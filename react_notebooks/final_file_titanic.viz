{"notebooks": [{"cells": [{"cell_id": 45, "code": "df_submission['Survived'] = model_svc_tune.predict(df_test)\ndf_submission", "class": "Data Export", "desc": "This code adds a 'Survived' column to the DataFrame `df_submission` by predicting the survival outcome using the tuned SVC model (`model_svc_tune`) on the `df_test` data.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.994651}}, {"cell_id": 47, "code": "df_submission.to_csv('df_submission.csv', index=False)", "class": "Data Export", "desc": "This code exports the `df_submission` DataFrame to a CSV file named 'df_submission.csv' without including the index.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9993338}}, {"cell_id": 66, "code": "df_submission.to_csv('df_submission_nn.csv', index=False)", "class": "Data Export", "desc": "This code exports the `df_submission` DataFrame to a CSV file named 'df_submission_nn.csv' without including the index.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9993351}}, {"cell_id": 1, "code": "#Train Data\ndf = pd.read_csv(\"../input/titanic/train.csv\")\ndf.head()", "class": "Data Extraction", "desc": "This code reads the train data from a CSV file into a pandas DataFrame and displays the first few rows using the `head()` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99950516}}, {"cell_id": 4, "code": "#Test Data\ndf_test = pd.read_csv('../input/titanic/test.csv')\ndf_test.head()", "class": "Data Extraction", "desc": "This code reads the test data from a CSV file into a pandas DataFrame and displays the first few rows using the `head()` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996246}}, {"cell_id": 29, "code": "X = df.drop(columns='Survived')\ny = df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n", "class": "Data Extraction", "desc": "This code separates the 'Survived' column as the target variable `y` and the remaining columns as the features `X`, then splits the data into training and testing sets using the `train_test_split` function from scikit-learn, with stratification and a specified random state for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.99518174}}, {"cell_id": 44, "code": "df_submission = pd.read_csv('../input/titanic/test.csv')\ndf_submission", "class": "Data Extraction", "desc": "This code reads the submission data from a CSV file into a pandas DataFrame named `df_submission` and displays its contents.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997209}}, {"cell_id": 57, "code": "X = df.drop(columns='Survived')\ny = df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape", "class": "Data Extraction", "desc": "This code separates the 'Survived' column as the target variable `y` and the remaining columns as the features `X`, then splits the data into training and testing sets using the `train_test_split` function from scikit-learn, with stratification and a specified random state for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9951421}}, {"cell_id": 61, "code": "df_submission = pd.read_csv('../input/titanic/test.csv')\ndf_submission", "class": "Data Extraction", "desc": "This code reads the submission data from a CSV file into a pandas DataFrame named `df_submission` and displays its contents.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997209}}, {"cell_id": 13, "code": "df.drop(columns=['PassengerId', 'Ticket', 'Cabin'], inplace=True)\ndf.head()", "class": "Data Transform", "desc": "This code removes the 'PassengerId', 'Ticket', and 'Cabin' columns from the DataFrame `df` using the `drop` method with `inplace=True`, and then displays the first few rows of the modified DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.999193}}, {"cell_id": 14, "code": "df_test.drop(columns=['PassengerId', 'Ticket', 'Cabin'], inplace=True)\ndf_test.head()", "class": "Data Transform", "desc": "This code removes the 'PassengerId', 'Ticket', and 'Cabin' columns from the DataFrame `df_test` using the `drop` method with `inplace=True`, and then displays the first few rows of the modified DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99913776}}, {"cell_id": 15, "code": "#Train Data\ndf.Age = pd.cut(df.Age, [0, 5, 12, 17, 30, 40, 80], labels = ['toddler', 'children', 'teenager', 'young adult', 'mature', 'elderly'])\ndf.head()", "class": "Data Transform", "desc": "This code transforms the 'Age' column in the DataFrame `df` by segmenting it into categorical age groups using the `pd.cut` function and assigning corresponding labels.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.76170206}}, {"cell_id": 16, "code": "#Test Data\ndf_test.Age = pd.cut(df_test.Age, [0, 5, 12, 17, 30, 40, 80], labels = ['toddler', 'children', 'teenager', 'young adult', 'mature', 'elderly'])\ndf_test.head()", "class": "Data Transform", "desc": "This code transforms the 'Age' column in the DataFrame `df_test` by segmenting it into categorical age groups using the `pd.cut` function and assigning corresponding labels.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.8833423}}, {"cell_id": 17, "code": "#Train Data\ndf.Fare = pd.cut(df.Fare, [0, 50, 100, 600], labels = ['cheap', 'medium', 'expensive'])\ndf.head()", "class": "Data Transform", "desc": "This code transforms the 'Fare' column in the DataFrame `df` by segmenting it into categorical fare ranges using the `pd.cut` function and assigning corresponding labels.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.82183254}}, {"cell_id": 18, "code": "#Test Data\ndf_test.Fare = pd.cut(df_test.Fare, [0, 50, 100, 600], labels = ['cheap', 'medium', 'expensive'])\ndf_test.head()", "class": "Data Transform", "desc": "This code transforms the 'Fare' column in the DataFrame `df_test` by segmenting it into categorical fare ranges using the `pd.cut` function and assigning corresponding labels.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9539027}}, {"cell_id": 19, "code": "#Train Data\ndf['IsAlone'] = (df.SibSp == 0) & (df.Parch == 0)\ndf.head()", "class": "Data Transform", "desc": "This code adds a new column 'IsAlone' to the DataFrame `df` to indicate whether a passenger is traveling alone, based on whether both 'SibSp' and 'Parch' columns are zero, using a conditional expression.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99740976}}, {"cell_id": 20, "code": "#Test Data\ndf_test['IsAlone'] = (df_test.SibSp == 0) & (df_test.Parch == 0)\ndf_test.head()", "class": "Data Transform", "desc": "This code adds a new column 'IsAlone' to the DataFrame `df_test` to indicate whether a passenger is traveling alone, based on whether both 'SibSp' and 'Parch' columns are zero, using a conditional expression.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9970663}}, {"cell_id": 21, "code": "#Train Data\ndf['Title'] = df.Name.apply(lambda x : x.split(\", \")[1].split(\".\")[0])\ndf.head()", "class": "Data Transform", "desc": "This code extracts the title from the 'Name' column in the DataFrame `df` by splitting the string and assigns it to a new column 'Title' using the `apply` method with a lambda function.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99911875}}, {"cell_id": 23, "code": "#Test Data\ndf_test['Title'] = df_test.Name.apply(lambda x : x.split(\", \")[1].split(\".\")[0])\ndf_test.head()", "class": "Data Transform", "desc": "This code extracts the title from the 'Name' column in the DataFrame `df_test` by splitting the string and assigns it to a new column 'Title' using the `apply` method with a lambda function.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9991943}}, {"cell_id": 24, "code": "def modif_title(x):\n  if x in ['Mr', 'Miss', 'Mrs', 'Master']:\n    return x\n  else:\n    return 'Other'\n\n#Train Data\ndf['Title'] = df['Title'].apply(modif_title)\ndf.head()", "class": "Data Transform", "desc": "This code modifies the 'Title' column in the DataFrame `df` by applying the `modif_title` function, which retains common titles and labels other titles as 'Other' using the `apply` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.49430394}}, {"cell_id": 26, "code": "#Test Data\ndf_test['Title'] = df_test['Title'].apply(modif_title)\ndf_test.head()", "class": "Data Transform", "desc": "This code modifies the 'Title' column in the DataFrame `df_test` by applying the `modif_title` function, which retains common titles and labels other titles as 'Other' using the `apply` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99860364}}, {"cell_id": 27, "code": "#Train Data\ndf.drop(columns='Name', inplace=True)\ndf.head()", "class": "Data Transform", "desc": "This code removes the 'Name' column from the DataFrame `df` using the `drop` method with `inplace=True` and then displays the first few rows of the modified DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9983292}}, {"cell_id": 28, "code": "#Test Data\ndf_test.drop(columns='Name', inplace=True)\ndf_test.head()", "class": "Data Transform", "desc": "This code removes the 'Name' column from the DataFrame `df_test` using the `drop` method with `inplace=True` and then displays the first few rows of the modified DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.999173}}, {"cell_id": 30, "code": "categorical_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder())\n])\n\npreprocessor = ColumnTransformer([\n    (\"categoric\", categorical_pipeline, ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'IsAlone','Title'])\n])", "class": "Data Transform", "desc": "This code defines a pipeline `categorical_pipeline` for processing categorical features that imputes missing values using the most frequent strategy and applies one-hot encoding, and then integrates it into a `ColumnTransformer` named `preprocessor` to specify the transformations for selected categorical columns.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9993753}}, {"cell_id": 42, "code": "categorical_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\"))\n])\n\npreprocessor = ColumnTransformer([\n    (\"categoric\", categorical_pipeline, ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'IsAlone','Title'])\n])", "class": "Data Transform", "desc": "This code defines a pipeline `categorical_pipeline` for processing categorical features that imputes missing values using the most frequent strategy and applies one-hot encoding while ignoring unknown categories, and then integrates it into a `ColumnTransformer` named `preprocessor` to specify the transformations for selected categorical columns.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99932027}}, {"cell_id": 46, "code": "df_submission = df_submission[['PassengerId', 'Survived']]\ndf_submission", "class": "Data Transform", "desc": "This code modifies the `df_submission` DataFrame to only include the 'PassengerId' and 'Survived' columns.", "testing": {"class": "Data_Export", "subclass": "prepare_output", "subclass_id": 55, "predicted_subclass_probability": 0.86127555}}, {"cell_id": 49, "code": "#Impute Missing Value\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimp.fit(df[['Age', 'Fare', 'Embarked']])\ndf[['Age', 'Fare', 'Embarked']] = imp.transform(df[['Age', 'Fare', 'Embarked']])", "class": "Data Transform", "desc": "This code imputes missing values in the 'Age', 'Fare', and 'Embarked' columns of the DataFrame `df` using the most frequent value strategy with the `SimpleImputer` from scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9343067}}, {"cell_id": 50, "code": "#Impute Missing Value\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimp.fit(df_test[['Age', 'Fare', 'Embarked']])\ndf_test[['Age', 'Fare', 'Embarked']] = imp.transform(df_test[['Age', 'Fare', 'Embarked']])", "class": "Data Transform", "desc": "This code imputes missing values in the 'Age', 'Fare', and 'Embarked' columns of the DataFrame `df_test` using the most frequent value strategy with the `SimpleImputer` from scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.49391738}}, {"cell_id": 51, "code": "df['IsAlone'] = df['IsAlone'].astype(np.int64)", "class": "Data Transform", "desc": "This code converts the 'IsAlone' column in the DataFrame `df` to the integer data type using the `astype` method and numpy's `int64`.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.985727}}, {"cell_id": 52, "code": "df_test['IsAlone'] = df_test['IsAlone'].astype(np.int64)", "class": "Data Transform", "desc": "This code converts the 'IsAlone' column in the DataFrame `df_test` to the integer data type using the `astype` method and numpy's `int64`.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.98643714}}, {"cell_id": 55, "code": "#Encode Categorical Data\ndf['Sex'].replace(to_replace=['female', 'male'], value=[0, 1], inplace=True)\ndf['Age'].replace(to_replace=['toddler', 'children', 'teenager', 'young adult', 'mature', 'elderly'], value=[1, 2, 3, 4, 5, 6], inplace=True)\ndf['Fare'].replace(to_replace=['cheap', 'medium', 'expensive'], value=[1, 2, 3], inplace=True)\ndf['Embarked'].replace(to_replace=['S', 'C', 'Q'], value=[1, 2, 3], inplace=True)\ndf['Title'].replace(to_replace=['Mr', 'Miss', 'Mrs', 'Master', 'Other'], value=[1, 2, 3, 4, 5], inplace=True)\ndf", "class": "Data Transform", "desc": "This code encodes categorical data in the DataFrame `df` by replacing specific string values in the 'Sex', 'Age', 'Fare', 'Embarked', and 'Title' columns with corresponding numerical values using the `replace` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.997387}}, {"cell_id": 56, "code": "#Encode Categorical Data\ndf_test['Sex'].replace(to_replace=['female', 'male'], value=[0, 1], inplace=True)\ndf_test['Age'].replace(to_replace=['toddler', 'children', 'teenager', 'young adult', 'mature', 'elderly'], value=[1, 2, 3, 4, 5, 6], inplace=True)\ndf_test['Fare'].replace(to_replace=['cheap', 'medium', 'expensive'], value=[1, 2, 3], inplace=True)\ndf_test['Embarked'].replace(to_replace=['S', 'C', 'Q'], value=[1, 2, 3], inplace=True)\ndf_test['Title'].replace(to_replace=['Mr', 'Miss', 'Mrs', 'Master', 'Other'], value=[1, 2, 3, 4, 5], inplace=True)\ndf_test", "class": "Data Transform", "desc": "This code encodes categorical data in the DataFrame `df_test` by replacing specific string values in the 'Sex', 'Age', 'Fare', 'Embarked', and 'Title' columns with corresponding numerical values using the `replace` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9925607}}, {"cell_id": 62, "code": "df_submission['Survived'] = model_tf.predict(df_test)\ndf_submission", "class": "Data Transform", "desc": "This code adds a 'Survived' column to the DataFrame `df_submission` by predicting the survival outcome using the TensorFlow model (`model_tf`) on the `df_test` data.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99467176}}, {"cell_id": 63, "code": "df_submission = df_submission[['PassengerId', 'Survived']]\ndf_submission", "class": "Data Transform", "desc": "This code modifies the `df_submission` DataFrame to only include the 'PassengerId' and 'Survived' columns.", "testing": {"class": "Data_Export", "subclass": "prepare_output", "subclass_id": 55, "predicted_subclass_probability": 0.86127555}}, {"cell_id": 64, "code": "df_submission['Survived'] = df_submission['Survived'].apply(lambda x : 0 if x < 0.5 else 1)", "class": "Data Transform", "desc": "This code updates the 'Survived' column in the `df_submission` DataFrame by applying a lambda function that classifies predictions as 0 if they are less than 0.5 and as 1 otherwise.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99894243}}, {"cell_id": 2, "code": "df.shape", "class": "Exploratory Data Analysis", "desc": "This code outputs the dimensions (number of rows and columns) of the DataFrame `df` using the `shape` attribute.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995491}}, {"cell_id": 3, "code": "df.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the total number of missing values for each column in the DataFrame `df` using the `isnull().sum()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9985019}}, {"cell_id": 5, "code": "df.Survived.value_counts()", "class": "Exploratory Data Analysis", "desc": "This code counts and displays the occurrences of each unique value in the 'Survived' column of the DataFrame `df` using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9994598}}, {"cell_id": 7, "code": "df['Age'].describe()", "class": "Exploratory Data Analysis", "desc": "This code generates and displays summary statistics for the 'Age' column in the DataFrame `df` using the `describe()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99945706}}, {"cell_id": 9, "code": "df['Fare'].describe()", "class": "Exploratory Data Analysis", "desc": "This code generates and displays summary statistics for the 'Fare' column in the DataFrame `df` using the `describe()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994549}}, {"cell_id": 22, "code": "df.Title.value_counts()", "class": "Exploratory Data Analysis", "desc": "This code counts and displays the occurrences of each unique value in the 'Title' column of the DataFrame `df` using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99943405}}, {"cell_id": 25, "code": "df.Title.value_counts()", "class": "Exploratory Data Analysis", "desc": "This code counts and displays the occurrences of each unique value in the 'Title' column of the DataFrame `df` using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99943405}}, {"cell_id": 48, "code": "df.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the total number of missing values for each column in the DataFrame `df` using the `isnull().sum()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9985019}}, {"cell_id": 53, "code": "df", "class": "Exploratory Data Analysis", "desc": "This code displays the contents of the DataFrame `df`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9990208}}, {"cell_id": 54, "code": "df_test", "class": "Exploratory Data Analysis", "desc": "This code displays the contents of the DataFrame `df_test`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9996252}}, {"cell_id": 65, "code": "df_submission", "class": "Exploratory Data Analysis", "desc": "This code displays the contents of the DataFrame `df_submission`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997551}}, {"cell_id": 0, "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n\nimport tensorflow as tf", "class": "Imports and Environment", "desc": "This code imports various libraries and modules including pandas, numpy, matplotlib, seaborn, and several scikit-learn and other machine learning packages such as XGBoost and TensorFlow.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.9993012}}, {"cell_id": 32, "code": "train_acc_knn = model_knn.score(X_train, y_train)\ntest_acc_knn = model_knn.score(X_test, y_test)\n\nprint('Train Accuracy : {}'.format(train_acc_knn))\nprint('Test Accuracy : {}'.format(test_acc_knn))", "class": "Model Evaluation", "desc": "This code evaluates the K-Nearest Neighbors classifier by calculating its accuracy on both the training and testing data using the `score` method and prints the resulting accuracy scores.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9968272}}, {"cell_id": 34, "code": "train_acc_lr = model_lr.score(X_train, y_train)\ntest_acc_lr = model_lr.score(X_test, y_test)\n\nprint('Train Accuracy : {}'.format(train_acc_lr))\nprint('Test Accuracy : {}'.format(test_acc_lr))", "class": "Model Evaluation", "desc": "This code evaluates the Logistic Regression classifier by calculating its accuracy on both the training and testing data using the `score` method and prints the resulting accuracy scores.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9961288}}, {"cell_id": 36, "code": "train_acc_svc = model_svc.score(X_train, y_train)\ntest_acc_svc = model_svc.score(X_test, y_test)\n\nprint('Train Accuracy : {}'.format(train_acc_svc))\nprint('Test Accuracy : {}'.format(test_acc_svc))", "class": "Model Evaluation", "desc": "This code evaluates the Support Vector Classifier (SVC) by calculating its accuracy on both the training and testing data using the `score` method and prints the resulting accuracy scores.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99677366}}, {"cell_id": 38, "code": "train_acc_rf = model_rf.score(X_train, y_train)\ntest_acc_rf = model_rf.score(X_test, y_test)\n\nprint('Train Accuracy : {}'.format(train_acc_rf))\nprint('Test Accuracy : {}'.format(test_acc_rf))", "class": "Model Evaluation", "desc": "This code evaluates the Random Forest classifier by calculating its accuracy on both the training and testing data using the `score` method and prints the resulting accuracy scores.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9961824}}, {"cell_id": 40, "code": "train_acc_xgb = model_xgb.score(X_train, y_train)\ntest_acc_xgb = model_xgb.score(X_test, y_test)\n\nprint('Train Accuracy : {}'.format(train_acc_xgb))\nprint('Test Accuracy : {}'.format(test_acc_xgb))", "class": "Model Evaluation", "desc": "This code evaluates the XGBoost classifier by calculating its accuracy on both the training and testing data using the `score` method and prints the resulting accuracy scores.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9973571}}, {"cell_id": 41, "code": "pd.DataFrame({'Model' : ['KNN', 'Logistic Regression', 'SVC',  'Random Forest', 'XGBoost'],\n    'Train Accuracy' : [train_acc_knn, train_acc_lr, train_acc_svc, train_acc_rf, train_acc_xgb],\n    'Test Accuracy' : [test_acc_knn, test_acc_lr, test_acc_svc, test_acc_rf, test_acc_xgb]\n})", "class": "Model Evaluation", "desc": "This code creates a pandas DataFrame to compare the train and test accuracy metrics of the KNN, Logistic Regression, SVC, Random Forest, and XGBoost models.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.97182345}}, {"cell_id": 31, "code": "model_knn = Pipeline([('prep', preprocessor), \n                     ('algo', KNeighborsClassifier())\n                     ])\nmodel_knn.fit(X_train, y_train)", "class": "Model Training", "desc": "This code creates a pipeline `model_knn` that preprocesses the data using the previously defined `preprocessor` and fits a K-Nearest Neighbors classifier to the training data (`X_train` and `y_train`) using the `fit` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9986652}}, {"cell_id": 33, "code": "model_lr = Pipeline([('prep', preprocessor), \n                     ('algo', LogisticRegression())\n                     ])\nmodel_lr.fit(X_train, y_train)", "class": "Model Training", "desc": "This code creates a pipeline `model_lr` that preprocesses the data using the previously defined `preprocessor` and fits a Logistic Regression classifier to the training data (`X_train` and `y_train`) using the `fit` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9994405}}, {"cell_id": 35, "code": "model_svc = Pipeline([('prep', preprocessor), \n                     ('algo', SVC())\n                     ])\nmodel_svc.fit(X_train, y_train)", "class": "Model Training", "desc": "This code creates a pipeline `model_svc` that preprocesses the data using the previously defined `preprocessor` and fits a Support Vector Classifier (SVC) to the training data (`X_train` and `y_train`) using the `fit` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9967314}}, {"cell_id": 37, "code": "model_rf = Pipeline([('prep', preprocessor), \n                     ('algo', RandomForestClassifier())\n                     ])\nmodel_rf.fit(X_train, y_train)", "class": "Model Training", "desc": "This code creates a pipeline `model_rf` that preprocesses the data using the previously defined `preprocessor` and fits a Random Forest classifier to the training data (`X_train` and `y_train`) using the `fit` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9993855}}, {"cell_id": 39, "code": "model_xgb = Pipeline([('prep', preprocessor), \n                     ('algo', XGBClassifier())\n                     ])\nmodel_xgb.fit(X_train, y_train)", "class": "Model Training", "desc": "This code creates a pipeline `model_xgb` that preprocesses the data using the previously defined `preprocessor` and fits an XGBoost classifier to the training data (`X_train` and `y_train`) using the `fit` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9987998}}, {"cell_id": 43, "code": "#SVC Hyperparameter Tuning\npipeline = Pipeline([\n    ('prep', preprocessor),\n    ('algo', SVC(probability=True, random_state=42))\n])\n\nparameter = {\n    'algo__gamma': np.logspace(-3, 3, 7),\n    'algo__C': np.logspace(-3, 3, 7)\n}\n\nmodel_svc_tune = GridSearchCV(pipeline, parameter, cv=10, n_jobs=-1, verbose=1)\nmodel_svc_tune.fit(X_train, y_train)\n\nprint(model_svc_tune.best_params_)\nprint(model_svc_tune.score(X_train, y_train)), print(model_svc_tune.score(X_test, y_test))", "class": "Model Training", "desc": "This code performs hyperparameter tuning on an SVC model using a pipeline that preprocesses the data, and searches for the best hyperparameters (`gamma` and `C`) using the `GridSearchCV` function with 10-fold cross-validation, then fits the model to the training data and prints the best parameters and model scores.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.99211216}}, {"cell_id": 58, "code": "model_tf = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu, \n                          input_shape=(9,)),\n    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n    tf.keras.layers.Dense(8, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nmodel_tf.summary()", "class": "Model Training", "desc": "This code defines a sequential neural network model using TensorFlow's Keras API, consisting of multiple dense layers with ReLU activation functions, a dropout layer to prevent overfitting, and a final dense layer with a sigmoid activation for binary classification, and then displays a summary of the model architecture.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.9876895}}, {"cell_id": 59, "code": "model_tf.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    metrics=['accuracy']\n)", "class": "Model Training", "desc": "This code compiles the TensorFlow Keras model `model_tf` by specifying the binary crossentropy loss function, Adam optimizer with a learning rate of 0.001, and accuracy as a metric for evaluation.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.99152327}}, {"cell_id": 60, "code": "model_tf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=1)\n", "class": "Model Training", "desc": "This code trains the TensorFlow Keras model `model_tf` on the training data (`X_train` and `y_train`) and evaluates it on the validation data (`X_test` and `y_test`) over 10 epochs, displaying progress with verbosity level 1.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9996891}}, {"cell_id": 6, "code": "ax = sns.countplot(data=df, x='Survived')\ntotal = len(df['Survived'])\nfor rec in ax.patches:\n    percentage = 100 * rec.get_height()/total\n    ax.text(rec.get_x() + rec.get_width() / 2, \n              rec.get_y() + rec.get_height(),\n              \"{:.1f}%\".format(percentage),\n              ha='center', \n              va='bottom')", "class": "Visualization", "desc": "This code creates a count plot of the 'Survived' column in the DataFrame `df` using Seaborn's `countplot` function, and annotates each bar with its percentage value.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9960206}}, {"cell_id": 8, "code": "sns.set_theme()\ng = sns.displot(data=df, x='Age', hue='Survived', kde=True, legend=False)\ng.fig.set_figwidth(9)\ng.fig.set_figheight(5)\nplt.legend(labels=['Survived', 'Not Survived'])\nplt.show()", "class": "Visualization", "desc": "This code generates a histogram with a kernel density estimate (KDE) overlay to visualize the distribution of the 'Age' column in the DataFrame `df`, differentiated by 'Survived' status, using Seaborn's `displot` function.", "testing": {"class": "Visualization", "subclass": "time_series", "subclass_id": 75, "predicted_subclass_probability": 0.74691087}}, {"cell_id": 10, "code": "g = sns.displot(data=df, x='Fare', bins=20, hue='Survived', kde=True, palette = \"rocket\", legend=False)\ng.fig.set_figwidth(7)\ng.fig.set_figheight(5)\nplt.legend(labels=['Survived', 'Not Survived'])\nplt.show()", "class": "Visualization", "desc": "This code generates a histogram with a kernel density estimate (KDE) overlay to visualize the distribution of the 'Fare' column in the DataFrame `df`, differentiated by 'Survived' status, using Seaborn's `displot` function.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99405044}}, {"cell_id": 11, "code": "cat_var= ['Pclass',  'Sex', 'SibSp', 'Parch', 'Embarked']\n\nfig, axes = plt.subplots(2, 3, figsize=(15,10))\n\nfor cat, ax in zip(cat_var, axes.flatten()):\n  sns.countplot(cat, data=df, hue='Survived', ax=ax)\n    \nplt.show()", "class": "Visualization", "desc": "This code creates count plots for multiple categorical variables ('Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked') to compare their distributions based on the 'Survived' status using Seaborn's `countplot` function and arranges these plots in a 2x3 grid using Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99782556}}, {"cell_id": 12, "code": "g = sns.displot(data=df, x='Fare', bins=20, hue='Embarked', kde=True)\ng.fig.set_figwidth(9)\ng.fig.set_figheight(5)\nplt.show()", "class": "Visualization", "desc": "This code generates a histogram with a kernel density estimate (KDE) overlay to visualize the distribution of the 'Fare' column in the DataFrame `df`, differentiated by 'Embarked' status, using Seaborn's `displot` function.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99618787}}], "notebook_id": 0, "notebook_name": "classification-titanic-kaggle.ipynb"}, {"cells": [{"cell_id": 49, "code": "sample = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\nsubmission = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nsubmission['Survived'] = y_pred\nsubmission = submission[['PassengerId','Survived']]\ndisplay(submission)", "class": "Data Export", "desc": "This code snippet loads a sample submission file, integrates the prediction probabilities into the test DataFrame as the 'Survived' column, and creates a submission DataFrame containing only 'PassengerId' and 'Survived' columns using Pandas.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99966645}}, {"cell_id": 51, "code": "submission['Survived'] = np.where(submission['Survived'] < 0.573, 0, 1)\nprint(submission['Survived'].mean())\ndisplay(submission.head(20))", "class": "Data Export", "desc": "This code snippet updates the 'Survived' column in the submission DataFrame by applying the optimized cutoff value of 0.573 to classify survival, prints the mean survival rate, and displays the first 20 rows of the submission DataFrame using Pandas and NumPy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.86286855}}, {"cell_id": 52, "code": "submission.to_csv('submission.csv', index=False)", "class": "Data Export", "desc": "This code snippet exports the final submission DataFrame to a CSV file named 'submission.csv' without including the index using Pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992442}}, {"cell_id": 1, "code": "# Save test data as dataframe and preview in order to get a sense of the what the data look like.\ntrain = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndisplay(train.head())", "class": "Data Extraction", "desc": "This code snippet reads the training data from a CSV file into a Pandas DataFrame and displays the first few rows to preview the data.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99953103}}, {"cell_id": 37, "code": "test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ndisplay(test.head())", "class": "Data Extraction", "desc": "This code snippet reads the test data from a CSV file into a Pandas DataFrame and displays the first few rows to preview the data.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9995247}}, {"cell_id": 4, "code": "# It appears that all titles (i.e. 'Mr.', 'Mrs.',...) can be found between a space and a period, so we will create a set of unique titles by searching\n# for this pattern in every name\ntitles = set()\nfor i in train['Name']:\n    title = re.search('\\s([a-zA-Z]+)\\.', i)\n    titles.add(title.group(1))\nprint(titles)", "class": "Data Transform", "desc": "This code snippet extracts unique titles (e.g., 'Mr.', 'Mrs.') from the 'Name' column of the training data by searching for patterns using regular expressions and stores them in a set.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.97765005}}, {"cell_id": 5, "code": "# Create new columns with an indicator for each title and test if the difference in survival rates for that column is statistically significant\n# use a copy of the dataset so as to not modify the original data\ncopy = train.copy()\nfor title in titles:\n    copy[title] = np.where(copy['Name'].str.contains(title), 1, 0)\n    display(copy[['Survived',title]].groupby([title]).agg({'Survived':['count','sum','mean']}))\n    t, p = stats.ttest_ind(a = copy[copy[title] == 0]['Survived'], b = copy[copy[title] == 1]['Survived'])\n    display(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code snippet creates new binary columns for each unique title in the dataset, computes aggregated survival statistics based on these columns, and performs t-tests to check if the difference in survival rates for each title is statistically significant, using Pandas, NumPy, and Scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.878474}}, {"cell_id": 6, "code": "# Many titles did not have sufficient frequency to use for predictions, but several may prove useful in our model\nfor title in titles.copy():\n    copy[title] = np.where(copy['Name'].str.contains(title), 1, 0)\n    t, p = stats.ttest_ind(a = copy[copy[title] == 0]['Survived'], b = copy[copy[title] == 1]['Survived'])\n    # After looking over the t-test results, we will keep all of the variables for which survival was significantly different amongs the two groups\n    if not p<0.05:\n        titles.remove(title)\ntitles = list(titles)\ndisplay(titles)\n# Add these new indicator columns to our actual dataset for these titles now that we have chosen them\nfor title in titles:\n    train[title] = np.where(copy['Name'].str.contains(title), 1, 0)", "class": "Data Transform", "desc": "This code snippet filters out the titles that do not have a statistically significant difference in survival rates, keeps the ones that do, and adds the corresponding binary indicator columns for these titles to the original dataset using Pandas, NumPy, and Scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99799144}}, {"cell_id": 9, "code": "# first we will try imputing the mean age into the null values\ntrain['Age_mean_imputed'] = np.where(train['Age'].isnull(),np.mean(train['Age']),train['Age'])\n# Now lets fit a bivariate regression model\nY, X = train['Survived'], train['Age_mean_imputed']\nage_model = sm.Logit(Y, X).fit()\ndisplay(age_model.summary().tables[1])", "class": "Data Transform", "desc": "This code snippet imputes the mean age into the null values of the 'Age' column, and then fits a bivariate logistic regression model using 'Survived' as the dependent variable and the mean-imputed 'Age' as the independent variable, finally displaying the summary of the model using Pandas and statsmodels.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.35666126}}, {"cell_id": 10, "code": "# second method is to use a regression model to predict the missing ages\nsub_data = train[~train['Age'].isnull()].copy()\n# We will use the columns that intuitively seem like they could relate to age\nY, X = np.array(sub_data['Age']).reshape(-1, 1), np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']])\n# fit the model\nage_model = lm.LinearRegression().fit(X, Y)\n#predict on the missing values\nsub_data = train[train['Age'].isnull()].copy()\nage_predictions = age_model.predict(np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']]))\n#add the missing values back into the dataframe\ntrain['Age_lm'] = train['Age']\nage_predictions_indexed = pd.DataFrame(age_predictions,index = train[train['Age_lm'].isnull()].index)[0]\ntrain['Age_lm'].fillna(age_predictions_indexed, inplace = True)\n\n# Now we will see how this predictor does at predicting Survival\nY, X = train['Survived'], train['Age_lm']\nage_model = sm.Logit(Y, X).fit()\ndisplay(age_model.summary().tables[1])", "class": "Data Transform", "desc": "This code snippet imputes missing ages by first fitting a linear regression model to predict 'Age' using relevant features and then replacing the missing age values with these predictions; subsequently, it fits a logistic regression model to predict 'Survived' using the imputed 'Age' values and displays the model summary using Pandas, scikit-learn, and statsmodels.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.62230337}}, {"cell_id": 13, "code": "# many values of SibSp have a very low frequency. Let's try simple indicator for any siblings or spouses\ntrain['SibSp_ind'] = np.where(train['SibSp'] > 0, 1, 0)\ndisplay(train[['Survived','SibSp_ind']].groupby(['SibSp_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = train[train['SibSp_ind'] == 1]['Survived'], b = train[train['SibSp_ind'] == 0]['Survived'])\ndisplay(f't-stat: {t}    p-value: {p}    p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code snippet creates a binary indicator column 'SibSp_ind' to represent the presence of any siblings or spouses, groups the data by this new column to display aggregated survival statistics, and performs a t-test to determine if the survival rates differ significantly based on this indicator using Pandas and Scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.92328453}}, {"cell_id": 15, "code": "# Let's consider an indicator for Parch as well due to low frequency of high values\ntrain['Parch_ind'] = np.where(train['Parch'] > 0, 1, 0)\ndisplay(train[['Survived','Parch_ind']].groupby(['Parch_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = train[train['Parch_ind'] == 1]['Survived'], b = train[train['Parch_ind'] == 0]['Survived'])\ndisplay(f't-stat: {t}    p-value: {p}    p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code snippet creates a binary indicator column 'Parch_ind' to represent whether the passenger had parents or children aboard, groups the data by this new column to display aggregated survival statistics, and performs a t-test to determine if the survival rates differ significantly based on this indicator using Pandas and Scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.953208}}, {"cell_id": 17, "code": "# let's look for patterns in the beginning letters of the ticket\nprefixes = set()\nfor i in train['Ticket']:\n    pref = re.search('([^\\s]+)\\s', i)\n    if pref is None: prefixes.add(\"None\")\n    else: prefixes.add(pref.group(1))\nprint(prefixes)", "class": "Data Transform", "desc": "This code snippet extracts unique prefixes from the 'Ticket' column by searching for patterns in the beginning letters using regular expressions and stores these prefixes in a set.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.31170517}}, {"cell_id": 18, "code": "# Create new columns with an indicator for each prefix and test if the difference in survival rates for that column is statistically significant\n# use a copy of the dataset so as to not modify the original data\ncopy = train.copy()\nfor pref in prefixes:\n    # Make sure to check for a space at the end of the prefix so we don't accidentally capture substrings\n    copy[pref] = np.where(copy['Ticket'].str.contains(f'{pref} '), 1, 0)\n    display(copy[['Survived',pref]].groupby([pref]).agg({'Survived':['count','sum','mean']}))\n    t, p = stats.ttest_ind(a = copy[copy[pref] == 0]['Survived'], b = copy[copy[pref] == 1]['Survived'])\n    display(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code snippet creates new binary indicator columns for each unique prefix from the 'Ticket' column, computes aggregated survival statistics based on these columns, and performs t-tests to check if the differences in survival rates for each prefix are statistically significant using Pandas, NumPy, and Scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.91176885}}, {"cell_id": 19, "code": "# There was a very low frequency of most prefixes. Let's compare prefixes to no prefix\ncopy = train.copy()\n# check if the ticket starts with any letter\ncopy['Ticket_prefix_ind'] = np.where(copy['Ticket'].str.startswith(tuple(prefixes)), 1, 0)\ndisplay(copy[['Survived','Ticket_prefix_ind']].groupby(['Ticket_prefix_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = copy[copy['Ticket_prefix_ind'] == 0]['Survived'], b = copy[copy['Ticket_prefix_ind'] == 1]['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code snippet creates a binary indicator column 'Ticket_prefix_ind' to represent whether the ticket starts with any identified prefix, groups the data by this new column to display aggregated survival statistics, and performs a t-test to determine if the survival rates differ significantly based on this indicator using Pandas and Scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.98258317}}, {"cell_id": 20, "code": "# as a whole, prefixes do not appear to be predictive. Let's just keep the two that were frequent enough and predictive on their own:\ncopy = train.copy()\n# check if the ticket starts with any letter\ncopy['Ticket_prefix_ind'] = np.where(copy['Ticket'].str.startswith(('C ','PC ')), 1, 0)\ndisplay(copy[['Survived','Ticket_prefix_ind']].groupby(['Ticket_prefix_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = copy[copy['Ticket_prefix_ind'] == 0]['Survived'], b = copy[copy['Ticket_prefix_ind'] == 1]['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code snippet refines the binary indicator column 'Ticket_prefix_ind' to represent whether the ticket starts with the frequent and predictive prefixes 'C ' or 'PC ', then groups the data by this new column to display aggregated survival statistics, and performs a t-test to determine if the survival rates differ significantly based on this refined indicator using Pandas and Scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.78813297}}, {"cell_id": 21, "code": "# since these two prefixes are frequent enough and have sufficiently different survival rates, we'll keep them in for consideration\ntrain['Ticket_PC_C'] = np.where(train['Ticket'].str.startswith(('C ','PC ')), 1, 0)", "class": "Data Transform", "desc": "This code snippet creates a column 'Ticket_PC_C' in the training data to indicate whether the ticket starts with the prefix 'C ' or 'PC ' using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99881774}}, {"cell_id": 23, "code": "train['Ticket_len5'] = train.Ticket.str.extract('(^\\d*)')\ntrain['Ticket_len5'] = np.where(train['Ticket_len5'].str.len() == 5, 1, 0)", "class": "Data Transform", "desc": "This code snippet creates a binary indicator column 'Ticket_len5' in the training data to represent whether the numerical part of the ticket has a length of 5 using Pandas and regular expressions.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9989471}}, {"cell_id": 25, "code": "# since the data are highly skewed with some outliers, let's bin the values into buckets using a decision tree and use that instead of the original column\nDT = tree.DecisionTreeClassifier(min_samples_leaf = 0.08, max_depth = 3)\nFare_tree = DT.fit(train['Fare'].to_frame(), train['Survived'])\ntrain['Fare_binned'] = Fare_tree.predict_proba(train['Fare'].to_frame())[:,1]\ndisplay(train[['Survived','Fare_binned','Fare']].groupby(['Fare_binned']).agg({'Survived':['count','sum','mean'],'Fare':['min','max','mean']}))", "class": "Data Transform", "desc": "This code snippet bins the 'Fare' values into buckets using a decision tree classifier, then creates a new column 'Fare_binned' based on the probabilities predicted by the model, and groups the data by 'Fare_binned' to display aggregated survival statistics and fare characteristics using Pandas and scikit-learn's DecisionTreeClassifier.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.41344222}}, {"cell_id": 27, "code": "# Let's merge all cabins that start with the same letter\ncopy = train.copy()\ncopy['Cabin_group'] = copy['Cabin'].str[:1]\ncopy['Cabin_group'].fillna('None', inplace = True)\ndisplay(copy[['Survived','Cabin_group']].groupby(['Cabin_group']).agg({'Survived':['count','sum','mean']}))", "class": "Data Transform", "desc": "This code snippet creates a new column 'Cabin_group' by extracting the first letter of the 'Cabin' values, fills null values with 'None', and groups the data by 'Cabin_group' to display aggregated survival statistics using Pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.3810074}}, {"cell_id": 28, "code": "# Since there is still a low frequency in many groups and it's unclear how groups might be meaningfully merged, let's mean_encode this column\ntrain['Cabin_group'] = train['Cabin'].str[:1]\ntrain['Cabin_group'].fillna('None', inplace = True)\nmeans = train[['Survived','Cabin_group']].groupby(['Cabin_group']).mean()\nmeans.rename({'Survived':'Cabin_ME'}, axis = 1, inplace = True)\ntrain = train.join(means, how = 'left', on = ['Cabin_group'])", "class": "Data Transform", "desc": "This code snippet performs mean encoding of the 'Cabin_group' column by calculating the mean survival rate for each group and creating a new column 'Cabin_ME' to store these encoded values, then joins this information back to the original dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.99588}}, {"cell_id": 30, "code": "# since there are only two null values for embarked, we will encode them with the mode\ntrain['Embarked'].fillna('S', inplace = True)\nprint(train['Embarked'].isnull().sum())\ndisplay(train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':['count','sum','mean']}))", "class": "Data Transform", "desc": "This code snippet fills the null values in the 'Embarked' column with 'S' (the mode of the column), verifies that no null values remain, and then groups the data by 'Embarked' to display aggregated survival statistics using Pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.4329066}}, {"cell_id": 31, "code": "# let's mean encode this column as well so that we can have all numerical columns\nmeans = train[['Survived','Embarked']].groupby(['Embarked']).mean()\nmeans.rename({'Survived':'Embarked_ME'}, axis = 1, inplace = True)\ntrain = train.join(means, how = 'left', on = ['Embarked'])", "class": "Data Transform", "desc": "This code snippet performs mean encoding of the 'Embarked' column by calculating the mean survival rate for each category and creating a new column 'Embarked_ME' to store these encoded values, then joins this information back to the original dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.9932888}}, {"cell_id": 32, "code": "# create our final data set with just the columns we'll be keeping\ndata = train[['Survived','Pclass','Mrs', 'Mr', 'Master', 'Miss','Sex','Age_lm','SibSp_ind','Parch_ind','Ticket_PC_C','Fare_binned','Cabin_ME','Embarked_ME','Ticket_len5']].copy()\n# clean up the names\ndata.rename({'Age_lm':'Age','SibSp_ind':'SibSp','Parch_ind':'Parch','Ticket_PC_C':'Ticket','Fare_binned':'Fare','Cabin_ME':'Cabin','Embarked_ME':'Embarked'}, axis = 1, inplace = True)\ndata['Female'] = np.where(data['Sex'] == 'female', 1, 0)\ndata.drop('Sex', axis = 1, inplace = True)\ndisplay(data.head())\ndisplay(data.columns)", "class": "Data Transform", "desc": "This code snippet creates a final dataset by selecting and renaming the relevant transformed columns, adding a binary column to represent the 'Sex' feature (specifically for females), and dropping the original 'Sex' column, then displays the head of the final dataset and its columns using Pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.85816985}}, {"cell_id": 38, "code": "# Update names column\nfor title in ['Mrs', 'Mr', 'Miss', 'Master']:\n    test[title] = np.where(test['Name'].str.contains(title), 1, 0)\nlen(test[test['Name'].isnull()])", "class": "Data Transform", "desc": "This code snippet updates the test DataFrame by creating binary indicator columns for each title ('Mrs', 'Mr', 'Miss', 'Master') based on the 'Name' column using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9953231}}, {"cell_id": 39, "code": "#Update Sex columns\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\nlen(test[test['Sex'].isnull()])", "class": "Data Transform", "desc": "This code snippet creates a binary indicator column 'Female' in the test DataFrame to represent whether the passenger's sex is female and checks for any null values in the 'Sex' column using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9913565}}, {"cell_id": 40, "code": "#Update SibSp\ntest['SibSp_ind'] = np.where(test['SibSp'] > 0, 1, 0)\nlen(test[test['SibSp'].isnull()])\n# display(test.head())", "class": "Data Transform", "desc": "This code snippet creates a binary indicator column 'SibSp_ind' in the test DataFrame to represent whether the passenger has any siblings or spouses aboard and checks for any null values in the 'SibSp' column using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99579054}}, {"cell_id": 41, "code": "#Update Parch\ntest['Parch_ind'] = np.where(test['Parch'] > 0, 1, 0)\nlen(test[test['Parch'].isnull()])\n# display(test.head())", "class": "Data Transform", "desc": "This code snippet creates a binary indicator column 'Parch_ind' in the test DataFrame to represent whether the passenger has any parents or children aboard and checks for any null values in the 'Parch' column using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99731416}}, {"cell_id": 42, "code": "#Update ticket\ntest['Ticket_PC_C'] = np.where(test['Ticket'].str.startswith(('C ','PC ')), 1, 0)\nlen(test[test['Ticket_PC_C'].isnull()])\n\ntest['Ticket_len5'] = test.Ticket.str.extract('(^\\d*)')\ntest['Ticket_len5'] = np.where(test['Ticket_len5'].str.len() == 5, 1, 0)", "class": "Data Transform", "desc": "This code snippet updates the test DataFrame by creating binary indicator columns 'Ticket_PC_C' to represent whether the ticket starts with 'C ' or 'PC ' and 'Ticket_len5' to indicate if the numerical part of the ticket has a length of 5 using Pandas and regular expressions.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9972178}}, {"cell_id": 43, "code": "#Update Fare\n#One null fare will be imputed with the median since outliers might skew the mean\ntest['Fare'].fillna(test['Fare'].median(), inplace = True)\n#Use the same decision tree we made before to impute the values\ntest['Fare_binned'] = Fare_tree.predict_proba(test['Fare'].to_frame())[:,1]\nlen(test[test['Fare'].isnull()])", "class": "Data Transform", "desc": "This code snippet imputes a null value in the 'Fare' column of the test DataFrame with the median fare, then uses the previously trained decision tree to create a new column 'Fare_binned' based on the predicted probabilities, and confirms there are no remaining null values using Pandas.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9609763}}, {"cell_id": 44, "code": "#Update Cabin\ntest['Cabin_group'] = test['Cabin'].str[:1]\ntest['Cabin_group'].fillna('None', inplace = True)\n#Mean encode based on the original training data\nmeans = train[['Survived','Cabin_group']].groupby(['Cabin_group']).mean()\nmeans.rename({'Survived':'Cabin_ME'}, axis = 1, inplace = True)\ntest = test.join(means, how = 'left', on = ['Cabin_group'])", "class": "Data Transform", "desc": "This code snippet updates the test DataFrame by creating a 'Cabin_group' column from the first letter of the 'Cabin' values, fills null values with 'None', and performs mean encoding 'Cabin_ME' based on the survival rates from the original training data, then joins this information back to the test dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.9975425}}, {"cell_id": 45, "code": "#Update Embarked\nlen(test[test['Embarked'].isnull()])\n# mean encode\nmeans = train[['Survived','Embarked']].groupby(['Embarked']).mean()\nmeans.rename({'Survived':'Embarked_ME'}, axis = 1, inplace = True)\ntest = test.join(means, how = 'left', on = ['Embarked'])", "class": "Data Transform", "desc": "This code snippet confirms there are no null values in the 'Embarked' column of the test DataFrame and then performs mean encoding on the 'Embarked' column based on the survival rates from the original training data, joining the encoded information back to the test dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.9983809}}, {"cell_id": 46, "code": "#Update Age column\nsub_data = test[~test['Age'].isnull()].copy()\n# We will use the columns that intuitively seem like they could relate to age\nY, X = np.array(sub_data['Age']).reshape(-1, 1), np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']])\n# fit the model\nage_model = lm.LinearRegression().fit(X, Y)\n#predict on the missing values\nsub_data = test[test['Age'].isnull()].copy()\nage_predictions = age_model.predict(np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']]))\n#add the missing values back into the dataframe\ntest['Age_lm'] = test['Age']\nage_predictions_indexed = pd.DataFrame(age_predictions,index = test[test['Age_lm'].isnull()].index)[0]\ntest['Age_lm'].fillna(age_predictions_indexed, inplace = True)\ndisplay(test.head())", "class": "Data Transform", "desc": "This code snippet imputes missing age values in the test DataFrame by fitting a linear regression model using relevant features, predicting the missing values, and updating the 'Age_lm' column with these predictions using Pandas and scikit-learn.", "testing": {"class": "Data_Export", "subclass": "prepare_output", "subclass_id": 55, "predicted_subclass_probability": 0.436364}}, {"cell_id": 47, "code": "# create our final data set with just the columns we'll be keeping\ntest = test[['Pclass','Mrs', 'Mr', 'Master', 'Miss','Sex','Age_lm','SibSp_ind','Parch_ind','Ticket_PC_C','Fare_binned','Cabin_ME','Embarked_ME','Ticket_len5']].copy()\n# clean up the names\ntest.rename({'Age_lm':'Age','SibSp_ind':'SibSp','Parch_ind':'Parch','Ticket_PC_C':'Ticket','Fare_binned':'Fare','Cabin_ME':'Cabin','Embarked_ME':'Embarked'}, axis = 1, inplace = True)\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\ntest.drop('Sex', axis = 1, inplace = True)\ndisplay(test.head())\ndisplay(test.columns)", "class": "Data Transform", "desc": "This code snippet creates the final test dataset by selecting and renaming relevant transformed columns, adding a binary column to represent the 'Sex' feature (specifically for females), and dropping the original 'Sex' column, then displays the head of the final dataset and its columns using Pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.96216625}}, {"cell_id": 2, "code": "# Initial review shows that Pclass appears to be predictive\ndisplay(train[['Survived','Pclass']].groupby(['Pclass']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Pclass'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet groups the training data by 'Pclass' to aggregate and display the count, sum, and mean of 'Survived', along with calculating and displaying the number and percentage of null values in 'Pclass' using Pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9523748}}, {"cell_id": 3, "code": "# The Names column contains too many unique values. We will investigate particular substrings.\npd.set_option(\"display.max_rows\", 20)\ndisplay(train['Name'])\nnulls = train['Name'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the 'Name' column of the training data to investigate unique values, sets the maximum rows to display to 20 using Pandas, and calculates and displays the number and percentage of null values in the 'Name' column.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.6983381}}, {"cell_id": 7, "code": "# Display unique values and how they survived\ndisplay(train[['Survived','Sex']].groupby(['Sex']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Sex'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')\n# Perform a t-test to check for statistically significant difference in survival rates\nt, p = stats.ttest_ind(a = train[train['Sex'] == 'female']['Survived'], b = train[train['Sex'] == 'male']['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Exploratory Data Analysis", "desc": "This code snippet groups the training data by 'Sex' to display aggregated survival statistics, checks for null values in the 'Sex' column, and performs a t-test to determine if the difference in survival rates between genders is statistically significant using Pandas and Scipy.", "testing": {"class": "Model_Evaluation", "subclass": "statistical_test", "subclass_id": 47, "predicted_subclass_probability": 0.93878275}}, {"cell_id": 8, "code": "# visualize distribution of ages\ntrain['Age'].hist()\n# check for null values\\\nnulls = train['Age'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet visualizes the distribution of the 'Age' column using a histogram and checks for null values in the 'Age' column, displaying their count and percentage using Pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9968941}}, {"cell_id": 12, "code": "# Initial review shows that SibSp appears to be predictive\ndisplay(train[['Survived','SibSp']].groupby(['SibSp']).agg({'Survived':['count','sum','mean']}))\nnulls = train['SibSp'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet groups the training data by 'SibSp' to display aggregated survival statistics and checks for null values in the 'SibSp' column, displaying their count and percentage using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.88131183}}, {"cell_id": 14, "code": "# Initial review shows that Parch appears to be predictive\ndisplay(train[['Survived','Parch']].groupby(['Parch']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Parch'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet groups the training data by 'Parch' to display aggregated survival statistics and checks for null values in the 'Parch' column, displaying their count and percentage using Pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9513562}}, {"cell_id": 16, "code": "# The Ticket column contains too many unique values. We will investigate particular substrings.\n#pd.set_option(\"display.max_rows\", None)\ndisplay(train['Ticket'])\nnulls = train['Ticket'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the 'Ticket' column to investigate its unique values and checks for null values in the 'Ticket' column, displaying their count and percentage using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.91641235}}, {"cell_id": 22, "code": "copy['Ticket_num'] = copy.Ticket.str.extract('(^\\d*)')\ncopy['Ticket_num'] = copy['Ticket_num'].str.len()\ndisplay(copy[['Survived','Ticket_num']].groupby(['Ticket_num']).agg({'Survived':['count','sum','mean']}))", "class": "Exploratory Data Analysis", "desc": "This code snippet extracts the numerical part of the 'Ticket' column, calculates its length, creates a new column 'Ticket_num' with these lengths, and groups the data by 'Ticket_num' to display aggregated survival statistics using Pandas and regular expressions.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.98412067}}, {"cell_id": 24, "code": "# visualize distribution of ages\ntrain['Fare'].hist(bins = 40)\n# check for null values\\\nnulls = train['Fare'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet visualizes the distribution of the 'Fare' column using a histogram with 40 bins and checks for null values in the 'Fare' column, displaying their count and percentage using Pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9981839}}, {"cell_id": 26, "code": "# Initial review shows that Parch appears to be predictive\nnulls = train['Cabin'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet checks for null values in the 'Cabin' column of the training data, displaying their count and percentage using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.96462893}}, {"cell_id": 29, "code": "# Initial review shows that Embarked appears to be predictive\ndisplay(train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Embarked'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code snippet groups the training data by 'Embarked' to display aggregated survival statistics and checks for null values in the 'Embarked' column, displaying their count and percentage using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.73904485}}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport itertools as it # to avoid nested for-loops\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # to search for substrings using regular expressions\nimport scipy.stats as stats # for performing t-tests for statistically significant difference in mean values\nfrom sklearn.model_selection import KFold # for k-fold validation of models\nfrom sklearn import metrics as met # for model evaluation metrics\nimport sklearn.linear_model as lm # for linear models\nfrom sklearn import tree # decision tree used to discretize some continuous variables\nimport seaborn as sns # for vizualiations\nimport statsmodels.api as sm # Used to create logistic regression models to check for statistical significance of continuous variables. This package returns confidence intervals for predictors\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This code snippet imports various Python libraries such as NumPy, Pandas, Scipy, scikit-learn, Seaborn, and statsmodels for data processing, machine learning, statistical analysis, and visualization, and also lists files available in a specified input directory using the os module.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99935275}}, {"cell_id": 11, "code": "# Finally, let's use ROC AUC to compare which version is superior\nmodel = lm.LogisticRegression(random_state = 8292010, solver = 'lbfgs')\nROC_AUC_mean = []\nROC_AUC_lm = []\n# We will do 5-fold validation when calculating the ROC_AUC\nkfold = KFold(10, shuffle = True, random_state = 1)\nfor tr, te in kfold.split(train):\n    # capture metrics for the first variation of the age with mean imputing:\n    X, y = np.array(train['Age_mean_imputed']).reshape(-1,1)[tr], np.array(train['Survived']).reshape(-1,1)[tr]\n    model.fit(X, y.ravel())\n    y_hat = model.predict_proba(X = np.array(train['Age_mean_imputed']).reshape(-1,1))[:,1][te]\n    y_true = np.array(train['Survived']).reshape(-1,1)[te]\n    ROC_AUC_mean.append(met.roc_auc_score(y_true, y_score = y_hat))\n    \n    # capture metrics for the second variation of the age with linear regression:\n    X, y = np.array(train['Age_lm']).reshape(-1,1)[tr], np.array(train['Survived']).reshape(-1,1)[tr]\n    model.fit(X, y.ravel())\n    y_hat = model.predict_proba(X = np.array(train['Age_lm']).reshape(-1,1))[:,1][te]\n    y_true = np.array(train['Survived']).reshape(-1,1)[te]\n    ROC_AUC_lm.append(met.roc_auc_score(y_true, y_score = y_hat))\ndisplay(np.average(ROC_AUC_mean))\ndisplay(np.average(ROC_AUC_lm))", "class": "Model Evaluation", "desc": "This code snippet compares the effectiveness of two imputation methods for 'Age' by calculating the ROC AUC scores using logistic regression with 10-fold cross-validation, and then averages and displays these scores for both versions (mean-imputed and linear regression-imputed 'Age') using scikit-learn.", "testing": {"class": "Model_Train", "subclass": "find_best_model_class", "subclass_id": 3, "predicted_subclass_probability": 0.44388086}}, {"cell_id": 35, "code": "np.max(df[7])", "class": "Model Evaluation", "desc": "This code snippet retrieves the maximum ROC AUC score corresponding to a `max_depth` value of 7 from the DataFrame containing ROC AUC scores for different parameter combinations using Pandas and NumPy.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9973348}}, {"cell_id": 50, "code": "# In order to choose a cutoff value for survival, we will tune the cutoff on our original model to maximize the matthews correlation coefficient\nX = data[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']]\ny = data['Survived']\ncutoffs = {}\n# try cutoff values from 0.1 to 0.9\nfor i in np.arange(0.3,0.7,0.001):\n    MCC = []\n    for tr, te in kfold.split(data):\n        X_train, X_test = X.loc[tr], X.loc[te]\n        y_train, y_test = y.loc[tr], y.loc[te]\n        fit = model.fit(X = X_train, y = y_train)\n        y_pred = fit.predict_proba(X_test)[:,1]\n        y_pred = np.where(y_pred < i, 0, 1)\n        MCC.append(met.matthews_corrcoef(y_test, y_pred))\n    cutoffs[i] = np.mean(MCC)\ndisplay(max(cutoffs, key = cutoffs.get))", "class": "Model Evaluation", "desc": "This code snippet tunes the cutoff threshold for predicting survival by iteratively testing values from 0.3 to 0.7 (with increments of 0.001) using 10-fold cross-validation, aiming to maximize the Matthews correlation coefficient (MCC) for the logistic regression model, and identifies the optimal cutoff value using scikit-learn and Pandas.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.6277582}}, {"cell_id": 33, "code": "X, y = data[['Pclass', 'Mrs', 'Mr', 'Master', 'Miss', 'Age','SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Female','Ticket_len5']], data['Survived']\n# create lists of parameters to try to figure out the optimal parameters\n# I started with much bigger ranges and narrowed it down to these\nmin_leafs = range(32,45)\nmax_depth = range(5,15)\n# create an empty dataframe in which to store ROC AUC score\ndf = pd.DataFrame(columns = max_depth, index = min_leafs)\n\n# for each combination of parameter values, figure out the ROC AUC using 10 fold validation\nfor leaf, depth in it.product(min_leafs, max_depth):\n    dtc = tree.DecisionTreeClassifier(min_samples_leaf = leaf, max_depth = depth)\n    kfold = KFold(10, shuffle = True, random_state = 1)\n    ROC_AUC_scores = []\n    for tr, te in kfold.split(data):\n        X_train, X_test = X.loc[tr], X.loc[te]\n        y_train, y_test = y.loc[tr], y.loc[te]\n        # fit the data\n        dtc = dtc.fit(X_train, y_train)\n        # get predictions on the test set\n        y_pred = dtc.predict_proba(X_test)[:,1]\n        # get the ROC on the test set\n        ROC_AUC_scores.append(met.roc_auc_score(y_test, y_pred))\n    # average the ROC from all folds\n    ROC_AUC = np.mean(ROC_AUC_scores)\n    # store average ROC AUC in dataframe\n    df.loc[leaf][depth] = float(ROC_AUC)", "class": "Model Training", "desc": "This code snippet tests various combinations of `min_samples_leaf` and `max_depth` parameters for a decision tree classifier by performing 10-fold cross-validation to compute and store the average ROC AUC scores for each combination using scikit-learn, Pandas, and itertools.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.8047089}}, {"cell_id": 36, "code": "X, y = data[['Pclass', 'Mrs', 'Mr', 'Master', 'Miss', 'Age','SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Female','Ticket_len5']], data['Survived']\n# create an empty list in which to store ROC AUC score\nROC_AUC = []\npredictors = X.columns\nmodel = lm.LogisticRegression(random_state = 8292010, solver = 'liblinear')\nkfold = KFold(10, shuffle = True, random_state = 1)\nselected = []\nmax_AUC = {}\n\n# starting with one predictor, figure out the ROC AUC in a bivariate model, then continuously add one predictor each time to maximize ROC AUC\nfor i in range(0, len(predictors)):\n    next_best = {}\n    for pred in [p for p in predictors if p not in selected]:\n        ROC_AUC_scores = []\n        X = data[selected + [pred]]\n        for tr, te in kfold.split(data):\n            X_train, X_test = X.loc[tr], X.loc[te]\n            y_train, y_test = y.loc[tr], y.loc[te]\n            fit = model.fit(X = X_train, y = y_train)\n            y_pred = fit.predict_proba(X_test)[:,1]\n            ROC_AUC_scores.append(met.roc_auc_score(y_test, y_pred))\n        next_best[pred] = np.mean(ROC_AUC_scores)\n    selected.append(max(next_best, key = next_best.get))\n    max_AUC[', '.join(selected)] = next_best[max(next_best, key = next_best.get)]\nfor i in max_AUC.keys():\n    print(f'{i}: {max_AUC[i]}')", "class": "Model Training", "desc": "This code snippet employs a stepwise selection approach to iteratively add predictors to a logistic regression model, optimizing for the best ROC AUC scores using 10-fold cross-validation, and logs the predictors in the optimal order along with their respective ROC AUC scores using Pandas, NumPy, and scikit-learn.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.6774396}}, {"cell_id": 48, "code": "model = lm.LogisticRegression(random_state = 8292010, solver = 'liblinear')\nX = data[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']]\ny = data['Survived']\nfit = model.fit(X = X, y = y)\n\ny_pred = fit.predict_proba(test[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']])[:,1]", "class": "Model Training", "desc": "This code snippet trains a logistic regression model to predict 'Survived' using selected features from the training dataset and generates prediction probabilities for the test dataset using the trained model with scikit-learn.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.87436324}}, {"cell_id": 34, "code": "# Looking at all of our parameter values, it looks like a max_depth = 7 and min_leaf_size = 38 are our best bets\ndf[df.columns] = df[df.columns].astype(float)\nsns.heatmap(df)", "class": "Visualization", "desc": "This code snippet visualizes the ROC AUC scores for different combinations of `max_depth` and `min_samples_leaf` parameters using a heatmap with Seaborn to identify optimal parameter values.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9982356}}], "notebook_id": 1, "notebook_name": "titanic-logistic-regression-and-decision-tree.ipynb"}, {"cells": [{"cell_id": 27, "code": "sub = pd.read_csv('../input/titanic/gender_submission.csv')\nsub['Survived'] = y_pred\nsub.to_csv('submission.csv', index=False)\n\nsub.head()", "class": "Data Export", "desc": "This code reads the gender submission CSV into a DataFrame `sub`, replaces its 'Survived' column with the predicted values `y_pred`, and saves the modified DataFrame to a new CSV file named 'submission.csv', then outputs the first five rows of `sub`.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99916327}}, {"cell_id": 0, "code": "import numpy as np\nimport pandas as pd\n\nfilename_train = \"/kaggle/input/titanic/train.csv\"\n\ntrain = pd.read_csv(filename_train)\ntrain\n", "class": "Data Extraction", "desc": "This code imports the `numpy` and `pandas` libraries and reads the Titanic training dataset from a CSV file into a pandas DataFrame named `train`.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99966395}}, {"cell_id": 1, "code": "filename_test = \"/kaggle/input/titanic/test.csv\"\ntest = pd.read_csv(filename_test)\ntest", "class": "Data Extraction", "desc": "This code reads the Titanic test dataset from a CSV file into a pandas DataFrame named `test`.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99973756}}, {"cell_id": 2, "code": "filename_submit=\"../input/titanic/gender_submission.csv\"\ngender_submission=pd.read_csv(filename_submit)\n\ngender_submission", "class": "Data Extraction", "desc": "This code reads the gender submission dataset for the Titanic from a CSV file into a pandas DataFrame named `gender_submission`.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996989}}, {"cell_id": 3, "code": "#\u5927\u6d5c\u5148\u751f\u306e\u8cea\u554f\nanimals = pd.DataFrame({'Cows': [12, 20, 3], 'Goats': [50, 19, 50]}, index=['Year 1', 'Year 2', 'Year 3'])\nanimals", "class": "Data Extraction", "desc": "This code creates a pandas DataFrame named `animals` containing data about the number of cows and goats over three years, with custom row indices labeled 'Year 1', 'Year 2', and 'Year 3'.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.99824834}}, {"cell_id": 4, "code": "#Goat\u304cmax\u306e\u6642\u306eTrue/Fales\u30ea\u30b9\u30c8\nprint(animals[\"Goats\"]==animals[\"Goats\"].max())\n\n#Goats\u304cmax\u306e\u6642\u306eCows\u306e\u5024\u3092Series\u3067\u8fd4\u3059\nprint(animals[\"Cows\"][animals[\"Goats\"]==animals[\"Goats\"].max()])\n\n#\u30d6\u30fc\u30eb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u53c2\u7167\u3068\u3044\u3046\u65b9\u6cd5", "class": "Data Transform", "desc": "This code uses boolean indexing to first print a boolean series where the \"Goats\" column values are equal to their maximum value, and then prints the corresponding \"Cows\" column values where this condition is true, demonstrating the method of boolean indexing.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9565232}}, {"cell_id": 5, "code": "data = pd.concat([train, test], sort=False)\ndata", "class": "Data Transform", "desc": "This code concatenates the training and test datasets along the vertical axis into a single DataFrame named `data` using the `pd.concat` function with sorting disabled.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9995253}}, {"cell_id": 7, "code": "data['Sex'].replace(['male', 'female'], [0, 1], inplace=True)\n", "class": "Data Transform", "desc": "This code replaces the categorical values 'male' and 'female' in the 'Sex' column of the `data` DataFrame with the numerical values 0 and 1, respectively, in place.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.79807115}}, {"cell_id": 8, "code": "data", "class": "Data Transform", "desc": "This code outputs the current state of the `data` DataFrame to display its content after the transformations have been applied.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997726}}, {"cell_id": 10, "code": "data['Embarked'].fillna(('S'), inplace=True)", "class": "Data Transform", "desc": "This code fills any missing values in the 'Embarked' column of the `data` DataFrame with the string 'S' in place using the `fillna` method.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.53473353}}, {"cell_id": 13, "code": "data['Embarked'] = data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)", "class": "Data Transform", "desc": "This code maps the categorical values 'S', 'C', and 'Q' in the 'Embarked' column of the `data` DataFrame to numerical values 0, 1, and 2, respectively, and converts the result to integer type.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.9373488}}, {"cell_id": 14, "code": "data", "class": "Data Transform", "desc": "This code outputs the current state of the `data` DataFrame to display its content after the latest transformations have been applied.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997726}}, {"cell_id": 15, "code": "data['Fare'].fillna(np.mean(data['Fare']), inplace=True)", "class": "Data Transform", "desc": "This code fills any missing values in the 'Fare' column of the `data` DataFrame with the mean value of the 'Fare' column, using the `fillna` method and `np.mean` function.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.7357263}}, {"cell_id": 16, "code": "age_avg = data['Age'].mean()\nage_std = data['Age'].std()\n\ndata['Age'].fillna(np.random.randint(age_avg - age_std, age_avg + age_std), inplace=True)", "class": "Data Transform", "desc": "This code fills any missing values in the 'Age' column of the `data` DataFrame with random integers within one standard deviation of the mean age value, using the `fillna` method and `np.random.randint` function.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99915504}}, {"cell_id": 17, "code": "data", "class": "Data Transform", "desc": "This code outputs the current state of the `data` DataFrame to display its content after filling the missing values in the 'Age' column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997726}}, {"cell_id": 18, "code": "delete_columns = ['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin']\ndata.drop(delete_columns, axis=1, inplace=True)", "class": "Data Transform", "desc": "This code drops the columns 'Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', and 'Cabin' from the `data` DataFrame inplace, using the `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99905545}}, {"cell_id": 19, "code": "train = data[:len(train)]\ntest = data[len(train):]", "class": "Data Transform", "desc": "This code splits the `data` DataFrame back into the original train and test DataFrames based on the length of the original `train` DataFrame.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.991172}}, {"cell_id": 20, "code": "y_train = train['Survived']\nX_train = train.drop('Survived', axis=1)\nX_test = test.drop('Survived', axis=1)", "class": "Data Transform", "desc": "This code separates the 'Survived' column as the target variable `y_train` from the feature matrix `X_train` in the `train` DataFrame, and similarly drops the 'Survived' column from the `test` DataFrame to create `X_test`.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99932396}}, {"cell_id": 21, "code": "X_train.head()", "class": "Data Transform", "desc": "This code outputs the first five rows of the `X_train` DataFrame to display its content.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99975497}}, {"cell_id": 22, "code": "y_train.head()", "class": "Data Transform", "desc": "This code outputs the first five values of the `y_train` Series to display its content.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997508}}, {"cell_id": 23, "code": "from sklearn.model_selection import train_test_split\n\n\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(X_train, y_train, test_size=0.3,\n                                 random_state=0, stratify=y_train)", "class": "Data Transform", "desc": "This code splits the `X_train` and `y_train` datasets into training and validation sets using the `train_test_split` function from scikit-learn, with 30% of the data allocated to the validation set, maintaining class distribution with `stratify=y_train` and a `random_state` of 0 for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9907714}}, {"cell_id": 24, "code": "categorical_features = ['Embarked', 'Pclass', 'Sex',\"Fare\"]", "class": "Data Transform", "desc": "This code creates a list named `categorical_features` that contains the names of the categorical feature columns 'Embarked', 'Pclass', 'Sex, and 'Fare'.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.998681}}, {"cell_id": 26, "code": "y_pred = (y_pred > 0.5).astype(int)\ny_pred[:10]", "class": "Data Transform", "desc": "This code converts the predicted probabilities `y_pred` from the LightGBM model to binary classification labels (0 or 1) based on a threshold of 0.5, and then outputs the first ten predictions.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.982947}}, {"cell_id": 6, "code": "data.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code calculates and outputs the total number of missing values in each column of the `data` DataFrame using the `isnull().sum()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99897987}}, {"cell_id": 9, "code": "data['Embarked'].value_counts()", "class": "Exploratory Data Analysis", "desc": "This code counts and outputs the occurrences of each unique value in the 'Embarked' column of the `data` DataFrame using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99953437}}, {"cell_id": 11, "code": "data['Embarked'].value_counts()", "class": "Exploratory Data Analysis", "desc": "This code counts and outputs the occurrences of each unique value in the 'Embarked' column of the `data` DataFrame after filling missing values, using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99953437}}, {"cell_id": 12, "code": "data.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code recalculates and outputs the total number of missing values in each column of the `data` DataFrame using the `isnull().sum()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99897987}}, {"cell_id": 25, "code": "import lightgbm as lgb\n\n\nlgb_train = lgb.Dataset(X_train, y_train,\n                                         categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train,\n                                         categorical_feature=categorical_features)\n\nparams = {\n    'objective': 'binary'\n}\n\nmodel = lgb.train(params, lgb_train,\n                               valid_sets=[lgb_train, lgb_eval],\n                               verbose_eval=10,\n                               num_boost_round=1000,\n                               early_stopping_rounds=10)\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)", "class": "Model Training", "desc": "This code initializes and trains a LightGBM model using the training data (`lgb_train`), validates it using the validation data (`lgb_eval`), and then makes predictions on the test dataset (`X_test`), employing early stopping and setting various training parameters.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9968785}}], "notebook_id": 2, "notebook_name": "notebook-titanic.ipynb"}, {"cells": [{"cell_id": 42, "code": "sub['Survived']=test_y", "class": "Data Export", "desc": "This snippet assigns the predictions stored in `test_y` to the `Survived` column of the `sub` DataFrame, preparing it for submission.", "testing": {"class": "Data_Export", "subclass": "prepare_output", "subclass_id": 55, "predicted_subclass_probability": 0.59170985}}, {"cell_id": 44, "code": "sub.to_csv('submission.csv',index=False)", "class": "Data Export", "desc": "This snippet exports the `sub` DataFrame to a CSV file named `submission.csv` without including the index, using the `to_csv` method in pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992668}}, {"cell_id": 1, "code": "train=pd.read_csv('../input/titanic/train.csv')", "class": "Data Extraction", "desc": "This snippet reads the training dataset from a CSV file located at `../input/titanic/train.csv` into a pandas DataFrame called `train`.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997099}}, {"cell_id": 27, "code": "test=pd.read_csv('../input/titanic/test.csv')", "class": "Data Extraction", "desc": "This snippet reads the test dataset from a CSV file located at `../input/titanic/test.csv` into a pandas DataFrame called `test`.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99972445}}, {"cell_id": 41, "code": "sub=pd.read_csv('../input/titanic/gender_submission.csv')", "class": "Data Extraction", "desc": "This snippet reads a CSV file located at `../input/titanic/gender_submission.csv` into a pandas DataFrame called `sub`.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99968815}}, {"cell_id": 9, "code": "# \u041a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nembark=pd.get_dummies(train['Embarked'])", "class": "Data Transform", "desc": "This snippet creates dummy/one-hot encoded variables for the categorical feature `Embarked` from the `train` DataFrame using the `pd.get_dummies` function.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9992428}}, {"cell_id": 10, "code": "gender=pd.get_dummies(train['Sex'])", "class": "Data Transform", "desc": "This snippet creates dummy/one-hot encoded variables for the categorical feature `Sex` from the `train` DataFrame using the `pd.get_dummies` function.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9990752}}, {"cell_id": 11, "code": "# \u041f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430 \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u044e\u0442\u0441\u044f \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c 30\ntrain['Age'].fillna(30,inplace=True)\ntrain['Age']=train['Age'].astype('int')", "class": "Data Transform", "desc": "This snippet fills missing values in the `Age` column of the `train` DataFrame with the value 30 and then converts the `Age` column to integer type using `fillna` and `astype` methods in pandas.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.97078043}}, {"cell_id": 12, "code": "# \u0412\u043e\u0437\u0440\u0430\u0441\u0442 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d \u043d\u0430 6 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0439\ndef age_grp(age):\n    if age<13:\n        return '<13'\n    elif (age>=13) &(age<18):\n        return '13-18'\n    elif (age>=18) &(age<=24):\n        return '18-24'\n    elif (age>=25) &(age<=34):\n        return '25-34'\n    elif (age>=35) &(age<=44):\n        return '35-44'\n    else:\n        return '45+'", "class": "Data Transform", "desc": "This snippet defines a function `age_grp` that categorizes individuals into six age groups based on their age.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.5255232}}, {"cell_id": 13, "code": "train['Age_grp']=train['Age'].apply(lambda x: age_grp(x))", "class": "Data Transform", "desc": "This snippet creates a new column `Age_grp` in the `train` DataFrame by applying the `age_grp` function to each value in the `Age` column using the `apply` method with a lambda function.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99622065}}, {"cell_id": 14, "code": "age=pd.get_dummies(train['Age_grp'])", "class": "Data Transform", "desc": "This snippet creates dummy/one-hot encoded variables for the categorical feature `Age_grp` from the `train` DataFrame using the `pd.get_dummies` function.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99901867}}, {"cell_id": 15, "code": "train_df=pd.concat([train,embark,gender,age],axis=1)", "class": "Data Transform", "desc": "This snippet concatenates the original `train` DataFrame with the newly created dummy/one-hot encoded DataFrames `embark`, `gender`, and `age` along the columns to form a new DataFrame `train_df` using the `pd.concat` function.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.999501}}, {"cell_id": 17, "code": "def is_var(val):\n    if val>0:\n        return 1\n    else:\n        return 0", "class": "Data Transform", "desc": "This snippet defines a function `is_var` that converts any positive value to 1 and any non-positive value to 0.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.5716193}}, {"cell_id": 18, "code": "# \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0432\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 - Family - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0447\u043b\u0435\u043d\u043e\u0432 \u043e\u0434\u043d\u043e\u0439 \u0441\u0435\u043c\u044c\u0438 \u043d\u0430 \u0422\u0438\u0442\u0430\u043d\u0438\u043a\u0435\ntrain_df['Family']=train_df['Parch'] + 1 + train_df['SibSp']\ntrain_df['Parch']=train_df['Parch'].apply(lambda x: is_var(x))\ntrain_df['SibSp']=train_df['SibSp'].apply(lambda x: is_var(x))", "class": "Data Transform", "desc": "This snippet creates a new feature `Family` in the `train_df` DataFrame by summing the values of `Parch`, `SibSp`, and adding 1 to represent family members on the Titanic, and then applies the `is_var` function to convert positive values in `Parch` and `SibSp` columns to 1, and non-positive values to 0 using the `apply` method with a lambda function.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99832696}}, {"cell_id": 19, "code": "sel_cols=['Fare', \n    'Pclass', 'SibSp',\n       'Parch',  'C', 'Q',\n       'S', 'female', 'male', '18-24', '25-34', '35-44', '45+', '<13','13-18', 'Family'\n]", "class": "Data Transform", "desc": "This snippet defines a list of selected column names `sel_cols` to be used for further analysis or modeling from the `train_df` DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.9982552}}, {"cell_id": 20, "code": "train_df.fillna(0,inplace=True)", "class": "Data Transform", "desc": "This snippet fills any remaining missing values in the `train_df` DataFrame with 0 using the `fillna` method with the `inplace=True` parameter to modify the DataFrame in place.", "testing": {"class": "Data_Transform", "subclass": "correct_missing_values", "subclass_id": 17, "predicted_subclass_probability": 0.7539002}}, {"cell_id": 21, "code": "X=train_df[sel_cols]", "class": "Data Transform", "desc": "This snippet creates a new DataFrame `X` that contains only the columns specified in the `sel_cols` list from the `train_df` DataFrame.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99719954}}, {"cell_id": 22, "code": "y=train_df['Survived']", "class": "Data Transform", "desc": "This snippet assigns the target variable `Survived` from the `train_df` DataFrame to the variable `y` for modeling purposes.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99920124}}, {"cell_id": 30, "code": "# \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\nembark=pd.get_dummies(test['Embarked'])", "class": "Data Transform", "desc": "This snippet creates dummy/one-hot encoded variables for the categorical feature `Embarked` from the `test` DataFrame using the `pd.get_dummies` function, similar to the processing performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9990907}}, {"cell_id": 31, "code": "gender=pd.get_dummies(test['Sex'])", "class": "Data Transform", "desc": "This snippet creates dummy/one-hot encoded variables for the categorical feature `Sex` from the `test` DataFrame using the `pd.get_dummies` function, similar to the processing performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.998933}}, {"cell_id": 32, "code": "test['Age'].fillna(30,inplace=True)\ntest['Age']=test['Age'].astype('int')", "class": "Data Transform", "desc": "This snippet fills missing values in the `Age` column of the `test` DataFrame with the value 30 and then converts the `Age` column to integer type using `fillna` and `astype` methods, similar to the processing performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.9835433}}, {"cell_id": 33, "code": "test['Age_grp']=test['Age'].apply(lambda x: age_grp(x))", "class": "Data Transform", "desc": "This snippet creates a new column `Age_grp` in the `test` DataFrame by applying the `age_grp` function to each value in the `Age` column using the `apply` method with a lambda function, similar to the processing performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99674535}}, {"cell_id": 34, "code": "age=pd.get_dummies(test['Age_grp'])", "class": "Data Transform", "desc": "This snippet creates dummy/one-hot encoded variables for the categorical feature `Age_grp` from the `test` DataFrame using the `pd.get_dummies` function, similar to the processing performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99887127}}, {"cell_id": 35, "code": "test_df=pd.concat([test,embark,gender,age],axis=1)", "class": "Data Transform", "desc": "This snippet concatenates the original `test` DataFrame with the newly created dummy/one-hot encoded DataFrames `embark`, `gender`, and `age` along the columns to form a new DataFrame `test_df` using the `pd.concat` function, similar to the processing performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9995338}}, {"cell_id": 36, "code": "test_df['Family']=test_df['Parch']+1+test_df['SibSp']\ntest_df['Parch']=test_df['Parch'].apply(lambda x: is_var(x))\ntest_df['SibSp']=test_df['SibSp'].apply(lambda x: is_var(x))", "class": "Data Transform", "desc": "This snippet creates a new feature `Family` in the `test_df` DataFrame by summing the values of `Parch`, `SibSp`, and adding 1 to represent family members on the Titanic, and then applies the `is_var` function to convert positive values in `Parch` and `SibSp` columns to 1, and non-positive values to 0 using the `apply` method with a lambda function, similar to the processing performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9401761}}, {"cell_id": 37, "code": "test_df.fillna(0,inplace=True)", "class": "Data Transform", "desc": "This snippet fills any remaining missing values in the `test_df` DataFrame with 0 using the `fillna` method with the `inplace=True` parameter to modify the DataFrame in place, similar to the processing performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "correct_missing_values", "subclass_id": 17, "predicted_subclass_probability": 0.90065837}}, {"cell_id": 38, "code": "test_X=test_df[sel_cols]", "class": "Data Transform", "desc": "This snippet creates a new DataFrame `test_X` that contains only the columns specified in the `sel_cols` list from the `test_df` DataFrame, similar to the feature selection performed on the training set.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.9941274}}, {"cell_id": 2, "code": "train.shape", "class": "Exploratory Data Analysis", "desc": "This snippet returns the dimensions of the `train` DataFrame, showing the number of rows and columns it contains.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.99950194}}, {"cell_id": 3, "code": "train.head()", "class": "Exploratory Data Analysis", "desc": "This snippet displays the first five rows of the `train` DataFrame to provide a preview of the dataset's structure and contents.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997507}}, {"cell_id": 4, "code": "# \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432\ntrain.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This snippet checks for missing values in each column of the `train` DataFrame by summing the number of null entries per column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9979207}}, {"cell_id": 5, "code": "# \u0421\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\ntrain['Survived'].value_counts()", "class": "Exploratory Data Analysis", "desc": "This snippet calculates the distribution of the target variable `Survived` in the `train` DataFrame by counting the occurrences of each unique value.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99951816}}, {"cell_id": 6, "code": "train['Survived'].value_counts(normalize=True)", "class": "Exploratory Data Analysis", "desc": "This snippet calculates the relative frequency distribution of the target variable `Survived` in the `train` DataFrame by normalizing the count of each unique value.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9995047}}, {"cell_id": 7, "code": "# \u041e\u0431\u0449\u0430\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445\ntrain.describe()", "class": "Exploratory Data Analysis", "desc": "This snippet generates descriptive statistics for the numerical features in the `train` DataFrame, including measures such as mean, standard deviation, minimum, and maximum values.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.999355}}, {"cell_id": 8, "code": "train['Sex'].value_counts()", "class": "Exploratory Data Analysis", "desc": "This snippet counts the occurrences of each unique value in the `Sex` column of the `train` DataFrame to examine the distribution of genders in the dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99953294}}, {"cell_id": 16, "code": "train_df.columns", "class": "Exploratory Data Analysis", "desc": "This snippet retrieves the list of column names in the `train_df` DataFrame to examine its structure and feature set.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9984144}}, {"cell_id": 28, "code": "test.shape", "class": "Exploratory Data Analysis", "desc": "This snippet returns the dimensions of the `test` DataFrame, showing the number of rows and columns it contains.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995401}}, {"cell_id": 29, "code": "test.columns", "class": "Exploratory Data Analysis", "desc": "This snippet retrieves the list of column names in the `test` DataFrame to examine its structure and feature set.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99861264}}, {"cell_id": 39, "code": "test_X.columns", "class": "Exploratory Data Analysis", "desc": "This snippet retrieves the list of column names in the `test_X` DataFrame to verify that the correct features have been selected for the test set.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99851304}}, {"cell_id": 43, "code": "sub.head()", "class": "Exploratory Data Analysis", "desc": "This snippet displays the first five rows of the `sub` DataFrame to provide a preview of the submission file's structure and contents.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974316}}, {"cell_id": 0, "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score", "class": "Imports and Environment", "desc": "This snippet imports necessary libraries and modules including `pandas`, `numpy`, `LogisticRegression` from `sklearn.linear_model`, `train_test_split` from `sklearn.model_selection`, and `accuracy_score` from `sklearn.metrics`.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.9993319}}, {"cell_id": 25, "code": "lr.score(train_X,train_y)", "class": "Model Evaluation", "desc": "This snippet evaluates the logistic regression model's performance on the training set by calculating its accuracy score using the `score` method.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99803406}}, {"cell_id": 26, "code": "lr.score(val_X,val_y)", "class": "Model Evaluation", "desc": "This snippet evaluates the logistic regression model's performance on the validation set by calculating its accuracy score using the `score` method.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.998086}}, {"cell_id": 40, "code": "test_y=lr.predict(test_X)", "class": "Model Evaluation", "desc": "This snippet generates predictions for the test set by applying the trained logistic regression model `lr` to the features in `test_X` using the `predict` method.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9937192}}, {"cell_id": 23, "code": "train_X,val_X,train_y,val_y=train_test_split(X,y,test_size=0.3,random_state=1)", "class": "Model Training", "desc": "This snippet splits the features `X` and target `y` into training and validation sets with 30% of the data reserved for validation, using the `train_test_split` function from `sklearn.model_selection` with a random seed of 1.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.998154}}, {"cell_id": 24, "code": "lr=LogisticRegression(max_iter=400)\nlr.fit(train_X,train_y)", "class": "Model Training", "desc": "This snippet initializes a `LogisticRegression` model with a maximum of 400 iterations and fits it to the training data `train_X` and `train_y` using the `fit` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9995912}}], "notebook_id": 3, "notebook_name": "ml2021-lab-2-titanic.ipynb"}, {"cells": [{"cell_id": 25, "code": "# Get predictions for each model and create submission files\nfor model in best_models:\n    predictions = best_models[model].predict(test_X)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('submission_' + model + '.csv', index=False)", "class": "Data Export", "desc": "This code snippet generates predictions for each model stored in `best_models` on the `test_X` dataset, creates a DataFrame containing `PassengerId` and `Survived` columns, and saves these predictions to CSV files named `submission_<model>.csv` using the `to_csv()` method from pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992563}}, {"cell_id": 1, "code": "train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv')", "class": "Data Extraction", "desc": "This code snippet reads the training and testing datasets from CSV files into pandas DataFrames named `train_data` and `test_data`.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997619}}, {"cell_id": 9, "code": "for data in [train_data, test_data]:\n    # Too many missing values\n    data.drop(['Cabin'], axis=1, inplace=True)\n    # Probably will not provide some useful information\n    data.drop(['Ticket', 'Fare'], axis=1, inplace=True)", "class": "Data Transform", "desc": "This code snippet removes the `Cabin`, `Ticket`, and `Fare` columns from both `train_data` and `test_data` DataFrames due to missing values and perceived lack of usefulness, using the `drop()` method from pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99880826}}, {"cell_id": 11, "code": "# Find the women and boys\nfor data in [train_data, test_data]:\n    data['Title'] = data.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\n    data['Woman_Or_Boy'] = (data.Title == 'Master') | (data.Sex == 'female')\n    data.drop('Title', axis=1, inplace=True)\n    data.drop('Name', axis=1, inplace=True)", "class": "Data Transform", "desc": "This code snippet creates a new column `Woman_Or_Boy` to identify women and boys in both `train_data` and `test_data` DataFrames based on the title extracted from the `Name` column, then drops the intermediate `Title` and original `Name` columns using string operations and the `drop()` method from pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.793178}}, {"cell_id": 12, "code": "# Encode 'Sex' and 'Woman_Or_Boy' columns\nlabel_encoder = LabelEncoder()\nfor data in [train_data, test_data]:\n    data['Sex'] = label_encoder.fit_transform(data['Sex'])\n    data['Woman_Or_Boy'] = label_encoder.fit_transform(data['Woman_Or_Boy'])", "class": "Data Transform", "desc": "This code snippet encodes the `Sex` and `Woman_Or_Boy` columns in both `train_data` and `test_data` DataFrames into numerical values using the `LabelEncoder` from scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9993673}}, {"cell_id": 13, "code": "# Merge two data to get the average Age and fill the column\nall_data = pd.concat([train_data, test_data])\naverage = all_data.Age.median()\nprint(\"Average Age: {0}\".format(average))\nfor data in [train_data, test_data]:\n    data.fillna(value={'Age': average}, inplace=True)", "class": "Data Transform", "desc": "This code snippet concatenates `train_data` and `test_data` DataFrames to calculate the median age, and then fills missing values in the `Age` column of both DataFrames with this median value using the `fillna()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.35051963}}, {"cell_id": 14, "code": "# Get the most common Embark and fill the column\nmost_common = all_data.Embarked.mode()\nprint(\"Most common Embarked value: {0}\".format(most_common[0]))\nfor data in [train_data, test_data]:\n    data.fillna(value={'Embarked': most_common[0]}, inplace=True)", "class": "Data Transform", "desc": "This code snippet identifies the most common value in the `Embarked` column from the concatenated DataFrame and fills the missing values in the `Embarked` column of both `train_data` and `test_data` DataFrames with this mode value using the `fillna()` method from pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.59436584}}, {"cell_id": 15, "code": "# Create categorical variable for traveling alone\n# Credits to https://www.kaggle.com/vaishvik25/titanic-eda-fe-3-model-decision-tree-viz\nfor data in [train_data, test_data]:\n    data['TravelAlone'] = np.where(data[\"SibSp\"] + data[\"Parch\"] > 0, 0, 1)\n    data.drop('SibSp', axis=1, inplace=True)\n    data.drop('Parch', axis=1, inplace=True)", "class": "Data Transform", "desc": "This code snippet creates a new column `TravelAlone` to indicate whether a passenger is traveling alone based on the sum of `SibSp` and `Parch`, then drops the `SibSp` and `Parch` columns from both `train_data` and `test_data` DataFrames using `np.where()` from NumPy and `drop()` from pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9941294}}, {"cell_id": 16, "code": "# Encode 'Embarked' column\none_hot_encoder = OneHotEncoder(sparse=False)\ndef encode_embarked(data):\n    encoded = pd.DataFrame(one_hot_encoder.fit_transform(data[['Embarked']]))\n    encoded.columns = one_hot_encoder.get_feature_names(['Embarked'])\n    data.drop(['Embarked'], axis=1, inplace=True)\n    data = data.join(encoded)\n    return data\ntrain_data = encode_embarked(train_data)\ntest_data = encode_embarked(test_data)", "class": "Data Transform", "desc": "This code snippet one-hot encodes the `Embarked` column in both `train_data` and `test_data` DataFrames using the `OneHotEncoder` from scikit-learn, replaces the original `Embarked` column with the encoded columns, and rejoins them to the respective DataFrames.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99923754}}, {"cell_id": 18, "code": "# Set X and y\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\ny = train_data['Survived']\ntest_X = test_data.drop(['PassengerId'], axis=1)", "class": "Data Transform", "desc": "This code snippet separates the `train_data` DataFrame into features `X` by dropping `Survived` and `PassengerId` columns, sets the target variable `y` to the `Survived` column, and similarly prepares the `test_X` DataFrame by dropping the `PassengerId` column.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99934286}}, {"cell_id": 2, "code": "train_data", "class": "Exploratory Data Analysis", "desc": "This code snippet outputs the contents of the `train_data` DataFrame to inspect the dataset. ", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997675}}, {"cell_id": 3, "code": "train_data.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates summary statistics for the numerical columns in the `train_data` DataFrame using the `describe()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994442}}, {"cell_id": 4, "code": "print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet prints the list of column names in the `train_data` DataFrame using the `columns.tolist()` method and a formatted string.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99567986}}, {"cell_id": 5, "code": "missing_values = train_data.isna().any()\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet identifies columns in the `train_data` DataFrame that contain missing values and prints their names using the `isna().any()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.92364883}}, {"cell_id": 6, "code": "print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()/len(train_data))))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the percentage of missing values in the `Age`, `Cabin`, and `Embarked` columns of the `train_data` DataFrame using the `isna().sum()` method and basic arithmetic operations.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99853253}}, {"cell_id": 7, "code": "duplicates = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the number of duplicated rows in the `train_data` DataFrame using the `duplicated().sum()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_duplicates", "subclass_id": 38, "predicted_subclass_probability": 0.85191345}}, {"cell_id": 8, "code": "categorical = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical))", "class": "Exploratory Data Analysis", "desc": "This code snippet identifies and prints the unique count of values for each column, sorted in ascending order, in the `train_data` DataFrame using the `nunique()` and `sort_values()` methods from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.84571636}}, {"cell_id": 10, "code": "train_data.tail()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the last five rows of the `train_data` DataFrame using the `tail()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997588}}, {"cell_id": 17, "code": "train_data.tail()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the last five rows of the `train_data` DataFrame using the `tail()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997588}}, {"cell_id": 20, "code": "print(\"Features: \\n{0} \".format(X.columns.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet prints the list of feature column names in the `X` DataFrame using the `columns.tolist()` method and the `format()` function.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.93314546}}, {"cell_id": 0, "code": "import numpy as np\nimport pandas as pd\n\n# Encoders\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Modelling\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\n# LightGBM\nfrom lightgbm import LGBMClassifier\n\n# Voting Classifier\nfrom sklearn.ensemble import VotingClassifier", "class": "Imports and Environment", "desc": "The code imports necessary libraries and tools for data manipulation (NumPy, pandas), data encoding (OneHotEncoder, LabelEncoder), model selection, training, tuning, and evaluation (RepeatedStratifiedKFold, train_test_split, GridSearchCV, accuracy_score), and different machine learning classifiers (RandomForestClassifier, XGBClassifier, LGBMClassifier, VotingClassifier).", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99930537}}, {"cell_id": 24, "code": "evaluate_model(best_model_voting.best_estimator_, 'voting')", "class": "Model Evaluation", "desc": "This code snippet evaluates the best estimator found by the grid search for the `VotingClassifier` using the `accuracy_score` and stores the model in the `best_models` dictionary by calling the `evaluate_model` function.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.86052555}}, {"cell_id": 19, "code": "# To store models created\nbest_models = {}\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ndef print_best_parameters(hyperparameters, best_parameters):\n    value = \"Best parameters: \"\n    for key in hyperparameters:\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n    if hyperparameters:\n        print(value[:-2])\n\ndef get_best_model(estimator, hyperparameters):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters,\n                               n_jobs=-1, cv=cv, scoring=\"accuracy\")\n    best_model = grid_search.fit(train_X, train_y)\n    best_parameters = best_model.best_estimator_.get_params()\n    print_best_parameters(hyperparameters, best_parameters)\n    return best_model\n\ndef evaluate_model(model, name):\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n    best_models[name] = model", "class": "Model Training", "desc": "This code snippet prepares to store the best models in a dictionary, splits the data into training and validation sets using `train_test_split`, defines helper functions to print best hyperparameters, perform grid search for hyperparameter tuning using `GridSearchCV`, and evaluate models using `accuracy_score`, and stores the trained model in the `best_models` dictionary.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.8173711}}, {"cell_id": 21, "code": "# I couldn't find a way to set fit_params of XGBClasssifier through GridSearchCV, so did a little trick.\n# https://stackoverflow.com/questions/35545733/how-do-you-use-fit-params-for-randomizedsearch-with-votingclassifier-in-sklearn\nclass MyXGBClassifier(XGBClassifier):\n    def fit(self, X, y=None):\n        return super(XGBClassifier, self).fit(X, y,\n                                              verbose=False,\n                                              early_stopping_rounds=40,\n                                              eval_metric='logloss',\n                                              eval_set=[(val_X, val_y)])", "class": "Model Training", "desc": "This code snippet defines a custom subclass `MyXGBClassifier` of `XGBClassifier` from XGBoost that overrides the `fit` method to include additional fit parameters such as `verbose`, `early_stopping_rounds`, `eval_metric`, and `eval_set` specifically for training with early stopping.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9982059}}, {"cell_id": 22, "code": "# Models from, https://www.kaggle.com/sfktrkl/titanic-hyperparameter-tuning-gridsearchcv\nrandomForest = RandomForestClassifier(random_state=1, n_estimators=20, max_features='auto',\n                                      criterion='gini', max_depth=4, min_samples_split=2,\n                                      min_samples_leaf=3)\nxgbClassifier = MyXGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor',\n                                use_label_encoder=False, learning_rate=0.4, gamma=0.4,\n                                max_depth=4, reg_lambda=0, reg_alpha=0.1)\nlgbmClassifier = LGBMClassifier(random_state=1, device='gpu', boosting_type='dart',\n                                num_leaves=8, learning_rate=0.1, n_estimators=100,\n                                reg_alpha=1, reg_lambda=1)\n\nclassifiers = [\n    ('randomForest', randomForest),\n    ('xgbClassifier', xgbClassifier),\n    ('lgbmClassifier', lgbmClassifier)\n]", "class": "Model Training", "desc": "This code snippet initializes three different classifiers: `RandomForestClassifier`, a custom `MyXGBClassifier`, and `LGBMClassifier` with specified hyperparameters and configurations, and stores them in a list of tuples named `classifiers`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.9895577}}, {"cell_id": 23, "code": "hyperparameters = {\n    'n_jobs'  : [-1],\n    'voting'  : ['hard', 'soft'],\n    'weights' : [(1, 1, 1),\n                (2, 1, 1), (1, 2, 1), (1, 1, 2),\n                (2, 2, 1), (1, 2, 2), (2, 1, 2),\n                (3, 2, 1), (1, 3, 2), (2, 1, 3), (3, 1, 2)]\n}\nestimator = VotingClassifier(estimators=classifiers)\nbest_model_voting = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet defines a set of hyperparameters for a `VotingClassifier`, which includes voting strategies and different weight combinations for the classifiers, initializes a `VotingClassifier` with the previously defined classifiers, and then uses the `get_best_model` function to perform a grid search for the best hyperparameters, storing the result in `best_model_voting`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.98003674}}], "notebook_id": 4, "notebook_name": "titanic-votingclassifier-with-gridsearchcv.ipynb"}, {"cells": [{"cell_id": 21, "code": "pred_data=pd.DataFrame(pred,columns=['Survived'])\ndata_target=pd.concat([passengerId,pred_data],axis=1)\ndata_target", "class": "Data Export", "desc": "This code snippet creates a DataFrame from the prediction results, merges it with the 'PassengerId' column, and stores the combined DataFrame in `data_target`.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.99849606}}, {"cell_id": 22, "code": "data_target.to_csv('my_submission.csv',index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Data Export", "desc": "This code snippet exports the `data_target` DataFrame to a CSV file named 'my_submission.csv' without including the index, and prints a confirmation message that the submission was successfully saved.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99907625}}, {"cell_id": 1, "code": "data=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndata.head(10)", "class": "Data Extraction", "desc": "This code snippet reads the Titanic training dataset from a CSV file into a pandas DataFrame and displays the first 10 rows.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99953437}}, {"cell_id": 16, "code": "test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest.head(10)", "class": "Data Extraction", "desc": "This code snippet reads the Titanic test dataset from a CSV file into a pandas DataFrame and displays the first 10 rows.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996283}}, {"cell_id": 3, "code": "data.drop([\"PassengerId\",\"Cabin\",\"Name\",\"Ticket\"],inplace=True,axis=1)\ndata['Age']=data['Age'].fillna(data['Age'].median())\ndata['Embarked']=data['Embarked'].fillna(data['Embarked'].mode()[0])\ndata['Fare'][data['Fare']>400]", "class": "Data Transform", "desc": "This code snippet removes unnecessary columns, fills missing 'Age' values with the median, fills missing 'Embarked' values with the mode, and selects 'Fare' values greater than 400 from the DataFrame.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.8541413}}, {"cell_id": 4, "code": "data[\"Sex\"]=data[\"Sex\"].map({\"female\":0,\"male\":1})\ndata=pd.get_dummies(data,drop_first=True)\ndata.head(10)", "class": "Data Transform", "desc": "This code snippet converts the 'Sex' column to numerical values using a mapping dictionary and applies one-hot encoding to the remaining categorical variables, dropping the first category to avoid multicollinearity, and then displays the first 10 rows of the transformed DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9988757}}, {"cell_id": 7, "code": "data.drop([\"Fare\",\"Embarked_Q\",\"SibSp\"],inplace=True,axis=1)\ndata.head()", "class": "Data Transform", "desc": "This code snippet removes the 'Fare', 'Embarked_Q', and 'SibSp' columns from the DataFrame and displays the first few rows to reflect these changes.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991449}}, {"cell_id": 8, "code": "mm_scale=MinMaxScaler()\ndata_scaled=pd.DataFrame(mm_scale.fit_transform(data),columns=data.columns)\ndata_scaled.head()", "class": "Data Transform", "desc": "This code snippet applies MinMax scaling to the DataFrame using scikit-learn's MinMaxScaler, creating a new DataFrame with scaled values and displaying the first few rows of the transformed data.", "testing": {"class": "Data_Transform", "subclass": "normalization", "subclass_id": 18, "predicted_subclass_probability": 0.97623605}}, {"cell_id": 9, "code": "yScaled=data[\"Survived\"]\nxScaled=data_scaled.drop(\"Survived\",axis=1)\nxScaled.head()", "class": "Data Transform", "desc": "This code snippet separates the 'Survived' column as the target variable `yScaled` and drops it from the scaled DataFrame to create the feature set `xScaled`, displaying the first few rows of the feature set.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99836594}}, {"cell_id": 13, "code": "xc.drop(\"Parch\",inplace=True,axis=1)\nxc.drop(\"const\",inplace=True,axis=1)", "class": "Data Transform", "desc": "This code snippet removes the 'Parch' and 'const' columns from the previously augmented feature set `xc`.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991861}}, {"cell_id": 17, "code": "passengerId=test['PassengerId']\ntest.drop([\"PassengerId\",\"Name\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\"],inplace=True,axis=1)\ntest.head()", "class": "Data Transform", "desc": "This code snippet extracts the 'PassengerId' column, then removes specified columns from the test DataFrame, and displays the first few rows of the transformed test DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99917966}}, {"cell_id": 18, "code": "test['Sex']=test['Sex'].map({\"female\":0,\"male\":1})\ntest=pd.get_dummies(test,drop_first=True)\ntest.head()", "class": "Data Transform", "desc": "This code snippet converts the 'Sex' column to numerical values and applies one-hot encoding to the remaining categorical variables in the test DataFrame, dropping the first category, and then displays the first few rows of the transformed test DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9989386}}, {"cell_id": 19, "code": "test.drop(\"Embarked_Q\",inplace=True,axis=1)\ntest[\"Age\"]=test[\"Age\"].fillna(test[\"Age\"].median())\ntest_scaled=pd.DataFrame(mm_scale.fit_transform(test),columns=test.columns)\ntest_scaled.head()", "class": "Data Transform", "desc": "This code snippet removes the 'Embarked_Q' column, fills missing 'Age' values with the median, and scales the remaining features in the test DataFrame using MinMax scaling, displaying the first few rows of the scaled test DataFrame.", "testing": {"class": "Data_Transform", "subclass": "normalization", "subclass_id": 18, "predicted_subclass_probability": 0.9365268}}, {"cell_id": 2, "code": "data.isnull().mean()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates the mean of missing values for each column in the DataFrame to understand the proportion of missing data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9989335}}, {"cell_id": 0, "code": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom xgboost import XGBClassifier\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))", "class": "Imports and Environment", "desc": "This code snippet imports necessary Python libraries and modules including numpy, pandas, matplotlib, scikit-learn, statsmodels, xgboost, seaborn, and os, and prints the paths of files in the '/kaggle/input' directory.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.9992461}}, {"cell_id": 11, "code": "vif=pd.DataFrame()\nvif['features']=xScaled.columns\nvif['VIF']=[variance_inflation_factor(xScaled.values,i) for i in range(xScaled.shape[1])]\nvif['VIF']=round(vif['VIF'],3)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif", "class": "Model Evaluation", "desc": "This code snippet calculates the Variance Inflation Factor (VIF) for each feature in the scaled feature set to assess multicollinearity, sorts the results by VIF values in descending order, and displays the VIF values rounded to three decimal places.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.28141785}}, {"cell_id": 12, "code": "xc=sm.add_constant(xScaled)\nlm=sm.OLS(yScaled,xc).fit()\nprint(lm.summary())", "class": "Model Evaluation", "desc": "This code snippet adds a constant to the feature set, fits an Ordinary Least Squares (OLS) regression model using statsmodels, and prints the model's summary to evaluate its performance and regression statistics.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.6782264}}, {"cell_id": 20, "code": "pred=model.predict(test_scaled)\npred", "class": "Model Evaluation", "desc": "This code snippet uses the trained XGBClassifier model to predict the target variable for the scaled test feature set and stores the predictions in the variable `pred`.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9937383}}, {"cell_id": 10, "code": "lm=LinearRegression()\nlm.fit(xScaled,yScaled)\nrfe=RFE(lm,4)\nrfe=rfe.fit(xScaled,yScaled)\nlist(zip(xScaled.columns,rfe.support_,rfe.ranking_))", "class": "Model Training", "desc": "This code snippet trains a Linear Regression model on the scaled feature set, applies Recursive Feature Elimination (RFE) to select the top 4 features, and then lists each feature alongside its support status and ranking.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.83992904}}, {"cell_id": 15, "code": "model=XGBClassifier(learning_rate=0.05, max_depth=4, n_estimators=81, nthread=-1, scale_pos_weight=1, random_state=14)\nmodel.fit(xc,yScaled)", "class": "Model Training", "desc": "This code snippet initializes an XGBClassifier with specified hyperparameters and trains it on the feature set `xc` and target variable `yScaled`.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.99963725}}, {"cell_id": 5, "code": "sns.pairplot(data,vars=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Sex\",\"Embarked_Q\",\"Embarked_S\"],kind=\"reg\",hue=\"Survived\")", "class": "Visualization", "desc": "This code snippet uses seaborn to create a pairplot of various features in the dataset, colored by the 'Survived' column, to visualize potential relationships and correlations between features.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99358493}}, {"cell_id": 6, "code": "correlation=data.corr()\nplt.figure(figsize=(14,12),dpi=80)\nsns.heatmap(correlation,annot=True)", "class": "Visualization", "desc": "This code snippet calculates the correlation matrix for the DataFrame and uses seaborn to create a heatmap, displaying the correlation coefficients with annotations, to visualize the strength and direction of relationships between features.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9986854}}, {"cell_id": 14, "code": "sns.countplot(yScaled)", "class": "Visualization", "desc": "This code snippet uses seaborn to create a count plot for the target variable `yScaled` to visualize the distribution of the 'Survived' classes.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99077016}}], "notebook_id": 5, "notebook_name": "titanic-with-xgboost.ipynb"}, {"cells": [{"cell_id": 29, "code": "y_pred = svc.predict(test_df)\nsubmission = pd.DataFrame({ 'PassengerId': test_dfn['PassengerId'],\n                            'Survived': y_pred })\nsubmission.to_csv(\"submission.csv\", index=False)", "class": "Data Export", "desc": "The code makes predictions on the test dataset using the trained SVC model, creates a submission DataFrame with 'PassengerId' and predicted 'Survived' values, and exports it to a CSV file named \"submission.csv\" using pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99937505}}, {"cell_id": 30, "code": "submission.head()", "class": "Data Export", "desc": "The code displays the first five rows of the 'submission' DataFrame using pandas' `head` method to verify the data before exporting.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974185}}, {"cell_id": 1, "code": "'''load training dataset'''\ntrain_df = pd.read_csv('../input/titanic/train.csv')\ntest_df = pd.read_csv('../input/titanic/test.csv')\nsubdf = pd.read_csv('../input/titanic/gender_submission.csv')", "class": "Data Extraction", "desc": "The code loads the training, testing, and submission datasets from CSV files using pandas' `read_csv` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997547}}, {"cell_id": 2, "code": "sub_df = subdf.drop(['PassengerId'],axis =1)\ntest_df =pd.concat([test_df,sub_df],axis = 1)\ndf = pd.concat([train_df,test_df],axis =0)\ndf.head()", "class": "Data Transform", "desc": "The code merges the submission data with the test dataset by dropping the 'PassengerId' column and concatenates the train and test datasets vertically using pandas. ", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9992582}}, {"cell_id": 4, "code": "df = df[['PassengerId','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Survived']]\ndf.head()", "class": "Data Transform", "desc": "The code rearranges the columns of the combined dataset to a specified order using pandas' indexing.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.78551346}}, {"cell_id": 11, "code": "lis=['Cabin','Name']\ndf= df.drop(lis ,axis=1)\ndf.head()", "class": "Data Transform", "desc": "The code drops the 'Cabin' and 'Name' columns from the combined dataset using pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9992561}}, {"cell_id": 14, "code": "cleaning = df.drop(['Survived'],axis = 1)\nSurvived = df['Survived']\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric_cols = cleaning.select_dtypes(include=numerics)\nnumeric_cols = numeric_cols.fillna(numeric_cols.mean())", "class": "Data Transform", "desc": "The code separates the 'Survived' column from the dataset, selects the numeric columns, and fills any missing values in these numeric columns with their respective mean values using pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.6854556}}, {"cell_id": 15, "code": "categorical = ['object']\ncategorical_cols = cleaning.select_dtypes(include=categorical)\ncategorical_cols = categorical_cols.fillna('none')\ncategorical_cols = pd.get_dummies(categorical_cols )", "class": "Data Transform", "desc": "The code selects the categorical columns of the dataset, fills any missing values in these columns with the string 'none', and converts them into dummy/indicator variables using pandas' `get_dummies` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99940133}}, {"cell_id": 16, "code": "cleaned = pd.concat([numeric_cols,categorical_cols],axis= 1)\ndf = pd.concat([cleaned,Survived],axis = 1)\ndf.head()", "class": "Data Transform", "desc": "The code concatenates the processed numeric and categorical columns along with the 'Survived' column to create the finalized dataset using pandas' `concat` method.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9990748}}, {"cell_id": 17, "code": "test_dfn = df.iloc[ 891 : ,:-1]\ntest_df = df.iloc[ 891 : ,:-1].values\ntest_dfn", "class": "Data Transform", "desc": "The code extracts the test dataset from the combined dataset by slicing from the 891st row onward and excluding the 'Survived' column, storing it both as a DataFrame and as a NumPy array using pandas' `iloc` method and the `values` attribute.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9969296}}, {"cell_id": 19, "code": "X = df.iloc[:,:-1].values\ny = df['Survived'].values", "class": "Data Transform", "desc": "The code separates the features and the target variable by extracting all columns except 'Survived' as a NumPy array `X` and the 'Survived' column as a NumPy array `y` using pandas' `iloc` method and the `values` attribute.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.9993187}}, {"cell_id": 20, "code": "scl = MinMaxScaler(feature_range = (0, 1))\nX = scl.fit_transform(X) \ntest_df = scl.fit_transform(test_df) ", "class": "Data Transform", "desc": "The code applies MinMax scaling to normalize the feature values of both the full features dataset `X` and the test dataset `test_df` to a range between 0 and 1 using scikit-learn's `MinMaxScaler`.", "testing": {"class": "Data_Transform", "subclass": "normalization", "subclass_id": 18, "predicted_subclass_probability": 0.9974043}}, {"cell_id": 21, "code": "X_train ,X_test ,y_train ,y_test = train_test_split(X, y , test_size = 0.3, random_state = 44)", "class": "Data Transform", "desc": "The code splits the dataset into training and testing sets with 70% of the data for training and 30% for testing using the `train_test_split` function from scikit-learn, with a specified random state for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9983181}}, {"cell_id": 3, "code": "list(df.columns.values)", "class": "Exploratory Data Analysis", "desc": "The code lists all column names of the combined dataset using the `columns.values` attribute in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9981401}}, {"cell_id": 5, "code": "df.shape", "class": "Exploratory Data Analysis", "desc": "The code outputs the dimensions (number of rows and columns) of the combined dataset using the `shape` attribute in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995491}}, {"cell_id": 6, "code": "df.describe()", "class": "Exploratory Data Analysis", "desc": "The code generates descriptive statistics for the numerical columns in the combined dataset using pandas' `describe` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994438}}, {"cell_id": 7, "code": "print(df.isnull().sum())\nsns.heatmap(df.isnull(),cbar=False, cmap='viridis')", "class": "Exploratory Data Analysis", "desc": "The code prints the number of missing values in each column using the `isnull().sum()` method in pandas and visualizes the missing values using a heatmap with seaborn.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.99889094}}, {"cell_id": 8, "code": "df['Survived'].value_counts()\nsns.countplot(df['Survived'])", "class": "Exploratory Data Analysis", "desc": "The code counts the occurrences of each value in the 'Survived' column using pandas' `value_counts` method and visualizes the distribution with a count plot using seaborn.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9774469}}, {"cell_id": 9, "code": "corr_matrix = df.corr()\ncmap = sns.diverging_palette(230, 20, as_cmap=True) \nsns.heatmap(corr_matrix, annot=None ,cmap=cmap)", "class": "Exploratory Data Analysis", "desc": "The code computes the correlation matrix of the combined dataset using pandas' `corr` method and visualizes it with a diverging palette heatmap using seaborn.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.99885654}}, {"cell_id": 10, "code": "corr_matrix.nlargest(5, 'Survived')['Survived'].index", "class": "Exploratory Data Analysis", "desc": "The code identifies the top five features most positively correlated with the 'Survived' column using pandas' `nlargest` method on the correlation matrix.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.5149967}}, {"cell_id": 12, "code": "sns.set_style('whitegrid')\nsns.countplot(x=df['Survived'],hue=df['Sex'],data=df)", "class": "Exploratory Data Analysis", "desc": "The code sets the seaborn plotting style to 'whitegrid' and creates a count plot to visualize the distribution of survival status ('Survived') split by gender ('Sex') using seaborn.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99833965}}, {"cell_id": 13, "code": "for i in df[['Age' ,'Pclass' ,'Fare','Parch']] :\n    print(i,'&','Survived')\n    df.hist(column=i, by='Survived')", "class": "Exploratory Data Analysis", "desc": "The code iterates over specified numerical columns and creates histograms to visualize the distribution of each column ('Age', 'Pclass', 'Fare', 'Parch') split by the 'Survived' status using pandas' `hist` method.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9965384}}, {"cell_id": 18, "code": "test_df", "class": "Exploratory Data Analysis", "desc": "The code outputs the NumPy array representation of the test dataset extracted from the combined dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99976057}}, {"cell_id": 0, "code": "import numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport warnings \nwarnings.filterwarnings('ignore')", "class": "Imports and Environment", "desc": "The code imports necessary libraries and modules for data manipulation (NumPy, pandas), visualization (seaborn, Matplotlib), machine learning tasks (various scikit-learn modules), and suppresses warnings.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.9989849}}, {"cell_id": 28, "code": "labels = np.arange(2)\nclf_report = classification_report(y_test,y_test_pred,labels=labels,target_names=('died','survived'), output_dict=True)\nhmap1 = sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\nprint ('Classification Report',hmap1)", "class": "Model Evaluation", "desc": "The code generates a classification report for the SVC model's performance on the test data, converts it into a DataFrame, and visualizes it using a heatmap with seaborn, displaying metrics for the 'died' and 'survived' classes.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9841629}}, {"cell_id": 22, "code": "#lgm = LogisticRegression()\n#lgm = lgm.fit(X_train,y_train)\n#y_tpred = lgm.predict(X_train)\n#y_pred = lgm.predict(X_test)\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n#print('test score :',accuracy_score(y_test , y_pred))\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n#print('report :',classification_report(y_test, y_pred ))\n                                                                                      # SCORE : 0.85", "class": "Model Training", "desc": "The code snippet, though commented out, trains a Logistic Regression model on the training data, makes predictions on both training and testing sets, and evaluates performance using accuracy score, confusion matrix, and classification report from scikit-learn.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.89561415}}, {"cell_id": 23, "code": "# rnc = RandomForestClassifier(n_estimators=100, max_depth=4,random_state=0)\n# rnc = rnc.fit(X_train,y_train)\n# y_tpred = rnc.predict(X_train)\n# y_pred = rnc.predict(X_test)\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n# print('test score :',accuracy_score(y_test , y_pred))\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n# print('report :',classification_report(y_test, y_pred ))\n                                                                                     # SCORE : 0.87", "class": "Model Training", "desc": "The code snippet, though commented out, trains a RandomForestClassifier with 100 estimators and a maximum depth of 4, evaluates its performance on the training and testing data using accuracy score, confusion matrix, and classification report from scikit-learn.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.8540831}}, {"cell_id": 24, "code": "# gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5,max_depth=3, random_state=140)\n# gbc = gbc.fit(X_train,y_train)\n# y_tpred = gbc.predict(X_train)\n# y_pred = gbc.predict(X_test)\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n# print('test score :',accuracy_score(y_test , y_pred))\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n# print('report :',classification_report(y_test, y_pred ))\n                                                                                   # SCORE : 0.85", "class": "Model Training", "desc": "The code snippet, though commented out, trains a GradientBoostingClassifier with 100 estimators, a learning rate of 0.5, and a maximum depth of 3, evaluating its performance on training and testing data using accuracy score, confusion matrix, and classification report from scikit-learn.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.96510017}}, {"cell_id": 25, "code": "# svc = SVC()\n# svc = svc.fit(X_train,y_train)\n# y_tpred = svc.predict(X_train)\n# y_pred = svc.predict(X_test)\n# print('train score :',accuracy_score(y_train ,y_tpred))\n# print('test score :',accuracy_score(y_test , y_pred))\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n# print('report :',classification_report(y_test, y_pred ))\n                                                                                   # SCORE : 0.86", "class": "Model Training", "desc": "The code snippet, though commented out, trains a Support Vector Classifier (SVC) on the training data and evaluates its performance on both training and testing data using accuracy score, confusion matrix, and classification report from scikit-learn.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.761462}}, {"cell_id": 26, "code": "#v1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=1 , C = 0.5 , tol = 0.001)\n#v2 = RandomForestClassifier(n_estimators=100, max_depth= 5,random_state=144)\n#v3 = SVC()\n#eclf = VotingClassifier(estimators=[('lr', v1), ('rf', v2), ('gnb', v3)],voting='hard')\n\n#for clf, label in zip([v1, v2, v3, eclf], ['Logistic Regression', 'Random Forest', 'SVC ', 'Ensemble ']): \n #   scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n  #  print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))", "class": "Model Training", "desc": "The code snippet, though commented out, sets up a VotingClassifier with Logistic Regression, RandomForestClassifier, and SVC as base estimators, and evaluates each individual model and the ensemble model using cross-validation to calculate accuracy scores with scikit-learn.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.936291}}, {"cell_id": 27, "code": "svc = SVC()\nsvc = svc.fit(X_train,y_train)\ny_train_pred = svc.predict(X_train)\ny_test_pred = svc.predict(X_test)\nprint('train score :',accuracy_score(y_train ,y_train_pred ))\nprint('test score :',accuracy_score(y_test , y_test_pred))\nprint('con matrix :',confusion_matrix(y_test, y_test_pred))\nprint('report :',classification_report(y_test, y_test_pred ))\ncon = confusion_matrix(y_test,y_test_pred)\nhmap =sns.heatmap(con,annot=True,fmt=\"d\")\nprint ('Confusion Matrix',hmap)", "class": "Model Training", "desc": "The code trains a Support Vector Classifier (SVC) on the training data, makes predictions on both training and testing data, evaluates the model's performance using accuracy score, confusion matrix, and classification report from scikit-learn, and visualizes the confusion matrix using a heatmap with seaborn.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.37717277}}], "notebook_id": 6, "notebook_name": "titanic-disaster-88-simple-explanation.ipynb"}, {"cells": [{"cell_id": 49, "code": "fix_df = pd.read_csv('/kaggle/input/titanic/test.csv')\nsubmit_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmit_df['PassengerId'] = fix_df['PassengerId']\nsubmit_df['Survived'] = predict\nsubmit_df.to_csv('titanic_extraction.csv', header=True, index=False)\nsubmit_df.head(10)", "class": "Data Export", "desc": "This code snippet creates a submission DataFrame with 'PassengerId' and 'Survived' columns from the original test dataset and the prediction results, writes it to a CSV file named 'titanic_extraction.csv', and displays the first ten rows.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992036}}, {"cell_id": 1, "code": "#import dataset\ndf = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf.head()", "class": "Data Extraction", "desc": "The code snippet reads the Titanic dataset from a CSV file into a pandas DataFrame and displays the first few rows using the `head()` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996679}}, {"cell_id": 2, "code": "df_test = pd.read_csv('/kaggle/input/titanic/test.csv')\ndf_test.head()", "class": "Data Extraction", "desc": "This code snippet reads the test dataset of the Titanic data from a CSV file into a pandas DataFrame and displays the first few rows using the `head()` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996685}}, {"cell_id": 14, "code": "#get the age data which is not NaN\nages = df[df['Age'].notnull()]['Age'].values\n\n#create the data for histogram\nages_hist = np.histogram(ages, bins=[0,10,20,30,40,50,60,70,80,90])\nages_hist", "class": "Data Transform", "desc": "This code snippet filters out the non-null 'Age' values from the dataset and creates a histogram data array of these ages using predefined bins with the `np.histogram` function from NumPy.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9616358}}, {"cell_id": 16, "code": "#fill the null values of 'Age'\nfor data in combined:\n    data.Age.fillna(data.Age.mean(), inplace = True)\n    data.Fare.fillna(data.Fare.mean(), inplace = True)", "class": "Data Transform", "desc": "This code snippet fills null values in the 'Age' and 'Fare' columns with their respective mean values in both training and test datasets using the `fillna` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.8799256}}, {"cell_id": 19, "code": "def take_initial(x):\n    return x[0]\n\ncabins = cabins.apply(take_initial)\ncabins_test = cabins_test.apply(take_initial)\nprint(cabins[:10])\nprint(cabins_test[:10])", "class": "Data Transform", "desc": "This code snippet transforms the 'Cabin' column entries by applying a function that extracts the first character of each entry using the `apply` method in pandas and prints the first ten transformed values for both the training and test datasets.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.7167116}}, {"cell_id": 23, "code": "#menggabungkan data keluarga menjadi 1 variabel 'Family Size'\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ndf[['Name','SibSp','Parch','FamilySize']]", "class": "Data Transform", "desc": "This code snippet creates a new column 'FamilySize' by summing the 'SibSp', 'Parch', and adding 1 to represent the total number of family members including the individual and displays selected columns to show the transformation.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9982066}}, {"cell_id": 24, "code": "df_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1\ndf_test[['Name','SibSp','Parch','FamilySize']]", "class": "Data Transform", "desc": "This code snippet creates a new column 'FamilySize' in the test dataset by summing the 'SibSp', 'Parch', and adding 1 to represent the total number of family members including the individual, then displays selected columns to show the transformation.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9989434}}, {"cell_id": 25, "code": "print('Training Data')\nprint('Sebelum drop :', df.shape)\ndf = df.dropna(subset=['Embarked'])\nprint('Setelah drop :', df.shape)\nprint()\n\nprint('Testing Data')\nprint('Sebelum drop :', df_test.shape)\ndf_test = df_test.dropna(subset=['Embarked'])\nprint('Setelah drop :', df_test.shape)\nprint()", "class": "Data Transform", "desc": "This code snippet removes rows with missing values in the 'Embarked' column from both the training and test datasets using the `dropna` method in pandas, and displays the shape of the DataFrame before and after the operation.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.65012956}}, {"cell_id": 26, "code": "embarked_one_hot = pd.get_dummies(df['Embarked'], prefix='Embarked')\ndf = pd.concat([df, embarked_one_hot], axis=1)", "class": "Data Transform", "desc": "This code snippet performs one-hot encoding on the 'Embarked' column of the training dataset using the `get_dummies` method in pandas and concatenates the resulting dummy columns to the original DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9990441}}, {"cell_id": 28, "code": "test_embarked_one_hot = pd.get_dummies(df_test['Embarked'], prefix='Embarked')\ndf_test = pd.concat([df_test, test_embarked_one_hot], axis=1)", "class": "Data Transform", "desc": "This code snippet performs one-hot encoding on the 'Embarked' column of the test dataset using the `get_dummies` method in pandas and concatenates the resulting dummy columns to the original DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99887747}}, {"cell_id": 30, "code": "df = df.drop('Embarked', axis=1)\ndf_test = df_test.drop('Embarked', axis=1)", "class": "Data Transform", "desc": "This code snippet removes the original 'Embarked' column from both the training and test datasets using the `drop` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.999225}}, {"cell_id": 31, "code": "df['Cabin'] = df['Cabin'].fillna('U')\ndf[['Name', 'Cabin']]", "class": "Data Transform", "desc": "This code snippet fills missing values in the 'Cabin' column with the placeholder 'U' and displays the 'Name' and 'Cabin' columns from the dataset.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.5407201}}, {"cell_id": 32, "code": "df_test['Cabin'] = df_test['Cabin'].fillna('U')\ndf_test[['Name', 'Cabin']]", "class": "Data Transform", "desc": "This code snippet fills missing values in the 'Cabin' column of the test dataset with the placeholder 'U' and displays the 'Name' and 'Cabin' columns.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.8625036}}, {"cell_id": 33, "code": "def take_initial(x):\n    return x[0]\n\ndf['Cabin'] = df['Cabin'].apply(take_initial)\nprint(df[['Name', 'Cabin']])\n\ndf_test['Cabin'] = df_test['Cabin'].apply(take_initial)\nprint(df_test[['Name', 'Cabin']])", "class": "Data Transform", "desc": "This code snippet transforms the 'Cabin' column in both the training and test datasets by applying a function that extracts the first character of each entry using the `apply` method in pandas and then prints the 'Name' and 'Cabin' columns.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9925022}}, {"cell_id": 35, "code": "cabin_remap = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\":6, \"G\":7, \"T\":8, \"U\":9}\ntest_cabin_remap = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\":6, \"G\":7, \"T\":8, \"U\":9}\ndf['Cabin'] = df['Cabin'].map(cabin_remap)\ndf_test['Cabin'] = df_test['Cabin'].map(test_cabin_remap)", "class": "Data Transform", "desc": "This code snippet maps the unique 'Cabin' values to numerical codes using a predefined dictionary for both the training and test datasets using the `map` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9991586}}, {"cell_id": 37, "code": "df['Title'] = df.Name.str.extract('([A-Za-z]+)\\.', expand = False)\ndf_test['Title'] = df_test.Name.str.extract('([A-Za-z]+)\\.', expand = False)\ndf.drop('Name', axis = 1, inplace = True)\ndf_test.drop('Name', axis = 1, inplace = True)\n\nprint(df.Title.value_counts())\nprint(df_test.Title.value_counts())", "class": "Data Transform", "desc": "This code snippet extracts titles from the 'Name' column and creates a new column 'Title' in both the training and test datasets, then drops the 'Name' column, and finally prints the frequency counts of each title.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9636613}}, {"cell_id": 38, "code": "#pengelompokan gelar yang langka\nunimportant = [ 'Dr', 'Rev', 'Major', 'Col', 'Mlle',\n       'Jonkheer', 'Mme', 'Lady', 'Capt', 'Don', 'Sir', 'the Countess','Ms']\n\ndf.Title = df.Title.replace(unimportant, 'Rare')\ndf_test.Title = df_test.Title.replace(unimportant, 'Rare')", "class": "Data Transform", "desc": "This code snippet replaces less common titles with the label 'Rare' in the 'Title' column for both the training and test datasets using the `replace` method in pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9640563}}, {"cell_id": 39, "code": "#konversi gelar ke bentuk integer\ntitle_remap = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndf['Title'] = df['Title'].map(title_remap)\n\ntest_title_remap = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndf_test['Title'] = df_test['Title'].map(test_title_remap)", "class": "Data Transform", "desc": "This code snippet converts the categorical titles in the 'Title' column to integer values using a predefined dictionary for both the training and test datasets by applying the `map` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9988896}}, {"cell_id": 40, "code": "sex_one_hot = pd.get_dummies(df['Sex'], prefix='Sex')\ndf = pd.concat([df, sex_one_hot], axis=1)\n\n#remove initial sex column \ndf = df.drop('Sex', axis=1)\n\ndf.columns", "class": "Data Transform", "desc": "This code snippet performs one-hot encoding on the 'Sex' column of the training dataset using the `get_dummies` method in pandas, concatenates the resulting dummy columns to the original DataFrame, and then removes the original 'Sex' column.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9992231}}, {"cell_id": 41, "code": "test_sex_one_hot = pd.get_dummies(df_test['Sex'], prefix='Sex')\ndf_test = pd.concat([df_test, test_sex_one_hot], axis=1)\n\n#remove initial sex column \ndf_test = df_test.drop('Sex', axis=1)\n\ndf.columns", "class": "Data Transform", "desc": "This code snippet performs one-hot encoding on the 'Sex' column of the test dataset using the `get_dummies` method in pandas, concatenates the resulting dummy columns to the original DataFrame, and then removes the original 'Sex' column.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9986179}}, {"cell_id": 42, "code": "df = df.drop(['PassengerId', 'Ticket', 'Title'], axis=1)\ndf_test = df_test.drop(['PassengerId', 'Ticket', 'Title'], axis=1)", "class": "Data Transform", "desc": "This code snippet removes the 'PassengerId', 'Ticket', and 'Title' columns from both the training and test datasets using the `drop` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99924254}}, {"cell_id": 43, "code": "X_train = df.drop(\"Survived\", axis=1)\nY_train = df[\"Survived\"]\nX_test = df_test\nprint(\"X_train shape: \",X_train.shape)\nprint(\"Y_train shape: \",Y_train.shape)\nprint(\"x_test shape: \",X_test.shape)", "class": "Data Transform", "desc": "This code snippet separates the 'Survived' column as the target variable `Y_train` and the remaining columns as the feature set `X_train` from the training dataset, and assigns the test dataset to `X_test` before printing their shapes.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.7103733}}, {"cell_id": 3, "code": "#check for data shape\ndf.shape", "class": "Exploratory Data Analysis", "desc": "This code snippet checks the dimensions of the dataset by displaying the number of rows and columns using the `shape` attribute of the pandas DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9996208}}, {"cell_id": 4, "code": "#check for data type\ndf.dtypes", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the data types of each column in the dataset using the `dtypes` attribute of the pandas DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_data_types", "subclass_id": 70, "predicted_subclass_probability": 0.99594927}}, {"cell_id": 5, "code": "#check for data type in test.csv\ndf_test.dtypes", "class": "Exploratory Data Analysis", "desc": "The code snippet displays the data types of each column in the test dataset using the `dtypes` attribute of the pandas DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_data_types", "subclass_id": 70, "predicted_subclass_probability": 0.9707183}}, {"cell_id": 6, "code": "#melakukan pengecekan terhadap data yang kosong\ncombined = [df, df_test]\nfor data in combined:\n    print(data.isnull().sum())\n    print()", "class": "Exploratory Data Analysis", "desc": "This code snippet checks for missing values in both the training and test datasets by summing up the null values for each column using the `isnull().sum()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.95363516}}, {"cell_id": 7, "code": "#check for null values on Age\ndf[df['Age'].isna()].head()", "class": "Exploratory Data Analysis", "desc": "This code snippet filters and displays the first few rows of the dataset where the 'Age' column contains null values using the `isna()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9984438}}, {"cell_id": 8, "code": "#check for survival values\nsurvived_count = df['Survived'].value_counts()\nsurvived_count", "class": "Exploratory Data Analysis", "desc": "This code snippet counts the occurrences of each unique value in the 'Survived' column of the dataset and stores the result in `survived_count` using the `value_counts()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99944824}}, {"cell_id": 11, "code": "#show the total number of each passengers class\npclass_count = df['Pclass'].value_counts()\npclass_count", "class": "Exploratory Data Analysis", "desc": "This code snippet counts the number of passengers in each class ('Pclass' column) and stores the result in `pclass_count` using the `value_counts()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9994443}}, {"cell_id": 17, "code": "#melihat data unik pada fitur 'Cabin'\ndf['Cabin'].unique()[:10]", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the first ten unique values in the 'Cabin' column of the dataset using the `unique()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.99743867}}, {"cell_id": 18, "code": "print('Training Data')\nprint('Sebelum dibersihkan:', df['Cabin'].shape)\ncabins = df['Cabin'].dropna()\n\nprint(\"Setelah dibersihkan:\", cabins.shape)\nprint()\n\nprint('Testing Data')\nprint('Sebelum dibersihkan:', df_test['Cabin'].shape)\ncabins_test = df_test['Cabin'].dropna()\n\nprint(\"Setelah dibersihkan:\", cabins_test.shape)", "class": "Exploratory Data Analysis", "desc": "This code snippet compares the number of entries in the 'Cabin' column before and after dropping the null values for both the training and test datasets using the `dropna()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.7360855}}, {"cell_id": 22, "code": "df['Fare'].describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet provides a statistical summary of the 'Fare' column in the dataset using the `describe()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994549}}, {"cell_id": 27, "code": "df[['Name', 'Embarked', 'Embarked_C', 'Embarked_Q', 'Embarked_S']].head()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the first few rows of the dataset, including the 'Name', 'Embarked', and the newly created one-hot encoded columns 'Embarked_C', 'Embarked_Q', and 'Embarked_S', using the `head()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9996408}}, {"cell_id": 29, "code": "df_test[['Name', 'Embarked', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the 'Name', 'Embarked', and the newly created one-hot encoded columns 'Embarked_C', 'Embarked_Q', and 'Embarked_S' from the test dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.98599225}}, {"cell_id": 34, "code": "df.Cabin.unique()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the unique values in the 'Cabin' column of the dataset using the `unique()` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.9976349}}, {"cell_id": 36, "code": "print(df['Cabin'])\nprint(df_test['Cabin'])", "class": "Exploratory Data Analysis", "desc": "This code snippet prints the 'Cabin' column from both the training and test datasets after mapping the cabin letters to numerical codes.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9778472}}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This code snippet imports essential Python libraries such as NumPy, pandas, matplotlib, and seaborn for data manipulation, visualization, and also lists the files available in the Kaggle input directory using the os module.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.9991904}}, {"cell_id": 44, "code": "import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.models import Sequential", "class": "Imports and Environment", "desc": "This code snippet imports TensorFlow and specific classes and methods from the Keras API within TensorFlow for building and training neural network models.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99933285}}, {"cell_id": 48, "code": "predict = model.predict(X_test)\n#since we have use sigmoid activation function in output layer\npredict = (predict > 0.5).astype(int).ravel()\nprint(predict)", "class": "Model Evaluation", "desc": "This code snippet generates predictions for the test dataset using the trained model, converts sigmoid output probabilities to binary predictions, and prints the results.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99033445}}, {"cell_id": 50, "code": "from sklearn.metrics import classification_report\n\nY_pred = (model.predict(X_train) > 0.5).astype(int)\n\n\nprint(classification_report(Y_train, Y_pred))", "class": "Model Evaluation", "desc": "This code snippet evaluates the performance of the trained model on the training dataset by generating predictions and printing a classification report with precision, recall, f1-score, and support using the `classification_report` method from sklearn.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.97872764}}, {"cell_id": 45, "code": "model = Sequential()\nmodel.add(Dense(units = 32, input_shape = (12,), activation = 'relu'))\nmodel.add(Dense(units =1 , activation = 'sigmoid'))", "class": "Model Training", "desc": "This code snippet defines a Sequential neural network model with one hidden layer of 32 units using the ReLU activation function and an output layer with 1 unit using the sigmoid activation function, intended for a binary classification task.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.99621934}}, {"cell_id": 46, "code": "model.summary()", "class": "Model Training", "desc": "This code snippet displays a summary of the Sequential model architecture, including details about each layer and the total number of parameters.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.99612254}}, {"cell_id": 47, "code": "model.compile(loss = tf.keras.losses.binary_crossentropy, optimizer = tf.keras.optimizers.Adam(), metrics = ['acc'])\nmodel.fit(X_train, Y_train, batch_size = 32, epochs = 50)", "class": "Model Training", "desc": "This code snippet compiles the model with binary cross-entropy loss, the Adam optimizer, and accuracy as a metric, then trains the model on the training data for 50 epochs with a batch size of 32 using the `fit` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.97769105}}, {"cell_id": 9, "code": "plt.figure(figsize=(4,5))\nplt.bar(survived_count.index, survived_count.values)\nplt.title('Grouped by Survival')\nplt.xticks([0,1],['Not Survived', 'Survived'])\n\n#show the text detail\nfor i, value in enumerate(survived_count.values):\n    plt.text(i, value-70, str(value), fontsize=12, color='white',\n            horizontalalignment='center', verticalalignment='center')\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a bar chart to visualize the count of survivors and non-survivors from the dataset, including textual annotations for the counts, using matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.998007}}, {"cell_id": 10, "code": "survived_sex = df[df['Survived']==1]['Sex'].value_counts()\n\nplt.figure(figsize=(4,5))\nplt.bar(survived_sex.index, survived_sex.values)\nplt.title('Survived female and male')\n\nfor i, value in enumerate(survived_sex.values):\n    plt.text(i, value-10, str(value), fontsize=12, color='white',\n            horizontalalignment='center', verticalalignment='center')\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a bar chart to visualize the number of male and female survivors in the dataset, including textual annotations for the counts, using matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99790275}}, {"cell_id": 12, "code": "#visualize passengger data in piechart\nplt.figure(figsize=(7,7))\nplt.title('Grouped by Pclass')\nplt.pie(pclass_count.values,\n        labels=['Class {}'.format(i) for i in pclass_count.index],\n        autopct='%1.1f%%', textprops={'fontsize':13})\n       \nplt.show()", "class": "Visualization", "desc": "This code snippet creates a pie chart to visualize the distribution of passengers across different classes ('Pclass') using the pie chart functionality in matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9904157}}, {"cell_id": 13, "code": "#visualize embarkation data in piechart\nembarked_count = df['Embarked'].value_counts()\n\nplt.figure(figsize=(7,7))\nplt.title('Grouped by embarkation')\nplt.pie(embarked_count.values,\n        labels=embarked_count.index,\n        autopct='%1.1f%%', textprops={'fontsize':13})\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a pie chart to visualize the distribution of passengers based on their embarkation points ('Embarked' column) using the pie chart functionality in matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9975122}}, {"cell_id": 15, "code": "#manual data labelling\nages_hist_labels = ['0-10', '11-20', '21-30',\n                    '31-40', '41-50', '51-60',\n                    '61-70','71-80', '81-90']\n\nplt.figure(figsize=(7,7))\nplt.title('Age Distribution')\nplt.bar(ages_hist_labels, ages_hist[0], color='Green')\nplt.xlabel('Age')\nplt.ylabel('No of passenger')\nfor i, bin in zip(ages_hist[0], range(9)):\n    plt.text(bin, i+5, str(int(i)), fontsize=12,\n            horizontalalignment='center', verticalalignment='center')\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a bar chart to visualize the distribution of passenger ages using predefined age bins, with manual labels and textual annotations for counts, using matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99748707}}, {"cell_id": 20, "code": "cabins_count = cabins.value_counts()\n\nplt.title('Cabin distribution')\nplt.bar(cabins_count.index, cabins_count.values)\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a bar chart to visualize the frequency distribution of the initials of the 'Cabin' values in the training dataset using matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99822444}}, {"cell_id": 21, "code": "plt.figure(figsize=(13,1))\nplt.title('Fare distribution')\nplt.boxplot(df['Fare'], vert=False)\nplt.yticks([])\nplt.xticks(range(0,600,50))\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a horizontal box plot to visualize the distribution of 'Fare' values in the dataset using matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9972312}}], "notebook_id": 7, "notebook_name": "titanic-extraction.ipynb"}, {"cells": [{"cell_id": 6, "code": "# Save test predictions to file\n#output = pd.DataFrame({'PassengerId': data.PassengerId,'Survived': preds})\n#output.to_csv('submission.csv', index=False)", "class": "Data Export", "desc": "The commented-out code creates a DataFrame with test predictions and saves it to a CSV file named 'submission.csv' using pandas' `to_csv` method.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.7032798}}, {"cell_id": 73, "code": "output = pd.DataFrame({'PassengerId': copy_test_data.PassengerId, 'Survived': preds2})\noutput.to_csv('submission2.csv', index=False)", "class": "Data Export", "desc": "The code creates a DataFrame with the passenger IDs from the original test dataset (`copy_test_data`) and the predicted survival values (`preds2`), then saves it to a CSV file named 'submission2.csv' using pandas' `to_csv` method.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99919873}}, {"cell_id": 2, "code": "data = pd.read_csv('/kaggle/input/titanic/train.csv')", "class": "Data Extraction", "desc": "The code reads the Titanic training dataset from a CSV file using pandas' `read_csv` method. ", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99973005}}, {"cell_id": 3, "code": "test_data = pd.read_csv('/kaggle/input/titanic/test.csv')", "class": "Data Extraction", "desc": "The code reads the Titanic test dataset from a CSV file using pandas' `read_csv` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99974674}}, {"cell_id": 74, "code": "sub = pd.read_csv('./submission2.csv')", "class": "Data Extraction", "desc": "The code reads the previously saved 'submission2.csv' file into a pandas DataFrame named `sub` using the `read_csv` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9995944}}, {"cell_id": 7, "code": "copy_data = data", "class": "Data Transform", "desc": "The code creates a copy of the Titanic training dataset by assigning the original DataFrame to a new variable.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.952716}}, {"cell_id": 8, "code": "copy_test_data = test_data", "class": "Data Transform", "desc": "The code creates a copy of the Titanic test dataset by assigning the original DataFrame to a new variable.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.77818143}}, {"cell_id": 18, "code": "dataset = data[['Sex', 'Age', 'SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']]", "class": "Data Transform", "desc": "The code creates a new DataFrame named `dataset` by selecting specific columns ('Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked') from the Titanic training dataset using pandas.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.8299218}}, {"cell_id": 31, "code": "data.drop(\"Cabin\", axis =1, inplace =True)", "class": "Data Transform", "desc": "The code removes the 'Cabin' column from the Titanic training dataset in place using pandas' `drop` method with `axis=1` and `inplace=True` parameters.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9992048}}, {"cell_id": 34, "code": "sex = pd.get_dummies(data[\"Sex\"], drop_first = True)\nsex.head(5)", "class": "Data Transform", "desc": "The code converts the 'Sex' column in the Titanic training dataset into dummy/indicator variables, dropping the first category, and displays the first 5 rows of the resulting DataFrame using the `get_dummies` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9990646}}, {"cell_id": 35, "code": "embarked = pd.get_dummies(data[\"Embarked\"], drop_first = True)\nembarked.head(5)", "class": "Data Transform", "desc": "The code converts the 'Embarked' column in the Titanic training dataset into dummy/indicator variables, dropping the first category, and displays the first 5 rows of the resulting DataFrame using the `get_dummies` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9991798}}, {"cell_id": 36, "code": "Pcl= pd.get_dummies(data[\"Pclass\"], drop_first = True)\nPcl.head(5)", "class": "Data Transform", "desc": "The code converts the 'Pclass' column in the Titanic training dataset into dummy/indicator variables, dropping the first category, and displays the first 5 rows of the resulting DataFrame using the `get_dummies` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99913234}}, {"cell_id": 39, "code": "data = pd.concat([data, sex, embarked, Pcl], axis = 1)\ndata.head()", "class": "Data Transform", "desc": "The code concatenates the original Titanic training dataset with the dummy variables for 'Sex', 'Embarked', and 'Pclass' along the column axis, and then displays the first few rows of the resulting DataFrame using pandas' `concat` and `head` methods.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9994272}}, {"cell_id": 41, "code": "data.drop([\"Sex\", \"Pclass\", \"PassengerId\", \"Embarked\", \"Name\"], axis =1, inplace = True)", "class": "Data Transform", "desc": "The code removes the 'Sex', 'Pclass', 'PassengerId', 'Embarked', and 'Name' columns from the Titanic training dataset in place using pandas' `drop` method with `axis=1` and `inplace=True` parameters.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991849}}, {"cell_id": 42, "code": "data.drop([\"Ticket\"], axis =1, inplace = True)", "class": "Data Transform", "desc": "The code removes the 'Ticket' column from the Titanic training dataset in place using pandas' `drop` method with `axis=1` and `inplace=True` parameters.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991635}}, {"cell_id": 44, "code": "data=data.fillna(data.mean())", "class": "Data Transform", "desc": "The code fills the null values in the Titanic training dataset with the mean of their respective columns using the `fillna` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.40510422}}, {"cell_id": 48, "code": "X = data.drop(\"Survived\", axis = 1)\ny = data[\"Survived\"]", "class": "Data Transform", "desc": "The code separates the features (X) from the target variable (y) in the Titanic training dataset by dropping the 'Survived' column for X and assigning the 'Survived' column to y using pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99935144}}, {"cell_id": 51, "code": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, test_size = 0.587)", "class": "Data Transform", "desc": "The code splits the features and target variable into training and testing sets, with a test size of 58.7% and a random seed of 1, using the `train_test_split` function from `sklearn.model_selection`.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9981351}}, {"cell_id": 62, "code": "test_data = test_data.drop(\"Cabin\", axis = 1)", "class": "Data Transform", "desc": "The code removes the 'Cabin' column from the Titanic test dataset using pandas' `drop` method with `axis=1`.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9992292}}, {"cell_id": 64, "code": "tsex = pd.get_dummies(test_data[\"Sex\"], drop_first = True)\ntembarked = pd.get_dummies(test_data[\"Embarked\"], drop_first = True)\ntPcl= pd.get_dummies(test_data[\"Pclass\"], drop_first = True)", "class": "Data Transform", "desc": "The code converts the 'Sex', 'Embarked', and 'Pclass' columns in the Titanic test dataset into dummy/indicator variables, dropping the first category for each, using the `get_dummies` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99921346}}, {"cell_id": 65, "code": "test_data = pd.concat([test_data, tsex, tembarked, tPcl], axis = 1)\ntest_data.head()", "class": "Data Transform", "desc": "The code concatenates the Titanic test dataset with the dummy variables for 'Sex', 'Embarked', and 'Pclass' along the column axis, and then displays the first few rows of the resulting DataFrame using pandas' `concat` and `head` methods.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.99946064}}, {"cell_id": 66, "code": "test_data.drop([\"Sex\", \"Pclass\", \"PassengerId\", \"Embarked\", \"Name\", \"Ticket\"], axis =1, inplace = True)", "class": "Data Transform", "desc": "The code removes the 'Sex', 'Pclass', 'PassengerId', 'Embarked', 'Name', and 'Ticket' columns from the Titanic test dataset in place using pandas' `drop` method with `axis=1` and `inplace=True` parameters.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9992144}}, {"cell_id": 69, "code": "test_data=test_data.fillna(data.mean())\ntest_data.isnull().sum()", "class": "Data Transform", "desc": "The code fills any remaining null values in the Titanic test dataset with the mean of their respective columns using the `fillna` method in pandas and then calculates and outputs the total number of null values in each column to confirm none remain, using the `isnull` and `sum` methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9830173}}, {"cell_id": 71, "code": "X_2 = data.drop(\"Survived\", axis = 1)\ny_2 = data[\"Survived\"]\nX_test_2 = test_data", "class": "Data Transform", "desc": "The code creates new feature sets `X_2` and `X_test_2` by dropping the 'Survived' column from the Titanic training dataset and assigns 'Survived' column as the target variable `y_2`, while using the entire modified test dataset as `X_test_2`.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.9993426}}, {"cell_id": 4, "code": "data.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the Titanic training dataset using the `head` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974936}}, {"cell_id": 5, "code": "data.shape", "class": "Exploratory Data Analysis", "desc": "The code outputs the dimensions (number of rows and columns) of the Titanic training dataset using the `shape` attribute in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995751}}, {"cell_id": 9, "code": "data.info()", "class": "Exploratory Data Analysis", "desc": "The code displays a concise summary of the Titanic training dataset, including the data types and non-null values for each column, using pandas' `info` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993351}}, {"cell_id": 10, "code": "classes = data.Pclass.unique()\nclasses.sort()\nlen(classes)", "class": "Exploratory Data Analysis", "desc": "The code extracts the unique values from the 'Pclass' column of the Titanic training dataset, sorts them, and then calculates the number of unique classes using the `unique`, `sort`, and `len` methods respectively.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.637069}}, {"cell_id": 11, "code": "classes", "class": "Exploratory Data Analysis", "desc": "The code outputs the sorted array of unique passenger classes from the 'Pclass' column of the Titanic training dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997011}}, {"cell_id": 12, "code": "data.describe()", "class": "Exploratory Data Analysis", "desc": "The code generates descriptive statistics of the Titanic training dataset, including measures such as mean, standard deviation, minimum, and maximum values for each numeric column, using pandas' `describe` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994498}}, {"cell_id": 13, "code": "print(\"Number of Passengers is equal to \" + str(len(data)))", "class": "Exploratory Data Analysis", "desc": "The code prints the number of passengers in the Titanic training dataset by converting the length of the DataFrame to a string and concatenating it with a descriptive message.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.69893026}}, {"cell_id": 14, "code": "data.groupby('Pclass').size()", "class": "Exploratory Data Analysis", "desc": "The code groups the Titanic training dataset by 'Pclass' and calculates the size of each group using pandas' `groupby` and `size` methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9654845}}, {"cell_id": 15, "code": "data.groupby('Sex').size()", "class": "Exploratory Data Analysis", "desc": "The code groups the Titanic training dataset by 'Sex' and calculates the size of each group using pandas' `groupby` and `size` methods.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.86233395}}, {"cell_id": 16, "code": "data.groupby('Survived').size()", "class": "Exploratory Data Analysis", "desc": "The code groups the Titanic training dataset by the 'Survived' column and calculates the size of each group using pandas' `groupby` and `size` methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.7214176}}, {"cell_id": 17, "code": "data.columns", "class": "Exploratory Data Analysis", "desc": "The code outputs the column labels of the Titanic training dataset using the `columns` attribute in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99858785}}, {"cell_id": 26, "code": "data.isnull()", "class": "Exploratory Data Analysis", "desc": "The code checks for null values in the Titanic training dataset and returns a DataFrame of the same shape indicating the presence of null values using pandas' `isnull` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9986425}}, {"cell_id": 27, "code": "data.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "The code calculates the total number of null values in each column of the Titanic training dataset using the `isnull` and `sum` methods in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99897987}}, {"cell_id": 30, "code": "data.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the Titanic training dataset using the `head` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974936}}, {"cell_id": 32, "code": "data.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the modified Titanic training dataset using the `head` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974936}}, {"cell_id": 33, "code": "len(data.columns)", "class": "Exploratory Data Analysis", "desc": "The code outputs the number of columns in the modified Titanic training dataset using the `len` function on the DataFrame's `columns` attribute.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9887581}}, {"cell_id": 37, "code": "data.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the modified Titanic training dataset using the `head` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974936}}, {"cell_id": 38, "code": "len(data.columns)", "class": "Exploratory Data Analysis", "desc": "The code outputs the number of columns in the modified Titanic training dataset using the `len` function on the DataFrame's `columns` attribute.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9887581}}, {"cell_id": 40, "code": "len(data.columns)", "class": "Exploratory Data Analysis", "desc": "The code outputs the number of columns in the transformed Titanic training dataset using the `len` function on the DataFrame's `columns` attribute.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9887581}}, {"cell_id": 43, "code": "len(data.columns)", "class": "Exploratory Data Analysis", "desc": "The code outputs the current number of columns in the transformed Titanic training dataset using the `len` function on the DataFrame's `columns` attribute.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9887581}}, {"cell_id": 46, "code": "data.head(5)", "class": "Exploratory Data Analysis", "desc": "The code displays the first five rows of the transformed Titanic training dataset using the `head` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997602}}, {"cell_id": 47, "code": "data.shape", "class": "Exploratory Data Analysis", "desc": "The code outputs the dimensions (number of rows and columns) of the transformed Titanic training dataset using the `shape` attribute in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995751}}, {"cell_id": 61, "code": "test_data.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "The code calculates and outputs the total number of null values in each column of the Titanic test dataset using the `isnull` and `sum` methods in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9989555}}, {"cell_id": 67, "code": "test_data.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the modified Titanic test dataset using the `head` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99975425}}, {"cell_id": 68, "code": "test_data.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "The code calculates and outputs the total number of null values in each column of the modified Titanic test dataset using the `isnull` and `sum` methods in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9989555}}, {"cell_id": 70, "code": "test_data.shape", "class": "Exploratory Data Analysis", "desc": "The code outputs the dimensions (number of rows and columns) of the modified Titanic test dataset using the `shape` attribute in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.99960977}}, {"cell_id": 75, "code": "sub.shape", "class": "Exploratory Data Analysis", "desc": "The code outputs the dimensions (number of rows and columns) of the `sub` DataFrame, which contains the contents of 'submission2.csv', using the `shape` attribute in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.99945813}}, {"cell_id": 76, "code": "sub.describe()", "class": "Exploratory Data Analysis", "desc": "The code generates descriptive statistics of the `sub` DataFrame, which includes measures such as mean, standard deviation, minimum, and maximum values for each numeric column, using pandas' `describe` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994567}}, {"cell_id": 77, "code": "sub.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the `sub` DataFrame using the `head` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974316}}, {"cell_id": 78, "code": "sub.info()", "class": "Exploratory Data Analysis", "desc": "The code displays a concise summary of the `sub` DataFrame, including the data types and non-null values for each column, using pandas' `info` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993374}}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "The code imports essential libraries such as NumPy for linear algebra, pandas for data processing, and uses the os module to list all files in a specified directory.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.99921954}}, {"cell_id": 1, "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport math\nfrom pandas.plotting import scatter_matrix", "class": "Imports and Environment", "desc": "The code imports several libraries including pandas for data manipulation, Matplotlib and Seaborn for visualization, NumPy for numerical operations, and additional functions including scatter_matrix from pandas.plotting. ", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99934846}}, {"cell_id": 49, "code": "from sklearn.linear_model import LogisticRegression", "class": "Imports and Environment", "desc": "The code imports the `LogisticRegression` class from the `sklearn.linear_model` module for building and training a logistic regression model.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99927217}}, {"cell_id": 50, "code": "from sklearn.model_selection import train_test_split", "class": "Imports and Environment", "desc": "The code imports the `train_test_split` function from the `sklearn.model_selection` module for splitting the dataset into training and testing sets.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99929905}}, {"cell_id": 55, "code": "from sklearn.metrics import classification_report", "class": "Imports and Environment", "desc": "The code imports the `classification_report` function from the `sklearn.metrics` module for evaluating the classification performance of the logistic regression model.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99928623}}, {"cell_id": 57, "code": "from sklearn.metrics import confusion_matrix", "class": "Imports and Environment", "desc": "The code imports the `confusion_matrix` function from the `sklearn.metrics` module for generating a confusion matrix to evaluate the performance of the logistic regression model.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.9992841}}, {"cell_id": 59, "code": "from sklearn.metrics import accuracy_score", "class": "Imports and Environment", "desc": "The code imports the `accuracy_score` function from the `sklearn.metrics` module for calculating the accuracy of the logistic regression model's predictions.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99928015}}, {"cell_id": 54, "code": "preds = model.predict(X_test)", "class": "Model Evaluation", "desc": "The code generates predictions on the test dataset (X_test) using the trained logistic regression model with the `predict` method.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9938067}}, {"cell_id": 56, "code": "classification_report(y_test,preds)", "class": "Model Evaluation", "desc": "The code generates a classification report that includes precision, recall, f1-score, and support for the predicted and actual values (y_test and preds) using the `classification_report` function from `sklearn.metrics`.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.997997}}, {"cell_id": 58, "code": "confusion_matrix(y_test, preds)", "class": "Model Evaluation", "desc": "The code generates a confusion matrix to evaluate the performance of the logistic regression model by comparing the actual values (y_test) with the predicted values (preds) using the `confusion_matrix` function from `sklearn.metrics`.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9977126}}, {"cell_id": 60, "code": "accuracy_score(y_test, preds)", "class": "Model Evaluation", "desc": "The code calculates the accuracy of the logistic regression model's predictions by comparing the true labels (y_test) with the predicted labels (preds) using the `accuracy_score` function from `sklearn.metrics`.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9978477}}, {"cell_id": 52, "code": "model = LogisticRegression(solver='liblinear')\nmodel_2 = LogisticRegression(solver='liblinear')", "class": "Model Training", "desc": "The code initializes two logistic regression models named `model` and `model_2` using the `LogisticRegression` class from `sklearn.linear_model` with the solver set to 'liblinear'.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.9973991}}, {"cell_id": 53, "code": "model.fit(X_train, y_train)", "class": "Model Training", "desc": "The code trains the `model` logistic regression on the training dataset (X_train, y_train) using the `fit` method from `sklearn.linear_model.LogisticRegression`.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9997054}}, {"cell_id": 72, "code": "model_2.fit(X_2, y_2)\npreds2 = model_2.predict(X_test_2)", "class": "Model Training", "desc": "The code trains the second logistic regression model (`model_2`) on the feature set `X_2` and target variable `y_2`, then generates predictions for the test dataset (`X_test_2`) using the `fit` and `predict` methods from `sklearn.linear_model.LogisticRegression`.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9029117}}, {"cell_id": 19, "code": "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\nplt.show()", "class": "Visualization", "desc": "The code generates a series of box plots for the columns in the `dataset` DataFrame, arranged in a 2x2 layout, using the `plot` method in pandas with Matplotlib for rendering.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9939215}}, {"cell_id": 20, "code": "scatter_matrix(dataset)\nplt.show()", "class": "Visualization", "desc": "The code creates a scatter plot matrix for the columns in the `dataset` DataFrame using the `scatter_matrix` function from `pandas.plotting` and displays it using Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9048239}}, {"cell_id": 21, "code": "sns.set_theme(style=\"darkgrid\")\nsns.countplot(x= \"Survived\", hue=\"Sex\", order=[1, 0], data = data).set(xticklabels=[\"Survived\", \"Not Survived\"])\nplt.xlabel(\"Survivors vs Non-Survivors Comparison\")\nplt.ylabel(\"Number of Passengers\")\nplt.title(\"Comparison\")\nplt.legend(labels=[\"Male\", \"Female\"])\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to compare the number of survivors versus non-survivors, with a hue based on the 'Sex' column, and customizes the plot with labels and a title before displaying it with Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.6843377}}, {"cell_id": 22, "code": "sns.set_theme(style=\"whitegrid\")\nsns.countplot(x= \"Survived\", hue=\"Pclass\", data = data).set(xticklabels=[\"Survived\", \"Not Survived\"])\nplt.xlabel(\"Survivors vs Non-Survivors Comparison\")\nplt.ylabel(\"Number of Passengers\")\nplt.title(\"Comparison\")\nplt.legend(title='Passenger Class', loc='upper right', labels=[\"First Class\", \"Second Class\", \"Third Class\"])\nplt.show()", "class": "Visualization", "desc": "The code uses Seaborn to create a count plot comparing the number of survivors versus non-survivors, with a hue based on the 'Pclass' column, and customizes the plot with labels, a title, and a legend before displaying it using Matplotlib.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.9569828}}, {"cell_id": 23, "code": "dataset.hist(figsize=(15,7.5))\nplt.show()", "class": "Visualization", "desc": "The code generates histograms for all columns in the `dataset` DataFrame with a specified figure size using the `hist` method in pandas and displays them using Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.98764926}}, {"cell_id": 24, "code": "data[\"Age\"].hist(figsize=(10,5))\nplt.show()", "class": "Visualization", "desc": "The code creates a histogram for the 'Age' column in the Titanic training dataset with a specified figure size using the `hist` method in pandas and displays it using Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9860421}}, {"cell_id": 25, "code": "data[\"Fare\"].plot.hist(figsize=(10,5), bins = 40)\nplt.show()", "class": "Visualization", "desc": "The code generates a histogram for the 'Fare' column in the Titanic training dataset with a specified figure size and 40 bins using the `plot.hist` method in pandas, and displays it using Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9954177}}, {"cell_id": 28, "code": "sns.heatmap(data.isnull(), yticklabels = False, cmap = \"plasma\")\nplt.show()", "class": "Visualization", "desc": "The code creates a heatmap to visualize the presence of null values in the Titanic training dataset using Seaborn's `heatmap` function with a 'plasma' colormap and hides y-axis tick labels, then displays it using Matplotlib.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.99902153}}, {"cell_id": 29, "code": "sns.set_theme(style=\"darkgrid\")\nsns.boxplot(x= \"Pclass\", y=\"Age\", data = data)\nplt.show()", "class": "Visualization", "desc": "The code creates a box plot to visualize the distribution of ages across different passenger classes ('Pclass') in the Titanic training dataset using Seaborn's `boxplot` function and displays it using Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99564457}}, {"cell_id": 45, "code": "sns.heatmap(data.isnull(), yticklabels = False, cmap = \"plasma\")\nplt.show()", "class": "Visualization", "desc": "The code creates a heatmap to visualize the presence of any remaining null values in the transformed Titanic training dataset using Seaborn's `heatmap` function with a 'plasma' colormap and hides y-axis tick labels, then displays it using Matplotlib.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.99902153}}, {"cell_id": 63, "code": "sns.heatmap(test_data.isnull(), yticklabels = False, cmap = \"plasma\")\nplt.show()", "class": "Visualization", "desc": "The code creates a heatmap to visualize the presence of null values in the Titanic test dataset using Seaborn's `heatmap` function with a 'plasma' colormap and hides y-axis tick labels, then displays it using Matplotlib.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9988809}}], "notebook_id": 8, "notebook_name": "logistic-regression-on-titanic-dataset-eda.ipynb"}, {"cells": [{"cell_id": 23, "code": "submit = pd.DataFrame({'PassengerId':test_df.index ,\n                      'Survived':preds})\nsubmit.to_csv('submission.csv', index=False)", "class": "Data Export", "desc": "This code creates a DataFrame with the test set's PassengerId and the predicted 'Survived' values, then exports this DataFrame to a CSV file named 'submission.csv' without including the index.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992084}}, {"cell_id": 1, "code": "#!kaggle competitions download -c titanic", "class": "Data Extraction", "desc": "The code uses a Kaggle command to download the Titanic dataset from the Kaggle competitions platform.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.8981032}}, {"cell_id": 2, "code": "# with zipfile.ZipFile('titanic.zip' , 'r') as file:\n#     file.extractall('data')", "class": "Data Extraction", "desc": "The code snippet extracts the contents of the 'titanic.zip' file into the 'data' directory using the `zipfile` module in Python.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.9961624}}, {"cell_id": 3, "code": "train_df = pd.read_csv('../input/titanic/train.csv' , index_col='PassengerId')\ntest_df = pd.read_csv('../input/titanic/test.csv' , index_col='PassengerId')\nsample= pd.read_csv('../input/titanic/gender_submission.csv')", "class": "Data Extraction", "desc": "This code loads the Titanic dataset's train, test, and sample submission CSV files into pandas DataFrames with the 'PassengerId' column set as the index.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997558}}, {"cell_id": 8, "code": "inputs = train_df.drop('Survived' ,axis=1)\ntargets = train_df.Survived", "class": "Data Transform", "desc": "The code snippet separates the feature columns (inputs) from the target column ('Survived') in the `train_df` DataFrame by dropping 'Survived' from `inputs` and assigning it to `targets`.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.47213688}}, {"cell_id": 9, "code": "num_cols = inputs.select_dtypes(include = ['int', 'float']).columns", "class": "Data Transform", "desc": "This code selects and stores the column names of numerical data types (integers and floats) from the `inputs` DataFrame in the `num_cols` variable.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9732161}}, {"cell_id": 18, "code": "train_inputs , val_inputs , train_targets ,val_targets = train_test_split(inputs[num_cols].fillna(method='ffill') , targets , test_size=0.1 , random_state=1)", "class": "Data Transform", "desc": "This code splits the numerical columns of the `inputs` DataFrame (with forward-filled missing values) and `targets` into training and validation sets using the `train_test_split` function from `sklearn` with a test size of 10% and a random state of 1.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9977585}}, {"cell_id": 4, "code": "train_df", "class": "Exploratory Data Analysis", "desc": "This code displays the contents of the `train_df` DataFrame, which contains the training data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99972683}}, {"cell_id": 5, "code": "train_df.shape , test_df.shape", "class": "Exploratory Data Analysis", "desc": "This code outputs the dimensions (number of rows and columns) of the `train_df` and `test_df` DataFrames.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.99963737}}, {"cell_id": 6, "code": "train_df.describe()", "class": "Exploratory Data Analysis", "desc": "This code provides summary statistics for the numerical columns in the `train_df` DataFrame using the `describe` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994492}}, {"cell_id": 7, "code": "train_df.info()", "class": "Exploratory Data Analysis", "desc": "This code displays a concise summary of the `train_df` DataFrame, including the number of non-null entries and the data types of each column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993624}}, {"cell_id": 10, "code": "num_cols", "class": "Exploratory Data Analysis", "desc": "This code displays the names of the columns in the `inputs` DataFrame that contain numerical data types.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99922514}}, {"cell_id": 11, "code": "inputs.Age.value_counts()", "class": "Exploratory Data Analysis", "desc": "This code provides the count of each unique value in the 'Age' column of the `inputs` DataFrame using the `value_counts` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.999488}}, {"cell_id": 13, "code": "inputs.corr()", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the correlation matrix for the numerical columns in the `inputs` DataFrame using the `corr` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993327}}, {"cell_id": 15, "code": "inputs[num_cols].info()", "class": "Exploratory Data Analysis", "desc": "This code displays a concise summary of the numerical columns in the `inputs` DataFrame, including the number of non-null entries and data types.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99931693}}, {"cell_id": 16, "code": "targets.shape", "class": "Exploratory Data Analysis", "desc": "This code provides the dimensions (number of rows and columns) of the `targets` Series.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.99957496}}, {"cell_id": 19, "code": "train_inputs.shape , train_targets.shape", "class": "Exploratory Data Analysis", "desc": "This code returns the dimensions (number of rows and columns) of the `train_inputs` and `train_targets` after the train-test split.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995982}}, {"cell_id": 21, "code": "(targets.value_counts()/targets.size)*100", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the percentage distribution of each unique value in the `targets` Series.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99953544}}, {"cell_id": 0, "code": "import zipfile\nimport os\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline", "class": "Imports and Environment", "desc": "The code imports and initializes essential libraries and tools such as `zipfile`, `os`, `pandas`, `numpy`, `matplotlib.pyplot`, and `seaborn` for data manipulation, analysis, and visualization.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.99939144}}, {"cell_id": 17, "code": "# ML imports \nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression", "class": "Imports and Environment", "desc": "This code imports essential machine learning modules and functions from the `sklearn` library, specifically `train_test_split` for splitting data and `LogisticRegression` for building logistic regression models.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99933535}}, {"cell_id": 22, "code": "preds = logreg.predict(test_df[num_cols].fillna(method='ffill'))", "class": "Model Evaluation", "desc": "This code generates predictions on the test dataset's numerical columns (with forward-filled missing values) using the previously trained logistic regression model.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99413484}}, {"cell_id": 20, "code": "logreg = LogisticRegression(random_state=1)\nlogreg.fit(train_inputs, train_targets)\nacc_log = round(logreg.score(val_inputs, val_targets) * 100, 2)\nprint(acc_log)", "class": "Model Training", "desc": "This code creates and fits a logistic regression model using the training data, then calculates and prints the accuracy of the model on the validation data rounded to two decimal places.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.99731016}}, {"cell_id": 12, "code": "plt.figure(figsize=(12 , 12))\nsns.countplot(y='Age',data=inputs)", "class": "Visualization", "desc": "This code snippet creates a count plot of the 'Age' column in the `inputs` DataFrame using Seaborn's `countplot` function, and sets the figure size to 12 by 12 inches with Matplotlib.", "testing": {"class": "Visualization", "subclass": "time_series", "subclass_id": 75, "predicted_subclass_probability": 0.9773536}}, {"cell_id": 14, "code": "plt.figure(figsize=(10,10))\nsns.heatmap(train_df.corr())", "class": "Visualization", "desc": "This code generates a heatmap to visualize the correlation matrix of the `train_df` DataFrame using Seaborn's `heatmap` function, with the figure size set to 10 by 10 inches via Matplotlib.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9980508}}], "notebook_id": 9, "notebook_name": "base-logistic-regression-model-with-numerical-col.ipynb"}, {"cells": [{"cell_id": 37, "code": "output = pd.DataFrame({'PassengerId':test_df.PassengerId, 'Survived':logistic_pred})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Data Export", "desc": "This code snippet creates a DataFrame with 'PassengerId' and predicted 'Survived' values, and exports it to a CSV file named 'submission.csv' without the index, followed by a confirmation print statement indicating the file was saved successfully.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99903464}}, {"cell_id": 1, "code": "train_df = pd.read_csv('../input/titanic/train.csv')\ntest_df = pd.read_csv('../input/titanic/test.csv')", "class": "Data Extraction", "desc": "This code snippet reads the training and test datasets from CSV files located in the specified directory into pandas DataFrames.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99975973}}, {"cell_id": 35, "code": "test_df[features]", "class": "Data Extraction", "desc": "This code snippet extracts and displays the specified feature columns from the test DataFrame using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99951756}}, {"cell_id": 17, "code": "from sklearn.preprocessing import LabelEncoder\n\nclass FE:\n    def __init__(self, dataframes):\n        self.dataframes = dataframes\n        \n    def remove_cols(self):\n        for df in self.dataframes:\n            try:\n                df.drop(['Cabin', 'Name', 'Ticket'], axis=1, inplace=True)\n            except:\n                pass\n        \n    def convert_sex(self):\n        for df in self.dataframes:\n            le = LabelEncoder()\n            df.Sex = le.fit_transform(df.Sex)\n            ", "class": "Data Transform", "desc": "This code snippet defines a class `FE` to handle feature engineering tasks such as removing specific columns and converting the 'Sex' column into numerical values using Label Encoding with scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9992322}}, {"cell_id": 18, "code": "fe = FE([train_df, test_df])\nfe.remove_cols()\nfe.convert_sex()", "class": "Data Transform", "desc": "This code snippet instantiates the `FE` class with the training and test DataFrames and applies the defined methods to remove specific columns and encode the 'Sex' column into numerical values.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.6193043}}, {"cell_id": 20, "code": "features = ['Pclass', 'Sex', 'SibSp', 'Parch']\nX = train_df[features]\ny = train_df['Survived']", "class": "Data Transform", "desc": "This code snippet selects 'Pclass', 'Sex', 'SibSp', and 'Parch' columns as features (X) and 'Survived' column as the target (y) from the training DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.999345}}, {"cell_id": 21, "code": "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=2)", "class": "Data Transform", "desc": "This code snippet splits the features and target data into training and testing sets with 25% of the data reserved for testing, using the `train_test_split` function from scikit-learn and setting a random state for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.99821234}}, {"cell_id": 2, "code": "print(train_df.shape, test_df.shape)", "class": "Exploratory Data Analysis", "desc": "This code snippet prints the dimensions (number of rows and columns) of the training and test datasets using the `.shape` attribute of the pandas DataFrames.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9990921}}, {"cell_id": 3, "code": "train_df.info()", "class": "Exploratory Data Analysis", "desc": "This code snippet provides a concise summary of the training DataFrame, including the data types and non-null counts for each column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993624}}, {"cell_id": 4, "code": "train_df.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates descriptive statistics for the numeric columns in the training DataFrame using the `.describe()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994492}}, {"cell_id": 5, "code": "train_df.isna().sum()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and displays the total number of missing values in each column of the training DataFrame using the `.isna().sum()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9990694}}, {"cell_id": 6, "code": "train_df.duplicated().sum()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and displays the number of duplicate rows in the training DataFrame using the `.duplicated().sum()` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_duplicates", "subclass_id": 38, "predicted_subclass_probability": 0.8890608}}, {"cell_id": 12, "code": "sexes = train_df.Sex.unique()\ncount = list()\n\nfor sex in sexes:\n    percentage = round(len(train_df[(train_df.Sex == sex) & (train_df.Survived == 1)]) / len(train_df[train_df.Sex == sex]), 3) * 100\n    count.append(percentage)\n    \nprint(count) ", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates the percentage of survivors for each unique value in the 'Sex' column and prints the computed percentages.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.86492133}}, {"cell_id": 14, "code": "p_classes = train_df.Pclass.unique()\npcts = list()\n\nfor p in p_classes:\n    percentage = round(len(train_df[(train_df.Pclass == p) & (train_df.Survived == 1)]) / len(train_df[train_df.Pclass == p]),3) * 100\n    pcts.append(percentage)\n    \nprint(pcts) ", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates the percentage of survivors for each unique value in the 'Pclass' column and prints the computed percentages.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.8156362}}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This code snippet imports various packages like NumPy, pandas, Matplotlib, Seaborn, and missingno for data manipulation, visualization, and handling missing values, and initializes environment-specific settings like filtering warnings and listing input data files using os.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.9989272}}, {"cell_id": 19, "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report", "class": "Imports and Environment", "desc": "This code snippet imports train-test splitting functions, cross-validation functions, and evaluation metrics like accuracy score and classification report from scikit-learn.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99933124}}, {"cell_id": 28, "code": "df_score = pd.DataFrame({'model':['Logistic Regression', 'KNN', 'Random Forest Classifier', 'Decision Tree Classifier', 'Support Vector Machine', 'Naive Bayes'],\n                        'accuracy':[lr_acc, knn_acc, rfc_acc, dtc_acc, clf_acc, gnb_acc]})", "class": "Model Evaluation", "desc": "This code snippet creates a pandas DataFrame named `df_score` that contains the names of the models and their corresponding accuracy scores collected from previous predictions.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.9986626}}, {"cell_id": 29, "code": "df_score.sort_values('accuracy', ascending=False)", "class": "Model Evaluation", "desc": "This code snippet sorts the `df_score` DataFrame in descending order based on the 'accuracy' column to rank the models by their accuracy performance.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9950781}}, {"cell_id": 30, "code": "df_score.accuracy.mean()", "class": "Model Evaluation", "desc": "This code snippet calculates and returns the mean accuracy of all the models listed in the `df_score` DataFrame using the `mean()` method on the 'accuracy' column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.6630979}}, {"cell_id": 31, "code": "model_names = ['Logistic Regression', 'KNN', 'Random Forest Classifier', 'Decision Tree Classifier', 'Support Vector Machine', 'Naive Bayes']\nmodel_selectors = [lr, knn, rfc, dtc, clf, gnb]\nscores_dict = {'model':[], 'accuracy':[]}\n\nfor label, selector in zip(model_names, model_selectors):\n    cv_mean_score = np.mean(cross_val_score(selector, X, y, cv=5))\n    scores_dict['model'].append(label)\n    scores_dict['accuracy'].append(cv_mean_score)    \n    \ndf_cv_score = pd.DataFrame(scores_dict)", "class": "Model Evaluation", "desc": "This code snippet performs 5-fold cross-validation for each model, calculates the mean cross-validation accuracy score, and stores the results in a DataFrame named `df_cv_score`.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.6588713}}, {"cell_id": 32, "code": "df_cv_score.sort_values('accuracy', ascending=False)", "class": "Model Evaluation", "desc": "This code snippet sorts the `df_cv_score` DataFrame in descending order based on the 'accuracy' column to rank the models by their cross-validated accuracy performance.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9968382}}, {"cell_id": 33, "code": "df_cv_score.accuracy.mean()", "class": "Model Evaluation", "desc": "This code snippet calculates and returns the mean cross-validated accuracy of all the models listed in the `df_cv_score` DataFrame using the `mean()` method on the 'accuracy' column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.6315649}}, {"cell_id": 22, "code": "from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(train_x, train_y)\n\nlr_pred = lr.predict(test_x)\n\nlr_acc = accuracy_score(lr_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Logistic Regression model using the `LogisticRegression` class from scikit-learn on the training data, predicts the target variable for the test data, and calculates the accuracy score of the predictions.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.78042775}}, {"cell_id": 23, "code": "from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=12)\nknn.fit(train_x, train_y)\n\nknn_pred = knn.predict(test_x)\n\nknn_acc = accuracy_score(knn_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a K-Nearest Neighbors (KNN) classifier with 12 neighbors using the `KNeighborsClassifier` class from scikit-learn on the training data, predicts the target variable for the test data, and calculates the accuracy score of the predictions.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.7452794}}, {"cell_id": 24, "code": "from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(train_x, train_y)\n\nrfc_pred = rfc.predict(test_x)\n\nrfc_acc = accuracy_score(rfc_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Random Forest classifier with 100 estimators using the `RandomForestClassifier` class from scikit-learn on the training data, predicts the target variable for the test data, and calculates the accuracy score of the predictions.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.8482741}}, {"cell_id": 25, "code": "from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(train_x, train_y)\n\ndtc_pred = dtc.predict(test_x)\n\ndtc_acc = accuracy_score(dtc_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Decision Tree classifier using the `DecisionTreeClassifier` class from scikit-learn on the training data, predicts the target variable for the test data, and calculates the accuracy score of the predictions.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.6479414}}, {"cell_id": 26, "code": "from sklearn.svm import SVC\n\nclf = SVC(kernel='linear')\nclf.fit(train_x, train_y)\n\nclf_pred = clf.predict(test_x)\n\nclf_acc = accuracy_score(clf_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Support Vector Classifier (SVC) with a linear kernel using the `SVC` class from scikit-learn on the training data, predicts the target variable for the test data, and calculates the accuracy score of the predictions.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.64344877}}, {"cell_id": 27, "code": "from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(train_x, train_y)\n\ngnb_pred = gnb.predict(test_x)\n\ngnb_acc = accuracy_score(gnb_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Gaussian Naive Bayes classifier using the `GaussianNB` class from scikit-learn on the training data, predicts the target variable for the test data, and calculates the accuracy score of the predictions.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.87931037}}, {"cell_id": 34, "code": "from sklearn.model_selection import GridSearchCV\n\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\nlogistic = GridSearchCV(estimator=model, param_grid={'solver':solvers, 'penalty':penalty, 'C':c_values}, cv=5)\nlogistic.fit(train_x, train_y)\n\ngrid_info = pd.DataFrame(logistic.cv_results_)\ngrid_info[['mean_test_score', 'param_solver', 'param_penalty', 'param_C']].sort_values('mean_test_score', ascending=False)", "class": "Model Training", "desc": "This code snippet uses GridSearchCV from scikit-learn to perform hyperparameter tuning on a Logistic Regression model, searching over specified solvers, penalty, and regularization values, and fits the model using 5-fold cross-validation before creating a DataFrame to display the mean test scores and corresponding hyperparameters.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.98615915}}, {"cell_id": 36, "code": "logistic_pred = logistic.predict(test_df[features])\nlogistic_pred", "class": "Model Training", "desc": "This code snippet uses the fitted Logistic Regression model from the grid search to predict the target variable for the test DataFrame based on the specified features.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9945187}}, {"cell_id": 7, "code": "msno.matrix(train_df)", "class": "Visualization", "desc": "This code snippet visualizes the missing values in the training DataFrame using the `msno.matrix()` function from the missingno library.", "testing": {"class": "Visualization", "subclass": "missing_values", "subclass_id": 34, "predicted_subclass_probability": 0.9554358}}, {"cell_id": 8, "code": "corr = train_df.corr()\nplt.figure(figsize=(15,8))\nsns.heatmap(corr, annot=True)", "class": "Visualization", "desc": "This code snippet calculates the correlation matrix of the training DataFrame using the `.corr()` method from pandas and visualizes it as a heatmap using Seaborn's `heatmap()` function and Matplotlib's `figure()` for size adjustment.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.99775463}}, {"cell_id": 9, "code": "for col in ['Age', 'SibSp', 'Parch', 'Fare']:\n    plt.title(col)\n    sns.boxplot(train_df[col])\n    plt.show()", "class": "Visualization", "desc": "This code snippet creates box plots for the 'Age', 'SibSp', 'Parch', and 'Fare' columns of the training DataFrame using Seaborn's `boxplot()` function and displays them with Matplotlib's `show()` function.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9927158}}, {"cell_id": 10, "code": "fig, axes = plt.subplots(3,3, figsize=(20,15))\n\nsns.countplot(ax=axes[0,0], data=train_df, x='Survived')\nsns.countplot(ax=axes[0,1], data=train_df, x='Sex')\nsns.countplot(ax=axes[0,2], data=train_df, x='Pclass')\nsns.histplot(ax=axes[1,0], data=train_df, x='Age')\nsns.histplot(ax=axes[1,1], data=train_df, x='Fare')\nsns.countplot(ax=axes[1,2], data=train_df, x='SibSp')\nsns.countplot(ax=axes[2,0], data=train_df, x='Parch')\nsns.countplot(ax=axes[2,1], data=train_df, x='Embarked')\n\nplt.tight_layout()\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a grid of subplots to display count plots and histograms for various columns ('Survived', 'Sex', 'Pclass', 'Age', 'Fare', 'SibSp', 'Parch', and 'Embarked') from the training DataFrame using Seaborn and Matplotlib for visualization.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9985378}}, {"cell_id": 11, "code": "sns.relplot(data=train_df, x='Age', y='Fare', hue='Sex', col='Survived') ", "class": "Visualization", "desc": "This code snippet creates a relational plot to visualize the relationship between 'Age' and 'Fare', colored by 'Sex' and faceted by 'Survived', using Seaborn's `relplot()` function.", "testing": {"class": "Visualization", "subclass": "time_series", "subclass_id": 75, "predicted_subclass_probability": 0.96493864}}, {"cell_id": 13, "code": "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(17,6))\nfig.suptitle('Survival Rates Between Males and Females\\n', fontsize=20)\n\nsns.countplot(ax=ax1, data=train_df, x='Sex', hue='Survived', edgecolor='black')\nax1.set_xticklabels(['Male','Female'])\nax1.legend(labels=['False', 'True'], title='Survived\\n')\nax1.set_xlabel('')\nax1.set_ylabel('Count')\n\nplt.pie(count, labels=['Male', 'Female'], explode=[0, 0.2], shadow=True, autopct='%1.1f%%', wedgeprops={'edgecolor':'black'})\n\nplt.tight_layout()", "class": "Visualization", "desc": "This code snippet creates a figure with two subplots to compare survival rates between males and females, using a count plot with Seaborn and a pie chart with Matplotlib, and includes a title for the figure.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99289554}}, {"cell_id": 15, "code": "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(17,6))\nfig.suptitle('Survival Rates in Different Ship Classes\\n', fontsize=20)\n\nsns.countplot(ax=ax1, data=train_df, x='Pclass', hue='Survived', edgecolor='black')\nax1.set_xticklabels(['First','Second', 'Third'])\nax1.legend(labels=['False', 'True'], title='Survived\\n')\nax1.set_xlabel('')\nax1.set_ylabel('Count')\n\nplt.pie(pcts, labels=['Third', 'First', 'Second'], explode=[0, 0., 0.2], shadow=True, autopct='%1.1f%%', startangle=90, wedgeprops={'edgecolor':'black'})\n\nplt.tight_layout()", "class": "Visualization", "desc": "This code snippet creates a figure with two subplots to compare survival rates among different ship classes, using a count plot with Seaborn and a pie chart with Matplotlib, and includes a title for the figure.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.98514646}}, {"cell_id": 16, "code": "plt.figure(figsize=(15,7))\nsns.histplot(data=train_df, x='Age', hue='Survived', multiple='stack')\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a stacked histogram to visualize the distribution of ages between survivors and non-survivors using Seaborn's `histplot()` function and adjusts the figure size with Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99855167}}], "notebook_id": 10, "notebook_name": "titanic-prediction.ipynb"}, {"cells": [{"cell_id": 33, "code": "submission.to_csv('submission.csv', index=False)", "class": "Data Export", "desc": "This code exports the `submission` DataFrame to a CSV file named 'submission.csv' without including the DataFrame index, using pandas' `to_csv` method.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992442}}, {"cell_id": 1, "code": "df = pd.read_csv('../input/titanic/train.csv')\ntest_df = pd.read_csv('../input/titanic/test.csv')", "class": "Data Extraction", "desc": "This code loads the Titanic dataset by reading train and test CSV files from the specified directory using pandas' `read_csv` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99975675}}, {"cell_id": 30, "code": "submission = pd.read_csv('../input/titanic/gender_submission.csv')\nsubmission # Remember that this example assumes that all women survived, and all men died.", "class": "Data Extraction", "desc": "This code loads a submission DataFrame by reading the 'gender_submission.csv' file from the specified directory using pandas' `read_csv` method and then outputs it to display its contents.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997315}}, {"cell_id": 6, "code": "complete_df = pd.concat([df, test_df])", "class": "Data Transform", "desc": "This code concatenates the training and test DataFrames (`df` and `test_df`) into a single DataFrame `complete_df` using pandas' `concat` method.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9995359}}, {"cell_id": 10, "code": "complete_df['Embarked'] = complete_df['Embarked'].fillna('C')", "class": "Data Transform", "desc": "This code fills missing values in the 'Embarked' column of the `complete_df` DataFrame with the value 'C' using pandas' `fillna` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.6962351}}, {"cell_id": 12, "code": "complete_df['Fare'] = complete_df.groupby('Pclass')['Fare'].transform(lambda val: val.fillna(val.median()))", "class": "Data Transform", "desc": "This code fills missing values in the 'Fare' column of the `complete_df` DataFrame with the median fare of the corresponding passenger class (`Pclass`) using pandas' `groupby`, `transform`, and `fillna` methods.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.3744527}}, {"cell_id": 13, "code": "complete_df.loc[complete_df['Sex']=='female','Age'] = complete_df[complete_df['Sex']=='female']['Age'].transform(lambda val: val.fillna(val.median()))\ncomplete_df.loc[complete_df['Sex']=='male', 'Age'] = complete_df[ complete_df['Sex']=='male' ]['Age'].transform(lambda val: val.fillna(val.median()))", "class": "Data Transform", "desc": "This code fills missing values in the 'Age' column of the `complete_df` DataFrame with the median age of the corresponding gender (`Sex`) using pandas' `loc`, `transform`, and `fillna` methods.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.51858294}}, {"cell_id": 15, "code": "X = complete_df[:891].drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'] ,axis=1)\nX", "class": "Data Transform", "desc": "This code creates a new DataFrame `X` containing the first 891 rows of `complete_df` and drops the columns 'PassengerId', 'Survived', 'Name', 'Ticket', and 'Cabin' using pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9975986}}, {"cell_id": 16, "code": "X = pd.get_dummies(X)\nX", "class": "Data Transform", "desc": "This code converts categorical variables in the DataFrame `X` into dummy/indicator variables using pandas' `get_dummies` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9991831}}, {"cell_id": 17, "code": "y = complete_df[:891]['Survived']", "class": "Data Transform", "desc": "This code creates a new Series `y` containing the 'Survived' column from the first 891 rows of `complete_df` using pandas' column indexing.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99917525}}, {"cell_id": 19, "code": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)", "class": "Data Transform", "desc": "This code splits the DataFrame `X` and Series `y` into training and testing sets using scikit-learn's `train_test_split` function with 33% of the data allocated for testing and a random state of 42.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.99822325}}, {"cell_id": 27, "code": "X_final = complete_df[891:].drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'] ,axis=1)\nX_final = pd.get_dummies(X_final)", "class": "Data Transform", "desc": "This code creates a new DataFrame `X_final` by extracting and transforming the rows after the initial 891 rows from `complete_df`, dropping specific columns, and converting categorical variables into dummy/indicator variables using pandas' `drop` and `get_dummies` methods.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9992072}}, {"cell_id": 31, "code": "submission['Survived'] = final_preds\nsubmission", "class": "Data Transform", "desc": "This code assigns the predicted survival labels (`final_preds`) to the 'Survived' column of the `submission` DataFrame and then displays its contents.", "testing": {"class": "Data_Export", "subclass": "prepare_output", "subclass_id": 55, "predicted_subclass_probability": 0.9914136}}, {"cell_id": 32, "code": "submission['Survived'] = submission['Survived'].astype(int)\nsubmission", "class": "Data Transform", "desc": "This code converts the values in the 'Survived' column of the `submission` DataFrame to integers using pandas' `astype` method and then displays its contents.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.98706263}}, {"cell_id": 2, "code": "df", "class": "Exploratory Data Analysis", "desc": "This code outputs the entire DataFrame `df` to display its contents.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9990208}}, {"cell_id": 3, "code": "test_df", "class": "Exploratory Data Analysis", "desc": "This code outputs the entire DataFrame `test_df` to display its contents.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99976057}}, {"cell_id": 4, "code": "df['Sex'].value_counts()", "class": "Exploratory Data Analysis", "desc": "This code counts the occurrences of unique values in the 'Sex' column of the `df` DataFrame using pandas' `value_counts` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99951005}}, {"cell_id": 7, "code": "complete_df.isnull().sum() ", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the total number of missing values for each column in the `complete_df` DataFrame using pandas' `isnull` and `sum` methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99892384}}, {"cell_id": 8, "code": "complete_df[complete_df['Embarked'].isnull()]", "class": "Exploratory Data Analysis", "desc": "This code filters and displays rows in the `complete_df` DataFrame where the 'Embarked' column has missing values using pandas' `isnull` method.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.7799185}}, {"cell_id": 11, "code": "complete_df[complete_df['Fare'].isnull()]", "class": "Exploratory Data Analysis", "desc": "This code filters and displays rows in the `complete_df` DataFrame where the 'Fare' column has missing values using pandas' `isnull` method.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.7051845}}, {"cell_id": 14, "code": "complete_df.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the total number of missing values for each column in the `complete_df` DataFrame using pandas' `isnull` and `sum` methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9989127}}, {"cell_id": 28, "code": "X_final", "class": "Exploratory Data Analysis", "desc": "This code outputs the `X_final` DataFrame to display its contents.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99975556}}, {"cell_id": 0, "code": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))", "class": "Imports and Environment", "desc": "This code imports necessary libraries such as NumPy, pandas, Matplotlib, and Seaborn, and lists files in the '/kaggle/input' directory using the 'os' module.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.9993911}}, {"cell_id": 18, "code": "from sklearn.model_selection import train_test_split", "class": "Imports and Environment", "desc": "This code imports the `train_test_split` function from the scikit-learn library for splitting data into training and testing sets.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99929905}}, {"cell_id": 20, "code": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV", "class": "Imports and Environment", "desc": "This code imports `RandomForestClassifier` from scikit-learn's ensemble module and `GridSearchCV` from scikit-learn's model_selection module for model training and hyperparameter tuning, respectively.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99927956}}, {"cell_id": 24, "code": "from sklearn.metrics import accuracy_score, classification_report", "class": "Imports and Environment", "desc": "This code imports the `accuracy_score` and `classification_report` metrics from scikit-learn to evaluate the performance of the model.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99931264}}, {"cell_id": 23, "code": "grid.best_params_", "class": "Model Evaluation", "desc": "This code retrieves and displays the best hyperparameters found by the GridSearchCV for the RandomForestClassifier.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.6292515}}, {"cell_id": 25, "code": "test_predictions = grid.predict(X_test)\nprint(accuracy_score(y_test, test_predictions))", "class": "Model Evaluation", "desc": "This code uses the best-estimator from GridSearchCV to predict the test set labels (`X_test`), and then prints the accuracy score by comparing the predictions with the true labels (`y_test`) using scikit-learn's `accuracy_score` function.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9948631}}, {"cell_id": 26, "code": "print(classification_report(y_test, test_predictions))", "class": "Model Evaluation", "desc": "This code prints the classification report, including precision, recall, F1-score, and support, for the test set predictions, using scikit-learn's `classification_report` function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9980178}}, {"cell_id": 21, "code": "param_grid = {'max_depth':[4,5,6,7,8,9,10]}", "class": "Model Training", "desc": "This code defines a dictionary `param_grid` specifying a range of values for the `max_depth` parameter of the RandomForestClassifier to be used in hyperparameter tuning with GridSearchCV.", "testing": {"class": "Model_Train", "subclass": "define_search_space", "subclass_id": 5, "predicted_subclass_probability": 0.9943967}}, {"cell_id": 22, "code": "forest = RandomForestClassifier(random_state=42)\ngrid = GridSearchCV(forest, param_grid, cv=10)\ngrid.fit(X_train, y_train)", "class": "Model Training", "desc": "This code initializes a `RandomForestClassifier` with a fixed random state, wraps it in a `GridSearchCV` object with a specified parameter grid and 10-fold cross-validation, and then fits it to the training data (`X_train`, `y_train`).", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.98969954}}, {"cell_id": 29, "code": "forest = RandomForestClassifier(max_depth=6, random_state=42)\nforest.fit(X,y)\nfinal_preds = forest.predict(X_final)", "class": "Model Training", "desc": "This code initializes a `RandomForestClassifier` with a maximum depth of 6 and a fixed random state, fits it on the entire training dataset (`X`, `y`), and then predicts the labels for `X_final`.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.88619727}}, {"cell_id": 5, "code": "sns.countplot(data=df, x='Survived', hue='Sex')", "class": "Visualization", "desc": "This code generates a count plot using Seaborn to visualize the number of survivors (`Survived` column) differentiated by gender (`Sex` column).", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99777454}}, {"cell_id": 9, "code": "sns.boxplot(data=complete_df[complete_df['Pclass']==1],y='Fare',x='Embarked')", "class": "Visualization", "desc": "This code generates a box plot using Seaborn to visualize the distribution of the `Fare` column for first-class passengers (`Pclass` equals 1), grouped by the `Embarked` column.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9974491}}], "notebook_id": 11, "notebook_name": "my-titanic-score-0-79186-with-random-forest.ipynb"}, {"cells": [{"cell_id": 1, "code": "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()", "class": "Data Extraction", "desc": "The code snippet reads the training data from a CSV file located at \"/kaggle/input/titanic/train.csv\" into a pandas DataFrame and displays the first few rows.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.999642}}, {"cell_id": 2, "code": "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()", "class": "Data Extraction", "desc": "The code snippet reads the test data from a CSV file located at \"/kaggle/input/titanic/test.csv\" into a pandas DataFrame and displays the first few rows.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99966204}}, {"cell_id": 6, "code": "print(\"Before\", train_data.shape, test_data.shape)\n\ntrain_data = train_data.drop(['Ticket', 'Cabin'], axis=1)\ntest_data = test_data.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_data, test_data]\n\n\"After\", train_data.shape, test_data.shape", "class": "Data Transform", "desc": "The code snippet removes the 'Ticket' and 'Cabin' columns from both the train and test datasets and creates a combined list of the modified datasets while printing their shapes before and after the transformation.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.33762202}}, {"cell_id": 7, "code": "combine = [train_data, test_data]", "class": "Data Transform", "desc": "The code snippet creates a list named `combine` that includes both the modified training and test datasets for subsequent data processing steps.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.9982835}}, {"cell_id": 9, "code": "for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])", "class": "Data Transform", "desc": "The code snippet extracts titles from the `Name` column in both the training and test datasets and creates a new `Title` column, then calculates the cross-tabulation of the `Title` and `Sex` columns in the training dataset.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.94240606}}, {"cell_id": 10, "code": "for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()", "class": "Data Transform", "desc": "The code snippet standardizes the `Title` column by replacing certain rare titles with 'Rare' and aligning similar titles, then calculates and displays the mean survival rate for each title group in the training dataset.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.6105874}}, {"cell_id": 11, "code": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()", "class": "Data Transform", "desc": "The code snippet maps the categorical `Title` values to numerical values using a predefined dictionary and replaces missing titles with 0 for both the training and test datasets, then displays the first few rows of the modified training dataset.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99322236}}, {"cell_id": 12, "code": "train_data = train_data.drop(['Name', 'PassengerId'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.shape, test_data.shape", "class": "Data Transform", "desc": "The code snippet removes the 'Name' and 'PassengerId' columns from the training dataset and the 'Name' column from the test dataset, then updates the combined datasets list and displays their shapes.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.5906634}}, {"cell_id": 13, "code": "#train_data['Sex'].fillNA(-1)\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_data.head()", "class": "Data Transform", "desc": "The code snippet maps the categorical `Sex` values to numerical values (`female` to 1, `male` to 0) and converts the `Sex` column to an integer type for both the training and test datasets, then displays the first few rows of the modified training dataset.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.97668195}}, {"cell_id": 14, "code": "guess_ages = np.zeros((2,3))\nguess_ages", "class": "Data Transform", "desc": "The code snippet initializes a 2x3 numpy array named `guess_ages` with zeros, which is likely intended for imputing missing age values later in the data processing steps.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.4285954}}, {"cell_id": 15, "code": "for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_data.head()", "class": "Data Transform", "desc": "The code snippet fills missing `Age` values in both the training and test datasets by guessing ages based on the median `Age` of passengers grouped by their `Sex` and `Pclass`, then converts the `Age` column to integer type.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.19565275}}, {"cell_id": 16, "code": "train_data['AgeBand'] = pd.cut(train_data['Age'], 5)\ntrain_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)", "class": "Data Transform", "desc": "The code snippet creates an `AgeBand` column in the training dataset by segmenting the `Age` column into 5 equal-sized ranges and then calculates and displays the mean survival rate for each age band, sorted by `AgeBand`.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.881014}}, {"cell_id": 17, "code": "for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_data.head()", "class": "Data Transform", "desc": "The code snippet categorizes the `Age` values in both the training and test datasets into discrete bands (0 to 3) based on specified age ranges, effectively replacing the `Age` column with these categorical values, and then displays the first few rows of the modified training dataset.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.998941}}, {"cell_id": 18, "code": "train_data = train_data.drop(['AgeBand'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()", "class": "Data Transform", "desc": "The code snippet removes the `AgeBand` column from the training dataset as it is no longer needed after age categorization, then updates the combined datasets list and displays the first few rows of the modified training dataset.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9964252}}, {"cell_id": 19, "code": "for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)", "class": "Data Transform", "desc": "The code snippet calculates a new column `FamilySize` by adding the values of `SibSp` and `Parch` columns and then adding 1 (to account for the individual themselves), and displays the mean survival rate for each family size in the training dataset, sorted by survival rate in descending order.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9927946}}, {"cell_id": 20, "code": "for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()", "class": "Data Transform", "desc": "The code snippet creates a new column `IsAlone` and sets it to 1 for passengers with a `FamilySize` of 1 (indicating they are alone) and sets it to 0 otherwise, then calculates and displays the mean survival rate based on the `IsAlone` column in the training dataset.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9402816}}, {"cell_id": 23, "code": "train_data = train_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_data = test_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_data, test_data]\n\ntrain_data.head()", "class": "Data Transform", "desc": "The code snippet removes the `Parch`, `SibSp`, and `FamilySize` columns from both the training and test datasets, then updates the combined datasets list and displays the first few rows of the modified training dataset.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.98982376}}, {"cell_id": 24, "code": "for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_data.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)", "class": "Data Transform", "desc": "The code snippet creates a new feature `Age*Class` by multiplying the `Age` and `Pclass` columns for both the training and test datasets, and then displays the first 10 rows of the `Age*Class`, `Age`, and `Pclass` columns from the training dataset.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99864763}}, {"cell_id": 3, "code": "women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)", "class": "Exploratory Data Analysis", "desc": "The code snippet calculates and prints the survival rate of women in the training data by filtering the DataFrame where the `Sex` column is 'female' and computing the proportion of the `Survived` column that is 1.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.45923898}}, {"cell_id": 4, "code": "men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)", "class": "Exploratory Data Analysis", "desc": "The code snippet calculates and prints the survival rate of men in the training data by filtering the DataFrame where the `Sex` column is 'male' and computing the proportion of the `Survived` column that is 1.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.59011316}}, {"cell_id": 8, "code": "print(train_data.columns.values)", "class": "Exploratory Data Analysis", "desc": "The code snippet prints all column names of the `train_data` DataFrame to display the features contained in the dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9982193}}, {"cell_id": 21, "code": "train_data.describe()", "class": "Exploratory Data Analysis", "desc": "The code snippet provides a statistical summary of the training dataset using the `describe` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994442}}, {"cell_id": 22, "code": "train_data.shape", "class": "Exploratory Data Analysis", "desc": "The code snippet outputs the shape of the training dataset, indicating the number of rows and columns it contains.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995814}}, {"cell_id": 25, "code": "train_data.describe()", "class": "Exploratory Data Analysis", "desc": "The code snippet provides a statistical summary of the modified training dataset using the `describe` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994442}}, {"cell_id": 26, "code": "train_data.head()", "class": "Exploratory Data Analysis", "desc": "The code snippet displays the first few rows of the modified training dataset using the `head` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997532}}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "The code snippet sets up the Python environment for data processing by importing the `numpy` and `pandas` libraries and lists all files in the specified input directory using the `os` module.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.99921954}}, {"cell_id": 5, "code": "from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Model Training", "desc": "The code snippet prepares the training and test datasets using one-hot encoding for categorical features, trains a RandomForestClassifier model using the training dataset, makes survival predictions on the test dataset, and saves the predictions to a CSV file named 'submission.csv'.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9991411}}, {"cell_id": 27, "code": "y = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Title\", \"IsAlone\", \"Age*Class\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Model Training", "desc": "The code snippet selects specific features from the training and test datasets, applies one-hot encoding for categorical variables, trains a RandomForestClassifier model, makes predictions on the test dataset, and saves the predictions to a CSV file named 'submission.csv'.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9991677}}, {"cell_id": 28, "code": "from sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n#model = SVC()\n#svc.fit(X,y)\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Title\", \"IsAlone\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n#model = DecisionTreeClassifier()\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Model Training", "desc": "The code snippet imports additional classifiers (SVC and DecisionTreeClassifier), selects features from the training and test datasets, applies one-hot encoding for categorical variables, trains a RandomForestClassifier model (with an option to switch to SVC or DecisionTreeClassifier), makes predictions on the test dataset, and saves the predictions to a CSV file named 'submission.csv'.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9964826}}], "notebook_id": 12, "notebook_name": "notebookda32490206.ipynb"}, {"cells": [{"cell_id": 41, "code": "#take backup for later use\ntrain_copy = combined[:train_len]\ntest_copy = combined[train_len:].reset_index(drop=True)\ntest_copy.drop(columns=['Survived'],inplace=True)", "class": "Data Export", "desc": "The code creates backup copies of the training and test sets from the combined dataset, resetting the index for the test set and removing the 'Survived' column from it using Pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.69678193}}, {"cell_id": 52, "code": "y_test_rfc = rfc.predict(test).astype(int)\ntest_out = pd.concat([test_copy['PassengerId'],pd.Series(y_test_rfc,name=\"Survived\")],axis=1)\ntest_out['Survived'] = test_out['Survived'].astype('int')\ntest_out.to_csv('submission.csv',index=False)", "class": "Data Export", "desc": "The code makes predictions on the test set using the trained Random Forest classifier, creates a DataFrame containing the 'PassengerId' and predicted 'Survived' status, and exports this DataFrame to a CSV file named 'submission.csv' using Pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9994411}}, {"cell_id": 3, "code": "train_df=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")", "class": "Data Extraction", "desc": "The code loads the train and test datasets from the Titanic dataset CSV files using Pandas' read_csv function.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99974495}}, {"cell_id": 4, "code": "train_len = len(train_df)\ncombined = train_df.append(test_df,ignore_index=True)\ncombined.fillna(np.nan)", "class": "Data Transform", "desc": "The code appends the test dataset to the training dataset to create a combined dataset and fills any missing values with NaN using Pandas and NumPy.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.99917126}}, {"cell_id": 13, "code": "combined['AgeGroup'] = 'adult'\ncombined.loc[combined['Name'].str.contains('Master'),'AgeGroup'] = \"child\"\ncombined.loc[combined['Age'] <= 14.0,'AgeGroup'] = \"child\"\ncombined.loc[(combined['Age'].isnull()) & (combined['Name'].str.contains('Miss')) & (combined['Parch'] != 0) ,'AgeGroup'] = \"child\"", "class": "Data Transform", "desc": "The code creates a new column 'AgeGroup' in the combined dataset, initially setting all entries to 'adult,' and then assigns 'child' to passengers whose names contain 'Master,' those aged 14 or younger, and those whose age is missing but are 'Miss' with non-zero parents/children (Parch) using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9986811}}, {"cell_id": 15, "code": "def Age(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    Sex=cols[2]\n    AgeGroup=cols[3]\n    if pd.isnull(Age):\n        if Pclass==1:\n            if Sex==\"male\":\n                if AgeGroup=='adult':\n                    return 42\n                else:\n                    return 7\n            elif Sex==\"female\":\n                if AgeGroup=='adult':\n                    return 37\n                else:\n                    return 8\n        elif Pclass==2:\n            if Sex==\"male\":\n                if AgeGroup=='adult':\n                    return 33\n                else:\n                    return 4\n            elif Sex==\"female\":\n                if AgeGroup=='adult':\n                    return 31\n                else:\n                    return 7\n        elif Pclass==3:\n            if Sex==\"male\":\n                if AgeGroup=='adult':\n                    return 29\n                else:\n                    return 7\n            elif Sex==\"female\":\n                if AgeGroup=='adult':\n                    return 27\n                else:\n                    return 5\n    else:\n        return Age\n    \ncombined[\"Age\"]=combined[[\"Age\",\"Pclass\",\"Sex\",\"AgeGroup\"]].apply(Age,axis=1)", "class": "Data Transform", "desc": "The code defines a function to impute missing age values based on passenger class (Pclass), gender (Sex), and age group (AgeGroup), and then applies this function to the combined dataset to update the \"Age\" column using Pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.980082}}, {"cell_id": 16, "code": "def AgeBand(col):\n    Age=col[0]\n    if Age <=7:\n        return \"0-7\"\n    elif Age <=14:\n        return \"8-14\"\n    elif Age <=21:\n        return \"15-21\"\n    elif Age <= 28:\n        return \"22-28\"\n    elif Age <= 35:\n        return \"29-35\"\n    elif Age <= 42:\n        return \"36-42\"\n    elif Age <= 49:\n        return \"43-49\"\n    elif Age <= 56:\n        return \"50-56\"\n    elif Age <= 63:\n        return \"57-63\"\n    else:\n        return \">=64\"\n\ncombined[\"AgeBand\"]=combined[[\"Age\"]].apply(AgeBand,axis=1)", "class": "Data Transform", "desc": "The code defines a function to categorize ages into specific age bands and applies this function to create a new column \"AgeBand\" in the combined dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.98803115}}, {"cell_id": 21, "code": "combined[combined['Embarked'].isnull()]['Embarked'] = combined['Embarked'].mode()", "class": "Data Transform", "desc": "The code fills missing values in the 'Embarked' column with the most frequent value (mode) of the column in the combined dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99778533}}, {"cell_id": 25, "code": "ticketCount = combined.groupby('Ticket')['PassengerId'].count().reset_index()\nticketCount.rename(columns={'PassengerId':'Count on Ticket'},inplace=True)\ncombined = combined.merge(ticketCount, on=\"Ticket\",how=\"left\")", "class": "Data Transform", "desc": "The code calculates the number of passengers per ticket, renames this count column to 'Count on Ticket,' and merges it back into the combined dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.9985952}}, {"cell_id": 26, "code": "combined['Diff'] = combined['FamilySize'] - combined['Count on Ticket']\ncombined['Family Status'] = combined.apply(lambda x:\"Has Family On Same Ticket\" if (x['FamilySize'] - x['Count on Ticket']) <= 0 else \"Family Not on same ticket\",axis=1)\n", "class": "Data Transform", "desc": "The code calculates the difference between family size and the number of passengers with the same ticket, stores the result in a new column 'Diff,' and creates a new column 'Family Status' based on whether the family is on the same ticket using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99793893}}, {"cell_id": 27, "code": "combined['Family Status'] = combined.apply(lambda x:\"Is Alone\" if (x['FamilySize']==1) & (x['Count on Ticket']==1)  else x['Family Status'],axis=1)", "class": "Data Transform", "desc": "The code updates the 'Family Status' column to label passengers as \"Is Alone\" if both their family size and ticket count are equal to one using the apply method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.996369}}, {"cell_id": 29, "code": "combined['Cabin Class'] = 'No Cabin'\n\ncombined['Cabin Class'] = combined.apply(lambda x: \"No Cabin\" if pd.isna(x[\"Cabin\"]) else x[\"Cabin\"][0] , axis=1)\n\n", "class": "Data Transform", "desc": "The code assigns a default 'No Cabin' value to a new column 'Cabin Class' and updates it to the first letter of the 'Cabin' value for passengers with non-null cabin information using the apply method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9755805}}, {"cell_id": 30, "code": "\ntickcab = combined[combined['Cabin Class'] != 'No Cabin'][['Ticket','Cabin Class']].drop_duplicates()\n\ntickcab = tickcab.rename(columns={'Cabin Class':'CabNam'})\ncombined = combined.merge(tickcab,how=\"left\",on=\"Ticket\")\n\ncombined['CabNam'].fillna('No Cabin')\n\ncombined['Cabin Class'] = combined.apply(lambda x:x['Cabin Class'] if x['Cabin Class'] != 'No Cabin' else x['CabNam'],axis=1)\ncombined.drop(columns=['CabNam'],inplace=True)\ncombined.drop_duplicates(inplace=True)\n", "class": "Data Transform", "desc": "The code updates the 'Cabin Class' column for passengers without original cabin information by using the cabin class of other passengers on the same ticket, merges this information back into the combined dataset, fills any remaining missing values with 'No Cabin,' and removes temporary columns and duplicate rows using Pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.6754502}}, {"cell_id": 33, "code": "combined[\"Fare\"] = combined[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\ncombined[\"Fare\"] = combined[\"Fare\"]/combined['Count on Ticket']", "class": "Data Transform", "desc": "The code transforms the \"Fare\" column by applying a logarithm to positive fare values, then normalizes the fare by dividing it by the number of passengers with the same ticket using Pandas and NumPy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99918026}}, {"cell_id": 36, "code": "companion = pd.pivot_table(combined, values='PassengerId',index=['Ticket'],columns=['AgeGroup'], aggfunc=\"count\").reset_index().fillna(0)\ncompanion.columns = ['Ticket','No. of Adult Companion', 'No. of Child Companion']\ncombined = combined.merge(companion, on='Ticket',how='left')", "class": "Data Transform", "desc": "The code creates a pivot table to count the number of adult and child companions per ticket, renames the columns for clarity, fills missing values with zero, and merges this information back into the combined dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.99741}}, {"cell_id": 37, "code": "combined.loc[combined['AgeGroup']=='adult','No. of Adult Companion'] = combined.loc[combined['AgeGroup']=='adult','No. of Adult Companion'] - 1\ncombined.loc[combined['AgeGroup']=='child','No. of Child Companion'] = combined.loc[combined['AgeGroup']=='child','No. of Child Companion'] - 1\n\ncombined['Companion'] = 'Adult & Child Companion'\ncombined['Companion'] = combined.apply(lambda x:'Only Adult Companion' if (x['No. of Adult Companion'] > 0) & (x['No. of Child Companion']==0) else x['Companion'],axis=1)\ncombined['Companion'] = combined.apply(lambda x:'Only Child Companion' if (x['No. of Adult Companion'] == 0) & (x['No. of Child Companion']>0) else x['Companion'],axis=1)\ncombined['Companion'] = combined.apply(lambda x:'No Companion' if (x['No. of Adult Companion'] == 0) & (x['No. of Child Companion']==0) else x['Companion'],axis=1)", "class": "Data Transform", "desc": "The code adjusts the companion count by subtracting one for the passenger themselves, then creates a new column 'Companion' to categorize whether passengers have adult, child, both types of companions, or no companions, using conditional logic and the apply method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.998919}}, {"cell_id": 42, "code": "combined.drop(columns=['PassengerId','Name','Age', 'AgeGroup','SibSp','Parch','Ticket','Cabin','Count on Ticket','Diff','No. of Adult Companion','No. of Child Companion'],inplace=True)", "class": "Data Transform", "desc": "The code removes specified columns from the combined dataset using the drop method in Pandas to streamline the dataset by excluding unnecessary or redundant data.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9992192}}, {"cell_id": 45, "code": "combined = pd.get_dummies(combined, columns = [\"Sex\",\"Embarked\",\"AgeBand\",\"Family Status\",\"Cabin Class\",\"Companion\"],drop_first=True)", "class": "Data Transform", "desc": "The code transforms categorical columns ('Sex', 'Embarked', 'AgeBand', 'Family Status', 'Cabin Class', and 'Companion') into dummy or one-hot encoded variables, dropping the first category in each column to avoid multicollinearity, using the get_dummies method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99923813}}, {"cell_id": 46, "code": "train = combined[:train_len]\ntest = combined[train_len:]\ntest.drop(columns=['Survived'],inplace=True)", "class": "Data Transform", "desc": "The code splits the combined dataset back into separate training and test sets, removing the 'Survived' column from the test set using Pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.993755}}, {"cell_id": 5, "code": "combined.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "The code calculates and displays the total number of missing values for each column in the combined dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99895346}}, {"cell_id": 6, "code": "train_df.describe()", "class": "Exploratory Data Analysis", "desc": "The code generates and displays summary statistics for the training dataset using the describe() method in Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994492}}, {"cell_id": 10, "code": "combined.groupby(['Pclass','Sex'])['Age'].mean()", "class": "Exploratory Data Analysis", "desc": "The code computes and displays the mean age of passengers grouped by passenger class (Pclass) and gender (Sex) using the groupby method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.99413943}}, {"cell_id": 14, "code": "combined[combined['Age'].notnull()].groupby(['Pclass','Sex','AgeGroup'])['Age'].mean()", "class": "Exploratory Data Analysis", "desc": "The code calculates and displays the mean age of passengers grouped by passenger class (Pclass), gender (Sex), and age group (AgeGroup) for entries with non-null ages using the groupby method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9977055}}, {"cell_id": 19, "code": "combined.groupby(['Pclass','Embarked'])['PassengerId'].count()", "class": "Exploratory Data Analysis", "desc": "The code calculates and displays the number of passengers grouped by passenger class (Pclass) and embarkation point (Embarked) using the groupby method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.99464333}}, {"cell_id": 35, "code": "combined.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the combined dataset to give an overview of its structure and contents using the head() method in Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997458}}, {"cell_id": 38, "code": "combined[combined[\"Survived\"].notnull()].groupby(['AgeGroup','Companion'])['PassengerId'].count()", "class": "Exploratory Data Analysis", "desc": "The code calculates and displays the count of passengers grouped by age group (AgeGroup) and companion type (Companion) for entries with non-null survival status using the groupby method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9981712}}, {"cell_id": 39, "code": "combined[combined[\"Survived\"].notnull()].groupby(['AgeGroup','Companion'])['Survived'].mean()", "class": "Exploratory Data Analysis", "desc": "The code calculates and displays the mean survival rate of passengers grouped by age group (AgeGroup) and companion type (Companion) for entries with non-null survival status using the groupby method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9978156}}, {"cell_id": 40, "code": "combined.columns", "class": "Exploratory Data Analysis", "desc": "The code displays the names of all columns in the combined dataset using the columns attribute in Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99846387}}, {"cell_id": 43, "code": "combined.columns", "class": "Exploratory Data Analysis", "desc": "The code displays the names of all remaining columns in the combined dataset after dropping specified columns using the columns attribute in Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99846387}}, {"cell_id": 44, "code": "combined.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the modified combined dataset to give an overview of its structure and contents using the head() method in Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997458}}, {"cell_id": 48, "code": "X_train.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the training feature set (X_train) to provide an overview of its structure and contents using the head() method in Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99975497}}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "The code imports essential libraries such as NumPy and Pandas for linear algebra and data processing respectively, and lists all files in the Kaggle input directory using the os library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.99921954}}, {"cell_id": 1, "code": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport time\nfrom collections import Counter\nfrom matplotlib import pyplot as plt\nfrom warnings import filterwarnings\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import norm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score,KFold\nimport plotly.express as px\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import feature_selection", "class": "Imports and Environment", "desc": "The code imports various libraries and modules for data processing, visualization, machine learning model training, evaluation, and optimization including Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn, Scipy, XGBoost, and Plotly.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.9984848}}, {"cell_id": 2, "code": "import warnings\nwarnings.filterwarnings('ignore')", "class": "Imports and Environment", "desc": "The code suppresses warnings to prevent them from appearing in the output using the warnings library.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.999143}}, {"cell_id": 50, "code": "report=classification_report(y_test,pred)\nprint(\"Decision Tree report \\n\",report)", "class": "Model Evaluation", "desc": "The code generates and prints a classification report, which includes precision, recall, f1-score, and support for the validation set predictions, using the classification_report function from Scikit-learn.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9974105}}, {"cell_id": 47, "code": "X = train.iloc[:,1:]\ny = train.iloc[:,0]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)", "class": "Model Training", "desc": "The code separates the training data into features (X) and target variable (y), and then splits these into training and validation sets (X_train, X_test, y_train, y_test) using the train_test_split function from the Scikit-learn library with a test size of 20% and a random state of 0.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.99681467}}, {"cell_id": 49, "code": "#Dtree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\npred_train = decision_tree.predict(X_train)\npred=decision_tree.predict(X_test)\npred_train_df=pd.DataFrame({\"Actual\":y_train,\"Pred\":pred_train})\npred_df=pd.DataFrame({\"Actual\":y_test,\"Pred\":pred})\ncm=confusion_matrix(y_test,pred)\ncm", "class": "Model Training", "desc": "The code trains a Decision Tree classifier on the training set (X_train, y_train), makes predictions on the training and validation sets, creates DataFrames to compare actual vs predicted values, and generates a confusion matrix for the validation set using Scikit-learn's DecisionTreeClassifier and confusion_matrix functions.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.25523284}}, {"cell_id": 51, "code": "#RFC\n\nrfc=ensemble.RandomForestClassifier(max_depth=6,random_state=0,n_estimators=64)\nrfc.fit(X_train, y_train)\npred_train = rfc.predict(X_train)\npred=rfc.predict(X_test)\npred_train_df=pd.DataFrame({\"Actual\":y_train,\"Pred\":pred_train})\npred_df=pd.DataFrame({\"Actual\":y_test,\"Pred\":pred})\n\ncm=confusion_matrix(y_test,pred)\nprint(cm)\n\nreport=classification_report(y_test,pred)\nprint(\"Random Forest report \\n\",report)", "class": "Model Training", "desc": "The code trains a Random Forest classifier with a maximum depth of 6, 64 estimators, and a random state of 0 on the training set (X_train, y_train), makes predictions on the training and validation sets, creates DataFrames to compare actual vs predicted values, and prints a confusion matrix and classification report for the validation set using Scikit-learn's ensemble.RandomForestClassifier, confusion_matrix, and classification_report functions.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.83314914}}, {"cell_id": 7, "code": "g = sns.barplot(x=\"Pclass\",y=\"Survived\",data=train_df)\ng.set_ylabel(\"Survival Probability\")", "class": "Visualization", "desc": "The code creates a bar plot to visualize the survival probability by passenger class (Pclass) using the Seaborn library and sets the y-axis label to \"Survival Probability\".", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.9456676}}, {"cell_id": 8, "code": "g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train_df)\ng.set_ylabel(\"Survival Probability\")", "class": "Visualization", "desc": "The code creates a bar plot to visualize the survival probability by gender (Sex) using the Seaborn library and sets the y-axis label to \"Survival Probability\".", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99116606}}, {"cell_id": 9, "code": "g = sns.barplot(x=\"Pclass\",y=\"Survived\",hue=\"Sex\",data=train_df)\ng.set_ylabel(\"Survival Probability\")", "class": "Visualization", "desc": "The code generates a bar plot to visualize the survival probability by passenger class (Pclass) and further differentiates by gender (Sex) using the Seaborn library, with the y-axis labeled as \"Survival Probability\".", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.8091258}}, {"cell_id": 11, "code": "sns.distplot(train_df[\"Age\"])", "class": "Visualization", "desc": "The code generates a distribution plot for the \"Age\" column in the training dataset using the distplot function from the Seaborn library.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9981388}}, {"cell_id": 12, "code": "g = sns.FacetGrid(train_df[train_df['Age'].notnull()], col='Sex',row='Pclass',hue=\"Survived\")\ng = g.map(sns.distplot, \"Age\",bins=10,hist_kws=dict(edgecolor=\"k\", linewidth=2),kde=False).add_legend()\n", "class": "Visualization", "desc": "The code creates a grid of histograms to visualize the age distribution for passengers of different sexes and passenger classes (Pclass), while coloring the histograms based on survival status, using the Seaborn library's FacetGrid and distplot functions.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9957695}}, {"cell_id": 17, "code": "g = sns.FacetGrid(combined[combined['Survived'].notnull()], col='Pclass')\ng = g.map(sns.barplot,\"AgeBand\",\"Survived\", order=[ \"0-7\",\"8-14\",\"15-21\",\"22-28\",\"29-35\",\"36-42\",\"43-49\",\"50-56\",\"57-63\",\">=64\"])\nfor axes in g.axes.flat:\n    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=70)\nplt.tight_layout()", "class": "Visualization", "desc": "The code generates a grid of bar plots to visualize the survival rates across different age bands (AgeBand) for each passenger class (Pclass), with rotated x-axis labels, using the Seaborn library's FacetGrid and barplot functions, and adjusts the layout with Matplotlib.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99827373}}, {"cell_id": 18, "code": "sns.barplot(\"Embarked\",\"Survived\",data=combined[combined['Survived'].notnull()])", "class": "Visualization", "desc": "The code creates a bar plot to visualize the survival rates based on the embarkation point (Embarked) using the Seaborn library.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9954508}}, {"cell_id": 20, "code": "sns.barplot(\"Pclass\",\"Survived\",hue=\"Embarked\",data=combined[combined['Survived'].notnull()],ci=None)\n", "class": "Visualization", "desc": "The code generates a bar plot to visualize the survival rates by passenger class (Pclass), further differentiated by embarkation point (Embarked), using the Seaborn library with confidence intervals turned off.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9985398}}, {"cell_id": 22, "code": "sns.barplot(\"SibSp\",\"Survived\",data=train_df, ci = None)", "class": "Visualization", "desc": "The code creates a bar plot to visualize the survival rates based on the number of siblings and spouses aboard (SibSp) using the Seaborn library with confidence intervals turned off.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9982626}}, {"cell_id": 23, "code": "sns.barplot(\"Parch\",\"Survived\",data=train_df, ci = None)", "class": "Visualization", "desc": "The code generates a bar plot to visualize the survival rates based on the number of parents and children aboard (Parch) using the Seaborn library with confidence intervals turned off.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9982613}}, {"cell_id": 24, "code": "combined['FamilySize'] = combined['SibSp'] + combined['Parch'] + 1  # +1 is to include the passenger him/herself\nsns.barplot(\"FamilySize\",\"Survived\",data=combined[combined['Survived'].notnull()], ci = None)", "class": "Visualization", "desc": "The code calculates the family size by adding siblings/spouses (SibSp) and parents/children (Parch) plus one to include the passenger, then generates a bar plot to visualize survival rates based on this family size using the Seaborn library with confidence intervals turned off.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99779296}}, {"cell_id": 28, "code": "g = sns.FacetGrid(combined[combined['Survived'].notnull()], col='Pclass')\ng.map(sns.barplot,\"Family Status\",\"Survived\",ci=None)\nfor axes in g.axes.flat:\n    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=70)\n", "class": "Visualization", "desc": "The code creates a grid of bar plots to visualize survival rates based on family status ('Family Status') across different passenger classes (Pclass), with rotated x-axis labels, using the Seaborn library's FacetGrid and barplot functions.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99821365}}, {"cell_id": 31, "code": "sns.barplot(\"Cabin Class\",\"Survived\",data=combined[combined[\"Survived\"].notnull()],ci=None)", "class": "Visualization", "desc": "The code generates a bar plot to visualize survival rates based on cabin class ('Cabin Class') using the Seaborn library with confidence intervals turned off.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9981097}}, {"cell_id": 32, "code": "g = sns.distplot(combined[\"Fare\"], color=\"m\",label=\"Skewness : %.2f\"%(combined[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")", "class": "Visualization", "desc": "The code creates a distribution plot for the \"Fare\" column, displaying its skewness in the legend, using the Seaborn library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99603015}}, {"cell_id": 34, "code": "g = sns.distplot(combined[\"Fare\"], color=\"m\",label=\"Skewness : %.2f\"%(combined[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")", "class": "Visualization", "desc": "The code generates a distribution plot for the transformed \"Fare\" column, displaying its skewness in the legend, using the Seaborn library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99603015}}], "notebook_id": 13, "notebook_name": "titanic-rfc-some-analysis-on-ticket.ipynb"}, {"cells": [{"cell_id": 1, "code": "train_data=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()", "class": "Data Extraction", "desc": "This code reads the 'train.csv' file from the specified directory using pandas and displays the first few rows of the dataset.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996675}}, {"cell_id": 2, "code": "test_data=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()", "class": "Data Extraction", "desc": "This code reads the 'test.csv' file from the specified directory using pandas and displays the first few rows of the dataset.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996755}}, {"cell_id": 10, "code": "test_data[\"CabinBool\"] = (test_data[\"Cabin\"].notnull().astype('int'))", "class": "Data Transform", "desc": "This code adds a new column 'CabinBool' to the test dataset, indicating whether each entry has cabin information by using pandas.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.9858393}}, {"cell_id": 11, "code": "train_data_edited=train_data.drop(columns=[\"Cabin\",\"Ticket\",\"Embarked\", \"Name\",\"Fare\"])\ntest_data_edited=test_data.drop(columns=[\"Cabin\",\"Ticket\",\"Embarked\", \"Name\",\"Fare\"])", "class": "Data Transform", "desc": "This code removes the columns 'Cabin', 'Ticket', 'Embarked', 'Name', and 'Fare' from both the training and test datasets using pandas' drop method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99705756}}, {"cell_id": 13, "code": "train_data_edited.fillna(train_data_edited.mean()[\"Age\"])\ntest_data_edited.fillna(test_data_edited.mean()[\"Age\"])\ntrain_data_edited.describe(include = \"all\")\n\n[df['Age'].fillna(df.groupby(['Pclass','Sex'])['Age'].transform('mean'), inplace=True) for df in [train_data_edited, test_data_edited]]", "class": "Data Transform", "desc": "This code fills missing 'Age' values in both training and test datasets with the mean age and then refines this by replacing missing 'Age' values with the mean age within each 'Pclass' and 'Sex' group using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99928135}}, {"cell_id": 3, "code": "women=train_data.loc[train_data.Sex=='female']['Survived']\nrate_women=sum(women)/len(women)\nprint(\"% of women who survived is \",rate_women)", "class": "Exploratory Data Analysis", "desc": "This code calculates and prints the survival rate of women in the training dataset by using pandas to filter and aggregate the 'Survived' column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.6733408}}, {"cell_id": 4, "code": "men=train_data.loc[train_data.Sex=='male']['Survived']\nrate_men=sum(men)/len(men)\nprint(\"% of men who survived is \",rate_men)", "class": "Exploratory Data Analysis", "desc": "This code calculates and prints the survival rate of men in the training dataset by using pandas to filter and aggregate the 'Survived' column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.587092}}, {"cell_id": 6, "code": "train_data.describe(include = \"all\")", "class": "Exploratory Data Analysis", "desc": "This code provides descriptive statistics for all columns in the training dataset using the `describe` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994178}}, {"cell_id": 7, "code": "print(pd.isnull(train_data).sum())", "class": "Exploratory Data Analysis", "desc": "This code prints the total number of missing values for each column in the training dataset using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.76591766}}, {"cell_id": 12, "code": "train_data_edited.groupby(['Sex','Pclass'])['Age'].agg(['mean', 'count'])\n#train_data_edited.groupby(['Title', 'Pclass'])['Age'].agg(['mean', 'count'])", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the mean and count of the 'Age' column grouped by 'Sex' and 'Pclass' using pandas' groupby and agg functions.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9827183}}, {"cell_id": 14, "code": "test_data_edited.describe(include = \"all\")", "class": "Exploratory Data Analysis", "desc": "This code provides descriptive statistics for all columns in the edited test dataset using the `describe` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99942374}}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This snippet imports necessary libraries like numpy and pandas and lists files under the '/kaggle/input' directory to set up the Kaggle environment.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.99923015}}, {"cell_id": 8, "code": "import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline", "class": "Imports and Environment", "desc": "This code imports the matplotlib and seaborn libraries and configures matplotlib to display plots inline within the Jupyter notebook.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.9993888}}, {"cell_id": 5, "code": "#from sklearn.ensemble import RandomForestClassifier\n\n#y = train_data[\"Survived\"]\n\n#features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n#X = pd.get_dummies(train_data[features])\n#X_test = pd.get_dummies(test_data[features])\n\n#model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n#model.fit(X, y)\n#predictions = model.predict(X_test)\n\n#output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n#output.to_csv('my_submission.csv', index=False)\n#print(\"Your submission was successfully saved!\")", "class": "Model Training", "desc": "This code, which is currently commented out, would perform feature selection, one-hot encoding, train a RandomForestClassifier on the training data, make predictions on the test data, and save the predictions to a CSV file using pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.38689268}}, {"cell_id": 15, "code": "from sklearn.ensemble import RandomForestClassifier\n\ny = train_data_edited[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\",\"Age\",\"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data_edited[features])\nX_test = pd.get_dummies(test_data_edited[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data_edited.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Model Training", "desc": "This code performs feature selection, one-hot encodes categorical variables, trains a RandomForestClassifier on the edited training data, makes predictions on the edited test data, and saves the predictions to a CSV file using pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9991767}}, {"cell_id": 9, "code": "train_data[\"CabinBool\"] = (train_data[\"Cabin\"].notnull().astype('int'))\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train_data)\nplt.show()", "class": "Visualization", "desc": "This code adds a new column 'CabinBool' to the training dataset indicating the presence of cabin information and uses seaborn's barplot to visualize the survival rates based on this new column.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.56851405}}], "notebook_id": 14, "notebook_name": "titanic-my-first-trial.ipynb"}, {"cells": [{"cell_id": 16, "code": "submit = pd.DataFrame({'PassengerId': final_test_data_id, 'Survived': prediction})", "class": "Data Export", "desc": "The code creates a pandas DataFrame named `submit` containing the 'PassengerId' and the corresponding survival predictions, preparing the data for submission.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.9983236}}, {"cell_id": 17, "code": "submit.to_csv(\"svm_titanic_sandorabad_7.csv\", index=False)", "class": "Data Export", "desc": "The code saves the `submit` DataFrame to a CSV file named \"svm_titanic_sandorabad_7.csv\" without the index, facilitating the submission of the predictions.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.999323}}, {"cell_id": 1, "code": "#Reading Data\ndata = pd.read_csv(\"../input/titanic/train.csv\") \nfinal_test_data = pd.read_csv(\"../input/titanic/test.csv\")\nfinal_test_data_id = np.array(final_test_data['PassengerId']) # we use it for our submission", "class": "Data Extraction", "desc": "The code reads the training and test datasets from CSV files using pandas and extracts the 'PassengerId' column from the test data into a numpy array for reference in the submission.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997137}}, {"cell_id": 3, "code": "# we are droppin less relevant columns.\ndata.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'],axis=1, inplace=True)\ndata", "class": "Data Transform", "desc": "The code removes less relevant columns 'PassengerId', 'Name', 'Ticket', and 'Cabin' from the dataset using the pandas `drop()` method to simplify the data for analysis and model training.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991437}}, {"cell_id": 5, "code": "mean_age = np.mean(data['Age'])\ndata['Age'] = data['Age'].fillna(mean_age)\ndata", "class": "Data Transform", "desc": "The code fills the missing values in the 'Age' column with the mean age using the pandas `fillna()` method and numpy's `mean()` function to handle missing data.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99761415}}, {"cell_id": 6, "code": "# We need to transform the 'Sex' data, 'male' = 1 'female' = 0\ndata[\"Sex\"].replace('male', 1, inplace=True)\ndata[\"Sex\"].replace('female', 0, inplace=True)", "class": "Data Transform", "desc": "The code transforms the categorical 'Sex' column by replacing 'male' with 1 and 'female' with 0 using the pandas `replace()` method to convert gender information into a numerical format suitable for machine learning models.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9895449}}, {"cell_id": 7, "code": "# C = Cherbourg, Q = Queenstown, S = Southampton, after reading: \"https://www.kaggle.com/brendan45774/titanic-top-solution/notebook\" we now that this feature is important\n\ndata['Cherbourg'] = data['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\ndata['Queenstown'] = data['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\ndata['Southampton'] = data['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\ndata.drop('Embarked', axis=1, inplace=True)", "class": "Data Transform", "desc": "The code converts the categorical 'Embarked' column into three binary columns 'Cherbourg', 'Queenstown', and 'Southampton' using the `apply()` method with a lambda function and then drops the original 'Embarked' column using the pandas `drop()` method, enhancing the dataset for the machine learning model.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9882433}}, {"cell_id": 8, "code": "X = np.array(data.drop(['Survived'], axis=1))\ny = np.array(data['Survived'])", "class": "Data Transform", "desc": "The code separates the features and the target variable by creating a numpy array `X` from the dataset excluding the 'Survived' column and another numpy array `y` for the 'Survived' column, preparing the data for model training.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.9993857}}, {"cell_id": 9, "code": "# We create the train, test splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)", "class": "Data Transform", "desc": "The code splits the dataset into training and testing sets using the `train_test_split` function from scikit-learn with 33% of the data allocated for testing, ensuring that the dataset is divided appropriately for training and evaluating the model.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9983494}}, {"cell_id": 12, "code": "# we apply everything that we used with the \"train.csv\" dataset to the \"test.csv\" dataset\nfinal_test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\nfinal_test_data['Age'].fillna(np.mean(final_test_data['Age']), inplace=True)\nfinal_test_data['Fare'].fillna(np.mean(final_test_data['Fare']), inplace=True)\nfinal_test_data[\"Sex\"].replace('male', 1, inplace=True)\nfinal_test_data[\"Sex\"].replace('female', 0, inplace=True)\nfinal_test_data['Cherbourg'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\nfinal_test_data['Queenstown'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\nfinal_test_data['Southampton'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\nfinal_test_data.drop('Embarked', axis=1, inplace=True)\n\n\nX_final_test = np.array(final_test_data)", "class": "Data Transform", "desc": "The code replicates the preprocessing steps performed on the training data for the test data by dropping irrelevant columns, filling missing values, transforming categorical data into numerical format, and finally converting the preprocessed DataFrame into a numpy array, `X_final_test`, for use in predictions.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99385875}}, {"cell_id": 2, "code": "data.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the dataset using the pandas `head()` method to provide an initial glimpse of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974936}}, {"cell_id": 4, "code": "# We have missing values for the column age, we are going to meke those values outliers so we can still use the data\nprint(data.info(), data.describe())", "class": "Exploratory Data Analysis", "desc": "The code prints the summary information and descriptive statistics of the dataset using the pandas `info()` and `describe()` methods to identify missing values and understand the basic characteristics of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994295}}, {"cell_id": 13, "code": "final_test_data", "class": "Exploratory Data Analysis", "desc": "The code displays the current state of the `final_test_data` DataFrame to verify the applied transformations and preprocessing steps.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997675}}, {"cell_id": 14, "code": "X_final_test", "class": "Exploratory Data Analysis", "desc": "The code would display the `X_final_test` numpy array, which contains the preprocessed test dataset ready for prediction.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99973935}}, {"cell_id": 18, "code": "# I based this nootebook on the tutorials of the youtuber: sentdex, \n# Specially on this series: \"https://pythonprogramming.net/machine-learning-tutorial-python-introduction/\" if someone is just starting as I am i really recomend it", "class": "Exploratory Data Analysis", "desc": "The code includes comments acknowledging the educational resources used, specifically the tutorials by the YouTuber \"sentdex,\" to provide context and recommendations for further learning.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.445148}}, {"cell_id": 0, "code": "import numpy as np\nfrom sklearn import preprocessing, svm\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd", "class": "Imports and Environment", "desc": "The code imports necessary libraries, including numpy for numerical operations, pandas for data manipulation, and scikit-learn modules for preprocessing, model training, and splitting the dataset.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99935955}}, {"cell_id": 11, "code": "# we test the model\nmodel_clf.fit(X_train, y_train)\naccuracy = model_clf.score(X_test, y_test)\nprint(accuracy)", "class": "Model Evaluation", "desc": "The code trains the Support Vector Classifier model on the training data using the `fit()` method and evaluates its accuracy on the test data with the `score()` method, printing the resulting accuracy.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.99956805}}, {"cell_id": 15, "code": "prediction = model_clf.predict(X_final_test)", "class": "Model Evaluation", "desc": "The code generates predictions for the preprocessed test dataset by using the `predict()` method of the trained Support Vector Classifier model.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99418724}}, {"cell_id": 10, "code": "# We create the model (classifier)\nmodel_clf = svm.SVC(kernel='linear')", "class": "Model Training", "desc": "The code initializes a Support Vector Classifier (SVC) with a linear kernel using scikit-learn's `svm.SVC` class, setting up the model for later training on the dataset.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.83985466}}], "notebook_id": 15, "notebook_name": "titanic-07.ipynb"}, {"cells": [{"cell_id": 29, "code": "adaSub = pd.DataFrame({'PassengerId': PassengerId, 'Survived':t_pred })\nadaSub.head()", "class": "Data Export", "desc": "The code creates a new DataFrame `adaSub` suitable for submission, containing the 'PassengerId' and the corresponding 'Survived' predictions, and then displays the first few rows of this DataFrame using the `head()` method.", "testing": {"class": "Data_Export", "subclass": "prepare_output", "subclass_id": 55, "predicted_subclass_probability": 0.9886446}}, {"cell_id": 30, "code": "adaSub.to_csv(\"1_Ada_Submission.csv\", index = False)", "class": "Data Export", "desc": "The code exports the `adaSub` DataFrame to a CSV file named \"1_Ada_Submission.csv\" without writing row indices using the `to_csv()` method with `index=False`.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992505}}, {"cell_id": 1, "code": "train = pd.read_csv(\"../input/train.csv\")\ntrain.head()", "class": "Data Extraction", "desc": "The code reads the training dataset from a CSV file into a Pandas DataFrame and displays the first few rows of the dataset using the `head()` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9871219}}, {"cell_id": 2, "code": "test = pd.read_csv(\"../input/test.csv\")\ntest.head()", "class": "Data Extraction", "desc": "The code reads the testing dataset from a CSV file into a Pandas DataFrame and displays the first few rows of the dataset using the `head()` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9992644}}, {"cell_id": 18, "code": "all_train = all_dummies[all_dummies['Survived'].notna()]\nall_train.info()", "class": "Data Extraction", "desc": "The code creates a new DataFrame `all_train` by extracting rows from `all_dummies` where 'Survived' is not null and provides a concise summary of this DataFrame, including data types and non-null counts using the `info()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.7295958}}, {"cell_id": 19, "code": "all_test = all_dummies[all_dummies['Survived'].isna()]\nall_test.info()", "class": "Data Extraction", "desc": "The code creates a new DataFrame `all_test` by extracting rows from `all_dummies` where 'Survived' is null and provides a concise summary of this DataFrame, including data types and non-null counts using the `info()` method.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.7275288}}, {"cell_id": 26, "code": "TestForPred = all_test.drop(['PassengerId', 'Survived'], axis = 1)", "class": "Data Extraction", "desc": "The code creates a new DataFrame `TestForPred` by dropping the 'PassengerId' and 'Survived' columns from the `all_test` DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99855393}}, {"cell_id": 28, "code": "PassengerId = all_test['PassengerId']", "class": "Data Extraction", "desc": "The code extracts the 'PassengerId' column from the `all_test` DataFrame for use in later stages, potentially for creating a submission file.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.9945315}}, {"cell_id": 5, "code": "all = pd.concat([train, test], sort = False)\nall.info()", "class": "Data Transform", "desc": "The code concatenates the training and testing datasets into a single DataFrame and provides a concise summary of the combined data including data types and non-null counts using the Pandas `concat` function and `info()` method.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.60922176}}, {"cell_id": 6, "code": "#Fill Missing numbers with median\nall['Age'] = all['Age'].fillna(value=all['Age'].median())\nall['Fare'] = all['Fare'].fillna(value=all['Fare'].median())", "class": "Data Transform", "desc": "The code fills missing values in the 'Age' and 'Fare' columns of the combined DataFrame with the median of their respective columns using the `fillna()` method in Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.72388726}}, {"cell_id": 9, "code": "all['Embarked'] = all['Embarked'].fillna('S')\nall.info()", "class": "Data Transform", "desc": "The code fills missing values in the 'Embarked' column of the combined DataFrame with the value 'S' and provides a summary of the updated dataset using the `info()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9427343}}, {"cell_id": 10, "code": "#Age\nall.loc[ all['Age'] <= 16, 'Age'] = 0\nall.loc[(all['Age'] > 16) & (all['Age'] <= 32), 'Age'] = 1\nall.loc[(all['Age'] > 32) & (all['Age'] <= 48), 'Age'] = 2\nall.loc[(all['Age'] > 48) & (all['Age'] <= 64), 'Age'] = 3\nall.loc[ all['Age'] > 64, 'Age'] = 4 ", "class": "Data Transform", "desc": "The code categorizes the 'Age' column in the combined DataFrame into discrete bins with integer labels representing different age ranges using conditional assignments with Pandas' `loc` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9980605}}, {"cell_id": 11, "code": "#Title\nimport re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+\\.)', name)\n    \n    if title_search:\n        return title_search.group(1)\n    return \"\"", "class": "Data Transform", "desc": "The code defines a function `get_title` that uses a regular expression to extract titles (e.g., Mr., Mrs., etc.) from the 'name' string, returning the matched title if found.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.961291}}, {"cell_id": 12, "code": "all['Title'] = all['Name'].apply(get_title)\nall['Title'].value_counts()", "class": "Data Transform", "desc": "The code extracts titles from the 'Name' column of the combined DataFrame using the `get_title` function and applies it to create a new 'Title' column, then displays the frequency of each title using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9991881}}, {"cell_id": 13, "code": "all['Title'] = all['Title'].replace(['Capt.', 'Dr.', 'Major.', 'Rev.'], 'Officer.')\nall['Title'] = all['Title'].replace(['Lady.', 'Countess.', 'Don.', 'Sir.', 'Jonkheer.', 'Dona.'], 'Royal.')\nall['Title'] = all['Title'].replace(['Mlle.', 'Ms.'], 'Miss.')\nall['Title'] = all['Title'].replace(['Mme.'], 'Mrs.')\nall['Title'].value_counts()", "class": "Data Transform", "desc": "The code replaces less common titles in the 'Title' column of the combined DataFrame with more generalized categories and then displays the frequency of each updated title category using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.97918713}}, {"cell_id": 14, "code": "#Cabin\nall['Cabin'] = all['Cabin'].fillna('Missing')\nall['Cabin'] = all['Cabin'].str[0]\nall['Cabin'].value_counts()", "class": "Data Transform", "desc": "The code fills missing values in the 'Cabin' column with the string 'Missing', reduces 'Cabin' entries to their first letter, and then displays the frequency of each unique cabin code using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9994407}}, {"cell_id": 15, "code": "#Family Size & Alone \nall['Family_Size'] = all['SibSp'] + all['Parch'] + 1\nall['IsAlone'] = 0\nall.loc[all['Family_Size']==1, 'IsAlone'] = 1\nall.head()", "class": "Data Transform", "desc": "The code creates a new 'Family_Size' column by summing 'SibSp', 'Parch', and adding 1, then creates an 'IsAlone' column to indicate whether a passenger is alone, setting it to 1 if 'Family_Size' equals 1, and finally displays the first few rows of the updated DataFrame using the `head()` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9981501}}, {"cell_id": 16, "code": "#Drop unwanted variables\nall_1 = all.drop(['Name', 'Ticket'], axis = 1)\nall_1.head()", "class": "Data Transform", "desc": "The code creates a new DataFrame `all_1` by dropping the 'Name' and 'Ticket' columns from the combined dataset and then displays the first few rows of the updated DataFrame using the `head()` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99922025}}, {"cell_id": 17, "code": "all_dummies = pd.get_dummies(all_1, drop_first = True)\nall_dummies.head()", "class": "Data Transform", "desc": "The code converts categorical variables in the `all_1` DataFrame into dummy/indicator variables using one-hot encoding with Pandas' `get_dummies()` function and drops the first level to avoid multicollinearity, then displays the first few rows of the resulting DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99894685}}, {"cell_id": 3, "code": "train.info()", "class": "Exploratory Data Analysis", "desc": "The code provides a concise summary of the training dataset, including the data types and non-null counts of each column using the `info()` method of a Pandas DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993617}}, {"cell_id": 4, "code": "test.info()", "class": "Exploratory Data Analysis", "desc": "The code provides a concise summary of the testing dataset, including the data types and non-null counts of each column using the `info()` method of a Pandas DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993563}}, {"cell_id": 7, "code": "all.info()", "class": "Exploratory Data Analysis", "desc": "The code provides a concise summary of the combined dataset, including the data types and non-null counts of each column using the `info()` method of a Pandas DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993235}}, {"cell_id": 0, "code": "import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))", "class": "Imports and Environment", "desc": "The code imports fundamental libraries including NumPy, Pandas, Matplotlib, and Seaborn, and lists the files in the specified input directory using os.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.999067}}, {"cell_id": 21, "code": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier", "class": "Imports and Environment", "desc": "The code imports the `AdaBoostClassifier` from `sklearn.ensemble` and the `DecisionTreeClassifier` from `sklearn.tree`, both of which are used for building a machine learning model.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.9992812}}, {"cell_id": 23, "code": "predictions = ada.predict(X_test)", "class": "Model Evaluation", "desc": "The code generates predictions on the test set by applying the trained `AdaBoostClassifier` model to the test features using the `predict()` method.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99481404}}, {"cell_id": 24, "code": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))", "class": "Model Evaluation", "desc": "The code imports the `classification_report` function from Scikit-learn's `metrics` module and prints a report summarizing the precision, recall, F1-score, and support for each class based on the true labels and predicted values.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9934075}}, {"cell_id": 25, "code": "print (f'Train Accuracy - : {ada.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {ada.score(X_test,y_test):.3f}')", "class": "Model Evaluation", "desc": "The code prints the training and testing accuracy of the `AdaBoostClassifier` model, with the accuracy scores formatted to three decimal places using the `score()` method.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99685717}}, {"cell_id": 20, "code": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(all_train.drop(['PassengerId','Survived'],axis=1), \n                                                    all_train['Survived'], test_size=0.30, \n                                                    random_state=101, stratify = all_train['Survived'])", "class": "Model Training", "desc": "The code splits the training data into training and testing subsets for both features and target variable using Scikit-learn's `train_test_split` function, with 30% of the data allocated for testing, ensuring stratified sampling based on the 'Survived' column, and a fixed random state for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.99786854}}, {"cell_id": 22, "code": "ada = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=100, random_state=0)\nada.fit(X_train,y_train)", "class": "Model Training", "desc": "The code initializes the `AdaBoostClassifier` with `DecisionTreeClassifier` base estimators, sets the number of estimators to 100 and the random state to 0 for reproducibility, then fits the model to the training data using the `fit()` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9996433}}, {"cell_id": 27, "code": "t_pred = ada.predict(TestForPred).astype(int)", "class": "Model Training", "desc": "The code generates integer predictions on the `TestForPred` dataset using the trained `AdaBoostClassifier` model and converts the predictions to integers using the `astype()` method.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.94950974}}, {"cell_id": 8, "code": "sns.catplot(x = 'Embarked', kind = 'count', data = all) #or all['Embarked'].value_counts()", "class": "Visualization", "desc": "The code creates a count plot of the 'Embarked' column in the combined dataset using Seaborn's `catplot` function to display the frequency of each category.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9461969}}], "notebook_id": 16, "notebook_name": "titanic-basic-solution-using-adaboost.ipynb"}], "metadata": {}}