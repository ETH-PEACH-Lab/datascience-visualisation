{"notebooks": [{"cells": [{"cell_id": 89, "code": "output = pd.DataFrame({'PassengerId': passenger_id, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Data Export", "desc": "The code creates a DataFrame 'output' with 'PassengerId' and 'Survived' columns based on the test predictions and saves it as a CSV file named 'submission.csv' without the index, and then prints a success message.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9988844}, "cluster": -1}, {"cell_id": 1, "code": "train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntrain_df.head()", "class": "Data Extraction", "desc": "The code reads a CSV file named 'train.csv' located in a specified directory into a pandas DataFrame and displays the first few rows of the data.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99962866}, "cluster": -1}, {"cell_id": 3, "code": "test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_df.head()", "class": "Data Extraction", "desc": "The code reads a CSV file named 'test.csv' located in a specified directory into a pandas DataFrame and displays the first few rows of the data.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996499}, "cluster": -1}, {"cell_id": 4, "code": "passenger_id = test_df['PassengerId']\npassenger_id", "class": "Data Extraction", "desc": "The code extracts the 'PassengerId' column from the 'test_df' DataFrame and stores it in the variable 'passenger_id', then displays its content.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.9961324}, "cluster": -1}, {"cell_id": 5, "code": "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n\ntrain_df = train_df[features + ['Survived']]\ntrain_df.head()", "class": "Data Transform", "desc": "The code selects specific columns ('Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked', and 'Survived') from the 'train_df' DataFrame and updates 'train_df' to include only these columns, then displays the first few rows of the resulting DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99079293}, "cluster": 5}, {"cell_id": 6, "code": "test_df = test_df[features]\ntest_df.head()", "class": "Data Transform", "desc": "The code selects specific columns ('Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', and 'Embarked') from the 'test_df' DataFrame and updates 'test_df' to include only these columns, then displays the first few rows of the resulting DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9990376}, "cluster": 5}, {"cell_id": 7, "code": "all_df = pd.concat((train_df, test_df), axis=0)", "class": "Data Transform", "desc": "The code concatenates the 'train_df' and 'test_df' DataFrames along the rows (axis=0) to form a combined DataFrame 'all_df'.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9995192}, "cluster": 8}, {"cell_id": 9, "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()", "class": "Data Transform", "desc": "The code splits the combined DataFrame 'all_df' back into 'train_df' and 'test_df' based on their original row indices, creating copies of each subset to ensure any future modifications do not affect the original data.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.89564276}, "cluster": 8}, {"cell_id": 18, "code": "print('Before encoding:\\n{}\\n'.format(all_df['Sex'].value_counts()))\nall_df['Sex'].replace({'male':0, 'female':1}, inplace=True)\nprint('After encoding:\\n{}'.format(all_df['Sex'].value_counts()))", "class": "Data Transform", "desc": "The code prints the value counts of the 'Sex' column before and after encoding the categorical values 'male' and 'female' into numerical values 0 and 1, respectively, within the 'all_df' DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9664374}, "cluster": 0}, {"cell_id": 19, "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()", "class": "Data Transform", "desc": "The code re-splits the combined DataFrame 'all_df' into 'train_df' and 'test_df' based on their original row indices, creating copies of each subset.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.89564276}, "cluster": 8}, {"cell_id": 25, "code": "all_df['Embarked'].fillna('S', inplace=True)\nall_df['Embarked'].isnull().any()", "class": "Data Transform", "desc": "The code fills missing values in the 'Embarked' column of the 'all_df' DataFrame with 'S' and checks if there are any remaining null values in this column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.993634}, "cluster": 1}, {"cell_id": 27, "code": "\"\"\"\n# We will encode `Embarked` with 'S' to 0, 'Q' to 1 and 'C' to 2\nprint('Before encoding:\\n{}\\n'.format(train_df['Embarked'].value_counts()))\ntrain_df['Embarked'].replace({'S':0, 'Q':1, 'C':2}, inplace=True)\nprint('After encoding:\\n{}'.format(train_df['Embarked'].value_counts()))\n\"\"\";", "class": "Data Transform", "desc": "The code prints the value counts of the 'Embarked' column before and after encoding the categorical values 'S', 'Q', and 'C' into numerical values 0, 1, and 2, respectively, within the 'train_df' DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.43857637}, "cluster": 0}, {"cell_id": 28, "code": "embark_dummies = pd.get_dummies(data=all_df['Embarked'], prefix='Embarked')\nembark_dummies.head()", "class": "Data Transform", "desc": "The code generates one-hot encoded dummy variables for the 'Embarked' column in the 'all_df' DataFrame and displays the first few rows of the resulting dummy DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9989791}, "cluster": 4}, {"cell_id": 29, "code": "all_df = pd.concat([all_df, embark_dummies], axis=1)\nall_df.drop(columns='Embarked', inplace=True)\nall_df.head()", "class": "Data Transform", "desc": "The code concatenates the one-hot encoded dummy variables for the 'Embarked' column to the 'all_df' DataFrame, drops the original 'Embarked' column, and displays the first few rows of the updated DataFrame.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.99898034}, "cluster": 4}, {"cell_id": 30, "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()", "class": "Data Transform", "desc": "The code re-splits the combined DataFrame 'all_df' back into 'train_df' and 'test_df' based on their original row indices, creating copies of each subset.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.89564276}, "cluster": 8}, {"cell_id": 32, "code": "all_df['Cabin'].fillna('U', inplace=True)\nall_df['Cabin'].isnull().any()", "class": "Data Transform", "desc": "The code fills missing values in the 'Cabin' column of the 'all_df' DataFrame with 'U' and checks if there are any remaining null values in this column.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.96726316}, "cluster": 1}, {"cell_id": 33, "code": "all_df['Cabin'] = all_df['Cabin'].apply(lambda x: x[0])\nall_df['Cabin'].value_counts()", "class": "Data Transform", "desc": "The code transforms the 'Cabin' values in the 'all_df' DataFrame by extracting the first character of each entry and then displays the frequency counts of these transformed values.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9995357}, "cluster": 3}, {"cell_id": 36, "code": "cabin_dummies = pd.get_dummies(data=all_df['Cabin'], prefix='Cabin')\ncabin_dummies.head()", "class": "Data Transform", "desc": "The code generates one-hot encoded dummy variables for the 'Cabin' column in the 'all_df' DataFrame and displays the first few rows of the resulting dummy DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99897337}, "cluster": 4}, {"cell_id": 37, "code": "all_df = pd.concat([all_df, cabin_dummies], axis=1)\nall_df.drop(columns='Cabin', inplace=True)\nall_df.head()", "class": "Data Transform", "desc": "The code concatenates the one-hot encoded dummy variables for the 'Cabin' column to the 'all_df' DataFrame, drops the original 'Cabin' column, and displays the first few rows of the updated DataFrame.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9989489}, "cluster": 4}, {"cell_id": 38, "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()", "class": "Data Transform", "desc": "The code re-splits the combined DataFrame 'all_df' back into 'train_df' and 'test_df' based on their original row indices, creating copies of each subset.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.89564276}, "cluster": 8}, {"cell_id": 43, "code": "\"\"\"\n# Choosing to impute with mean.\ntrain_df.groupby(by=['Pclass', 'Parch'])['Age'].transform('mean').isnull().sum()\ntrain_df['Age'].fillna(train_df.groupby(by=['Pclass', 'Sex', 'Parch'])['Age'].transform('mean'), inplace=True)\n\"\"\";", "class": "Data Transform", "desc": "The code fills missing values in the 'Age' column of the 'train_df' DataFrame using the mean 'Age' calculated from groupings based on ['Pclass', 'Sex', 'Parch'].", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.84770185}, "cluster": 0}, {"cell_id": 46, "code": "scaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(train_df)\nscaled_df = pd.DataFrame(data=scaled_df, columns=train_df.columns)\nscaled_df.head()", "class": "Data Transform", "desc": "The code applies MinMax scaling to the features in the 'train_df' DataFrame, transforms the scaled data into a DataFrame with the same column names, and displays the first few rows of the scaled DataFrame.", "testing": {"class": "Data_Transform", "subclass": "normalization", "subclass_id": 18, "predicted_subclass_probability": 0.9299693}, "cluster": 4}, {"cell_id": 47, "code": "imputer = KNNImputer(n_neighbors=3)\nimputed_df = imputer.fit_transform(scaled_df)\n#imputer_df = pd.DataFrame(data=imputed_df, columns=features)\nimputed_df = scaler.inverse_transform(imputed_df)\nimputed_df = pd.DataFrame(data=imputed_df, columns=train_df.columns)", "class": "Data Transform", "desc": "The code performs K-Nearest Neighbors imputation on the scaled DataFrame to fill in missing values, then inversely transforms the imputed data back to the original scale, and finally converts the imputed array into a DataFrame with the same column names as the original 'train_df'.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.37421712}, "cluster": 2}, {"cell_id": 49, "code": "train_df['Age'] = imputed_df['Age']", "class": "Data Transform", "desc": "The code updates the 'Age' column in the 'train_df' DataFrame with the imputed age values from 'imputed_df'.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99938726}, "cluster": 8}, {"cell_id": 50, "code": "scaled_df = scaler.transform(test_df)\nscaled_df = pd.DataFrame(data=scaled_df, columns=test_df.columns)\nscaled_df.head()", "class": "Data Transform", "desc": "The code applies the previously fitted MinMaxScaler to the 'test_df' DataFrame, transforms the scaled data into a DataFrame with the same column names, and displays the first few rows of the scaled DataFrame.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.26208633}, "cluster": 4}, {"cell_id": 51, "code": "imputed_df = imputer.transform(scaled_df)\n#imputer_df = pd.DataFrame(data=imputed_df, columns=all_df[891:].columns)\nimputed_df = scaler.inverse_transform(imputed_df)\nimputed_df = pd.DataFrame(data=imputed_df, columns=all_df.iloc[891:].columns)\nimputed_df.head()", "class": "Data Transform", "desc": "The code performs K-Nearest Neighbors imputation on the scaled 'test_df' DataFrame to fill in missing values, then inversely transforms the imputed data back to the original scale and converts the imputed array into a DataFrame with the same column names as the original 'all_df.iloc[891:]', and displays the first few rows of this imputed DataFrame.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.39289653}, "cluster": 2}, {"cell_id": 52, "code": "test_df['Age'] = imputed_df['Age']", "class": "Data Transform", "desc": "The code updates the 'Age' column in the 'test_df' DataFrame with the imputed age values from 'imputed_df'.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9993936}, "cluster": 8}, {"cell_id": 53, "code": "all_df.iloc[:891] = train_df.copy()\nall_df.iloc[891:] = test_df.copy()", "class": "Data Transform", "desc": "The code updates the 'all_df' DataFrame by copying the modified 'train_df' and 'test_df' into their respective original row indices within 'all_df'.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.49698666}, "cluster": 8}, {"cell_id": 54, "code": "all_df['Age_band'] = all_df['Age'].apply(lambda x: math.ceil(x / 5))\nall_df.sample(10)", "class": "Data Transform", "desc": "The code creates a new column 'Age_band' in the 'all_df' DataFrame by applying a lambda function that calculates the ceiling of each 'Age' value divided by 5, and it then displays a random sample of 10 rows from the updated DataFrame.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99901354}, "cluster": 4}, {"cell_id": 55, "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()", "class": "Data Transform", "desc": "The code re-splits the combined DataFrame 'all_df' back into 'train_df' and 'test_df' based on their original row indices, creating copies of each subset.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.89564276}, "cluster": 8}, {"cell_id": 60, "code": "all_df['Fare'].fillna(train_df.Fare.mean(), inplace=True)", "class": "Data Transform", "desc": "The code fills missing values in the 'Fare' column of the 'all_df' DataFrame with the mean 'Fare' value from the 'train_df' DataFrame.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.45188195}, "cluster": 8}, {"cell_id": 61, "code": "def create_fare_class(x):\n    if x >= 100:\n        fare_class = 1\n    elif x >= 80 and x < 100:\n        fare_class = 2\n    elif x >= 60 and x < 80:\n        fare_class = 3\n    elif x >= 40 and x < 60:\n        fare_class = 4\n    elif x >= 20 and x < 40:\n        fare_class = 5\n    else:\n        fare_class = 6\n\n    return fare_class", "class": "Data Transform", "desc": "The code defines a function `create_fare_class` that categorizes 'Fare' values into six different classes based on specified fare ranges.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.9775144}, "cluster": 7}, {"cell_id": 62, "code": "all_df['Fare_class'] = all_df['Fare'].apply(create_fare_class)\nall_df.head()", "class": "Data Transform", "desc": "The code creates a new column 'Fare_class' in the 'all_df' DataFrame by applying the `create_fare_class` function to categorize the 'Fare' values and then displays the first few rows of the updated DataFrame.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99655885}, "cluster": 9}, {"cell_id": 63, "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()", "class": "Data Transform", "desc": "The code re-splits the combined DataFrame 'all_df' back into 'train_df' and 'test_df' based on their original row indices, creating copies of each subset.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.89564276}, "cluster": 8}, {"cell_id": 69, "code": "X_train = train_df.drop(columns=['Age', 'Fare', 'Survived'])\nX_train.head()", "class": "Data Transform", "desc": "The code creates a new DataFrame 'X_train' by dropping the 'Age', 'Fare', and 'Survived' columns from the 'train_df' DataFrame and displays the first few rows of the resulting DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99889225}, "cluster": 5}, {"cell_id": 70, "code": "y_train = train_df['Survived'].astype('int')\ny_train.head()", "class": "Data Transform", "desc": "The code creates a new Series 'y_train' containing the 'Survived' column from the 'train_df' DataFrame, converting it to integer type, and displays the first few rows of the resulting Series.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.6704867}, "cluster": 4}, {"cell_id": 72, "code": "X_test = test_df.drop(columns=['Age', 'Fare', 'Survived'])\nX_test.head()", "class": "Data Transform", "desc": "The code creates a new DataFrame 'X_test' by dropping the 'Age', 'Fare', and 'Survived' columns from the 'test_df' DataFrame and displays the first few rows of the resulting DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.999015}, "cluster": 5}, {"cell_id": 74, "code": "cv_score_columns = ['logreg', 'rforest', 'adaboost', 'gboost', 'xg']\n# cv_score_columns = ['logreg', 'rforest', 'adaboost', 'gboost']\ncv_score_df = pd.DataFrame(columns=cv_score_columns)\ncv_score_df", "class": "Data Transform", "desc": "The code initializes a pandas DataFrame 'cv_score_df' with columns named after different classifiers ('logreg', 'rforest', 'adaboost', 'gboost', 'xg') meant to store cross-validation scores.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.99424237}, "cluster": 0}, {"cell_id": 2, "code": "train_df.sample(10)", "class": "Exploratory Data Analysis", "desc": "The code randomly selects and displays 10 rows from the 'train_df' DataFrame to provide a quick look at a subset of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99976045}, "cluster": 0}, {"cell_id": 8, "code": "all_df.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the combined DataFrame 'all_df' to provide an initial view of the concatenated data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997588}, "cluster": 0}, {"cell_id": 11, "code": "train_df.info()", "class": "Exploratory Data Analysis", "desc": "The code generates a concise summary of the 'train_df' DataFrame, providing information on the data types, non-null counts, and memory usage.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993624}, "cluster": 1}, {"cell_id": 12, "code": "train_df.describe()", "class": "Exploratory Data Analysis", "desc": "The code produces summary statistics for the numerical columns in the 'train_df' DataFrame, including measures like mean, standard deviation, min, and max values.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994492}, "cluster": 1}, {"cell_id": 13, "code": "print(train_df['Survived'].value_counts())\nprint(train_df['Survived'].value_counts(normalize=True))", "class": "Exploratory Data Analysis", "desc": "The code prints both the absolute and normalized (relative) frequency counts of the 'Survived' column in the 'train_df' DataFrame to understand the distribution of the target variable.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99950135}, "cluster": 1}, {"cell_id": 15, "code": "women = train_df.loc[train_df.Sex == 'female']['Survived']\nrate_women = sum(women)/len(women)\n\nprint('% of women who survived: {:.3f}'.format(rate_women))", "class": "Exploratory Data Analysis", "desc": "The code calculates and prints the survival rate of women in the 'train_df' DataFrame by filtering rows where 'Sex' is 'female' and computing the ratio of those who survived.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.31332606}, "cluster": 1}, {"cell_id": 16, "code": "men = train_df.loc[train_df.Sex == 'male']['Survived']\nrate_men = sum(men)/len(men)\n\nprint('% of men who survived: {:.3f}'.format(rate_men))", "class": "Exploratory Data Analysis", "desc": "The code calculates and prints the survival rate of men in the 'train_df' DataFrame by filtering rows where 'Sex' is 'male' and computing the ratio of those who survived.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.35764214}, "cluster": 1}, {"cell_id": 20, "code": "print(train_df['Pclass'].value_counts())\nprint(train_df['Pclass'].value_counts(normalize=True))", "class": "Exploratory Data Analysis", "desc": "The code prints both the absolute and normalized (relative) frequency counts of the 'Pclass' column in the 'train_df' DataFrame to understand the distribution of passenger classes.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9995016}, "cluster": 1}, {"cell_id": 23, "code": "train_df['Embarked'].value_counts()", "class": "Exploratory Data Analysis", "desc": "The code calculates and displays the frequency counts of the 'Embarked' column in the 'train_df' DataFrame to understand the distribution of embarkation points.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9995072}, "cluster": 1}, {"cell_id": 24, "code": "print('Percentage of null values in Embarked column: {:.3g}%'.format(train_df['Embarked'].isnull().sum() / len(train_df) * 100))\nprint('Percentage of null values in Embarked column: {:.3g}%'.format(test_df['Embarked'].isnull().sum() / len(test_df) * 100))", "class": "Exploratory Data Analysis", "desc": "The code calculates and prints the percentage of null values in the 'Embarked' column for both 'train_df' and 'test_df' DataFrames to understand the extent of missing data in this feature.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9974777}, "cluster": 1}, {"cell_id": 31, "code": "print('Percentage of null values in Cabin column: {:.3g}%'.format(train_df['Cabin'].isnull().sum() / len(train_df) * 100))\nprint('Percentage of null values in Cabin column: {:.3g}%'.format(test_df['Cabin'].isnull().sum() / len(test_df) * 100))", "class": "Exploratory Data Analysis", "desc": "The code calculates and prints the percentage of null values in the 'Cabin' column for both 'train_df' and 'test_df' DataFrames to understand the extent of missing data in this feature.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.98824114}, "cluster": 1}, {"cell_id": 34, "code": "all_df['Cabin'].unique()", "class": "Exploratory Data Analysis", "desc": "The code displays the unique values in the 'Cabin' column of the 'all_df' DataFrame to understand the variety of cabin designations.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.99807227}, "cluster": 1}, {"cell_id": 40, "code": "temp_df = train_df[train_df.Age.isnull()]\nprint(\"The number of null values in Age column is {}\".format(len(temp_df)))", "class": "Exploratory Data Analysis", "desc": "The code creates a temporary DataFrame 'temp_df' containing rows from 'train_df' where the 'Age' column is null and prints the number of such null values.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.39663118}, "cluster": 1}, {"cell_id": 45, "code": "train_df.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the 'train_df' DataFrame to provide an overview of the current state of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997545}, "cluster": 0}, {"cell_id": 48, "code": "imputed_df.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "The code calculates and displays the count of null values for each column in the 'imputed_df' DataFrame to ensure no missing data remains after imputation.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9989617}, "cluster": 1}, {"cell_id": 56, "code": "train_df[train_df.Fare==0]", "class": "Exploratory Data Analysis", "desc": "The code filters and displays the rows from the 'train_df' DataFrame where the 'Fare' value is zero to examine entries with potentially erroneous or missing fare data.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.97212875}, "cluster": 1}, {"cell_id": 66, "code": "all_df.columns", "class": "Exploratory Data Analysis", "desc": "The code displays the column names of the 'all_df' DataFrame to provide an overview of the data's structure.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9983329}, "cluster": 0}, {"cell_id": 68, "code": "train_df.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the 'train_df' DataFrame to provide an overview of the current state of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997545}, "cluster": 0}, {"cell_id": 71, "code": "test_df.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first few rows of the 'test_df' DataFrame to provide an overview of the current state of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997558}, "cluster": 0}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "The code imports several essential libraries including NumPy, pandas, math, and tqdm, and lists all input data files in a specified directory using the os library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.9992084}, "cluster": -1}, {"cell_id": 10, "code": "import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline", "class": "Imports and Environment", "desc": "The code imports the Matplotlib library as plt and the Seaborn library as sns for plotting and visualizations, and ensures that plots are displayed inline in Jupyter notebooks.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.9993888}, "cluster": -1}, {"cell_id": 44, "code": "from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import KNNImputer", "class": "Imports and Environment", "desc": "The code imports the MinMaxScaler from scikit-learn for feature scaling and the KNNImputer for missing value imputation using k-nearest neighbors.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.999292}, "cluster": -1}, {"cell_id": 73, "code": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nrandom_state=2", "class": "Imports and Environment", "desc": "The code imports several classification models including Logistic Regression, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, and XGBClassifier from XGBoost, as well as model evaluation tools like cross_val_score and GridSearchCV from scikit-learn, while setting a random state variable.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.84664047}, "cluster": -1}, {"cell_id": 76, "code": "models = [logreg, rforest, adaboost, gboost, xg]\ncv_scores = []\nfor model in models:\n    cvs_mean = (cross_val_score(model, X_train, y_train, cv=5)).mean()\n    cv_scores.append(cvs_mean)", "class": "Model Evaluation", "desc": "The code evaluates five different models (LogisticRegression, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, XGBClassifier) on the training set using 5-fold cross-validation and stores the mean cross-validation scores in the 'cv_scores' list.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.8604194}, "cluster": -1}, {"cell_id": 77, "code": "cv_score_df.loc['base'] = cv_scores\ncv_score_df", "class": "Model Evaluation", "desc": "The code adds the calculated mean cross-validation scores to the 'cv_score_df' DataFrame under the row labeled 'base' and displays the DataFrame.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9977927}, "cluster": 0}, {"cell_id": 80, "code": "best_model_df = pd.DataFrame(columns=cv_score_columns)\nbest_model_list = []\ncv_scores = []", "class": "Model Evaluation", "desc": "The code initializes an empty DataFrame 'best_model_df' with columns corresponding to different classifiers and creates two empty lists, 'best_model_list' to store the best models and 'cv_scores' to store their cross-validation scores.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.99682707}, "cluster": 1}, {"cell_id": 81, "code": "%%time\nfor m, p in tqdm(zip(gridsearch_dict['model_list'], gridsearch_dict['param_list'])):\n    clf = GridSearchCV(m, p)\n    clf.fit(X_train, y_train)\n    best_model_list.append({'best_model':clf.best_estimator_, 'best_param':clf.best_params_})\n    cv_scores.append(clf.best_score_)", "class": "Model Evaluation", "desc": "The code performs grid search cross-validation using GridSearchCV on each model with its corresponding parameter grid to find the best estimator and parameters, fitting the model to the training data, storing the best model and parameters in 'best_model_list', and the cross-validation scores in 'cv_scores', all while utilizing the tqdm library to display the progress.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.99058175}, "cluster": 1}, {"cell_id": 82, "code": "best_model_list", "class": "Model Evaluation", "desc": "The code outputs the 'best_model_list', which contains dictionaries of the best estimators and their respective best parameters obtained from the grid search cross-validation for each model.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9995352}, "cluster": -1}, {"cell_id": 83, "code": "cv_scores", "class": "Model Evaluation", "desc": "The code outputs the 'cv_scores' list, which contains the best cross-validation scores obtained from the grid search cross-validation for each model.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99977154}, "cluster": -1}, {"cell_id": 84, "code": "best_model_df.loc['best'] = best_model_list\ncv_score_df.loc['best'] = cv_scores", "class": "Model Evaluation", "desc": "The code adds the best model estimators to the 'best_model_df' DataFrame under the row labeled 'best' and the best cross-validation scores to the 'cv_score_df' DataFrame under the row labeled 'best'.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.8962994}, "cluster": -1}, {"cell_id": 85, "code": "cv_score_df", "class": "Model Evaluation", "desc": "The code outputs the 'cv_score_df' DataFrame, which contains the mean cross-validation scores and the best cross-validation scores for each model.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997156}, "cluster": -1}, {"cell_id": 88, "code": "predictions = model_best.predict(X_test)", "class": "Model Evaluation", "desc": "The code uses the trained best RandomForestClassifier model to make predictions on the 'X_test' dataset and stores the predicted values in the 'predictions' variable.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9945082}, "cluster": 0}, {"cell_id": 75, "code": "logreg = LogisticRegression(solver='liblinear', random_state=random_state)\nrforest = RandomForestClassifier(random_state=random_state)\nadaboost = AdaBoostClassifier(random_state=random_state)\ngboost = GradientBoostingClassifier(random_state=random_state)\nxg = xgb.XGBClassifier(use_label_encoder=False, random_state=random_state)", "class": "Model Training", "desc": "The code initializes five classifier models: LogisticRegression, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, and XGBClassifier, each with the specified random state for reproducibility.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.97994995}, "cluster": -1}, {"cell_id": 78, "code": "logreg = LogisticRegression(solver='liblinear', random_state=random_state)\nrforest = RandomForestClassifier(random_state=random_state)\nadaboost = AdaBoostClassifier(random_state=random_state)\ngboost = GradientBoostingClassifier(random_state=random_state)\nxg = xgb.XGBClassifier(use_label_encoder=False, random_state=random_state)", "class": "Model Training", "desc": "The code reinitializes the LogisticRegression, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, and XGBClassifier models, each with the specified random state for reproducibility.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.97994995}, "cluster": -1}, {"cell_id": 79, "code": "gridsearch_dict = {'model_list':models,\n                   'param_list':[{'penalty':('l1', 'l2'), 'C':(.001, .01, .1)},\n                                 {'n_estimators':(100, 200, 300),'criterion':('gini', 'entropy'), 'max_depth':(2, 3, 4, 5)},\n                                 {'n_estimators':(50, 100, 200, 300), 'learning_rate':(.001, .01, .1)},\n                                 {'loss':('deviance', 'exponential'), 'learning_rate':(.001, .01), 'n_estimators':(100, 200, 300)},\n                                 {'n_estimators':(100, 200, 300), 'learning_rate':(.001, .01, .1)}\n                                ]\n                   }", "class": "Model Training", "desc": "The code creates a dictionary 'gridsearch_dict' containing the list of initialized models and a corresponding list of parameter grids for each model, which will be used for hyperparameter tuning during the grid search process.", "testing": {"class": "Model_Train", "subclass": "define_search_space", "subclass_id": 5, "predicted_subclass_probability": 0.9896735}, "cluster": 0}, {"cell_id": 86, "code": "model_best = best_model_df.iloc[0]['rforest']['best_model']", "class": "Model Training", "desc": "The code selects the best RandomForestClassifier model from the 'best_model_df' DataFrame.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.913111}, "cluster": -1}, {"cell_id": 87, "code": "model_best.fit(X_train, y_train)", "class": "Model Training", "desc": "The code trains the best RandomForestClassifier model using the training data, 'X_train' and 'y_train'.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9997105}, "cluster": 0}, {"cell_id": 14, "code": "sns.countplot(x=train_df['Survived'])\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to visualize the distribution of the 'Survived' column values in the 'train_df' DataFrame and displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99577636}, "cluster": 1}, {"cell_id": 17, "code": "sns.countplot(x='Survived', data=train_df, hue='Sex' )\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to visualize the count of 'Survived' values in the 'train_df' DataFrame, separated by the 'Sex' attribute, and displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99815875}, "cluster": 1}, {"cell_id": 21, "code": "sns.countplot(x='Survived', data=train_df, hue='Pclass')\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to visualize the count of 'Survived' values in the 'train_df' DataFrame, separated by the 'Pclass' attribute, and displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99799454}, "cluster": 1}, {"cell_id": 22, "code": "sns.catplot(x='Pclass', data=train_df, hue='Sex',\n            col='Survived', kind='count')\nplt.show()", "class": "Visualization", "desc": "The code creates a categorical plot using Seaborn to visualize the count of 'Pclass' values in the 'train_df' DataFrame, separated by 'Sex' and faceted by 'Survived', and displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9954602}, "cluster": 1}, {"cell_id": 26, "code": "sns.countplot(x='Embarked', data=train_df, hue='Survived')\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to visualize the count of 'Embarked' values in the 'train_df' DataFrame, separated by 'Survived', and displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9850413}, "cluster": 1}, {"cell_id": 35, "code": "sns.countplot(x='Cabin', data=all_df.iloc[:891], hue='Survived')\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to visualize the count of 'Cabin' values in the 'train_df' subset of the 'all_df' DataFrame, separated by 'Survived', and displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9963553}, "cluster": 1}, {"cell_id": 39, "code": "plt.figure(figsize=(15, 4))\nplt.subplot(121)\nsns.histplot(x='Age', data=train_df, hue='Survived', multiple='stack')\nplt.subplot(122)\nsns.histplot(x='Age', data=train_df, hue='Survived', element='poly')\n#sns.histplot(x='Age', data=train_df, ax=ax2, color='g')\nplt.show()", "class": "Visualization", "desc": "The code creates a figure with two subplots using Seaborn and Matplotlib, where it displays histograms of the 'Age' distribution in the 'train_df' DataFrame separated by 'Survived', with the first subplot showing stacked histograms and the second subplot showing histograms with overlapping elements, then displays the plots.", "testing": {"class": "Visualization", "subclass": "time_series", "subclass_id": 75, "predicted_subclass_probability": 0.8408765}, "cluster": 1}, {"cell_id": 41, "code": "plt.figure(figsize=(18, 10))\nplt.subplot(231)\nsns.countplot(x='Pclass', data=temp_df)\nplt.subplot(232)\nsns.countplot(x='Sex', data=temp_df)\nplt.subplot(233)\nsns.countplot(x='SibSp', data=temp_df)\nplt.subplot(234)\nsns.countplot(x='Parch', data=temp_df)\nplt.subplot(235)\nsns.histplot(x='Fare', data=temp_df)\nplt.show()", "class": "Visualization", "desc": "The code creates a figure with multiple subplots using Seaborn and Matplotlib to display the distributions of 'Pclass', 'Sex', 'SibSp', 'Parch', and 'Fare' within the subset of rows from 'train_df' where the 'Age' column is null, and then displays the plots.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99794}, "cluster": 0}, {"cell_id": 42, "code": "\"\"\"\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.histplot(data=train_df.groupby(by=['Pclass', 'Parch', 'SibSp'])['Age'].transform('median'), ax=ax1, bins=20, kde=True, color='b')\nsns.histplot(data=train_df.Age, ax=ax1, kde=True, color='r')\n\nsns.histplot(data=train_df.groupby(by=['Pclass', 'Parch', 'Sex'])['Age'].transform('median'), ax=ax2, bins=20, kde=True, color='b')\nsns.histplot(data=train_df.Age, ax=ax2, kde=True, color='r')\n\nplt.show()\n\"\"\";", "class": "Visualization", "desc": "The code creates a figure with two subplots using Seaborn and Matplotlib to display histograms and KDE plots comparing the overall distribution of 'Age' in 'train_df' with the distributions of median 'Age' values grouped by ['Pclass', 'Parch', 'SibSp'] in the first subplot and ['Pclass', 'Parch', 'Sex'] in the second subplot, and then displays the plots.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9966239}, "cluster": 0}, {"cell_id": 57, "code": "plt.figure(figsize=(18, 12))\nplt.subplot(211)\nsns.histplot(x='Fare', data=train_df, bins=50, hue='Survived', multiple='stack')\nplt.subplot(212)\nsns.histplot(x='Fare', data=train_df, bins=50, hue='Survived', element='poly')\nplt.show()", "class": "Visualization", "desc": "The code creates a figure with two subplots using Seaborn and Matplotlib to display histograms of the 'Fare' distribution in the 'train_df' DataFrame separated by 'Survived', with the first subplot showing stacked histograms and the second subplot showing histograms with overlapping elements, and then displays the plots.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9983432}, "cluster": 1}, {"cell_id": 58, "code": "plt.figure(figsize=(18, 6))\nsns.scatterplot(x='Age', y='Fare', data=train_df, hue='Survived', size='Fare', sizes=(20, 150))\nplt.show()", "class": "Visualization", "desc": "The code creates a scatter plot using Seaborn to visualize the relationship between 'Age' and 'Fare' in the 'train_df' DataFrame, with points colored by 'Survived' status and sized proportionally to 'Fare', then displays the plot.", "testing": {"class": "Visualization", "subclass": "time_series", "subclass_id": 75, "predicted_subclass_probability": 0.97067386}, "cluster": 1}, {"cell_id": 59, "code": "plt.figure(figsize=(18, 6))\nax = plt.subplot()\n\nax.scatter(train_df[train_df.Survived==1]['Age'], train_df[train_df.Survived==1]['Fare'],\n           c='g', s=train_df[train_df.Survived==1]['Fare'])\nax.scatter(train_df[train_df.Survived==0]['Age'], train_df[train_df.Survived==0]['Fare'],\n           c='r', s=train_df[train_df.Survived==0]['Fare'])\nplt.show()", "class": "Visualization", "desc": "The code creates a scatter plot using Matplotlib to visualize the relationship between 'Age' and 'Fare' in the 'train_df' DataFrame, with points colored green for those who survived and red for those who did not survive, and sized proportionally to 'Fare', then displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9976399}, "cluster": 1}, {"cell_id": 64, "code": "sns.countplot(x='Fare_class', data=all_df, hue='Survived')\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to visualize the count of 'Fare_class' values in the 'all_df' DataFrame, separated by 'Survived', and displays the plot.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.4513631}, "cluster": 1}, {"cell_id": 65, "code": "sns.countplot(x='SibSp', data=train_df, hue='Survived')\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to visualize the count of 'SibSp' values in the 'train_df' DataFrame, separated by 'Survived', and displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99788505}, "cluster": 1}, {"cell_id": 67, "code": "sns.countplot(x='Parch', data=train_df, hue='Survived')\nplt.show()", "class": "Visualization", "desc": "The code creates a count plot using Seaborn to visualize the count of 'Parch' values in the 'train_df' DataFrame, separated by 'Survived', and displays the plot.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99725753}, "cluster": 1}], "notebook_id": 0, "notebook_name": "titanic-submissions.ipynb", "user": "titanic-submissions.ipynb"}, {"cells": [{"cell_id": 37, "code": "output = pd.DataFrame({'PassengerId':test_df.PassengerId, 'Survived':logistic_pred})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Data Export", "desc": "This code snippet creates a DataFrame with 'PassengerId' and the predicted 'Survived' values, and exports it to a CSV file named 'submission.csv' using `pandas`.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99903464}, "cluster": -1}, {"cell_id": 1, "code": "train_df = pd.read_csv('../input/titanic/train.csv')\ntest_df = pd.read_csv('../input/titanic/test.csv')", "class": "Data Extraction", "desc": "This code snippet loads the training and testing datasets from CSV files using the `pandas` library.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99975973}, "cluster": -1}, {"cell_id": 20, "code": "features = ['Pclass', 'Sex', 'SibSp', 'Parch']\nX = train_df[features]\ny = train_df['Survived']", "class": "Data Extraction", "desc": "This code snippet selects the features 'Pclass', 'Sex', 'SibSp', and 'Parch' as `X` and the target 'Survived' as `y` from the training DataFrame.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.999345}, "cluster": -1}, {"cell_id": 35, "code": "test_df[features]", "class": "Data Extraction", "desc": "This code snippet extracts the specified feature columns 'Pclass', 'Sex', 'SibSp', and 'Parch' from the test DataFrame using `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99951756}, "cluster": -1}, {"cell_id": 17, "code": "from sklearn.preprocessing import LabelEncoder\n\nclass FE:\n    def __init__(self, dataframes):\n        self.dataframes = dataframes\n        \n    def remove_cols(self):\n        for df in self.dataframes:\n            try:\n                df.drop(['Cabin', 'Name', 'Ticket'], axis=1, inplace=True)\n            except:\n                pass\n        \n    def convert_sex(self):\n        for df in self.dataframes:\n            le = LabelEncoder()\n            df.Sex = le.fit_transform(df.Sex)\n            ", "class": "Data Transform", "desc": "This code snippet defines a class `FE` for feature engineering, which includes methods to remove specified columns and convert the 'Sex' column to numerical values using the `LabelEncoder` from `sklearn.preprocessing`.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9992322}, "cluster": 2}, {"cell_id": 18, "code": "fe = FE([train_df, test_df])\nfe.remove_cols()\nfe.convert_sex()", "class": "Data Transform", "desc": "This code snippet creates an instance of the `FE` class and applies the `remove_cols` and `convert_sex` methods to remove certain columns and encode the 'Sex' column in both the training and testing DataFrames.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.6193043}, "cluster": 5}, {"cell_id": 21, "code": "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=2)", "class": "Data Transform", "desc": "This code snippet splits the dataset into training and testing sets with 25% of the data reserved for testing, using the `train_test_split` function from `sklearn.model_selection`.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.99821234}, "cluster": 2}, {"cell_id": 2, "code": "print(train_df.shape, test_df.shape)", "class": "Exploratory Data Analysis", "desc": "This code snippet prints the dimensions (number of rows and columns) of the training and testing datasets using the `shape` attribute of the DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9990921}, "cluster": 1}, {"cell_id": 3, "code": "train_df.info()", "class": "Exploratory Data Analysis", "desc": "This code snippet provides a concise summary of the training DataFrame, including the data types and non-null counts for each column, using the `info()` method from the `pandas` library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993624}, "cluster": 1}, {"cell_id": 4, "code": "train_df.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates descriptive statistics for the numeric columns in the training DataFrame using the `describe()` method from the `pandas` library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994492}, "cluster": 1}, {"cell_id": 5, "code": "train_df.isna().sum()", "class": "Exploratory Data Analysis", "desc": "This code snippet counts the number of missing values in each column of the training DataFrame using the `isna().sum()` method chain from the `pandas` library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9990694}, "cluster": 1}, {"cell_id": 6, "code": "train_df.duplicated().sum()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates the number of duplicate rows in the training DataFrame using the `duplicated().sum()` method chain from the `pandas` library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_duplicates", "subclass_id": 38, "predicted_subclass_probability": 0.8890608}, "cluster": 1}, {"cell_id": 12, "code": "sexes = train_df.Sex.unique()\ncount = list()\n\nfor sex in sexes:\n    percentage = round(len(train_df[(train_df.Sex == sex) & (train_df.Survived == 1)]) / len(train_df[train_df.Sex == sex]), 3) * 100\n    count.append(percentage)\n    \nprint(count) ", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the survival rate percentages for each unique value in the 'Sex' column of the training DataFrame by iterating through unique sexes and computing the ratio of survivors to the total count per sex.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.86492133}, "cluster": 1}, {"cell_id": 14, "code": "p_classes = train_df.Pclass.unique()\npcts = list()\n\nfor p in p_classes:\n    percentage = round(len(train_df[(train_df.Pclass == p) & (train_df.Survived == 1)]) / len(train_df[train_df.Pclass == p]),3) * 100\n    pcts.append(percentage)\n    \nprint(pcts) ", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the survival rate percentages for each unique value in the 'Pclass' column of the training DataFrame by iterating through the unique passenger classes and computing the ratio of survivors to the total count per class.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.8156362}, "cluster": 1}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This code snippet imports essential libraries including `numpy`, `pandas`, `matplotlib`, `seaborn`, and `missingno`, configures warning settings, and lists files in the Kaggle input directory.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.9989272}, "cluster": -1}, {"cell_id": 19, "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report", "class": "Imports and Environment", "desc": "This code snippet imports `train_test_split` and `cross_val_score` from `sklearn.model_selection`, and `accuracy_score` and `classification_report` from `sklearn.metrics` for use in model evaluation and training.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99933124}, "cluster": -1}, {"cell_id": 28, "code": "df_score = pd.DataFrame({'model':['Logistic Regression', 'KNN', 'Random Forest Classifier', 'Decision Tree Classifier', 'Support Vector Machine', 'Naive Bayes'],\n                        'accuracy':[lr_acc, knn_acc, rfc_acc, dtc_acc, clf_acc, gnb_acc]})", "class": "Model Evaluation", "desc": "This code snippet creates a DataFrame to store and compare the accuracy scores of different models, including Logistic Regression, KNN, Random Forest Classifier, Decision Tree Classifier, Support Vector Machine, and Naive Bayes, using `pandas`.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.9986626}, "cluster": -1}, {"cell_id": 29, "code": "df_score.sort_values('accuracy', ascending=False)", "class": "Model Evaluation", "desc": "This code snippet sorts the DataFrame `df_score` in descending order based on the 'accuracy' column to identify the model with the highest accuracy using the `sort_values` method from the `pandas` library.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9950781}, "cluster": -1}, {"cell_id": 30, "code": "df_score.accuracy.mean()", "class": "Model Evaluation", "desc": "This code snippet calculates and prints the mean accuracy of the different models by computing the average of the 'accuracy' column in the `df_score` DataFrame using the `mean` method from the `pandas` library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.6630979}, "cluster": -1}, {"cell_id": 31, "code": "model_names = ['Logistic Regression', 'KNN', 'Random Forest Classifier', 'Decision Tree Classifier', 'Support Vector Machine', 'Naive Bayes']\nmodel_selectors = [lr, knn, rfc, dtc, clf, gnb]\nscores_dict = {'model':[], 'accuracy':[]}\n\nfor label, selector in zip(model_names, model_selectors):\n    cv_mean_score = np.mean(cross_val_score(selector, X, y, cv=5))\n    scores_dict['model'].append(label)\n    scores_dict['accuracy'].append(cv_mean_score)    \n    \ndf_cv_score = pd.DataFrame(scores_dict)", "class": "Model Evaluation", "desc": "This code snippet performs 5-fold cross-validation for each of the listed models to compute their mean cross-validation scores, stores these scores in a dictionary, and creates a DataFrame `df_cv_score` to compare the models' accuracy using `cross_val_score` from `sklearn.model_selection` and `numpy`.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.6588713}, "cluster": 1}, {"cell_id": 32, "code": "df_cv_score.sort_values('accuracy', ascending=False)", "class": "Model Evaluation", "desc": "This code snippet sorts the DataFrame `df_cv_score` in descending order based on the 'accuracy' column to determine which model has the highest average cross-validation accuracy using the `sort_values` method from the `pandas` library.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9968382}, "cluster": -1}, {"cell_id": 33, "code": "df_cv_score.accuracy.mean()", "class": "Model Evaluation", "desc": "This code snippet calculates and prints the mean of the cross-validation accuracy scores for all the models by computing the average of the 'accuracy' column in the `df_cv_score` DataFrame using the `mean` method from the `pandas` library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.6315649}, "cluster": -1}, {"cell_id": 22, "code": "from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(train_x, train_y)\n\nlr_pred = lr.predict(test_x)\n\nlr_acc = accuracy_score(lr_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Logistic Regression model using the training data, generates predictions on the test data, and calculates the accuracy score using `LogisticRegression`, `fit`, `predict` from `sklearn.linear_model`, and `accuracy_score` from `sklearn.metrics`.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.78042775}, "cluster": -1}, {"cell_id": 23, "code": "from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=12)\nknn.fit(train_x, train_y)\n\nknn_pred = knn.predict(test_x)\n\nknn_acc = accuracy_score(knn_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a K-Nearest Neighbors classifier with 12 neighbors using the training data, generates predictions on the test data, and calculates the accuracy score using `KNeighborsClassifier` from `sklearn.neighbors` and `accuracy_score` from `sklearn.metrics`.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.7452794}, "cluster": -1}, {"cell_id": 24, "code": "from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(train_x, train_y)\n\nrfc_pred = rfc.predict(test_x)\n\nrfc_acc = accuracy_score(rfc_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Random Forest classifier with 100 estimators using the training data, generates predictions on the test data, and calculates the accuracy score using `RandomForestClassifier` from `sklearn.ensemble` and `accuracy_score` from `sklearn.metrics`.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.8482741}, "cluster": -1}, {"cell_id": 25, "code": "from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(train_x, train_y)\n\ndtc_pred = dtc.predict(test_x)\n\ndtc_acc = accuracy_score(dtc_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Decision Tree classifier using the training data, generates predictions on the test data, and calculates the accuracy score using `DecisionTreeClassifier` from `sklearn.tree` and `accuracy_score` from `sklearn.metrics`.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.6479414}, "cluster": -1}, {"cell_id": 26, "code": "from sklearn.svm import SVC\n\nclf = SVC(kernel='linear')\nclf.fit(train_x, train_y)\n\nclf_pred = clf.predict(test_x)\n\nclf_acc = accuracy_score(clf_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Support Vector Classifier with a linear kernel using the training data, generates predictions on the test data, and calculates the accuracy score using `SVC` from `sklearn.svm` and `accuracy_score` from `sklearn.metrics`.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.64344877}, "cluster": -1}, {"cell_id": 27, "code": "from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(train_x, train_y)\n\ngnb_pred = gnb.predict(test_x)\n\ngnb_acc = accuracy_score(gnb_pred, test_y)", "class": "Model Training", "desc": "This code snippet trains a Gaussian Naive Bayes classifier using the training data, generates predictions on the test data, and calculates the accuracy score using `GaussianNB` from `sklearn.naive_bayes` and `accuracy_score` from `sklearn.metrics`.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.87931037}, "cluster": -1}, {"cell_id": 34, "code": "from sklearn.model_selection import GridSearchCV\n\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\nlogistic = GridSearchCV(estimator=model, param_grid={'solver':solvers, 'penalty':penalty, 'C':c_values}, cv=5)\nlogistic.fit(train_x, train_y)\n\ngrid_info = pd.DataFrame(logistic.cv_results_)\ngrid_info[['mean_test_score', 'param_solver', 'param_penalty', 'param_C']].sort_values('mean_test_score', ascending=False)", "class": "Model Training", "desc": "This code snippet performs hyperparameter tuning on a Logistic Regression model using `GridSearchCV` to optimize the solver and regularization parameters, fits the optimized model to the training data, and creates a DataFrame to display the cross-validation results sorted by the mean test score using `pandas`.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.98615915}, "cluster": 0}, {"cell_id": 36, "code": "logistic_pred = logistic.predict(test_df[features])\nlogistic_pred", "class": "Model Training", "desc": "This code snippet generates predictions for the test DataFrame using the best-estimated Logistic Regression model found through GridSearchCV.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9945187}, "cluster": 0}, {"cell_id": 7, "code": "msno.matrix(train_df)", "class": "Visualization", "desc": "This code snippet generates a matrix visualization of missing data in the training DataFrame using the `matrix()` function from the `missingno` library.", "testing": {"class": "Visualization", "subclass": "missing_values", "subclass_id": 34, "predicted_subclass_probability": 0.9554358}, "cluster": 1}, {"cell_id": 8, "code": "corr = train_df.corr()\nplt.figure(figsize=(15,8))\nsns.heatmap(corr, annot=True)", "class": "Visualization", "desc": "This code snippet calculates the correlation matrix of the training DataFrame using the `corr()` method from `pandas`, and then visualizes it with a heatmap using the `seaborn` library combined with `matplotlib` for figure sizing.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.99775463}, "cluster": 0}, {"cell_id": 9, "code": "for col in ['Age', 'SibSp', 'Parch', 'Fare']:\n    plt.title(col)\n    sns.boxplot(train_df[col])\n    plt.show()", "class": "Visualization", "desc": "This code snippet creates and displays boxplots for the 'Age', 'SibSp', 'Parch', and 'Fare' columns in the training DataFrame using the `boxplot()` function from the `seaborn` library.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9927158}, "cluster": 0}, {"cell_id": 10, "code": "fig, axes = plt.subplots(3,3, figsize=(20,15))\n\nsns.countplot(ax=axes[0,0], data=train_df, x='Survived')\nsns.countplot(ax=axes[0,1], data=train_df, x='Sex')\nsns.countplot(ax=axes[0,2], data=train_df, x='Pclass')\nsns.histplot(ax=axes[1,0], data=train_df, x='Age')\nsns.histplot(ax=axes[1,1], data=train_df, x='Fare')\nsns.countplot(ax=axes[1,2], data=train_df, x='SibSp')\nsns.countplot(ax=axes[2,0], data=train_df, x='Parch')\nsns.countplot(ax=axes[2,1], data=train_df, x='Embarked')\n\nplt.tight_layout()\nplt.show()", "class": "Visualization", "desc": "This code snippet creates a 3x3 grid of subplots to visualize the distributions of various features in the training DataFrame, using `countplot` for categorical variables and `histplot` for numerical variables from the `seaborn` library, and adjusts the layout using `matplotlib`.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9985378}, "cluster": 0}, {"cell_id": 11, "code": "sns.relplot(data=train_df, x='Age', y='Fare', hue='Sex', col='Survived') ", "class": "Visualization", "desc": "This code snippet creates a relational plot to visualize the relationship between 'Age' and 'Fare' variables, with points colored by 'Sex' and faceted by 'Survived', using the `relplot()` function from the `seaborn` library.", "testing": {"class": "Visualization", "subclass": "time_series", "subclass_id": 75, "predicted_subclass_probability": 0.96493864}, "cluster": 0}, {"cell_id": 13, "code": "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(17,6))\nfig.suptitle('Survival Rates Between Males and Females\\n', fontsize=20)\n\nsns.countplot(ax=ax1, data=train_df, x='Sex', hue='Survived', edgecolor='black')\nax1.set_xticklabels(['Male','Female'])\nax1.legend(labels=['False', 'True'], title='Survived\\n')\nax1.set_xlabel('')\nax1.set_ylabel('Count')\n\nplt.pie(count, labels=['Male', 'Female'], explode=[0, 0.2], shadow=True, autopct='%1.1f%%', wedgeprops={'edgecolor':'black'})\n\nplt.tight_layout()", "class": "Visualization", "desc": "This code snippet creates a dual visualization: a count plot showing the count of survivors and non-survivors by sex using `seaborn` and `matplotlib`, and a pie chart displaying the survival rate percentages for each sex using `matplotlib`.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99289554}, "cluster": -1}, {"cell_id": 15, "code": "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(17,6))\nfig.suptitle('Survival Rates in Different Ship Classes\\n', fontsize=20)\n\nsns.countplot(ax=ax1, data=train_df, x='Pclass', hue='Survived', edgecolor='black')\nax1.set_xticklabels(['First','Second', 'Third'])\nax1.legend(labels=['False', 'True'], title='Survived\\n')\nax1.set_xlabel('')\nax1.set_ylabel('Count')\n\nplt.pie(pcts, labels=['Third', 'First', 'Second'], explode=[0, 0., 0.2], shadow=True, autopct='%1.1f%%', startangle=90, wedgeprops={'edgecolor':'black'})\n\nplt.tight_layout()", "class": "Visualization", "desc": "This code snippet creates a dual visualization: a count plot showing the count of survivors and non-survivors by passenger class using `seaborn` and `matplotlib`, and a pie chart displaying the survival rate percentages for each class using `matplotlib`.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.98514646}, "cluster": -1}, {"cell_id": 16, "code": "plt.figure(figsize=(15,7))\nsns.histplot(data=train_df, x='Age', hue='Survived', multiple='stack')\nplt.show()", "class": "Visualization", "desc": "This code snippet generates a stacked histogram to visualize the distribution of ages, differentiated by survival status, using the `histplot()` function from the `seaborn` library.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99855167}, "cluster": 1}], "notebook_id": 1, "notebook_name": "titanic-prediction.ipynb", "user": "titanic-prediction.ipynb"}, {"cells": [{"cell_id": 41, "code": "#take backup for later use\n\ntrain_copy = combined[:train_len]\n\ntest_copy = combined[train_len:].reset_index(drop=True)\n\ntest_copy.drop(columns=['Survived'],inplace=True)", "class": "Data Export", "desc": "This code creates backup copies of the combined DataFrame, splitting it back into training and test sets, and drops the 'Survived' column from the test set for later use.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.69678193}, "cluster": -1}, {"cell_id": 52, "code": "y_test_rfc = rfc.predict(test).astype(int)\n\ntest_out = pd.concat([test_copy['PassengerId'],pd.Series(y_test_rfc,name=\"Survived\")],axis=1)\n\ntest_out['Survived'] = test_out['Survived'].astype('int')\n\ntest_out.to_csv('submission.csv',index=False)", "class": "Data Export", "desc": "This code snippet uses the trained Random Forest classifier to predict survival on the test set, concatenates the predictions with the 'PassengerId' column, and exports the results to a CSV file named 'submission.csv'.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9994411}, "cluster": -1}, {"cell_id": 3, "code": "train_df=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")", "class": "Data Extraction", "desc": "The code reads the training and test datasets from CSV files located in the specified directory into pandas DataFrame objects.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99974495}, "cluster": -1}, {"cell_id": 4, "code": "train_len = len(train_df)\n\ncombined = train_df.append(test_df,ignore_index=True)\n\ncombined.fillna(np.nan)", "class": "Data Transform", "desc": "The code snippet concatenates the training and test DataFrames into a single DataFrame, assigns it to the variable `combined`, and replaces missing values with NaN using numpy.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.99917126}, "cluster": 9}, {"cell_id": 13, "code": "combined['AgeGroup'] = 'adult'\n\ncombined.loc[combined['Name'].str.contains('Master'),'AgeGroup'] = \"child\"\n\ncombined.loc[combined['Age'] <= 14.0,'AgeGroup'] = \"child\"\n\ncombined.loc[(combined['Age'].isnull()) & (combined['Name'].str.contains('Miss')) & (combined['Parch'] != 0) ,'AgeGroup'] = \"child\"", "class": "Data Transform", "desc": "The code snippet creates a new column 'AgeGroup' in the combined DataFrame, with default values set to 'adult', and categorizes certain rows as 'child' based on criteria related to the 'Name', 'Age', and 'Parch' columns.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9986811}, "cluster": 6}, {"cell_id": 15, "code": "def Age(cols):\n\n    Age=cols[0]\n\n    Pclass=cols[1]\n\n    Sex=cols[2]\n\n    AgeGroup=cols[3]\n\n    if pd.isnull(Age):\n\n        if Pclass==1:\n\n            if Sex==\"male\":\n\n                if AgeGroup=='adult':\n\n                    return 42\n\n                else:\n\n                    return 7\n\n            elif Sex==\"female\":\n\n                if AgeGroup=='adult':\n\n                    return 37\n\n                else:\n\n                    return 8\n\n        elif Pclass==2:\n\n            if Sex==\"male\":\n\n                if AgeGroup=='adult':\n\n                    return 33\n\n                else:\n\n                    return 4\n\n            elif Sex==\"female\":\n\n                if AgeGroup=='adult':\n\n                    return 31\n\n                else:\n\n                    return 7\n\n        elif Pclass==3:\n\n            if Sex==\"male\":\n\n                if AgeGroup=='adult':\n\n                    return 29\n\n                else:\n\n                    return 7\n\n            elif Sex==\"female\":\n\n                if AgeGroup=='adult':\n\n                    return 27\n\n                else:\n\n                    return 5\n\n    else:\n\n        return Age\n\n    \n\ncombined[\"Age\"]=combined[[\"Age\",\"Pclass\",\"Sex\",\"AgeGroup\"]].apply(Age,axis=1)", "class": "Data Transform", "desc": "This code defines an `Age` function to impute missing age values based on passenger class, gender, and age group averages, then applies this function to the combined DataFrame to fill in the missing 'Age' values.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.980082}, "cluster": 4}, {"cell_id": 16, "code": "def AgeBand(col):\n\n    Age=col[0]\n\n    if Age <=7:\n\n        return \"0-7\"\n\n    elif Age <=14:\n\n        return \"8-14\"\n\n    elif Age <=21:\n\n        return \"15-21\"\n\n    elif Age <= 28:\n\n        return \"22-28\"\n\n    elif Age <= 35:\n\n        return \"29-35\"\n\n    elif Age <= 42:\n\n        return \"36-42\"\n\n    elif Age <= 49:\n\n        return \"43-49\"\n\n    elif Age <= 56:\n\n        return \"50-56\"\n\n    elif Age <= 63:\n\n        return \"57-63\"\n\n    else:\n\n        return \">=64\"\n\n\n\ncombined[\"AgeBand\"]=combined[[\"Age\"]].apply(AgeBand,axis=1)", "class": "Data Transform", "desc": "The code defines an `AgeBand` function to categorize ages into specified ranges and applies this function to create a new 'AgeBand' column in the combined DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.98803115}, "cluster": 4}, {"cell_id": 21, "code": "combined[combined['Embarked'].isnull()]['Embarked'] = combined['Embarked'].mode()", "class": "Data Transform", "desc": "The code fills the missing 'Embarked' values in the combined DataFrame with the most frequent value (mode) from the 'Embarked' column.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99778533}, "cluster": 1}, {"cell_id": 25, "code": "ticketCount = combined.groupby('Ticket')['PassengerId'].count().reset_index()\n\nticketCount.rename(columns={'PassengerId':'Count on Ticket'},inplace=True)\n\ncombined = combined.merge(ticketCount, on=\"Ticket\",how=\"left\")", "class": "Data Transform", "desc": "This code snippet calculates the number of passengers with the same ticket number, renames the resulting count as 'Count on Ticket', and merges this data back into the combined DataFrame.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.9985952}, "cluster": 3}, {"cell_id": 26, "code": "combined['Diff'] = combined['FamilySize'] - combined['Count on Ticket']\n\ncombined['Family Status'] = combined.apply(lambda x:\"Has Family On Same Ticket\" if (x['FamilySize'] - x['Count on Ticket']) <= 0 else \"Family Not on same ticket\",axis=1)\n", "class": "Data Transform", "desc": "This code snippet creates a new column 'Diff' to capture the difference between family size and count on the same ticket, and another column 'Family Status' to indicate if a passenger has family members on the same ticket or not, based on this difference.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99793893}, "cluster": -1}, {"cell_id": 27, "code": "combined['Family Status'] = combined.apply(lambda x:\"Is Alone\" if (x['FamilySize']==1) & (x['Count on Ticket']==1)  else x['Family Status'],axis=1)", "class": "Data Transform", "desc": "This code snippet updates the 'Family Status' column to categorize passengers as \"Is Alone\" if both their family size and count on ticket are 1.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.996369}, "cluster": -1}, {"cell_id": 29, "code": "combined['Cabin Class'] = 'No Cabin'\n\n\n\ncombined['Cabin Class'] = combined.apply(lambda x: \"No Cabin\" if pd.isna(x[\"Cabin\"]) else x[\"Cabin\"][0] , axis=1)\n\n\n", "class": "Data Transform", "desc": "This code snippet creates a new column 'Cabin Class' to classify the cabin class based on the first letter of the 'Cabin' value, or assigns 'No Cabin' if the 'Cabin' value is missing.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9755805}, "cluster": 8}, {"cell_id": 30, "code": "\n\ntickcab = combined[combined['Cabin Class'] != 'No Cabin'][['Ticket','Cabin Class']].drop_duplicates()\n\n\n\ntickcab = tickcab.rename(columns={'Cabin Class':'CabNam'})\n\ncombined = combined.merge(tickcab,how=\"left\",on=\"Ticket\")\n\n\n\ncombined['CabNam'].fillna('No Cabin')\n\n\n\ncombined['Cabin Class'] = combined.apply(lambda x:x['Cabin Class'] if x['Cabin Class'] != 'No Cabin' else x['CabNam'],axis=1)\n\ncombined.drop(columns=['CabNam'],inplace=True)\n\ncombined.drop_duplicates(inplace=True)\n", "class": "Data Transform", "desc": "This code snippet creates a temporary DataFrame with unique ticket and cabin class combinations, merges it back into the combined DataFrame to fill missing cabin class values based on tickets, then cleans up temporary columns and drops duplicate rows.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.6754502}, "cluster": 3}, {"cell_id": 33, "code": "combined[\"Fare\"] = combined[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\ncombined[\"Fare\"] = combined[\"Fare\"]/combined['Count on Ticket']", "class": "Data Transform", "desc": "This code snippet applies log transformation to the \"Fare\" column to reduce skewness and then normalizes the fare by dividing it by the 'Count on Ticket' for each passenger in the combined DataFrame.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99918026}, "cluster": 4}, {"cell_id": 36, "code": "companion = pd.pivot_table(combined, values='PassengerId',index=['Ticket'],columns=['AgeGroup'], aggfunc=\"count\").reset_index().fillna(0)\n\ncompanion.columns = ['Ticket','No. of Adult Companion', 'No. of Child Companion']\n\ncombined = combined.merge(companion, on='Ticket',how='left')", "class": "Data Transform", "desc": "This code snippet creates a pivot table to count the number of adult and child companions per ticket and merges these counts back into the combined DataFrame, renaming the columns appropriately.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.99741}, "cluster": 3}, {"cell_id": 37, "code": "combined.loc[combined['AgeGroup']=='adult','No. of Adult Companion'] = combined.loc[combined['AgeGroup']=='adult','No. of Adult Companion'] - 1\n\ncombined.loc[combined['AgeGroup']=='child','No. of Child Companion'] = combined.loc[combined['AgeGroup']=='child','No. of Child Companion'] - 1\n\n\n\ncombined['Companion'] = 'Adult & Child Companion'\n\ncombined['Companion'] = combined.apply(lambda x:'Only Adult Companion' if (x['No. of Adult Companion'] > 0) & (x['No. of Child Companion']==0) else x['Companion'],axis=1)\n\ncombined['Companion'] = combined.apply(lambda x:'Only Child Companion' if (x['No. of Adult Companion'] == 0) & (x['No. of Child Companion']>0) else x['Companion'],axis=1)\n\ncombined['Companion'] = combined.apply(lambda x:'No Companion' if (x['No. of Adult Companion'] == 0) & (x['No. of Child Companion']==0) else x['Companion'],axis=1)", "class": "Data Transform", "desc": "This code snippet adjusts the count of companions by subtracting the passenger themselves, then categorizes companions into different types ('Adult & Child Companion', 'Only Adult Companion', 'Only Child Companion', 'No Companion') based on the adjusted counts and creates a new column 'Companion' to reflect these categories.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.998919}, "cluster": 2}, {"cell_id": 42, "code": "combined.drop(columns=['PassengerId','Name','Age', 'AgeGroup','SibSp','Parch','Ticket','Cabin','Count on Ticket','Diff','No. of Adult Companion','No. of Child Companion'],inplace=True)", "class": "Data Transform", "desc": "This code snippet drops several columns from the combined DataFrame that are deemed unnecessary for subsequent analysis or model training.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9992192}, "cluster": 3}, {"cell_id": 45, "code": "combined = pd.get_dummies(combined, columns = [\"Sex\",\"Embarked\",\"AgeBand\",\"Family Status\",\"Cabin Class\",\"Companion\"],drop_first=True)", "class": "Data Transform", "desc": "This code snippet converts categorical columns into one-hot encoded variables, dropping the first category to avoid the dummy variable trap, and updates the combined DataFrame with these new binary columns.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99923813}, "cluster": 3}, {"cell_id": 46, "code": "train = combined[:train_len]\n\ntest = combined[train_len:]\n\ntest.drop(columns=['Survived'],inplace=True)", "class": "Data Transform", "desc": "This code snippet splits the combined DataFrame back into training and test sets based on the original training length, and drops the 'Survived' column from the test set.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.993755}, "cluster": 4}, {"cell_id": 5, "code": "combined.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and displays the total number of missing values in each column of the combined DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99895346}, "cluster": 1}, {"cell_id": 6, "code": "train_df.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates and displays descriptive statistics for the numerical columns in the training DataFrame using pandas' `describe()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994492}, "cluster": 1}, {"cell_id": 10, "code": "combined.groupby(['Pclass','Sex'])['Age'].mean()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and displays the mean age for each combination of passenger class and gender by grouping the combined DataFrame.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.99413943}, "cluster": 1}, {"cell_id": 14, "code": "combined[combined['Age'].notnull()].groupby(['Pclass','Sex','AgeGroup'])['Age'].mean()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and displays the mean age for each combination of passenger class, gender, and age group by grouping the combined DataFrame where the 'Age' is not null.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9977055}, "cluster": 1}, {"cell_id": 19, "code": "combined.groupby(['Pclass','Embarked'])['PassengerId'].count()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and displays the count of passengers for each combination of passenger class and embarkation point by grouping the combined DataFrame.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.99464333}, "cluster": 1}, {"cell_id": 35, "code": "combined.head()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the first few rows of the combined DataFrame using the `head()` method to get an overview of the current state of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997458}, "cluster": 1}, {"cell_id": 38, "code": "combined[combined[\"Survived\"].notnull()].groupby(['AgeGroup','Companion'])['PassengerId'].count()", "class": "Exploratory Data Analysis", "desc": "This code snippet counts the number of passengers by age group and companion type for passengers with known survival status by grouping the combined DataFrame.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9981712}, "cluster": 1}, {"cell_id": 39, "code": "combined[combined[\"Survived\"].notnull()].groupby(['AgeGroup','Companion'])['Survived'].mean()", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and displays the mean survival rate for each combination of age group and companion type by grouping the combined DataFrame where the survival status is known.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9978156}, "cluster": 1}, {"cell_id": 40, "code": "combined.columns", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the column names of the combined DataFrame using the `columns` attribute to provide an overview of the available features.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99846387}, "cluster": 1}, {"cell_id": 43, "code": "combined.columns", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the current column names of the combined DataFrame using the `columns` attribute to verify the remaining features after dropping multiple columns.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99846387}, "cluster": 1}, {"cell_id": 44, "code": "combined.head()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the first few rows of the current combined DataFrame using the `head()` method to review the remaining data after dropping several columns.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997458}, "cluster": 1}, {"cell_id": 48, "code": "X_train.head()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the first few rows of the training features DataFrame `X_train` using the `head()` method to inspect the structure and content of the training data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99975497}, "cluster": 1}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# For example, here's several helpful packages to load\n\n\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "The code snippet imports essential libraries such as NumPy and pandas, and lists all files in the specified input directory on a Kaggle environment using the os package.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.99921954}, "cluster": -1}, {"cell_id": 1, "code": "import pandas as pd\n\nimport numpy as np\n\nimport seaborn as sns\n\nimport time\n\nfrom collections import Counter\n\nfrom matplotlib import pyplot as plt\n\nfrom warnings import filterwarnings\n\nfrom sklearn import model_selection\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom scipy.stats import norm\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import cross_val_score,KFold\n\nimport plotly.express as px\n\nfrom plotly.offline import plot, iplot, init_notebook_mode\n\ninit_notebook_mode(connected=True)\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn import feature_selection", "class": "Imports and Environment", "desc": "This code snippet imports various libraries and modules for data analysis, visualization, preprocessing, and machine learning tasks, including pandas, NumPy, seaborn, matplotlib, scikit-learn, XGBoost, and Plotly, and initializes Plotly in notebook mode.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.9984848}, "cluster": -1}, {"cell_id": 2, "code": "import warnings\n\nwarnings.filterwarnings('ignore')", "class": "Imports and Environment", "desc": "The code snippet suppresses warnings by instructing Python to ignore them using the warnings library.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.999143}, "cluster": -1}, {"cell_id": 50, "code": "report=classification_report(y_test,pred)\n\nprint(\"Decision Tree report \\n\",report)", "class": "Model Evaluation", "desc": "This code snippet generates and prints a classification report, which includes precision, recall, f1-score, and support metrics for the decision tree model's test predictions using scikit-learn's `classification_report` function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9974105}, "cluster": 0}, {"cell_id": 47, "code": "X = train.iloc[:,1:]\n\ny = train.iloc[:,0]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)", "class": "Model Training", "desc": "This code snippet separates features and target variable from the training DataFrame, and then splits them into training and testing sets using an 80-20 split with a fixed random state for reproducibility, employing scikit-learn's `train_test_split` function.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.99681467}, "cluster": 0}, {"cell_id": 49, "code": "#Dtree\n\ndecision_tree = DecisionTreeClassifier()\n\ndecision_tree.fit(X_train, y_train)\n\npred_train = decision_tree.predict(X_train)\n\npred=decision_tree.predict(X_test)\n\npred_train_df=pd.DataFrame({\"Actual\":y_train,\"Pred\":pred_train})\n\npred_df=pd.DataFrame({\"Actual\":y_test,\"Pred\":pred})\n\ncm=confusion_matrix(y_test,pred)\n\ncm", "class": "Model Training", "desc": "This code snippet trains a decision tree classifier on the training data, makes predictions on both the training and testing sets, creates DataFrames to compare actual and predicted values, and computes the confusion matrix for the test predictions using scikit-learn's `DecisionTreeClassifier` and `confusion_matrix` functions.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.25523284}, "cluster": 0}, {"cell_id": 51, "code": "#RFC\n\n\n\nrfc=ensemble.RandomForestClassifier(max_depth=6,random_state=0,n_estimators=64)\n\nrfc.fit(X_train, y_train)\n\npred_train = rfc.predict(X_train)\n\npred=rfc.predict(X_test)\n\npred_train_df=pd.DataFrame({\"Actual\":y_train,\"Pred\":pred_train})\n\npred_df=pd.DataFrame({\"Actual\":y_test,\"Pred\":pred})\n\n\n\ncm=confusion_matrix(y_test,pred)\n\nprint(cm)\n\n\n\nreport=classification_report(y_test,pred)\n\nprint(\"Random Forest report \\n\",report)", "class": "Model Training", "desc": "This code snippet trains a Random Forest classifier with specified parameters on the training data, makes predictions on both training and testing sets, creates DataFrames to compare actual and predicted values, computes the confusion matrix, and generates a classification report using scikit-learn's `RandomForestClassifier`, `confusion_matrix`, and `classification_report` functions.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.83314914}, "cluster": 0}, {"cell_id": 7, "code": "g = sns.barplot(x=\"Pclass\",y=\"Survived\",data=train_df)\n\ng.set_ylabel(\"Survival Probability\")", "class": "Visualization", "desc": "This code snippet creates a bar plot to visualize the survival probability by passenger class using seaborn's `barplot` function and sets the y-axis label to \"Survival Probability\".", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.9456676}, "cluster": 0}, {"cell_id": 8, "code": "g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train_df)\n\ng.set_ylabel(\"Survival Probability\")", "class": "Visualization", "desc": "This code snippet creates a bar plot to visualize the survival probability by gender using seaborn's `barplot` function and sets the y-axis label to \"Survival Probability\".", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99116606}, "cluster": 0}, {"cell_id": 9, "code": "g = sns.barplot(x=\"Pclass\",y=\"Survived\",hue=\"Sex\",data=train_df)\n\ng.set_ylabel(\"Survival Probability\")", "class": "Visualization", "desc": "This code snippet creates a bar plot to visualize the survival probability by passenger class and gender using seaborn's `barplot` function with the `hue` parameter and sets the y-axis label to \"Survival Probability\".", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.8091258}, "cluster": 0}, {"cell_id": 11, "code": "sns.distplot(train_df[\"Age\"])", "class": "Visualization", "desc": "This code snippet generates and displays a distribution plot for the \"Age\" column in the training DataFrame using seaborn's `distplot` function.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9981388}, "cluster": 1}, {"cell_id": 12, "code": "g = sns.FacetGrid(train_df[train_df['Age'].notnull()], col='Sex',row='Pclass',hue=\"Survived\")\n\ng = g.map(sns.distplot, \"Age\",bins=10,hist_kws=dict(edgecolor=\"k\", linewidth=2),kde=False).add_legend()\n", "class": "Visualization", "desc": "This code snippet creates a FacetGrid of distribution plots for the \"Age\" column, separated by gender and passenger class, and colored by survival status using seaborn's `FacetGrid` and `distplot` functions.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9957695}, "cluster": 0}, {"cell_id": 17, "code": "g = sns.FacetGrid(combined[combined['Survived'].notnull()], col='Pclass')\n\ng = g.map(sns.barplot,\"AgeBand\",\"Survived\", order=[ \"0-7\",\"8-14\",\"15-21\",\"22-28\",\"29-35\",\"36-42\",\"43-49\",\"50-56\",\"57-63\",\">=64\"])\n\nfor axes in g.axes.flat:\n\n    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=70)\n\nplt.tight_layout()", "class": "Visualization", "desc": "This code creates a FacetGrid of bar plots to visualize the survival probability by age band and passenger class using seaborn's `barplot` function and customizes the x-axis labels' orientation for better readability.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99827373}, "cluster": -1}, {"cell_id": 18, "code": "sns.barplot(\"Embarked\",\"Survived\",data=combined[combined['Survived'].notnull()])", "class": "Visualization", "desc": "This code snippet creates a bar plot to visualize the survival probability by embarkation point using seaborn's `barplot` function on non-null survival data from the combined DataFrame.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9954508}, "cluster": -1}, {"cell_id": 20, "code": "sns.barplot(\"Pclass\",\"Survived\",hue=\"Embarked\",data=combined[combined['Survived'].notnull()],ci=None)\n", "class": "Visualization", "desc": "This code snippet creates a bar plot to visualize the survival probability by passenger class and embarkation point, using seaborn's `barplot` function with the `hue` parameter, on non-null survival data from the combined DataFrame.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9985398}, "cluster": -1}, {"cell_id": 22, "code": "sns.barplot(\"SibSp\",\"Survived\",data=train_df, ci = None)", "class": "Visualization", "desc": "This code snippet creates a bar plot to visualize the survival probability by the number of siblings or spouses aboard using seaborn's `barplot` function on the training DataFrame.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9982626}, "cluster": -1}, {"cell_id": 23, "code": "sns.barplot(\"Parch\",\"Survived\",data=train_df, ci = None)", "class": "Visualization", "desc": "This code snippet creates a bar plot to visualize the survival probability by the number of parents or children aboard using seaborn's `barplot` function on the training DataFrame.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9982613}, "cluster": -1}, {"cell_id": 24, "code": "combined['FamilySize'] = combined['SibSp'] + combined['Parch'] + 1  # +1 is to include the passenger him/herself\n\nsns.barplot(\"FamilySize\",\"Survived\",data=combined[combined['Survived'].notnull()], ci = None)", "class": "Visualization", "desc": "This code snippet creates a new column 'FamilySize' in the combined DataFrame to denote the total family size and then creates a bar plot to visualize the survival probability by family size using seaborn's `barplot` function.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99779296}, "cluster": 0}, {"cell_id": 28, "code": "g = sns.FacetGrid(combined[combined['Survived'].notnull()], col='Pclass')\n\ng.map(sns.barplot,\"Family Status\",\"Survived\",ci=None)\n\nfor axes in g.axes.flat:\n\n    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=70)\n", "class": "Visualization", "desc": "This code creates a FacetGrid of bar plots to visualize the survival probability by family status and passenger class, using seaborn's `barplot` function, and customizes the x-axis labels' orientation for better readability.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99821365}, "cluster": -1}, {"cell_id": 31, "code": "sns.barplot(\"Cabin Class\",\"Survived\",data=combined[combined[\"Survived\"].notnull()],ci=None)", "class": "Visualization", "desc": "This code snippet creates a bar plot to visualize the survival probability by cabin class using seaborn's `barplot` function on non-null survival data from the combined DataFrame.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9981097}, "cluster": -1}, {"cell_id": 32, "code": "g = sns.distplot(combined[\"Fare\"], color=\"m\",label=\"Skewness : %.2f\"%(combined[\"Fare\"].skew()))\n\ng = g.legend(loc=\"best\")", "class": "Visualization", "desc": "This code snippet generates a distribution plot for the \"Fare\" column, calculates and displays the skewness of the fare distribution as a legend using seaborn's `distplot` function.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99603015}, "cluster": 0}, {"cell_id": 34, "code": "g = sns.distplot(combined[\"Fare\"], color=\"m\",label=\"Skewness : %.2f\"%(combined[\"Fare\"].skew()))\n\ng = g.legend(loc=\"best\")", "class": "Visualization", "desc": "This code snippet generates a distribution plot for the normalized \"Fare\" column, calculates and displays the skewness of the transformed fare distribution as a legend using seaborn's `distplot` function.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99603015}, "cluster": 0}], "notebook_id": 2, "notebook_name": "titanic-rfc-some-analysis-on-ticket.ipynb", "user": "titanic-rfc-some-analysis-on-ticket.ipynb"}, {"cells": [{"cell_id": 49, "code": "sample = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\nsubmission = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nsubmission['Survived'] = y_pred\nsubmission = submission[['PassengerId','Survived']]\ndisplay(submission)", "class": "Data Export", "desc": "This code creates a submission DataFrame by reading passenger IDs from the test dataset and assigning the predicted survival probabilities, then displays the prepared submission DataFrame using pandas.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99966645}, "cluster": -1}, {"cell_id": 52, "code": "submission.to_csv('submission.csv', index=False)", "class": "Data Export", "desc": "This code saves the final submission DataFrame to a CSV file named 'submission.csv' without including the index using pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992442}, "cluster": -1}, {"cell_id": 1, "code": "# Save test data as dataframe and preview in order to get a sense of the what the data look like.\ntrain = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndisplay(train.head())", "class": "Data Extraction", "desc": "This code reads the training data from a CSV file using pandas and displays the first few rows of the dataset.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99953103}, "cluster": -1}, {"cell_id": 37, "code": "test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ndisplay(test.head())", "class": "Data Extraction", "desc": "This code reads the test data from a CSV file using pandas and displays the first few rows of the dataset.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9995247}, "cluster": -1}, {"cell_id": 4, "code": "# It appears that all titles (i.e. 'Mr.', 'Mrs.',...) can be found between a space and a period, so we will create a set of unique titles by searching\n# for this pattern in every name\ntitles = set()\nfor i in train['Name']:\n    title = re.search('\\s([a-zA-Z]+)\\.', i)\n    titles.add(title.group(1))\nprint(titles)", "class": "Data Transform", "desc": "This code extracts unique titles from the `Name` column by searching for patterns between a space and a period using regular expressions and saves them into a set.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.97765005}, "cluster": 7}, {"cell_id": 5, "code": "# Create new columns with an indicator for each title and test if the difference in survival rates for that column is statistically significant\n# use a copy of the dataset so as to not modify the original data\ncopy = train.copy()\nfor title in titles:\n    copy[title] = np.where(copy['Name'].str.contains(title), 1, 0)\n    display(copy[['Survived',title]].groupby([title]).agg({'Survived':['count','sum','mean']}))\n    t, p = stats.ttest_ind(a = copy[copy[title] == 0]['Survived'], b = copy[copy[title] == 1]['Survived'])\n    display(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code creates new columns in a copy of the dataset for each title, computes survival statistics grouped by these titles, and performs t-tests to check for statistically significant differences in survival rates using numpy and scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.878474}, "cluster": 2}, {"cell_id": 6, "code": "# Many titles did not have sufficient frequency to use for predictions, but several may prove useful in our model\nfor title in titles.copy():\n    copy[title] = np.where(copy['Name'].str.contains(title), 1, 0)\n    t, p = stats.ttest_ind(a = copy[copy[title] == 0]['Survived'], b = copy[copy[title] == 1]['Survived'])\n    # After looking over the t-test results, we will keep all of the variables for which survival was significantly different amongs the two groups\n    if not p<0.05:\n        titles.remove(title)\ntitles = list(titles)\ndisplay(titles)\n# Add these new indicator columns to our actual dataset for these titles now that we have chosen them\nfor title in titles:\n    train[title] = np.where(copy['Name'].str.contains(title), 1, 0)", "class": "Data Transform", "desc": "This code filters out titles with insufficient frequency by keeping only those with a statistically significant difference in survival rates, and then adds these indicator columns to the original dataset using numpy and scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99799144}, "cluster": 2}, {"cell_id": 9, "code": "# first we will try imputing the mean age into the null values\ntrain['Age_mean_imputed'] = np.where(train['Age'].isnull(),np.mean(train['Age']),train['Age'])\n# Now lets fit a bivariate regression model\nY, X = train['Survived'], train['Age_mean_imputed']\nage_model = sm.Logit(Y, X).fit()\ndisplay(age_model.summary().tables[1])", "class": "Data Transform", "desc": "This code imputes the mean age for null values in the `Age` column and fits a logistic regression model to predict survival using the imputed age values, displaying the model summary table using pandas and statsmodels.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.35666126}, "cluster": 2}, {"cell_id": 10, "code": "# second method is to use a regression model to predict the missing ages\nsub_data = train[~train['Age'].isnull()].copy()\n# We will use the columns that intuitively seem like they could relate to age\nY, X = np.array(sub_data['Age']).reshape(-1, 1), np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']])\n# fit the model\nage_model = lm.LinearRegression().fit(X, Y)\n#predict on the missing values\nsub_data = train[train['Age'].isnull()].copy()\nage_predictions = age_model.predict(np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']]))\n#add the missing values back into the dataframe\ntrain['Age_lm'] = train['Age']\nage_predictions_indexed = pd.DataFrame(age_predictions,index = train[train['Age_lm'].isnull()].index)[0]\ntrain['Age_lm'].fillna(age_predictions_indexed, inplace = True)\n\n# Now we will see how this predictor does at predicting Survival\nY, X = train['Survived'], train['Age_lm']\nage_model = sm.Logit(Y, X).fit()\ndisplay(age_model.summary().tables[1])", "class": "Data Transform", "desc": "This code imputes missing `Age` values using a linear regression model based on related features like `Fare`, titles, `SibSp`, and `Parch`, and then fits a logistic regression model to predict survival using the imputed age values, displaying the model summary table using pandas, sklearn, and statsmodels.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.62230337}, "cluster": 2}, {"cell_id": 13, "code": "# many values of SibSp have a very low frequency. Let's try simple indicator for any siblings or spouses\ntrain['SibSp_ind'] = np.where(train['SibSp'] > 0, 1, 0)\ndisplay(train[['Survived','SibSp_ind']].groupby(['SibSp_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = train[train['SibSp_ind'] == 1]['Survived'], b = train[train['SibSp_ind'] == 0]['Survived'])\ndisplay(f't-stat: {t}    p-value: {p}    p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code creates a new binary indicator column `SibSp_ind` to denote the presence of siblings or spouses, evaluates its relation to survival by grouping and calculating survival statistics, and performs a t-test to check for statistically significant differences in survival rates using numpy, pandas, and scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.92328453}, "cluster": 2}, {"cell_id": 15, "code": "# Let's consider an indicator for Parch as well due to low frequency of high values\ntrain['Parch_ind'] = np.where(train['Parch'] > 0, 1, 0)\ndisplay(train[['Survived','Parch_ind']].groupby(['Parch_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = train[train['Parch_ind'] == 1]['Survived'], b = train[train['Parch_ind'] == 0]['Survived'])\ndisplay(f't-stat: {t}    p-value: {p}    p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code creates a new binary indicator column `Parch_ind` to denote the presence of parents or children, evaluates its relation to survival by grouping and calculating survival statistics, and performs a t-test to check for statistically significant differences in survival rates using numpy, pandas, and scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.953208}, "cluster": 2}, {"cell_id": 17, "code": "# let's look for patterns in the beginning letters of the ticket\nprefixes = set()\nfor i in train['Ticket']:\n    pref = re.search('([^\\s]+)\\s', i)\n    if pref is None: prefixes.add(\"None\")\n    else: prefixes.add(pref.group(1))\nprint(prefixes)", "class": "Data Transform", "desc": "This code extracts unique prefixes from the `Ticket` column by searching for patterns at the beginning of the ticket strings using regular expressions and saves them into a set.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.31170517}, "cluster": 7}, {"cell_id": 18, "code": "# Create new columns with an indicator for each prefix and test if the difference in survival rates for that column is statistically significant\n# use a copy of the dataset so as to not modify the original data\ncopy = train.copy()\nfor pref in prefixes:\n    # Make sure to check for a space at the end of the prefix so we don't accidentally capture substrings\n    copy[pref] = np.where(copy['Ticket'].str.contains(f'{pref} '), 1, 0)\n    display(copy[['Survived',pref]].groupby([pref]).agg({'Survived':['count','sum','mean']}))\n    t, p = stats.ttest_ind(a = copy[copy[pref] == 0]['Survived'], b = copy[copy[pref] == 1]['Survived'])\n    display(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code creates new columns in a copy of the dataset for each extracted ticket prefix, computes survival statistics grouped by these prefixes, and performs t-tests to check for statistically significant differences in survival rates using numpy and scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.91176885}, "cluster": 2}, {"cell_id": 19, "code": "# There was a very low frequency of most prefixes. Let's compare prefixes to no prefix\ncopy = train.copy()\n# check if the ticket starts with any letter\ncopy['Ticket_prefix_ind'] = np.where(copy['Ticket'].str.startswith(tuple(prefixes)), 1, 0)\ndisplay(copy[['Survived','Ticket_prefix_ind']].groupby(['Ticket_prefix_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = copy[copy['Ticket_prefix_ind'] == 0]['Survived'], b = copy[copy['Ticket_prefix_ind'] == 1]['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code creates a new binary indicator column `Ticket_prefix_ind` to denote whether the ticket starts with any letter prefix, evaluates its relation to survival by grouping and calculating survival statistics, and performs a t-test to check for statistically significant differences in survival rates using numpy and scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.98258317}, "cluster": 2}, {"cell_id": 20, "code": "# as a whole, prefixes do not appear to be predictive. Let's just keep the two that were frequent enough and predictive on their own:\ncopy = train.copy()\n# check if the ticket starts with any letter\ncopy['Ticket_prefix_ind'] = np.where(copy['Ticket'].str.startswith(('C ','PC ')), 1, 0)\ndisplay(copy[['Survived','Ticket_prefix_ind']].groupby(['Ticket_prefix_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = copy[copy['Ticket_prefix_ind'] == 0]['Survived'], b = copy[copy['Ticket_prefix_ind'] == 1]['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Data Transform", "desc": "This code creates a new binary indicator column `Ticket_prefix_ind` to denote whether the ticket starts with the frequently occurring and predictive prefixes 'C ' or 'PC ', evaluates its relation to survival by grouping and calculating survival statistics, and performs a t-test to check for statistically significant differences in survival rates using numpy and scipy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.78813297}, "cluster": 2}, {"cell_id": 21, "code": "# since these two prefixes are frequent enough and have sufficiently different survival rates, we'll keep them in for consideration\ntrain['Ticket_PC_C'] = np.where(train['Ticket'].str.startswith(('C ','PC ')), 1, 0)", "class": "Data Transform", "desc": "This code adds a new column `Ticket_PC_C` to the dataset to denote whether the ticket starts with the prefixes 'C ' or 'PC ', indicating these will be kept for consideration in further analysis using numpy and pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99881774}, "cluster": 2}, {"cell_id": 22, "code": "copy['Ticket_num'] = copy.Ticket.str.extract('(^\\d*)')\ncopy['Ticket_num'] = copy['Ticket_num'].str.len()\ndisplay(copy[['Survived','Ticket_num']].groupby(['Ticket_num']).agg({'Survived':['count','sum','mean']}))", "class": "Data Transform", "desc": "This code extracts the numerical part from the `Ticket` column, calculates its length, and evaluates the relation of this length to survival by grouping and calculating survival statistics using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.98412067}, "cluster": 2}, {"cell_id": 23, "code": "train['Ticket_len5'] = train.Ticket.str.extract('(^\\d*)')\ntrain['Ticket_len5'] = np.where(train['Ticket_len5'].str.len() == 5, 1, 0)", "class": "Data Transform", "desc": "This code adds a new binary indicator column `Ticket_len5` to the dataset to denote whether the numerical part of the ticket is exactly five digits long using numpy and pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9989471}, "cluster": 2}, {"cell_id": 25, "code": "# since the data are highly skewed with some outliers, let's bin the values into buckets using a decision tree and use that instead of the original column\nDT = tree.DecisionTreeClassifier(min_samples_leaf = 0.08, max_depth = 3)\nFare_tree = DT.fit(train['Fare'].to_frame(), train['Survived'])\ntrain['Fare_binned'] = Fare_tree.predict_proba(train['Fare'].to_frame())[:,1]\ndisplay(train[['Survived','Fare_binned','Fare']].groupby(['Fare_binned']).agg({'Survived':['count','sum','mean'],'Fare':['min','max','mean']}))", "class": "Data Transform", "desc": "This code bins the highly skewed `Fare` values using a decision tree classifier to predict the probability of survival, and evaluates the relation of these binned values to survival by grouping and calculating survival and fare statistics using pandas and scikit-learn's tree module.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.41344222}, "cluster": 2}, {"cell_id": 27, "code": "# Let's merge all cabins that start with the same letter\ncopy = train.copy()\ncopy['Cabin_group'] = copy['Cabin'].str[:1]\ncopy['Cabin_group'].fillna('None', inplace = True)\ndisplay(copy[['Survived','Cabin_group']].groupby(['Cabin_group']).agg({'Survived':['count','sum','mean']}))", "class": "Data Transform", "desc": "This code creates a new column `Cabin_group` by extracting the first letter of the `Cabin` column and filling null values with 'None', then evaluates the relation of these cabin groups to survival by grouping and calculating survival statistics using pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.3810074}, "cluster": 2}, {"cell_id": 28, "code": "# Since there is still a low frequency in many groups and it's unclear how groups might be meaningfully merged, let's mean_encode this column\ntrain['Cabin_group'] = train['Cabin'].str[:1]\ntrain['Cabin_group'].fillna('None', inplace = True)\nmeans = train[['Survived','Cabin_group']].groupby(['Cabin_group']).mean()\nmeans.rename({'Survived':'Cabin_ME'}, axis = 1, inplace = True)\ntrain = train.join(means, how = 'left', on = ['Cabin_group'])", "class": "Data Transform", "desc": "This code creates a new column `Cabin_group` by extracting the first letter of the `Cabin` column and filling null values with 'None', calculates mean survival rates for each cabin group, and performs mean encoding by joining this mean survival rate back to the original dataset using pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.99588}, "cluster": 2}, {"cell_id": 30, "code": "# since there are only two null values for embarked, we will encode them with the mode\ntrain['Embarked'].fillna('S', inplace = True)\nprint(train['Embarked'].isnull().sum())\ndisplay(train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':['count','sum','mean']}))", "class": "Data Transform", "desc": "This code fills the null values in the `Embarked` column with the mode 'S' and re-evaluates the relation of `Embarked` to survival by grouping and calculating survival statistics using pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.4329066}, "cluster": 1}, {"cell_id": 31, "code": "# let's mean encode this column as well so that we can have all numerical columns\nmeans = train[['Survived','Embarked']].groupby(['Embarked']).mean()\nmeans.rename({'Survived':'Embarked_ME'}, axis = 1, inplace = True)\ntrain = train.join(means, how = 'left', on = ['Embarked'])", "class": "Data Transform", "desc": "This code calculates mean survival rates for each `Embarked` category, performs mean encoding, and joins this mean survival rate back to the original dataset to convert the `Embarked` column into numerical format using pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.9932888}, "cluster": 2}, {"cell_id": 32, "code": "# create our final data set with just the columns we'll be keeping\ndata = train[['Survived','Pclass','Mrs', 'Mr', 'Master', 'Miss','Sex','Age_lm','SibSp_ind','Parch_ind','Ticket_PC_C','Fare_binned','Cabin_ME','Embarked_ME','Ticket_len5']].copy()\n# clean up the names\ndata.rename({'Age_lm':'Age','SibSp_ind':'SibSp','Parch_ind':'Parch','Ticket_PC_C':'Ticket','Fare_binned':'Fare','Cabin_ME':'Cabin','Embarked_ME':'Embarked'}, axis = 1, inplace = True)\ndata['Female'] = np.where(data['Sex'] == 'female', 1, 0)\ndata.drop('Sex', axis = 1, inplace = True)\ndisplay(data.head())\ndisplay(data.columns)", "class": "Data Transform", "desc": "This code creates a final dataset by selecting relevant columns, renaming them for clarity, and creating a new binary column `Female` to indicate gender, then dropping the `Sex` column to ensure all columns are numerical using pandas and numpy.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.85816985}, "cluster": 2}, {"cell_id": 38, "code": "# Update names column\nfor title in ['Mrs', 'Mr', 'Miss', 'Master']:\n    test[title] = np.where(test['Name'].str.contains(title), 1, 0)\nlen(test[test['Name'].isnull()])", "class": "Data Transform", "desc": "This code updates the test dataset by adding new binary indicator columns for specific titles (`Mrs`, `Mr`, `Miss`, `Master`) based on the `Name` column using pandas and numpy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9953231}, "cluster": 2}, {"cell_id": 39, "code": "#Update Sex columns\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\nlen(test[test['Sex'].isnull()])", "class": "Data Transform", "desc": "This code adds a new binary indicator column `Female` to the test dataset to denote gender and checks for any null values in the `Sex` column using pandas and numpy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9913565}, "cluster": 2}, {"cell_id": 40, "code": "#Update SibSp\ntest['SibSp_ind'] = np.where(test['SibSp'] > 0, 1, 0)\nlen(test[test['SibSp'].isnull()])\n# display(test.head())", "class": "Data Transform", "desc": "This code adds a new binary indicator column `SibSp_ind` to the test dataset to denote the presence of siblings or spouses and checks for any null values in the `SibSp` column using pandas and numpy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99579054}, "cluster": 2}, {"cell_id": 41, "code": "#Update Parch\ntest['Parch_ind'] = np.where(test['Parch'] > 0, 1, 0)\nlen(test[test['Parch'].isnull()])\n# display(test.head())", "class": "Data Transform", "desc": "This code adds a new binary indicator column `Parch_ind` to the test dataset to denote the presence of parents or children and checks for any null values in the `Parch` column using pandas and numpy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99731416}, "cluster": 2}, {"cell_id": 42, "code": "#Update ticket\ntest['Ticket_PC_C'] = np.where(test['Ticket'].str.startswith(('C ','PC ')), 1, 0)\nlen(test[test['Ticket_PC_C'].isnull()])\n\ntest['Ticket_len5'] = test.Ticket.str.extract('(^\\d*)')\ntest['Ticket_len5'] = np.where(test['Ticket_len5'].str.len() == 5, 1, 0)", "class": "Data Transform", "desc": "This code adds two new binary indicator columns to the test dataset: `Ticket_PC_C` to denote tickets starting with 'C ' or 'PC ', and `Ticket_len5` to denote tickets with exactly five digits, and checks for any null values in the `Ticket_PC_C` column using pandas and numpy.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9972178}, "cluster": 2}, {"cell_id": 43, "code": "#Update Fare\n#One null fare will be imputed with the median since outliers might skew the mean\ntest['Fare'].fillna(test['Fare'].median(), inplace = True)\n#Use the same decision tree we made before to impute the values\ntest['Fare_binned'] = Fare_tree.predict_proba(test['Fare'].to_frame())[:,1]\nlen(test[test['Fare'].isnull()])", "class": "Data Transform", "desc": "This code imputes the single null value in the `Fare` column of the test dataset with the median fare, uses the previously trained decision tree to bin the `Fare` values, and checks for any remaining null values in the `Fare` column using pandas.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9609763}, "cluster": 2}, {"cell_id": 44, "code": "#Update Cabin\ntest['Cabin_group'] = test['Cabin'].str[:1]\ntest['Cabin_group'].fillna('None', inplace = True)\n#Mean encode based on the original training data\nmeans = train[['Survived','Cabin_group']].groupby(['Cabin_group']).mean()\nmeans.rename({'Survived':'Cabin_ME'}, axis = 1, inplace = True)\ntest = test.join(means, how = 'left', on = ['Cabin_group'])", "class": "Data Transform", "desc": "This code creates a new column `Cabin_group` in the test dataset by extracting the first letter of the `Cabin` column, fills null values with 'None', and mean encodes this column based on the original training data, joining the encoded values back to the test dataset using pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.9975425}, "cluster": 2}, {"cell_id": 45, "code": "#Update Embarked\nlen(test[test['Embarked'].isnull()])\n# mean encode\nmeans = train[['Survived','Embarked']].groupby(['Embarked']).mean()\nmeans.rename({'Survived':'Embarked_ME'}, axis = 1, inplace = True)\ntest = test.join(means, how = 'left', on = ['Embarked'])", "class": "Data Transform", "desc": "This code checks for null values in the `Embarked` column of the test dataset and then performs mean encoding of the `Embarked` column based on the original training data, joining these encoded values back to the test dataset using pandas.", "testing": {"class": "Data_Transform", "subclass": "merge", "subclass_id": 32, "predicted_subclass_probability": 0.9983809}, "cluster": 2}, {"cell_id": 46, "code": "#Update Age column\nsub_data = test[~test['Age'].isnull()].copy()\n# We will use the columns that intuitively seem like they could relate to age\nY, X = np.array(sub_data['Age']).reshape(-1, 1), np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']])\n# fit the model\nage_model = lm.LinearRegression().fit(X, Y)\n#predict on the missing values\nsub_data = test[test['Age'].isnull()].copy()\nage_predictions = age_model.predict(np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']]))\n#add the missing values back into the dataframe\ntest['Age_lm'] = test['Age']\nage_predictions_indexed = pd.DataFrame(age_predictions,index = test[test['Age_lm'].isnull()].index)[0]\ntest['Age_lm'].fillna(age_predictions_indexed, inplace = True)\ndisplay(test.head())", "class": "Data Transform", "desc": "This code imputes missing `Age` values in the test dataset using a linear regression model based on related features like `Fare`, titles, `SibSp`, and `Parch`, and adds these imputed age values back into the dataset as a new column `Age_lm` using pandas and scikit-learn.", "testing": {"class": "Data_Export", "subclass": "prepare_output", "subclass_id": 55, "predicted_subclass_probability": 0.436364}, "cluster": 2}, {"cell_id": 47, "code": "# create our final data set with just the columns we'll be keeping\ntest = test[['Pclass','Mrs', 'Mr', 'Master', 'Miss','Sex','Age_lm','SibSp_ind','Parch_ind','Ticket_PC_C','Fare_binned','Cabin_ME','Embarked_ME','Ticket_len5']].copy()\n# clean up the names\ntest.rename({'Age_lm':'Age','SibSp_ind':'SibSp','Parch_ind':'Parch','Ticket_PC_C':'Ticket','Fare_binned':'Fare','Cabin_ME':'Cabin','Embarked_ME':'Embarked'}, axis = 1, inplace = True)\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\ntest.drop('Sex', axis = 1, inplace = True)\ndisplay(test.head())\ndisplay(test.columns)", "class": "Data Transform", "desc": "This code creates a final test dataset by selecting relevant columns, renaming them for clarity, and creating a new binary column `Female` to indicate gender, then dropping the `Sex` column to ensure all columns are numerical using pandas and numpy.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.96216625}, "cluster": 2}, {"cell_id": 51, "code": "submission['Survived'] = np.where(submission['Survived'] < 0.573, 0, 1)\nprint(submission['Survived'].mean())\ndisplay(submission.head(20))", "class": "Data Transform", "desc": "This code applies a cutoff value of 0.573 to the predicted survival probabilities in the submission DataFrame, converting them to binary survival predictions and then prints the mean survival rate and displays the first 20 rows of the submission DataFrame using numpy and pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.86286855}, "cluster": 2}, {"cell_id": 2, "code": "# Initial review shows that Pclass appears to be predictive\ndisplay(train[['Survived','Pclass']].groupby(['Pclass']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Pclass'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code performs an initial review of the `Pclass` feature by showing the count, sum, and mean of `Survived` grouped by `Pclass` and checks for null values in the `Pclass` column in the dataset using pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9523748}, "cluster": 1}, {"cell_id": 3, "code": "# The Names column contains too many unique values. We will investigate particular substrings.\npd.set_option(\"display.max_rows\", 20)\ndisplay(train['Name'])\nnulls = train['Name'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code displays the `Name` column of the dataset to investigate substrings in names, checks for null values in this column using pandas, and sets the display option to show up to 20 rows.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.6983381}, "cluster": 1}, {"cell_id": 7, "code": "# Display unique values and how they survived\ndisplay(train[['Survived','Sex']].groupby(['Sex']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Sex'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')\n# Perform a t-test to check for statistically significant difference in survival rates\nt, p = stats.ttest_ind(a = train[train['Sex'] == 'female']['Survived'], b = train[train['Sex'] == 'male']['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')", "class": "Exploratory Data Analysis", "desc": "This code displays survival statistics grouped by gender, checks for null values in the `Sex` column, and performs a t-test to check for statistically significant differences in survival rates between males and females using pandas and scipy.", "testing": {"class": "Model_Evaluation", "subclass": "statistical_test", "subclass_id": 47, "predicted_subclass_probability": 0.93878275}, "cluster": 1}, {"cell_id": 8, "code": "# visualize distribution of ages\ntrain['Age'].hist()\n# check for null values\\\nnulls = train['Age'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code visualizes the distribution of the `Age` column using a histogram and checks for null values in the `Age` column using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9968941}, "cluster": 1}, {"cell_id": 12, "code": "# Initial review shows that SibSp appears to be predictive\ndisplay(train[['Survived','SibSp']].groupby(['SibSp']).agg({'Survived':['count','sum','mean']}))\nnulls = train['SibSp'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code performs an initial review of the `SibSp` feature by showing the count, sum, and mean of `Survived` grouped by `SibSp` and checks for null values in the `SibSp` column in the dataset using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.88131183}, "cluster": 1}, {"cell_id": 14, "code": "# Initial review shows that Parch appears to be predictive\ndisplay(train[['Survived','Parch']].groupby(['Parch']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Parch'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code performs an initial review of the `Parch` feature by showing the count, sum, and mean of `Survived` grouped by `Parch` and checks for null values in the `Parch` column in the dataset using pandas.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9513562}, "cluster": 1}, {"cell_id": 16, "code": "# The Ticket column contains too many unique values. We will investigate particular substrings.\n#pd.set_option(\"display.max_rows\", None)\ndisplay(train['Ticket'])\nnulls = train['Ticket'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code displays the `Ticket` column of the dataset to investigate substrings in ticket values and checks for null values in this column using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.91641235}, "cluster": 1}, {"cell_id": 24, "code": "# visualize distribution of ages\ntrain['Fare'].hist(bins = 40)\n# check for null values\\\nnulls = train['Fare'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code visualizes the distribution of the `Fare` column using a histogram with 40 bins and checks for null values in the `Fare` column using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9981839}, "cluster": 1}, {"cell_id": 26, "code": "# Initial review shows that Parch appears to be predictive\nnulls = train['Cabin'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code checks for null values in the `Cabin` column of the dataset using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.96462893}, "cluster": 1}, {"cell_id": 29, "code": "# Initial review shows that Embarked appears to be predictive\ndisplay(train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Embarked'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')", "class": "Exploratory Data Analysis", "desc": "This code performs an initial review of the `Embarked` feature by showing the count, sum, and mean of `Survived` grouped by `Embarked` and checks for null values in the `Embarked` column in the dataset using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.73904485}, "cluster": 1}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport itertools as it # to avoid nested for-loops\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # to search for substrings using regular expressions\nimport scipy.stats as stats # for performing t-tests for statistically significant difference in mean values\nfrom sklearn.model_selection import KFold # for k-fold validation of models\nfrom sklearn import metrics as met # for model evaluation metrics\nimport sklearn.linear_model as lm # for linear models\nfrom sklearn import tree # decision tree used to discretize some continuous variables\nimport seaborn as sns # for vizualiations\nimport statsmodels.api as sm # Used to create logistic regression models to check for statistical significance of continuous variables. This package returns confidence intervals for predictors\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This code imports several Python libraries, including itertools, numpy, pandas, re, scipy, sklearn, seaborn, and statsmodels, and lists the files in the `/kaggle/input` directory.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99935275}, "cluster": -1}, {"cell_id": 11, "code": "# Finally, let's use ROC AUC to compare which version is superior\nmodel = lm.LogisticRegression(random_state = 8292010, solver = 'lbfgs')\nROC_AUC_mean = []\nROC_AUC_lm = []\n# We will do 5-fold validation when calculating the ROC_AUC\nkfold = KFold(10, shuffle = True, random_state = 1)\nfor tr, te in kfold.split(train):\n    # capture metrics for the first variation of the age with mean imputing:\n    X, y = np.array(train['Age_mean_imputed']).reshape(-1,1)[tr], np.array(train['Survived']).reshape(-1,1)[tr]\n    model.fit(X, y.ravel())\n    y_hat = model.predict_proba(X = np.array(train['Age_mean_imputed']).reshape(-1,1))[:,1][te]\n    y_true = np.array(train['Survived']).reshape(-1,1)[te]\n    ROC_AUC_mean.append(met.roc_auc_score(y_true, y_score = y_hat))\n    \n    # capture metrics for the second variation of the age with linear regression:\n    X, y = np.array(train['Age_lm']).reshape(-1,1)[tr], np.array(train['Survived']).reshape(-1,1)[tr]\n    model.fit(X, y.ravel())\n    y_hat = model.predict_proba(X = np.array(train['Age_lm']).reshape(-1,1))[:,1][te]\n    y_true = np.array(train['Survived']).reshape(-1,1)[te]\n    ROC_AUC_lm.append(met.roc_auc_score(y_true, y_score = y_hat))\ndisplay(np.average(ROC_AUC_mean))\ndisplay(np.average(ROC_AUC_lm))", "class": "Model Evaluation", "desc": "This code uses 10-fold cross-validation to compare the performance of two logistic regression models with different ways of imputing missing age values by calculating the ROC AUC scores for each method using sklearn and pandas.", "testing": {"class": "Model_Train", "subclass": "find_best_model_class", "subclass_id": 3, "predicted_subclass_probability": 0.44388086}, "cluster": 0}, {"cell_id": 35, "code": "np.max(df[7])", "class": "Model Evaluation", "desc": "This code evaluates the performance of the decision tree classifier by retrieving the maximum ROC AUC score from the DataFrame for a specific max_depth value (7) using numpy.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9973348}, "cluster": 0}, {"cell_id": 50, "code": "# In order to choose a cutoff value for survival, we will tune the cutoff on our original model to maximize the matthews correlation coefficient\nX = data[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']]\ny = data['Survived']\ncutoffs = {}\n# try cutoff values from 0.1 to 0.9\nfor i in np.arange(0.3,0.7,0.001):\n    MCC = []\n    for tr, te in kfold.split(data):\n        X_train, X_test = X.loc[tr], X.loc[te]\n        y_train, y_test = y.loc[tr], y.loc[te]\n        fit = model.fit(X = X_train, y = y_train)\n        y_pred = fit.predict_proba(X_test)[:,1]\n        y_pred = np.where(y_pred < i, 0, 1)\n        MCC.append(met.matthews_corrcoef(y_test, y_pred))\n    cutoffs[i] = np.mean(MCC)\ndisplay(max(cutoffs, key = cutoffs.get))", "class": "Model Evaluation", "desc": "This code tunes the cutoff value for survival prediction by iteratively evaluating the Matthews correlation coefficient (MCC) using 10-fold cross-validation on the original model, searching for the cutoff value that maximizes the MCC using numpy, pandas, and scikit-learn.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.6277582}, "cluster": 1}, {"cell_id": 33, "code": "X, y = data[['Pclass', 'Mrs', 'Mr', 'Master', 'Miss', 'Age','SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Female','Ticket_len5']], data['Survived']\n# create lists of parameters to try to figure out the optimal parameters\n# I started with much bigger ranges and narrowed it down to these\nmin_leafs = range(32,45)\nmax_depth = range(5,15)\n# create an empty dataframe in which to store ROC AUC score\ndf = pd.DataFrame(columns = max_depth, index = min_leafs)\n\n# for each combination of parameter values, figure out the ROC AUC using 10 fold validation\nfor leaf, depth in it.product(min_leafs, max_depth):\n    dtc = tree.DecisionTreeClassifier(min_samples_leaf = leaf, max_depth = depth)\n    kfold = KFold(10, shuffle = True, random_state = 1)\n    ROC_AUC_scores = []\n    for tr, te in kfold.split(data):\n        X_train, X_test = X.loc[tr], X.loc[te]\n        y_train, y_test = y.loc[tr], y.loc[te]\n        # fit the data\n        dtc = dtc.fit(X_train, y_train)\n        # get predictions on the test set\n        y_pred = dtc.predict_proba(X_test)[:,1]\n        # get the ROC on the test set\n        ROC_AUC_scores.append(met.roc_auc_score(y_test, y_pred))\n    # average the ROC from all folds\n    ROC_AUC = np.mean(ROC_AUC_scores)\n    # store average ROC AUC in dataframe\n    df.loc[leaf][depth] = float(ROC_AUC)", "class": "Model Training", "desc": "This code initializes features and target variables, defines a range of parameters for decision tree classifiers, performs 10-fold cross-validation to evaluate the ROC AUC score for each combination of parameters, and stores the average ROC AUC scores in a DataFrame using pandas, numpy, itertools, and scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.8047089}, "cluster": 0}, {"cell_id": 36, "code": "X, y = data[['Pclass', 'Mrs', 'Mr', 'Master', 'Miss', 'Age','SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Female','Ticket_len5']], data['Survived']\n# create an empty list in which to store ROC AUC score\nROC_AUC = []\npredictors = X.columns\nmodel = lm.LogisticRegression(random_state = 8292010, solver = 'liblinear')\nkfold = KFold(10, shuffle = True, random_state = 1)\nselected = []\nmax_AUC = {}\n\n# starting with one predictor, figure out the ROC AUC in a bivariate model, then continuously add one predictor each time to maximize ROC AUC\nfor i in range(0, len(predictors)):\n    next_best = {}\n    for pred in [p for p in predictors if p not in selected]:\n        ROC_AUC_scores = []\n        X = data[selected + [pred]]\n        for tr, te in kfold.split(data):\n            X_train, X_test = X.loc[tr], X.loc[te]\n            y_train, y_test = y.loc[tr], y.loc[te]\n            fit = model.fit(X = X_train, y = y_train)\n            y_pred = fit.predict_proba(X_test)[:,1]\n            ROC_AUC_scores.append(met.roc_auc_score(y_test, y_pred))\n        next_best[pred] = np.mean(ROC_AUC_scores)\n    selected.append(max(next_best, key = next_best.get))\n    max_AUC[', '.join(selected)] = next_best[max(next_best, key = next_best.get)]\nfor i in max_AUC.keys():\n    print(f'{i}: {max_AUC[i]}')", "class": "Model Training", "desc": "This code performs stepwise feature selection using logistic regression with 10-fold cross-validation to iteratively add predictors that maximize the ROC AUC score, storing the results in a dictionary and printing them using pandas, numpy, and scikit-learn.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.6774396}, "cluster": 0}, {"cell_id": 48, "code": "model = lm.LogisticRegression(random_state = 8292010, solver = 'liblinear')\nX = data[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']]\ny = data['Survived']\nfit = model.fit(X = X, y = y)\n\ny_pred = fit.predict_proba(test[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']])[:,1]", "class": "Model Training", "desc": "This code trains a logistic regression model using selected features from the final dataset to predict survival and applies the trained model to predict probabilities for the test dataset using pandas and scikit-learn.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.87436324}, "cluster": 0}, {"cell_id": 34, "code": "# Looking at all of our parameter values, it looks like a max_depth = 7 and min_leaf_size = 38 are our best bets\ndf[df.columns] = df[df.columns].astype(float)\nsns.heatmap(df)", "class": "Visualization", "desc": "This code visualizes the ROC AUC scores for different combinations of decision tree parameters using a heatmap with seaborn to identify the best parameter values.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9982356}, "cluster": -1}], "notebook_id": 3, "notebook_name": "titanic-logistic-regression-and-decision-tree.ipynb", "user": "titanic-logistic-regression-and-decision-tree.ipynb"}, {"cells": [{"cell_id": 30, "code": "#Creating Dataframe to store the Ids with Prediction\noutput=pd.DataFrame({'PassengerId':test_id,'Survived':pred})\nprint(output)", "class": "Data Export", "desc": "This code snippet creates a DataFrame storing the 'PassengerId' and the corresponding survival predictions, then prints the DataFrame using Pandas.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.9984794}, "cluster": -1}, {"cell_id": 31, "code": "output.to_csv(\"Titanic_Survival.csv\",index=False)\nprint(\"Completed\")", "class": "Data Export", "desc": "This code snippet saves the DataFrame containing the survival predictions to a CSV file named \"Titanic_Survival.csv\" and confirms completion by printing \"Completed\" using Pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992403}, "cluster": -1}, {"cell_id": 1, "code": "#importing training data\ntrain_data=pd.read_csv(\"../input/titanic/train.csv\")\ntrain_data.head(10)", "class": "Data Extraction", "desc": "This code loads the training dataset from a CSV file located in the specified directory and displays the first 10 rows using Pandas.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9995395}, "cluster": -1}, {"cell_id": 16, "code": "#Importing testing data\ntest_data=pd.read_csv(\"../input/titanic/test.csv\")\ntest_data.head(10)", "class": "Data Extraction", "desc": "This code snippet loads the testing dataset from a CSV file located in the specified directory and displays the first 10 rows using Pandas.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99959296}, "cluster": -1}, {"cell_id": 4, "code": "#Checking columns with missing data\nMiss_Percent=100*(train_data.isnull().sum()/len(train_data))\n\n#Creating a dataframe to show percentage of missing data and its respective data column in table\nDataFrame=pd.DataFrame(Miss_Percent)\nmiss_percent_table=DataFrame.rename(columns={0:'% of Missing Values'})\nMissPercent=miss_percent_table\n\n#Displaying Missing Value table\nMissPercent", "class": "Data Transform", "desc": "This code snippet calculates the percentage of missing data for each column in the training dataset, creates a DataFrame to display these percentages, and then shows this DataFrame.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.99713886}, "cluster": 7}, {"cell_id": 6, "code": "#Replacing Null Value with unknown as it is Categorical data \ntrain_data['Cabin']=train_data['Cabin'].fillna('Unknown')", "class": "Data Transform", "desc": "This code snippet replaces null values in the 'Cabin' column of the training dataset with the string 'Unknown' using Pandas.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.72266513}, "cluster": 1}, {"cell_id": 8, "code": "#Replacing the null value with Median \ntrain_data['Age']=train_data['Age'].fillna(train_data['Age'].median())", "class": "Data Transform", "desc": "This code snippet replaces null values in the 'Age' column of the training dataset with the median age value using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.89056116}, "cluster": 1}, {"cell_id": 10, "code": "train_data['Embarked']=train_data['Embarked'].fillna('Unknown')", "class": "Data Transform", "desc": "This code snippet replaces null values in the 'Embarked' column of the training dataset with the string 'Unknown' using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.8414222}, "cluster": 1}, {"cell_id": 11, "code": "#Dropping PassengerID, Name and Ticket because they will not have bigger impact predicting the survival \ntrain_data=train_data.drop(['PassengerId','Name','Ticket'],axis=1)", "class": "Data Transform", "desc": "This code snippet drops the 'PassengerId', 'Name', and 'Ticket' columns from the training dataset using Pandas, under the assumption that these columns have minimal impact on predicting survival.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9049642}, "cluster": 6}, {"cell_id": 14, "code": "#Encoding Catergorical data\nencoder = LabelEncoder()\ntrain_data['Sex'] = encoder.fit_transform(train_data['Sex'])\ntrain_data['Cabin'] = encoder.fit_transform(train_data['Cabin'])\ntrain_data['Embarked']=encoder.fit_transform(train_data['Embarked'])\ntrain_data.head(10)", "class": "Data Transform", "desc": "This code snippet encodes categorical columns 'Sex', 'Cabin', and 'Embarked' in the training dataset into numerical values using scikit-learn's LabelEncoder and displays the first 10 rows of the transformed dataset.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99937207}, "cluster": 9}, {"cell_id": 19, "code": "#Replacing the null value with Median \ntest_data['Age']=test_data['Age'].fillna(test_data['Age'].median())", "class": "Data Transform", "desc": "This code snippet replaces null values in the 'Age' column of the testing dataset with the median age value using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9261921}, "cluster": 1}, {"cell_id": 21, "code": "#Replacing the null value with Median \ntest_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].median())", "class": "Data Transform", "desc": "This code snippet replaces null values in the 'Fare' column of the testing dataset with the median fare value using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9895257}, "cluster": 1}, {"cell_id": 23, "code": "#Replacing Null Value with unknown as it is Categorical data \ntest_data['Cabin']=test_data['Cabin'].fillna('Unknown')", "class": "Data Transform", "desc": "This code snippet replaces null values in the 'Cabin' column of the testing dataset with the string 'Unknown' using Pandas.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.66284883}, "cluster": 1}, {"cell_id": 24, "code": "#Dropping Columns because bescause they were dropped in Training data\ntest_id=test_data['PassengerId']\ntest_data=test_data.drop(['PassengerId','Name','Ticket'],axis=1)", "class": "Data Transform", "desc": "This code snippet drops the 'PassengerId', 'Name', and 'Ticket' columns from the testing dataset and stores the 'PassengerId' column in a separate variable `test_id` using Pandas, aligning the structure with the training data.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9989949}, "cluster": 6}, {"cell_id": 25, "code": "#Encoding Testing data\ntest_data['Sex'] = encoder.fit_transform(test_data['Sex'])\ntest_data['Cabin'] = encoder.fit_transform(test_data['Cabin'])\ntest_data['Embarked']=encoder.fit_transform(test_data['Embarked'])", "class": "Data Transform", "desc": "This code snippet encodes the categorical columns 'Sex', 'Cabin', and 'Embarked' in the testing dataset into numerical values using scikit-learn's LabelEncoder.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99932957}, "cluster": 9}, {"cell_id": 27, "code": "#Allocating variable for testing data\nx_test=test_data", "class": "Data Transform", "desc": "This code snippet assigns the processed testing dataset to the variable `x_test` using Pandas.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99759275}, "cluster": 1}, {"cell_id": 2, "code": "train_data.info()", "class": "Exploratory Data Analysis", "desc": "This code snippet uses Pandas to display a concise summary of the training dataset, including the data types, non-null values, and memory usage.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99936634}, "cluster": 1}, {"cell_id": 3, "code": "train_data.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates descriptive statistics for the training dataset, including measures such as mean, standard deviation, and percentiles, using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994442}, "cluster": 1}, {"cell_id": 5, "code": "#Checking Cabin column\ntrain_data['Cabin'].unique()", "class": "Exploratory Data Analysis", "desc": "This code snippet examines the unique values present in the 'Cabin' column of the training dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.99826425}, "cluster": 1}, {"cell_id": 7, "code": "#Checking Age column\ntrain_data['Age'].unique()", "class": "Exploratory Data Analysis", "desc": "This code snippet examines the unique values present in the 'Age' column of the training dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.9981658}, "cluster": 1}, {"cell_id": 9, "code": "#Checking Embarked Column\ntrain_data['Embarked'].unique()", "class": "Exploratory Data Analysis", "desc": "This code snippet examines the unique values present in the 'Embarked' column of the training dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.99811125}, "cluster": 1}, {"cell_id": 17, "code": "test_data.isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code snippet uses Pandas to count the number of null values in each column of the testing dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9989555}, "cluster": 1}, {"cell_id": 18, "code": "#Checking Age data column\ntest_data['Age'].unique()", "class": "Exploratory Data Analysis", "desc": "This code snippet examines the unique values present in the 'Age' column of the testing dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.99808854}, "cluster": 1}, {"cell_id": 20, "code": "#Checking Fare data column\ntest_data['Fare'].unique()", "class": "Exploratory Data Analysis", "desc": "This code snippet examines the unique values present in the 'Fare' column of the testing dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.99812144}, "cluster": 1}, {"cell_id": 22, "code": "#Checking Cabin Column\ntest_data['Cabin'].unique()", "class": "Exploratory Data Analysis", "desc": "This code snippet examines the unique values present in the 'Cabin' column of the testing dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_unique_values", "subclass_id": 57, "predicted_subclass_probability": 0.9981199}, "cluster": 1}, {"cell_id": 26, "code": "#Checking the encoded test data\ntest_data.head(10)", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the first 10 rows of the encoded testing dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99911183}, "cluster": 1}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Libraries for Data Manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Libraries for Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\n%matplotlib inline\n\n#Library for modelling\nfrom sklearn.preprocessing import LabelEncoder #Encodes Categorical Data into Numerical\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This code snippet imports essential libraries for data manipulation, visualization, and modeling\u2014such as NumPy, Pandas, Seaborn, Matplotlib, and scikit-learn\u2014and lists input data files from a specified directory.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.9978358}, "cluster": -1}, {"cell_id": 29, "code": "#Testing the model with test data\npred = random_forest.predict(x_test)", "class": "Model Evaluation", "desc": "This code snippet uses the trained RandomForestClassifier model to predict the target variable for the testing dataset (`x_test`) using scikit-learn.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99370307}, "cluster": 0}, {"cell_id": 15, "code": "#Setting training data into x_train and y_train\nx_train=train_data.drop('Survived',axis=1)\ny_train=train_data['Survived']\n\n#Shapes of x_train,y_train and test data\nx_train.shape, y_train.shape,", "class": "Model Training", "desc": "This code snippet separates the features and target variable in the training dataset into `x_train` and `y_train` respectively, and displays their shapes using Pandas.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.9980792}, "cluster": 0}, {"cell_id": 28, "code": "random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(x_train, y_train)\nrandom_forest.score(x_train, y_train)", "class": "Model Training", "desc": "This code snippet initializes a RandomForestClassifier with 100 estimators, fits the model to the training data (`x_train`, `y_train`), and then calculates the training accuracy score using scikit-learn.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9990081}, "cluster": 1}, {"cell_id": 12, "code": "#Barplot to show the total survivals based on Gender in each Passenger class\nplt.figure(figsize=(12,12))\nsns.barplot(x=\"Pclass\", y=\"Survived\",hue=\"Sex\", data=train_data)", "class": "Visualization", "desc": "This code snippet generates a bar plot using Seaborn and Matplotlib to display the total number of survivors based on gender across different passenger classes.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9939272}, "cluster": -1}, {"cell_id": 13, "code": "#Calculating Correlation \ncorrelation=train_data.corr()\n#Plotting the Correlation in HeatMap for the data columns which has correlation value more than 0.4\nplt.figure(figsize=(12,12))\nCorr_Heatmap=sns.heatmap(correlation,annot=True,cmap=\"GnBu\")", "class": "Visualization", "desc": "This code snippet calculates the correlation matrix for the training dataset using Pandas and then generates a heatmap of the correlations using Seaborn and Matplotlib, highlighting values greater than 0.4.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9982987}, "cluster": 2}], "notebook_id": 4, "notebook_name": "titanic-survival-prediction-using-rfclassifier.ipynb", "user": "titanic-survival-prediction-using-rfclassifier.ipynb"}, {"cells": [{"cell_id": 46, "code": "# Get predictions for each model and create submission files\nfor model in best_models:\n    predictions = best_models[model].predict(test_X)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('submission_' + model + '.csv', index=False)", "class": "Data Export", "desc": "This code snippet generates predictions for each trained model, creates a DataFrame with the `PassengerId` and `Survived` columns, and exports the predictions to CSV files named `submission_<model>.csv` using `pandas`.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992563}, "cluster": -1}, {"cell_id": 1, "code": "train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv')", "class": "Data Extraction", "desc": "This code snippet loads the training and testing datasets for the Titanic dataset from CSV files using `pandas`.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997619}, "cluster": -1}, {"cell_id": 9, "code": "def clean_data(data):\n    # Too many missing values\n    data.drop(['Cabin'], axis=1, inplace=True)\n    \n    # Probably will not provide some useful information\n    data.drop(['Name', 'Ticket', 'Fare', 'Embarked'], axis=1, inplace=True)\n    \n    return data\n    \ntrain_data = clean_data(train_data)\ntest_data = clean_data(test_data)", "class": "Data Transform", "desc": "This code snippet defines and applies a `clean_data` function that removes columns with too many missing values or less useful information (`Cabin`, `Name`, `Ticket`, `Fare`, `Embarked`) from both `train_data` and `test_data` using `pandas`.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9989342}, "cluster": 7}, {"cell_id": 11, "code": "train_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\ntest_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\n\n# Merge two data to get the average Age and fill the column\nall_data = pd.concat([train_data, test_data])\naverage = all_data.Age.median()\nprint(\"Average Age: {0}\".format(average))\ntrain_data.fillna(value={'Age': average}, inplace=True)\ntest_data.fillna(value={'Age': average}, inplace=True)", "class": "Data Transform", "desc": "This code snippet replaces the categorical `Sex` column with numerical values, concatenates `train_data` and `test_data` to calculate and print the median `Age`, and fills missing `Age` values with this median in both datasets using `pandas`.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.29412758}, "cluster": 2}, {"cell_id": 13, "code": "# Set X and y\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\ny = train_data['Survived']\ntest_X = test_data.drop(['PassengerId'], axis=1)", "class": "Data Transform", "desc": "This code snippet sets up the feature matrix `X` and the target vector `y` for the training data by dropping the `Survived` and `PassengerId` columns, and prepares the test data feature matrix `test_X` by dropping the `PassengerId` column using `pandas`.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99934286}, "cluster": 2}, {"cell_id": 2, "code": "train_data", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the content of the training dataset, `train_data`, to understand its structure and contents using `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997675}, "cluster": 1}, {"cell_id": 3, "code": "train_data.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates descriptive statistical summaries of the numerical features in the `train_data` dataset using the `describe()` method from `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994442}, "cluster": 1}, {"cell_id": 4, "code": "print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet prints out the column names of the `train_data` dataset using the `columns` attribute and `tolist()` method from `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99567986}, "cluster": 1}, {"cell_id": 5, "code": "missing_values = train_data.isna().any()\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet identifies and prints the column names in the `train_data` dataset that contain missing values using the `isna()` and `any()` methods from `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.92364883}, "cluster": 1}, {"cell_id": 6, "code": "print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()/len(train_data))))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the percentage of missing values for the `Age`, `Cabin`, and `Embarked` columns in the `train_data` dataset using `pandas` methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99853253}, "cluster": 1}, {"cell_id": 7, "code": "duplicates = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the number of duplicate rows in the `train_data` dataset using the `duplicated().sum()` method from `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_duplicates", "subclass_id": 38, "predicted_subclass_probability": 0.85191345}, "cluster": 1}, {"cell_id": 8, "code": "categorical = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical))", "class": "Exploratory Data Analysis", "desc": "This code snippet identifies and prints the categorical variables in the `train_data` dataset by calculating the number of unique values in each column and sorting them in ascending order using `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.84571636}, "cluster": 1}, {"cell_id": 10, "code": "train_data.tail()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the last few rows of the cleaned `train_data` dataset using the `tail()` method from `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997588}, "cluster": 1}, {"cell_id": 12, "code": "train_data.tail()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the last few rows of the transformed `train_data` dataset using the `tail()` method from `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997588}, "cluster": 1}, {"cell_id": 15, "code": "print(\"Features: \\n{0} \".format(X.columns.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet prints out the feature names (column names) of the `X` dataset using the `columns` attribute and `tolist()` method from `pandas`.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.93314546}, "cluster": 1}, {"cell_id": 0, "code": "import numpy as np\nimport pandas as pd\n\n# Modelling\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\n\n# KNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Perceptron\nfrom sklearn.linear_model import Perceptron\n\n# Support Vector Machines\nfrom sklearn.svm import SVC\n\n# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\n# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\n# LightGBM\nfrom lightgbm import LGBMClassifier", "class": "Imports and Environment", "desc": "This code snippet imports the necessary libraries and machine learning models from packages such as `numpy`, `pandas`, `sklearn`, `xgboost`, and `lightgbm`.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.9993038}, "cluster": -1}, {"cell_id": 17, "code": "evaluate_model(best_model_logistic.best_estimator_, 'logistic')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained logistic regression model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.7345922}, "cluster": 2}, {"cell_id": 19, "code": "evaluate_model(best_model_gaussian_nb.best_estimator_, 'gaussian_nb')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Gaussian Naive Bayes model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.2824658}, "cluster": 2}, {"cell_id": 21, "code": "evaluate_model(best_model_multinominal_nb.best_estimator_, 'multinominal_nb')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Multinomial Naive Bayes model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.20433934}, "cluster": 2}, {"cell_id": 23, "code": "evaluate_model(best_model_complement_nb.best_estimator_, 'complement_nb')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Complement Naive Bayes model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.8224627}, "cluster": 2}, {"cell_id": 25, "code": "evaluate_model(best_model_bernoulli_nb.best_estimator_, 'bernoulli_nb')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Bernoulli Naive Bayes model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.4510578}, "cluster": 2}, {"cell_id": 27, "code": "evaluate_model(best_model_kneighbors.best_estimator_, 'kneighbors')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained k-Nearest Neighbors model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.7882486}, "cluster": 2}, {"cell_id": 29, "code": "evaluate_model(best_model_perceptron.best_estimator_, 'perceptron')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Perceptron model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.6991431}, "cluster": 2}, {"cell_id": 31, "code": "evaluate_model(best_model_svc.best_estimator_, 'svc')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Support Vector Classifier (SVC) model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.73722243}, "cluster": 2}, {"cell_id": 33, "code": "evaluate_model(best_model_sgd.best_estimator_, 'sgd')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Stochastic Gradient Descent (SGD) model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.698045}, "cluster": 2}, {"cell_id": 35, "code": "evaluate_model(best_model_gbc.best_estimator_, 'gbc')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Gradient Boosting Classifier (GBC) model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.7379423}, "cluster": 2}, {"cell_id": 37, "code": "evaluate_model(best_model_adaboost.best_estimator_, 'adaboost')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained AdaBoost model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.8056865}, "cluster": 2}, {"cell_id": 39, "code": "evaluate_model(best_model_decision_tree.best_estimator_, 'decision_tree')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Decision Tree model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.8723382}, "cluster": 2}, {"cell_id": 41, "code": "evaluate_model(best_model_random_forest.best_estimator_, 'random_forest')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained Random Forest model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.65981984}, "cluster": 2}, {"cell_id": 43, "code": "evaluate_model(best_model_xgb.best_estimator_, 'xgb')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained XGBoost model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.74224275}, "cluster": 2}, {"cell_id": 45, "code": "evaluate_model(best_model_lgbm.best_estimator_, 'lgbm')", "class": "Model Evaluation", "desc": "This code snippet evaluates the trained LightGBM model by calculating and printing the accuracy score, and stores the model in the `best_models` dictionary using `evaluate_model`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.7132252}, "cluster": 2}, {"cell_id": 14, "code": "# To store models created\nbest_models = {}\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ndef print_best_parameters(hyperparameters, best_parameters):\n    value = \"Best parameters: \"\n    for key in hyperparameters:\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n    if hyperparameters:\n        print(value[:-2])\n\ndef get_best_model(estimator, hyperparameters, fit_params={}):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters, n_jobs=-1, cv=cv, scoring=\"accuracy\")\n    best_model = grid_search.fit(train_X, train_y, **fit_params)\n    best_parameters = best_model.best_estimator_.get_params()\n    print_best_parameters(hyperparameters, best_parameters)\n    return best_model\n\ndef evaluate_model(model, name):\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n    best_models[name] = model", "class": "Model Training", "desc": "This code snippet initializes the `best_models` dictionary for storing trained models, splits the data into training and validation sets using `train_test_split`, and defines functions for printing the best hyperparameters (`print_best_parameters`), hyperparameter tuning and model training (`get_best_model`), and evaluating models (`evaluate_model`) using methods from `sklearn`.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.8947038}, "cluster": 2}, {"cell_id": 16, "code": "# https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\nhyperparameters = {\n    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n    'penalty' : ['l2'],\n    'C'       : [100, 10, 1.0, 0.1, 0.01]\n}\nestimator = LogisticRegression(random_state=1)\nbest_model_logistic = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `LogisticRegression`, initializes the model with a random state, and finds the best hyperparameters using the `get_best_model` function, which performs grid search with cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.99286205}, "cluster": 2}, {"cell_id": 18, "code": "# https://www.analyticsvidhya.com/blog/2021/01/gaussian-naive-bayes-with-hyperpameter-tuning/\nhyperparameters = {\n    'var_smoothing': np.logspace(0, -9, num=100)\n}\nestimator = GaussianNB()\nbest_model_gaussian_nb = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `GaussianNB` (specifically `var_smoothing`), initializes the model, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.94975966}, "cluster": 2}, {"cell_id": 20, "code": "# https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6\nhyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = MultinomialNB()\nbest_model_multinominal_nb = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `MultinomialNB` (including `alpha` and `fit_prior`), initializes the model, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.8767018}, "cluster": 2}, {"cell_id": 22, "code": "hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n    'norm'      : [True, False]\n}\nestimator = ComplementNB()\nbest_model_complement_nb = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `ComplementNB` (including `alpha`, `fit_prior`, and `norm`), initializes the model, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.8994227}, "cluster": 2}, {"cell_id": 24, "code": "hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = BernoulliNB()\nbest_model_bernoulli_nb = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `BernoulliNB` (including `alpha` and `fit_prior`), initializes the model, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.6762601}, "cluster": 2}, {"cell_id": 26, "code": "# https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f\nhyperparameters = {\n    'n_neighbors' : list(range(1,5)),\n    'weights'     : ['uniform', 'distance'],\n    'algorithm'   : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size'   : list(range(1,10)),\n    'p'           : [1,2]\n}\nestimator = KNeighborsClassifier()\nbest_model_kneighbors = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `KNeighborsClassifier` (including `n_neighbors`, `weights`, `algorithm`, `leaf_size`, and `p`), initializes the model, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.76489675}, "cluster": 2}, {"cell_id": 28, "code": "# https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/\n# https://machinelearningmastery.com/manually-optimize-hyperparameters/\nhyperparameters = {\n    'penalty'  : ['l1', 'l2', 'elasticnet'],\n    'eta0'     : [0.0001, 0.001, 0.01, 0.1, 1.0],\n    'max_iter' : list(range(50, 200, 50))\n}\nestimator = Perceptron(random_state=1)\nbest_model_perceptron = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `Perceptron` (including `penalty`, `eta0`, and `max_iter`), initializes the model with a random state, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.9484464}, "cluster": 2}, {"cell_id": 30, "code": "# https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n# https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167\nhyperparameters = {\n    'C'      : [0.1, 1, 10, 100],\n    'gamma'  : [0.0001, 0.001, 0.01, 0.1, 1],\n    'kernel' : ['rbf']\n}\nestimator = SVC(random_state=1)\nbest_model_svc = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `SVC` (including `C`, `gamma`, and `kernel`), initializes the model with a random state, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.8621029}, "cluster": 2}, {"cell_id": 32, "code": "# https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4\n# https://www.knowledgehut.com/tutorials/machine-learning/hyperparameter-tuning-machine-learning\nhyperparameters = {\n    'loss'    : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n    'penalty' : ['l1', 'l2', 'elasticnet'],\n    'alpha'   : [0.01, 0.1, 1, 10]\n}\nestimator = SGDClassifier(random_state=1, early_stopping=True)\nbest_model_sgd = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `SGDClassifier` (including `loss`, `penalty`, and `alpha`), initializes the model with a random state and early stopping, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.9670545}, "cluster": 2}, {"cell_id": 34, "code": "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\nhyperparameters = {\n    'loss'          : ['deviance', 'exponential'],\n    'learning_rate' : [0.01, 0.1, 0.2, 0.3],\n    'n_estimators'  : [50, 100, 200],\n    'subsample'     : [0.1, 0.2, 0.5, 1.0],\n    'max_depth'     : [2, 3, 4, 5]\n}\nestimator = GradientBoostingClassifier(random_state=1)\nbest_model_gbc = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `GradientBoostingClassifier` (including `loss`, `learning_rate`, `n_estimators`, `subsample`, and `max_depth`), initializes the model with a random state, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.95942765}, "cluster": 2}, {"cell_id": 36, "code": "# https://medium.com/@chaudhurysrijani/tuning-of-adaboost-with-computational-complexity-8727d01a9d20\nhyperparameters = {\n    'n_estimators'  : [10, 50, 100, 500],\n    'learning_rate' : [0.001, 0.01, 0.1, 1.0]\n}\nestimator = AdaBoostClassifier(random_state=1)\nbest_model_adaboost = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `AdaBoostClassifier` (including `n_estimators` and `learning_rate`), initializes the model with a random state, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.9942344}, "cluster": 2}, {"cell_id": 38, "code": "# https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n# https://www.kaggle.com/gauravduttakiit/hyperparameter-tuning-in-decision-trees\nhyperparameters = {\n    'criterion'         : ['gini', 'entropy'],\n    'splitter'          : ['best', 'random'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = DecisionTreeClassifier(random_state=1)\nbest_model_decision_tree = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `DecisionTreeClassifier` (including `criterion`, `splitter`, `max_depth`, `min_samples_split`, and `min_samples_leaf`), initializes the model with a random state, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.8945561}, "cluster": 2}, {"cell_id": 40, "code": "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n# https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/\nhyperparameters = {\n    'n_estimators'      : list(range(10, 50, 10)),\n    'max_features'      : ['auto', 'sqrt', 'log2'],\n    'criterion'         : ['gini', 'entropy'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = RandomForestClassifier(random_state=1)\nbest_model_random_forest = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `RandomForestClassifier` (including `n_estimators`, `max_features`, `criterion`, `max_depth`, `min_samples_split`, and `min_samples_leaf`), initializes the model with a random state, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "define_search_space", "subclass_id": 5, "predicted_subclass_probability": 0.47099417}, "cluster": 2}, {"cell_id": 42, "code": "# https://towardsdatascience.com/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d\nhyperparameters = {\n    'learning_rate' : [0.3, 0.4, 0.5],\n    'gamma'         : [0, 0.4, 0.8],\n    'max_depth'     : [2, 3, 4],\n    'reg_lambda'    : [0, 0.1, 1],\n    'reg_alpha'     : [0.1, 1]\n}\nfit_params = {\n    'verbose'               : False,\n    'early_stopping_rounds' : 40,\n    'eval_metric'           : 'logloss',\n    'eval_set'              : [(val_X, val_y)]\n}\nestimator = XGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False)\nbest_model_xgb = get_best_model(estimator, hyperparameters, fit_params)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `XGBClassifier` (including `learning_rate`, `gamma`, `max_depth`, `reg_lambda`, and `reg_alpha`), initializes the model with specific settings for GPU usage and seeds, and finds the best hyperparameters using the `get_best_model` function with grid search, cross-validation, and additional fit parameters using `sklearn` and `xgboost`.", "testing": {"class": "Model_Train", "subclass": "init_hyperparams", "subclass_id": 59, "predicted_subclass_probability": 0.80115277}, "cluster": 2}, {"cell_id": 44, "code": "# https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\nhyperparameters = {\n    'boosting_type' : ['gbdt', 'dart', 'goss'],\n    'num_leaves'    : [4, 8, 16, 32],\n    'learning_rate' : [0.01, 0.1, 1],\n    'n_estimators'  : [25, 50, 100],\n    'reg_alpha'     : [0, 0.1, 1],\n    'reg_lambda'    : [0, 0.1, 1],\n}\nestimator = LGBMClassifier(random_state=1, device='gpu')\nbest_model_lgbm = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet sets up a grid of hyperparameters for `LGBMClassifier` (including `boosting_type`, `num_leaves`, `learning_rate`, `n_estimators`, `reg_alpha`, and `reg_lambda`), initializes the model with a random state and GPU support, and finds the best hyperparameters using the `get_best_model` function with grid search and cross-validation using `sklearn`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.9681083}, "cluster": 2}], "notebook_id": 5, "notebook_name": "titanic-hyperparameter-tuning-gridsearchcv.ipynb", "user": "titanic-hyperparameter-tuning-gridsearchcv.ipynb"}, {"cells": [{"cell_id": 29, "code": "predictions = rfc1.predict(test_data[features])\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Data Export", "desc": "This code snippet generates predictions for the test dataset using the fitted RandomForestClassifier, creates a DataFrame with 'PassengerId' and 'Survived' columns, and exports it to a CSV file named 'submission.csv' using the pandas `to_csv` method.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99908555}, "cluster": -1}, {"cell_id": 1, "code": "train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')", "class": "Data Extraction", "desc": "This code snippet reads the training and test datasets from CSV files into pandas DataFrames.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997478}, "cluster": -1}, {"cell_id": 4, "code": "train_data['female'] = pd.get_dummies(train_data['Sex'])['female']\ntest_data['female'] = pd.get_dummies(test_data['Sex'])['female']", "class": "Data Transform", "desc": "This code snippet creates a new binary column 'female' in both the train and test datasets by applying one-hot encoding on the 'Sex' column using pandas' `get_dummies` function.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99779177}, "cluster": 9}, {"cell_id": 5, "code": "sum(train_data['Age'].isnull())\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())", "class": "Data Transform", "desc": "This code snippet counts the number of missing values in the 'Age' column and then fills any missing values in the 'Age' column of both the training and test datasets with the mean of the respective column using pandas' `fillna` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.97469246}, "cluster": 1}, {"cell_id": 10, "code": "train_data['class1'] = pd.get_dummies(train_data.Pclass)[1]\ntest_data['class1'] = pd.get_dummies(test_data.Pclass)[1]\ntrain_data['class2'] = pd.get_dummies(train_data.Pclass)[2]\ntest_data['class2'] = pd.get_dummies(test_data.Pclass)[2]", "class": "Data Transform", "desc": "This code snippet creates new binary columns 'class1' and 'class2' in both the train and test datasets by applying one-hot encoding on the 'Pclass' column using pandas' `get_dummies` function.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9973912}, "cluster": 9}, {"cell_id": 12, "code": "sibs = train_data.loc[train_data.SibSp <= 1]['Survived']\nprint(sum(sibs)/len(sibs))\ntrain_data['many_sibs'] = (train_data.SibSp > 1)*1\ntest_data['many_sibs'] = (test_data.SibSp > 1)*1", "class": "Data Transform", "desc": "This code snippet calculates and prints the survival rate of passengers with one or no siblings/spouses and creates a new binary column 'many_sibs' indicating if a passenger has more than one sibling/spouse in both train and test datasets.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9980178}, "cluster": -1}, {"cell_id": 14, "code": "bins = [0.42, 15, 30, 50,80]\ntrain_data['bin_age'] = pd.cut(x=train_data.Age, bins=bins)\ntest_data['bin_age'] = pd.cut(x=test_data.Age, bins=bins)", "class": "Data Transform", "desc": "This code snippet creates a new column 'bin_age' in both the train and test datasets that categorizes passengers' ages into bins using the `cut` function from pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.5303526}, "cluster": 2}, {"cell_id": 15, "code": "train_data['young'] = pd.get_dummies(train_data.bin_age).iloc[:,0]\ntest_data['young'] = pd.get_dummies(test_data.bin_age).iloc[:,0]\ntrain_data['senior'] = pd.get_dummies(train_data.bin_age).iloc[:,3]\ntest_data['senior'] = pd.get_dummies(test_data.bin_age).iloc[:,3]", "class": "Data Transform", "desc": "This code snippet creates new binary columns 'young' and 'senior' in both the train and test datasets by applying one-hot encoding to the 'bin_age' column using pandas' `get_dummies` function.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99718374}, "cluster": 9}, {"cell_id": 25, "code": "test_data.Fare = test_data.Fare.fillna(test_data.Fare.mean())", "class": "Data Transform", "desc": "This code snippet fills any missing values in the 'Fare' column of the test dataset with the mean of the 'Fare' column using pandas' `fillna` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.27125782}, "cluster": 1}, {"cell_id": 2, "code": "train_data.head()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the first five rows of the training dataset using the pandas `head` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997532}, "cluster": 1}, {"cell_id": 3, "code": "women = train_data.loc[train_data.Sex == 'female']['Survived']\nprint('Women survived',sum(women)/len(women))\n\nmen = train_data.loc[train_data.Sex == 'male']['Survived']\nprint('Men survived',sum(men)/len(men))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the survival rates of women and men from the training dataset using pandas' `loc` method.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.29674977}, "cluster": 1}, {"cell_id": 7, "code": "high_fare = train_data.loc[train_data.Fare > 100]['Survived']\nprint('High fare survivors',sum(high_fare)/len(high_fare))\nlow_fare = train_data.loc[train_data.Fare < 32]['Survived']\nprint('High fare survivors',sum(low_fare)/len(low_fare))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the survival rates of passengers with high fare (greater than 100) and low fare (less than 32) from the training dataset using pandas' `loc` method.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.97302115}, "cluster": 1}, {"cell_id": 8, "code": "pclass1 = train_data.loc[train_data.Pclass == 1]['Survived']\nprint('Class1',sum(pclass1)/len(pclass1))\npclass2 = train_data.loc[train_data.Pclass == 2]['Survived']\nprint('Class2',sum(pclass2)/len(pclass2))\npclass3 = train_data.loc[train_data.Pclass == 3]['Survived']\nprint('Class3',sum(pclass3)/len(pclass3))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the survival rates for passengers in each class (1st, 2nd, and 3rd) from the training dataset using pandas' `loc` method.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.92158645}, "cluster": 1}, {"cell_id": 9, "code": "sum(test_data.Pclass.isna())", "class": "Exploratory Data Analysis", "desc": "This code snippet counts the number of missing values in the 'Pclass' column of the test dataset using pandas' `isna` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99908054}, "cluster": 1}, {"cell_id": 11, "code": "sum(test_data.SibSp.isna())", "class": "Exploratory Data Analysis", "desc": "This code snippet counts the number of missing values in the 'SibSp' column of the test dataset using pandas' `isna` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9991391}, "cluster": 1}, {"cell_id": 13, "code": "young = train_data.loc[train_data.Age <= 15]['Survived']\nprint(sum(young)/len(young))\n\nold = train_data.loc[train_data.Age >=40]['Survived']\nprint(sum(old)/len(old))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the survival rates for passengers aged 15 or younger and those aged 40 or older from the training dataset using pandas' `loc` method.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.9821845}, "cluster": 1}, {"cell_id": 16, "code": "train_data.corr()['Survived']\n\nfeatures = ['Pclass', 'Fare', 'female', 'class1', 'class2', 'many_sibs', 'young', 'senior']", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and displays the correlation of each feature with the 'Survived' column in the training dataset, and defines a list of selected features for further analysis or modeling.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99760026}, "cluster": 1}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This code snippet imports essential libraries for data analysis (numpy, pandas) and visualization (matplotlib, seaborn), and lists files in the Kaggle input directory using the os module.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.999241}, "cluster": -1}, {"cell_id": 17, "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix", "class": "Imports and Environment", "desc": "This code snippet imports the `train_test_split` function, `LogisticRegression` model, and evaluation metrics `accuracy_score` and `confusion_matrix` from the `sklearn` library.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99930274}, "cluster": -1}, {"cell_id": 26, "code": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n#CV_rfc = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)\n#CV_rfc.fit(X_train, y_train)", "class": "Imports and Environment", "desc": "This code snippet imports `cross_val_score`, `GridSearchCV`, `RandomForestClassifier`, `accuracy_score` from `sklearn`, initializes a parameter grid for hyperparameter tuning in a RandomForestClassifier, and comments out the initialization and fitting of `GridSearchCV`.", "testing": {"class": "Model_Train", "subclass": "define_search_space", "subclass_id": 5, "predicted_subclass_probability": 0.85772324}, "cluster": -1}, {"cell_id": 20, "code": "accuracy_score(y_pred, y_test)", "class": "Model Evaluation", "desc": "This code snippet computes the accuracy score of the logistic regression model by comparing the predicted values (`y_pred`) with the actual test target values (`y_test`) using the `accuracy_score` function from `sklearn`.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99797374}, "cluster": -1}, {"cell_id": 21, "code": "confusion_matrix(y_pred, y_test)", "class": "Model Evaluation", "desc": "This code snippet calculates the confusion matrix for the logistic regression model's predictions on the test data using the `confusion_matrix` function from `sklearn`.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9979062}, "cluster": 0}, {"cell_id": 23, "code": "accuracy_score(y_pred, y_test)", "class": "Model Evaluation", "desc": "This code snippet computes the accuracy score of the Random Forest classifier by comparing the predicted values (`y_pred`) with the actual test target values (`y_test`) using the `accuracy_score` function from `sklearn`.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99797374}, "cluster": -1}, {"cell_id": 24, "code": "confusion_matrix(y_pred, y_test)", "class": "Model Evaluation", "desc": "This code snippet calculates the confusion matrix for the Random Forest classifier's predictions on the test data using the `confusion_matrix` function from `sklearn`.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9979062}, "cluster": 0}, {"cell_id": 18, "code": "X = train_data[features]\ny = train_data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)", "class": "Model Training", "desc": "This code snippet splits the training dataset into training and testing sets for features and target variable using the `train_test_split` function from `sklearn` with a test size of 33% and a random state of 0.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9982145}, "cluster": 0}, {"cell_id": 19, "code": "log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\ny_pred", "class": "Model Training", "desc": "This code snippet initializes a `LogisticRegression` model, fits it to the training data, and predicts the target variable for the test data.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.7683394}, "cluster": 0}, {"cell_id": 22, "code": "from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)", "class": "Model Training", "desc": "This code snippet initializes a `RandomForestClassifier` with 100 estimators and a maximum depth of 5, fits it to the training data, and predicts the target variable for the test data.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.977549}, "cluster": 0}, {"cell_id": 27, "code": "#CV_rfc.best_params_", "class": "Model Training", "desc": "This code snippet (currently commented out) retrieves the best hyperparameters for the RandomForestClassifier identified by the `GridSearchCV` after fitting the model.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.9968646}, "cluster": 0}, {"cell_id": 28, "code": "rfc1=RandomForestClassifier(random_state=42, max_features='log2', n_estimators= 200, max_depth=6, criterion='entropy')\nrfc1.fit(X_train, y_train)", "class": "Model Training", "desc": "This code snippet initializes a `RandomForestClassifier` with specific hyperparameters (`max_features='log2'`, `n_estimators=200`, `max_depth=6`, `criterion='entropy'`, `random_state=42`) and fits it to the training data.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9996785}, "cluster": 1}, {"cell_id": 6, "code": "plt.subplot(1,2,1)\nsns.histplot(train_data.Age)\nplt.subplot(1,2,2)\nsns.histplot(test_data.Age)", "class": "Visualization", "desc": "This code snippet creates two side-by-side histograms to visualize the distribution of the 'Age' column in both the training and test datasets using Matplotlib's `subplot` and Seaborn's `histplot` functions.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99291444}, "cluster": 0}], "notebook_id": 6, "notebook_name": "titanic-tutorial-vmindel.ipynb", "user": "titanic-tutorial-vmindel.ipynb"}, {"cells": [{"cell_id": 16, "code": "submit = pd.DataFrame({'PassengerId': final_test_data_id, 'Survived': prediction})", "class": "Data Export", "desc": "This code creates a pandas DataFrame named `submit` containing the 'PassengerId' and the corresponding 'Survived' predictions for submission.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.9983236}, "cluster": -1}, {"cell_id": 17, "code": "submit.to_csv(\"svm_titanic_sandorabad_7.csv\", index=False)", "class": "Data Export", "desc": "This code exports the `submit` DataFrame to a CSV file named \"svm_titanic_sandorabad_7.csv\" without including the index.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.999323}, "cluster": -1}, {"cell_id": 1, "code": "#Reading Data\ndata = pd.read_csv(\"../input/titanic/train.csv\") \nfinal_test_data = pd.read_csv(\"../input/titanic/test.csv\")\nfinal_test_data_id = np.array(final_test_data['PassengerId']) # we use it for our submission", "class": "Data Extraction", "desc": "This code reads the training and test datasets from CSV files using pandas and extracts the PassengerId column from the test data into a NumPy array for later use in submission.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997137}, "cluster": -1}, {"cell_id": 3, "code": "# we are droppin less relevant columns.\ndata.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'],axis=1, inplace=True)\ndata", "class": "Data Transform", "desc": "This code removes the 'PassengerId', 'Name', 'Ticket', and 'Cabin' columns from the training dataset using the pandas `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991437}, "cluster": 6}, {"cell_id": 5, "code": "mean_age = np.mean(data['Age'])\ndata['Age'] = data['Age'].fillna(mean_age)\ndata", "class": "Data Transform", "desc": "This code fills the missing values in the 'Age' column of the training dataset with the mean age using NumPy's `mean` function and pandas' `fillna` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99761415}, "cluster": 1}, {"cell_id": 6, "code": "# We need to transform the 'Sex' data, 'male' = 1 'female' = 0\ndata[\"Sex\"].replace('male', 1, inplace=True)\ndata[\"Sex\"].replace('female', 0, inplace=True)", "class": "Data Transform", "desc": "This code converts the 'Sex' column values in the training dataset from categorical 'male' and 'female' to numerical 1 and 0, respectively, using the pandas `replace` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9895449}, "cluster": 0}, {"cell_id": 7, "code": "# C = Cherbourg, Q = Queenstown, S = Southampton, after reading: \"https://www.kaggle.com/brendan45774/titanic-top-solution/notebook\" we now that this feature is important\n\ndata['Cherbourg'] = data['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\ndata['Queenstown'] = data['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\ndata['Southampton'] = data['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\ndata.drop('Embarked', axis=1, inplace=True)", "class": "Data Transform", "desc": "This code transforms the 'Embarked' column into three separate binary columns ('Cherbourg', 'Queenstown', 'Southampton') using lambda functions with pandas `apply` method and then drops the original 'Embarked' column.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9882433}, "cluster": 5}, {"cell_id": 8, "code": "X = np.array(data.drop(['Survived'], axis=1))\ny = np.array(data['Survived'])", "class": "Data Transform", "desc": "This code separates the features and target variable by converting the training dataset excluding the 'Survived' column into a NumPy array `X` and the 'Survived' column into a NumPy array `y`.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.9993857}, "cluster": 2}, {"cell_id": 12, "code": "# we apply everything that we used with the \"train.csv\" dataset to the \"test.csv\" dataset\nfinal_test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\nfinal_test_data['Age'].fillna(np.mean(final_test_data['Age']), inplace=True)\nfinal_test_data['Fare'].fillna(np.mean(final_test_data['Fare']), inplace=True)\nfinal_test_data[\"Sex\"].replace('male', 1, inplace=True)\nfinal_test_data[\"Sex\"].replace('female', 0, inplace=True)\nfinal_test_data['Cherbourg'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\nfinal_test_data['Queenstown'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\nfinal_test_data['Southampton'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\nfinal_test_data.drop('Embarked', axis=1, inplace=True)\n\n\nX_final_test = np.array(final_test_data)", "class": "Data Transform", "desc": "This code preprocesses the test dataset by dropping irrelevant columns, filling missing values for 'Age' and 'Fare', converting 'Sex' to a numerical format, transforming the 'Embarked' column into binary columns, and then converts the final transformed test data into a NumPy array `X_final_test`.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99385875}, "cluster": 2}, {"cell_id": 2, "code": "data.head()", "class": "Exploratory Data Analysis", "desc": "This code displays the first five rows of the training dataset using the pandas `head` method to get an initial look at the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974936}, "cluster": 0}, {"cell_id": 4, "code": "# We have missing values for the column age, we are going to meke those values outliers so we can still use the data\nprint(data.info(), data.describe())", "class": "Exploratory Data Analysis", "desc": "This code outputs the summary information and descriptive statistics of the training dataset using the pandas `info` and `describe` methods to assess data types and detect missing values.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994295}, "cluster": 1}, {"cell_id": 13, "code": "final_test_data", "class": "Exploratory Data Analysis", "desc": "This code outputs the current state of the final test dataset to inspect the applied transformations.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997675}, "cluster": 0}, {"cell_id": 14, "code": "X_final_test", "class": "Exploratory Data Analysis", "desc": "This code outputs the variable `X_final_test` which contains the test dataset transformed and converted into a NumPy array for model prediction.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99973935}, "cluster": 1}, {"cell_id": 0, "code": "import numpy as np\nfrom sklearn import preprocessing, svm\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd", "class": "Imports and Environment", "desc": "This code imports the necessary libraries including NumPy, pandas, and scikit-learn modules for preprocessing, support vector machine (SVM) modeling, and data splitting.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99935955}, "cluster": -1}, {"cell_id": 18, "code": "# I based this nootebook on the tutorials of the youtuber: sentdex, \n# Specially on this series: \"https://pythonprogramming.net/machine-learning-tutorial-python-introduction/\" if someone is just starting as I am i really recomend it", "class": "Imports and Environment", "desc": "This code provides a reference to the YouTube tutorial series by Sentdex, specifically mentioning the machine learning series, as a helpful resource for beginners.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.445148}, "cluster": -1}, {"cell_id": 11, "code": "# we test the model\nmodel_clf.fit(X_train, y_train)\naccuracy = model_clf.score(X_test, y_test)\nprint(accuracy)", "class": "Model Evaluation", "desc": "This code trains the SVM model on the training data using the `fit` method and then evaluates its accuracy on the test data using the `score` method, printing the resulting accuracy.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.99956805}, "cluster": -1}, {"cell_id": 15, "code": "prediction = model_clf.predict(X_final_test)", "class": "Model Evaluation", "desc": "This code generates predictions for the test dataset using the previously trained SVM model by calling its `predict` method with `X_final_test` as input.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99418724}, "cluster": 2}, {"cell_id": 9, "code": "# We create the train, test splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)", "class": "Model Training", "desc": "This code splits the dataset into training and testing sets using scikit-learn's `train_test_split` function with 33% of the data reserved for testing.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9983494}, "cluster": 0}, {"cell_id": 10, "code": "# We create the model (classifier)\nmodel_clf = svm.SVC(kernel='linear')", "class": "Model Training", "desc": "This code initializes a Support Vector Classifier (SVC) model with a linear kernel using scikit-learn's `svm.SVC` class.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.83985466}, "cluster": 1}], "notebook_id": 7, "notebook_name": "titanic-07.ipynb", "user": "titanic-07.ipynb"}, {"cells": [{"cell_id": 25, "code": "# Get predictions for each model and create submission files\n\nfor model in best_models:\n\n    predictions = best_models[model].predict(test_X)\n\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n\n    output.to_csv('submission_' + model + '.csv', index=False)", "class": "Data Export", "desc": "This code snippet generates predictions for each model in the `best_models` dictionary, creates a DataFrame with `PassengerId` and `Survived` columns, and exports it as a CSV file named `submission_<model>.csv`.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992563}, "cluster": -1}, {"cell_id": 1, "code": "train_data = pd.read_csv('../input/titanic/train.csv')\n\ntest_data = pd.read_csv('../input/titanic/test.csv')", "class": "Data Extraction", "desc": "This code snippet reads the training and test datasets for the Titanic survival prediction task from CSV files using pandas' `read_csv` function.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997619}, "cluster": -1}, {"cell_id": 9, "code": "for data in [train_data, test_data]:\n\n    # Too many missing values\n\n    data.drop(['Cabin'], axis=1, inplace=True)\n\n    # Probably will not provide some useful information\n\n    data.drop(['Ticket', 'Fare'], axis=1, inplace=True)", "class": "Data Transform", "desc": "This code snippet drops the 'Cabin', 'Ticket', and 'Fare' columns from both the `train_data` and `test_data` DataFrames using pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99880826}, "cluster": 5}, {"cell_id": 11, "code": "# Find the women and boys\n\nfor data in [train_data, test_data]:\n\n    data['Title'] = data.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\n\n    data['Woman_Or_Boy'] = (data.Title == 'Master') | (data.Sex == 'female')\n\n    data.drop('Title', axis=1, inplace=True)\n\n    data.drop('Name', axis=1, inplace=True)", "class": "Data Transform", "desc": "This code snippet creates a new column `Title` by extracting titles from the `Name` column, then creates a `Woman_Or_Boy` column to indicate whether a passenger is a woman or a boy, and finally drops the `Title` and `Name` columns from both the `train_data` and `test_data` DataFrames.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.793178}, "cluster": 5}, {"cell_id": 12, "code": "# Encode 'Sex' and 'Woman_Or_Boy' columns\n\nlabel_encoder = LabelEncoder()\n\nfor data in [train_data, test_data]:\n\n    data['Sex'] = label_encoder.fit_transform(data['Sex'])\n\n    data['Woman_Or_Boy'] = label_encoder.fit_transform(data['Woman_Or_Boy'])", "class": "Data Transform", "desc": "This code snippet encodes the 'Sex' and 'Woman_Or_Boy' columns in both `train_data` and `test_data` DataFrames using `LabelEncoder` from sklearn.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9993673}, "cluster": 5}, {"cell_id": 13, "code": "# Merge two data to get the average Age and fill the column\n\nall_data = pd.concat([train_data, test_data])\n\naverage = all_data.Age.median()\n\nprint(\"Average Age: {0}\".format(average))\n\nfor data in [train_data, test_data]:\n\n    data.fillna(value={'Age': average}, inplace=True)", "class": "Data Transform", "desc": "This code snippet merges the `train_data` and `test_data` DataFrames to calculate the median age, then fills the missing values in the `Age` column with this median value for both datasets.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.35051963}, "cluster": 5}, {"cell_id": 14, "code": "# Get the most common Embark and fill the column\n\nmost_common = all_data.Embarked.mode()\n\nprint(\"Most common Embarked value: {0}\".format(most_common[0]))\n\nfor data in [train_data, test_data]:\n\n    data.fillna(value={'Embarked': most_common[0]}, inplace=True)", "class": "Data Transform", "desc": "This code snippet finds the most common value in the `Embarked` column and fills the missing values in this column with the most common value for both the `train_data` and `test_data` DataFrames.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.59436584}, "cluster": 5}, {"cell_id": 15, "code": "# Create categorical variable for traveling alone\n\n# Credits to https://www.kaggle.com/vaishvik25/titanic-eda-fe-3-model-decision-tree-viz\n\nfor data in [train_data, test_data]:\n\n    data['TravelAlone'] = np.where(data[\"SibSp\"] + data[\"Parch\"] > 0, 0, 1)\n\n    data.drop('SibSp', axis=1, inplace=True)\n\n    data.drop('Parch', axis=1, inplace=True)", "class": "Data Transform", "desc": "This code snippet creates a new categorical variable `TravelAlone` to indicate if a passenger is traveling alone, drops the `SibSp` and `Parch` columns from both `train_data` and `test_data` DataFrames, based on the condition that a passenger is alone if the sum of `SibSp` and `Parch` is zero.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9941294}, "cluster": 5}, {"cell_id": 16, "code": "# Encode 'Embarked' column\n\none_hot_encoder = OneHotEncoder(sparse=False)\n\ndef encode_embarked(data):\n\n    encoded = pd.DataFrame(one_hot_encoder.fit_transform(data[['Embarked']]))\n\n    encoded.columns = one_hot_encoder.get_feature_names(['Embarked'])\n\n    data.drop(['Embarked'], axis=1, inplace=True)\n\n    data = data.join(encoded)\n\n    return data\n\ntrain_data = encode_embarked(train_data)\n\ntest_data = encode_embarked(test_data)", "class": "Data Transform", "desc": "This code snippet encodes the `Embarked` column using `OneHotEncoder` from sklearn, creating new columns for each category, and drops the original `Embarked` column before merging the encoded columns back into both the `train_data` and `test_data` DataFrames.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99923754}, "cluster": 5}, {"cell_id": 18, "code": "# Set X and y\n\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\n\ny = train_data['Survived']\n\ntest_X = test_data.drop(['PassengerId'], axis=1)", "class": "Data Transform", "desc": "This code snippet separates features (X) and the target variable (y) from the `train_data` DataFrame by dropping `Survived` and `PassengerId` columns, and prepares the feature set `test_X` from `test_data` by dropping the `PassengerId` column.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99934286}, "cluster": 2}, {"cell_id": 2, "code": "train_data", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the contents of the `train_data` DataFrame to provide an initial look at the training dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997675}, "cluster": 1}, {"cell_id": 3, "code": "train_data.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates summary statistics of the numerical columns in the `train_data` DataFrame using the `describe` method from pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994442}, "cluster": 1}, {"cell_id": 4, "code": "print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet prints the names of all columns in the `train_data` DataFrame by converting the columns to a list and formatting it into a string.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.99567986}, "cluster": 1}, {"cell_id": 5, "code": "missing_values = train_data.isna().any()\n\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet identifies columns in the `train_data` DataFrame that have missing values by checking for NaN entries and prints their names as a list.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.92364883}, "cluster": 1}, {"cell_id": 6, "code": "print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()/len(train_data))))\n\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()/len(train_data))))\n\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()/len(train_data))))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the percentage of missing values in the `Age`, `Cabin`, and `Embarked` columns of the `train_data` DataFrame.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99853253}, "cluster": 1}, {"cell_id": 7, "code": "duplicates = train_data.duplicated().sum()\n\nprint('Duplicates in train data: {0}'.format(duplicates))", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the number of duplicate rows in the `train_data` DataFrame by using the `duplicated` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_duplicates", "subclass_id": 38, "predicted_subclass_probability": 0.85191345}, "cluster": 1}, {"cell_id": 8, "code": "categorical = train_data.nunique().sort_values(ascending=True)\n\nprint('Categorical variables in train data: \\n{0}'.format(categorical))", "class": "Exploratory Data Analysis", "desc": "This code snippet identifies and prints the number of unique values in each column of the `train_data` DataFrame, ordered in ascending order, to help identify categorical variables.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.84571636}, "cluster": 1}, {"cell_id": 10, "code": "train_data.tail()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the last few rows of the `train_data` DataFrame to review the recent changes or data structure.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997588}, "cluster": 0}, {"cell_id": 17, "code": "train_data.tail()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the last few rows of the `train_data` DataFrame to review the recent changes or data structure.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997588}, "cluster": 0}, {"cell_id": 20, "code": "print(\"Features: \\n{0} \".format(X.columns.tolist()))", "class": "Exploratory Data Analysis", "desc": "This code snippet prints the names of all feature columns in the `X` DataFrame by converting the column names to a list and formatting it into a string.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.93314546}, "cluster": 1}, {"cell_id": 0, "code": "import numpy as np\n\nimport pandas as pd\n\n\n\n# Encoders\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\n# Modelling\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\n\n\n\n# Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n# XGBoost\n\nfrom xgboost import XGBClassifier\n\n\n\n# LightGBM\n\nfrom lightgbm import LGBMClassifier\n\n\n\n# Voting Classifier\n\nfrom sklearn.ensemble import VotingClassifier", "class": "Imports and Environment", "desc": "This code snippet imports various libraries and modules including NumPy, pandas, sklearn for preprocessing, model selection, and evaluation, as well as machine learning models RandomForestClassifier, XGBClassifier, LGBMClassifier, and VotingClassifier.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99930537}, "cluster": -1}, {"cell_id": 24, "code": "evaluate_model(best_model_voting.best_estimator_, 'voting')", "class": "Model Evaluation", "desc": "This code snippet evaluates the best voting classifier model obtained from the grid search by printing its accuracy and storing it in the `best_models` dictionary under the name 'voting'.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.86052555}, "cluster": 2}, {"cell_id": 19, "code": "# To store models created\n\nbest_models = {}\n\n\n\n# Split data\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n\n\ndef print_best_parameters(hyperparameters, best_parameters):\n\n    value = \"Best parameters: \"\n\n    for key in hyperparameters:\n\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n\n    if hyperparameters:\n\n        print(value[:-2])\n\n\n\ndef get_best_model(estimator, hyperparameters):\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters,\n\n                               n_jobs=-1, cv=cv, scoring=\"accuracy\")\n\n    best_model = grid_search.fit(train_X, train_y)\n\n    best_parameters = best_model.best_estimator_.get_params()\n\n    print_best_parameters(hyperparameters, best_parameters)\n\n    return best_model\n\n\n\ndef evaluate_model(model, name):\n\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n\n    best_models[name] = model", "class": "Model Training", "desc": "This code snippet initializes a dictionary to store the best models, splits the training data into training and validation sets using `train_test_split`, defines functions to print the best hyperparameters, retrieve the best model using `GridSearchCV` with `RepeatedStratifiedKFold` cross-validation, and evaluates the model by printing its accuracy and storing it in the `best_models` dictionary.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.8173711}, "cluster": -1}, {"cell_id": 21, "code": "# I couldn't find a way to set fit_params of XGBClasssifier through GridSearchCV, so did a little trick.\n\n# https://stackoverflow.com/questions/35545733/how-do-you-use-fit-params-for-randomizedsearch-with-votingclassifier-in-sklearn\n\nclass MyXGBClassifier(XGBClassifier):\n\n    def fit(self, X, y=None):\n\n        return super(XGBClassifier, self).fit(X, y,\n\n                                              verbose=False,\n\n                                              early_stopping_rounds=40,\n\n                                              eval_metric='logloss',\n\n                                              eval_set=[(val_X, val_y)])", "class": "Model Training", "desc": "This code snippet defines a custom class `MyXGBClassifier` that inherits from `XGBClassifier` and overrides the `fit` method to include additional fit parameters such as `early_stopping_rounds`, `eval_metric`, and `eval_set`, which are used during model training.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9982059}, "cluster": -1}, {"cell_id": 22, "code": "# Models from, https://www.kaggle.com/sfktrkl/titanic-hyperparameter-tuning-gridsearchcv\n\nrandomForest = RandomForestClassifier(random_state=1, n_estimators=20, max_features='auto',\n\n                                      criterion='gini', max_depth=4, min_samples_split=2,\n\n                                      min_samples_leaf=3)\n\nxgbClassifier = MyXGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor',\n\n                                use_label_encoder=False, learning_rate=0.4, gamma=0.4,\n\n                                max_depth=4, reg_lambda=0, reg_alpha=0.1)\n\nlgbmClassifier = LGBMClassifier(random_state=1, device='gpu', boosting_type='dart',\n\n                                num_leaves=8, learning_rate=0.1, n_estimators=100,\n\n                                reg_alpha=1, reg_lambda=1)\n\n\n\nclassifiers = [\n\n    ('randomForest', randomForest),\n\n    ('xgbClassifier', xgbClassifier),\n\n    ('lgbmClassifier', lgbmClassifier)\n\n]", "class": "Model Training", "desc": "This code snippet initializes three classifiers\u2014`RandomForestClassifier`, `MyXGBClassifier`, and `LGBMClassifier`\u2014with specified hyperparameters and stores them in a list of tuples called `classifiers`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.9895577}, "cluster": 2}, {"cell_id": 23, "code": "hyperparameters = {\n\n    'n_jobs'  : [-1],\n\n    'voting'  : ['hard', 'soft'],\n\n    'weights' : [(1, 1, 1),\n\n                (2, 1, 1), (1, 2, 1), (1, 1, 2),\n\n                (2, 2, 1), (1, 2, 2), (2, 1, 2),\n\n                (3, 2, 1), (1, 3, 2), (2, 1, 3), (3, 1, 2)]\n\n}\n\nestimator = VotingClassifier(estimators=classifiers)\n\nbest_model_voting = get_best_model(estimator, hyperparameters)", "class": "Model Training", "desc": "This code snippet specifies hyperparameters for the `VotingClassifier`, creates an instance of `VotingClassifier` with the previously defined classifiers, and retrieves the best voting classifier model using the `get_best_model` function with the specified hyperparameters.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.98003674}, "cluster": 2}], "notebook_id": 8, "notebook_name": "titanic-votingclassifier-with-gridsearchcv.ipynb", "user": "titanic-votingclassifier-with-gridsearchcv.ipynb"}, {"cells": [{"cell_id": 45, "code": "# Save file\ntitanic_train.to_pickle('./train.clean.pkl')", "class": "Data Export", "desc": "This snippet saves the cleaned dataset to a pickle file using Pandas' `to_pickle` method.", "testing": {"class": "Model_Train", "subclass": "load_pretrained", "subclass_id": 30, "predicted_subclass_probability": 0.9738529}, "cluster": -1}, {"cell_id": 58, "code": "titanic_test.to_pickle('./test.clean.pkl')", "class": "Data Export", "desc": "This snippet saves the cleaned test dataset to a pickle file using Pandas' `to_pickle` method.", "testing": {"class": "Model_Train", "subclass": "load_pretrained", "subclass_id": 30, "predicted_subclass_probability": 0.9730775}, "cluster": -1}, {"cell_id": 74, "code": "submission = pd.DataFrame({'Survived': predictions}, index = test_clean.PassengerId)\nsubmission.to_csv('./submission.csv')", "class": "Data Export", "desc": "This snippet creates a DataFrame containing the predictions and the `PassengerId` from the test dataset, and exports this DataFrame to a CSV file for submission using Pandas' `to_csv` method.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99883705}, "cluster": -1}, {"cell_id": 1, "code": "# load the train dataset.\ntitanic_train = pd.read_csv(\"../input/titanic/train.csv\")\n\n# summary\nprint('This dataset has {} observations with {} variables each.' .format(titanic_train.shape[0], titanic_train.shape[1]))\n\n# print first 10 observations\ntitanic_train.head(10)", "class": "Data Extraction", "desc": "This snippet loads the train dataset from a CSV file using Pandas, prints a summary of the dataset's dimensions, and displays the first 10 observations.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99967563}, "cluster": -1}, {"cell_id": 46, "code": "titanic_test = pd.read_csv('../input/titanic/test.csv')", "class": "Data Extraction", "desc": "This snippet loads the test dataset from a CSV file using Pandas and stores it in the `titanic_test` DataFrame.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997478}, "cluster": -1}, {"cell_id": 60, "code": "# load clean trainset\ntrain_clean = pd.read_pickle('./train.clean.pkl')\n\n# obtain covariates and dependent variable\ntrain_clean_X = train_clean.loc[:,['Pclass','Sex','Age','Fare','Embarked','Cabin_letter','Age_known','Ticket_number','Family_members']]\ntrain_clean_Y = train_clean.loc[:,'Survived']\n\n# get dummies\ntrain_clean_X = pd.get_dummies(train_clean_X, drop_first=True)", "class": "Data Extraction", "desc": "This snippet loads the cleaned training dataset from a pickle file, separates the covariates and the dependent variable, and converts categorical variables into dummy/indicator variables using Pandas.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.87908113}, "cluster": -1}, {"cell_id": 3, "code": "# Generate a new variable (Cabin_known) which equals one if the cabin of the passenger is known.\ntitanic_train[\"Cabin_known\"] = np.where(titanic_train[\"Cabin\"].isna() == False, 1, 0)\nsurvived_cabinknown = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Cabin_known\"], margins = True)\nsurvived_cabinknown.columns = [\"Unkown\",\"Known\",\"Row total\"]\nsurvived_cabinknown.index= [\"Died\",\"Survived\",\"Column total\"]\nsurvived_cabinknown/survived_cabinknown.loc[\"Column total\"] #gives us relative figures", "class": "Data Transform", "desc": "This snippet generates a new variable `Cabin_known` to indicate whether a cabin number is known, and then creates a crosstab to analyze the relationship between survival and cabin knowledge, finally computing the relative figures.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.7482262}, "cluster": 2}, {"cell_id": 4, "code": "titanic_train[\"Cabin_letter\"] = titanic_train[\"Cabin\"].str.slice(start=0,stop=1) # Gets the cabin letter\nsurvived_cabinletter = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Cabin_letter\"], margins = True)\nsurvived_cabinletter.columns = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\", \"Row total\"]\nsurvived_cabinletter.index = [\"Died\", \"Survived\", \"Column total\"]\nsurvived_cabinletter # We need the absolute frequencies to see whether the differences are statistically significant", "class": "Data Transform", "desc": "This snippet creates a new variable `Cabin_letter` by extracting the first letter from the `Cabin` column and then generates a crosstab to analyze the relationship between survival and cabin letters, displaying the absolute frequencies.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.98749954}, "cluster": 2}, {"cell_id": 5, "code": "survived_cabinletter/survived_cabinletter.loc[\"Column total\"]", "class": "Data Transform", "desc": "This snippet computes and displays the relative frequencies of survived versus died passengers based on their cabin letters by normalizing the crosstab results.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9450264}, "cluster": 7}, {"cell_id": 6, "code": "titanic_train.loc[titanic_train['Cabin_letter'].isin([\"A\",\"C\",\"F\",\"G\",\"T\"]), 'Cabin_letter'] = 'known low'\ntitanic_train.loc[titanic_train['Cabin_letter'].isin([\"B\",\"D\",\"E\"]), 'Cabin_letter'] = 'known high'\ntitanic_train.loc[titanic_train['Cabin_letter'].isna(), 'Cabin_letter'] = 'unkown'", "class": "Data Transform", "desc": "This snippet categorizes the `Cabin_letter` variable into three groups: 'known low', 'known high', and 'unknown', based on specified cabin letters and missing values.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.98631454}, "cluster": 0}, {"cell_id": 7, "code": "# drop Cabin_known\ntitanic_train.drop(columns='Cabin', inplace = True)\n# drop Cabin_letter\ntitanic_train.drop(columns='Cabin_known', inplace = True)", "class": "Data Transform", "desc": "This snippet drops the `Cabin` and `Cabin_known` columns from the dataset using Pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99904495}, "cluster": 9}, {"cell_id": 8, "code": "titanic_train[\"Age_known\"] = np.where(titanic_train[\"Age\"].isna() == False, 1, 0)\nsurvived_ageknown = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Age_known\"])\nsurvived_ageknown", "class": "Data Transform", "desc": "This snippet generates a new variable `Age_known` to indicate whether the age of passengers is known, and creates a crosstab to analyze the relationship between survival and age knowledge.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.72090036}, "cluster": 2}, {"cell_id": 9, "code": "# titanic_train[\"Age\"].max() # check max age (80)\nbins = [0, 5, 12, 18, 35, 55, 100]\ntitanic_train[\"Age_group\"] = pd.cut(titanic_train[\"Age\"], bins)\nsurvived_agegroup = pd.crosstab(index = titanic_train[\"Survived\"], \n                           columns = titanic_train[\"Age_group\"])\nsurvived_agegroup", "class": "Data Transform", "desc": "This snippet creates an `Age_group` variable by categorizing passengers' ages into specified bins and generates a crosstab to analyze the relationship between survival and age groups.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9908936}, "cluster": 2}, {"cell_id": 11, "code": "titanic_train[\"Age\"].fillna(round(titanic_train[\"Age\"].mean()), inplace=True)\n\n# drop Age_group\ntitanic_train.drop(columns='Age_group', inplace = True)", "class": "Data Transform", "desc": "This snippet fills missing values in the `Age` column with the rounded mean age and drops the `Age_group` column from the dataset.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9941328}, "cluster": 5}, {"cell_id": 13, "code": "survived_embarked = pd.crosstab(index = titanic_train[\"Survived\"], \n                           columns = titanic_train[\"Embarked\"])\nsurvived_embarked", "class": "Data Transform", "desc": "This snippet generates a crosstab to analyze the relationship between survival and the `Embarked` variable using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.98169005}, "cluster": 2}, {"cell_id": 14, "code": "titanic_train.loc[titanic_train['Embarked'].isna(), 'Embarked'] = 'C'", "class": "Data Transform", "desc": "This snippet fills missing values in the `Embarked` column with the value 'C'.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9946273}, "cluster": 8}, {"cell_id": 17, "code": "# drop 'Name'\ntitanic_train.drop(['Name'], axis=1, inplace=True)\n\n# 'Ticket_letter' = 1 if 'Ticket' starts with a letter\ntitanic_train['Ticket_letter'] = np.where(titanic_train['Ticket'].str.isnumeric() == False, 1, 0)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_letter\"])\nsurvived_Ticketletter", "class": "Data Transform", "desc": "This snippet drops the `Name` column, creates a new variable `Ticket_letter` indicating whether the ticket starts with a letter, and generates a crosstab to analyze the relationship between survival and the `Ticket_letter`.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.32447293}, "cluster": 2}, {"cell_id": 18, "code": "# extract first digit from 'Ticket'\ntitanic_train.loc[titanic_train['Ticket_letter'] == 0, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_train.loc[titanic_train['Ticket_letter'] == 1, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_number\"])\nsurvived_Ticketletter", "class": "Data Transform", "desc": "This snippet extracts the first digit from the `Ticket` variable (differentiating based on whether the ticket starts with a letter) into a new variable `Ticket_number`, and generates a crosstab to analyze the relationship between survival and `Ticket_number`.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9830366}, "cluster": 2}, {"cell_id": 20, "code": "titanic_train.loc[titanic_train['Ticket'].str.split().str.len() == 3, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket'].str.split().str.len() == 3].str.split().str[2].str.slice(start=0,stop=1)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_number\"])\nsurvived_Ticketletter", "class": "Data Transform", "desc": "This snippet updates the `Ticket_number` variable for tickets that split into three parts by extracting the first digit of the third part, and then generates a crosstab to analyze the relationship between survival and `Ticket_number`.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.6221314}, "cluster": 2}, {"cell_id": 22, "code": "# convert 'Ticket_number' into numeric format \ntitanic_train['Ticket_number'] = pd.to_numeric(titanic_train['Ticket_number'])\n\n# check whether 'Ticket_number' == 'Pclass'\ntitanic_train['Pclass_Ticket_equal'] = np.where(titanic_train['Pclass'] == titanic_train['Ticket_number'], 1, 0)\n\nsum(titanic_train['Pclass_Ticket_equal'])/titanic_train.shape[0]", "class": "Data Transform", "desc": "This snippet converts the `Ticket_number` variable into a numeric format, creates a new variable `Pclass_Ticket_equal` indicating whether the `Pclass` matches the `Ticket_number`, and calculates the proportion of matches within the dataset.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.60356903}, "cluster": 2}, {"cell_id": 23, "code": "# drop 'Ticket', Ticket_letter' and 'Pclass_Ticket_equal'\ntitanic_train.drop(['Ticket','Ticket_letter','Pclass_Ticket_equal'], axis=1, inplace=True)", "class": "Data Transform", "desc": "This snippet drops the `Ticket`, `Ticket_letter`, and `Pclass_Ticket_equal` columns from the dataset using Pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9992398}, "cluster": 9}, {"cell_id": 24, "code": "sum(titanic_train['Ticket_number'].isna()) # 4 missing values\n\ntitanic_train.loc[np.isnan(titanic_train['Ticket_number']),'Ticket_number'] = 4 ", "class": "Data Transform", "desc": "This snippet identifies and fills the four missing values in the `Ticket_number` column with the value 4.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.92739946}, "cluster": 8}, {"cell_id": 27, "code": "# divide 'Fare' into different groups\nbins = [-1, 10, 30, 70, 550]  # the interval starts in -1 to include passengers with a fare of 0 (workers?)   \ntitanic_train[\"Fare_group\"] = pd.cut(titanic_train[\"Fare\"], bins)\nsurvived_faregroup = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Fare_group\"], margins = True)\nsurvived_faregroup", "class": "Data Transform", "desc": "This snippet categorizes the `Fare` variable into different groups based on specified bins and generates a crosstab to analyze the relationship between survival and fare groups.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9928417}, "cluster": 2}, {"cell_id": 28, "code": "pclass_faregroup = pd.crosstab(index = titanic_train[\"Pclass\"], columns = titanic_train[\"Fare_group\"], margins = True)\npclass_faregroup", "class": "Data Transform", "desc": "This snippet generates a crosstab to analyze the relationship between passenger class (`Pclass`) and fare groups.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.786694}, "cluster": 2}, {"cell_id": 29, "code": "survived_pclass = pd.crosstab(index = titanic_train[\"Survived\"], columns = titanic_train[\"Pclass\"], margins = True)\nsurvived_pclass", "class": "Data Transform", "desc": "This snippet generates a crosstab to analyze the relationship between survival and passenger class (`Pclass`) and includes row and column totals.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9923334}, "cluster": 2}, {"cell_id": 30, "code": "titanic_train.drop([\"Fare_group\"], axis = 1, inplace = True)", "class": "Data Transform", "desc": "This snippet drops the `Fare_group` column from the dataset using Pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99926406}, "cluster": 9}, {"cell_id": 38, "code": "titanic_train[\"Family_members\"] = titanic_train[\"Family_members\"].map({0:'none', 1:'few', 2:'few', 3:'few', 4:'many', 5:'many', 6:'many', 7:'many', 10:'many'})\n\ntitanic_train.drop(columns=['SibSp','Parch'], inplace = True)", "class": "Data Transform", "desc": "This snippet reclassifies the `Family_members` variable into categories of 'none', 'few', and 'many', and then drops the `SibSp` and `Parch` columns from the dataset using Pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99915195}, "cluster": 0}, {"cell_id": 43, "code": "titanic_train.Pclass = pd.Categorical(titanic_train.Pclass)\ntitanic_train.Sex = pd.Categorical(titanic_train.Sex)\ntitanic_train.Embarked = pd.Categorical(titanic_train.Embarked)\ntitanic_train.Cabin_letter = pd.Categorical(titanic_train.Cabin_letter)\ntitanic_train.Age_known = pd.Categorical(titanic_train.Age_known)\ntitanic_train.Family_members = pd.Categorical(titanic_train.Family_members)", "class": "Data Transform", "desc": "This snippet converts the `Pclass`, `Sex`, `Embarked`, `Cabin_letter`, `Age_known`, and `Family_members` columns into categorical data types using Pandas.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.38450193}, "cluster": 7}, {"cell_id": 48, "code": "titanic_test[\"Cabin_letter\"] = titanic_test[\"Cabin\"].str.slice(start=0,stop=1) # Gets the cabin letter\ntitanic_test.loc[titanic_test['Cabin_letter'].isin([\"A\",\"C\",\"F\",\"G\",\"T\"]), 'Cabin_letter'] = 'known low'\ntitanic_test.loc[titanic_test['Cabin_letter'].isin([\"B\",\"D\",\"E\"]), 'Cabin_letter'] = 'known high'\ntitanic_test.loc[titanic_test['Cabin_letter'].isna(), 'Cabin_letter'] = 'unkown'\n\n# drop Cabin\ntitanic_test.drop(columns='Cabin', inplace = True)", "class": "Data Transform", "desc": "This snippet extracts the first letter of the `Cabin` column into a new variable `Cabin_letter`, categorizes it, and then drops the `Cabin` column from the test dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9957533}, "cluster": 2}, {"cell_id": 49, "code": "titanic_test[\"Age_known\"] = np.where(titanic_test[\"Age\"].isna() == False, 1, 0)\n\ntitanic_test[\"Age\"].fillna(round(titanic_test[\"Age\"].mean()), inplace=True)", "class": "Data Transform", "desc": "This snippet creates a new variable `Age_known` to indicate whether the age of passengers is known and fills missing values in the `Age` column with the rounded mean age.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99483806}, "cluster": 2}, {"cell_id": 50, "code": "titanic_test.loc[titanic_test['Embarked'].isna(), 'Embarked'] = 'C'", "class": "Data Transform", "desc": "This snippet fills missing values in the `Embarked` column of the test dataset with the value 'C'.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9909113}, "cluster": 8}, {"cell_id": 51, "code": "# 'Ticket_letter' = 1 if 'Ticket' starts with a letter\ntitanic_test['Ticket_letter'] = np.where(titanic_test['Ticket'].str.isnumeric() == False, 1, 0)\n\n# extract first digit from 'Ticket'\ntitanic_test.loc[titanic_test['Ticket_letter'] == 0, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_test.loc[titanic_test['Ticket_letter'] == 1, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# extract first digit from 'Ticket'\ntitanic_test.loc[titanic_test['Ticket_letter'] == 0, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_test.loc[titanic_test['Ticket_letter'] == 1, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# extract first digit from 'Ticket' in case of three parts ticket number \ntitanic_test.loc[titanic_test['Ticket'].str.split().str.len() == 3, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket'].str.split().str.len() == 3].str.split().str[2].str.slice(start=0,stop=1)\n\n# convert 'Ticket_number' into numeric format \ntitanic_test['Ticket_number'] = pd.to_numeric(titanic_test['Ticket_number'])", "class": "Data Transform", "desc": "This snippet creates a new variable `Ticket_letter` indicating whether the ticket starts with a letter, extracts the first digit from the ticket into `Ticket_number`, converts `Ticket_number` into a numeric format, and handles special cases for tickets that split into three parts.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99679714}, "cluster": 2}, {"cell_id": 52, "code": "titanic_test.drop(['Name','Ticket','Ticket_letter'],1,inplace=True)", "class": "Data Transform", "desc": "This snippet drops the `Name`, `Ticket`, and `Ticket_letter` columns from the test dataset using Pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99920505}, "cluster": 9}, {"cell_id": 54, "code": "titanic_test[\"Family_members\"] = titanic_test[\"Family_members\"].map({0:'none', 1:'few', 2:'few', 3:'few', 4:'many', 5:'many', 6:'many', 7:'many', 10:'many'})\n\n# drop 'Parch' and 'SibSp'\ntitanic_test.drop(columns=['SibSp','Parch'], inplace = True)", "class": "Data Transform", "desc": "This snippet reclassifies the `Family_members` variable into categories of 'none', 'few', and 'many', and then drops the `Parch` and `SibSp` columns from the test dataset using Pandas' `drop` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99894744}, "cluster": 0}, {"cell_id": 56, "code": "titanic_test.loc[titanic_test['Fare'].isna(),'Fare'] = titanic_test[\"Fare\"].median()", "class": "Data Transform", "desc": "This snippet fills missing values in the `Fare` column with the median fare value in the test dataset using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9961583}, "cluster": 1}, {"cell_id": 57, "code": "titanic_test.Pclass = pd.Categorical(titanic_test.Pclass)\ntitanic_test.Sex = pd.Categorical(titanic_test.Sex)\ntitanic_test.Embarked = pd.Categorical(titanic_test.Embarked)\ntitanic_test.Cabin_letter = pd.Categorical(titanic_test.Cabin_letter)\ntitanic_test.Age_known = pd.Categorical(titanic_test.Age_known)\ntitanic_test.Family_members = pd.Categorical(titanic_test.Family_members)", "class": "Data Transform", "desc": "This snippet converts the `Pclass`, `Sex`, `Embarked`, `Cabin_letter`, `Age_known`, and `Family_members` columns in the test dataset into categorical data types using Pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.7381678}, "cluster": 7}, {"cell_id": 2, "code": "#Get the number of missing observations for each variable\ntitanic_train.isna().sum()", "class": "Exploratory Data Analysis", "desc": "This snippet calculates and displays the number of missing observations for each variable in the dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9988305}, "cluster": 1}, {"cell_id": 10, "code": "round(titanic_train[\"Age\"].mean())", "class": "Exploratory Data Analysis", "desc": "This snippet calculates and rounds the mean age of passengers in the dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9984812}, "cluster": 1}, {"cell_id": 12, "code": "titanic_train[titanic_train['Embarked'].isna()]", "class": "Exploratory Data Analysis", "desc": "This snippet filters and displays rows in the dataset where the `Embarked` column has missing values using Pandas.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.65819347}, "cluster": 1}, {"cell_id": 15, "code": "# Have a look at the first rows.\ntitanic_train.head(10)", "class": "Exploratory Data Analysis", "desc": "This snippet displays the first 10 rows of the dataset using Pandas' `head` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9996778}, "cluster": 1}, {"cell_id": 16, "code": "#Statistical description of numerical varibles\nprint('\\nStatistical description of dataset:')\ntitanic_train.describe() # It will only show up numerical variables", "class": "Exploratory Data Analysis", "desc": "This snippet prints the statistical description of numerical variables in the dataset using Pandas' `describe` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994048}, "cluster": 1}, {"cell_id": 19, "code": "titanic_train.loc[titanic_train['Ticket_number']=='B']", "class": "Exploratory Data Analysis", "desc": "This snippet filters and displays rows in the dataset where the `Ticket_number` column has a value of 'B' using Pandas.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.99537355}, "cluster": 1}, {"cell_id": 21, "code": "titanic_train.head()", "class": "Exploratory Data Analysis", "desc": "This snippet displays the first 5 rows of the dataset using Pandas' `head` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99975556}, "cluster": 1}, {"cell_id": 31, "code": "titanic_train.groupby('Survived').mean()", "class": "Exploratory Data Analysis", "desc": "This snippet calculates and displays the mean of each variable grouped by survival status using Pandas' `groupby` and `mean` methods.", "testing": {"class": "Data_Transform", "subclass": "groupby", "subclass_id": 60, "predicted_subclass_probability": 0.9952874}, "cluster": 1}, {"cell_id": 32, "code": "titanic_train.drop(['PassengerId'], axis=1, inplace=False).corr()", "class": "Exploratory Data Analysis", "desc": "This snippet computes and displays the correlation matrix of the dataset (excluding the `PassengerId` column) using Pandas' `corr` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.81973666}, "cluster": 1}, {"cell_id": 34, "code": "pd.crosstab(titanic_train.Survived,titanic_train.SibSp)", "class": "Exploratory Data Analysis", "desc": "This snippet generates a crosstab to analyze the relationship between survival status and the number of siblings/spouses aboard (`SibSp`) using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99778974}, "cluster": -1}, {"cell_id": 36, "code": "pd.crosstab(titanic_train.Survived,titanic_train.Parch)", "class": "Exploratory Data Analysis", "desc": "This snippet generates a crosstab to analyze the relationship between survival status and the number of parents/children aboard (`Parch`) using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9976615}, "cluster": -1}, {"cell_id": 40, "code": "pd.crosstab(titanic_train.Embarked,titanic_train.Survived)", "class": "Exploratory Data Analysis", "desc": "This snippet generates a crosstab to analyze the relationship between embarkation points (`Embarked`) and survival status using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99811065}, "cluster": -1}, {"cell_id": 42, "code": "pd.crosstab(titanic_train.Sex,titanic_train.Survived)", "class": "Exploratory Data Analysis", "desc": "This snippet generates a crosstab to analyze the relationship between gender (`Sex`) and survival status using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9979779}, "cluster": -1}, {"cell_id": 44, "code": "titanic_train.info()", "class": "Exploratory Data Analysis", "desc": "This snippet displays a summary of the dataset including the data types and non-null counts of each column using Pandas' `info` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99935824}, "cluster": 1}, {"cell_id": 47, "code": "titanic_test.head()", "class": "Exploratory Data Analysis", "desc": "This snippet displays the first 5 rows of the test dataset using Pandas' `head` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997546}, "cluster": 1}, {"cell_id": 53, "code": "titanic_test['Family_members'] = titanic_test['SibSp'] + titanic_test['Parch']\n\ntitanic_test['Family_members'].value_counts()", "class": "Exploratory Data Analysis", "desc": "This snippet creates a new variable `Family_members` by summing `SibSp` and `Parch` and then displays the counts of each unique value using Pandas' `value_counts` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9993037}, "cluster": 1}, {"cell_id": 55, "code": "titanic_test.isna().sum()", "class": "Exploratory Data Analysis", "desc": "This snippet calculates and displays the number of missing values for each variable in the test dataset using Pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9990232}, "cluster": 1}, {"cell_id": 61, "code": "train_clean_X.head()", "class": "Exploratory Data Analysis", "desc": "This snippet displays the first 5 rows of the covariates in the training dataset using Pandas' `head` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997551}, "cluster": 1}, {"cell_id": 75, "code": "submission.value_counts()", "class": "Exploratory Data Analysis", "desc": "This snippet displays the counts of unique values in the `submission` DataFrame, which contains the predictions for the `Survived` variable, using Pandas' `value_counts` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99947196}, "cluster": 1}, {"cell_id": 0, "code": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')", "class": "Imports and Environment", "desc": "This snippet imports necessary libraries including NumPy for numerical operations, Pandas for data manipulation, Seaborn and Matplotlib for visualization, and hides warnings to keep the output clean.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.9993629}, "cluster": -1}, {"cell_id": 59, "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# LPM and logistic regression\nfrom sklearn import linear_model\n\n# lDA and QDA\nfrom sklearn import discriminant_analysis\n\n# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# grid search crossvalidation\nfrom sklearn.model_selection import GridSearchCV\n\n# decision trees\nfrom sklearn.tree import DecisionTreeClassifier\n\n# bagging\nfrom sklearn.ensemble import BaggingClassifier\n\n# random forests\nfrom sklearn.ensemble import RandomForestClassifier\n\n# gradient boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# neural networks\nfrom sklearn.neural_network import MLPClassifier\n\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# SVM\nfrom sklearn.svm import SVC\n\n# model selection\nfrom sklearn.model_selection import cross_val_score\n\n# voting classifier\nfrom sklearn.ensemble import VotingClassifier", "class": "Imports and Environment", "desc": "This snippet imports various libraries and modules from Scikit-learn for different machine learning algorithms and tools, including regression, classification, model selection, preprocessing, and ensemble methods.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.999298}, "cluster": -1}, {"cell_id": 62, "code": "# create vector containing classifiers\n\nclassifiers = []\n\n# logistic regression\nclassifiers.append(linear_model.LogisticRegression())\n\n# linear discriminant analysis (LDA)\nclassifiers.append(discriminant_analysis.LinearDiscriminantAnalysis())\n\n# quadratic discriminant analysis (QDA)\nclassifiers.append(discriminant_analysis.QuadraticDiscriminantAnalysis())\n\n# KNN\nclassifiers.append(KNeighborsClassifier())\n\n# decision trees\nclassifiers.append(DecisionTreeClassifier())\n\n# bagging\nclassifiers.append(BaggingClassifier())\n\n# random forests\nclassifiers.append(RandomForestClassifier())\n\n# gradient boosting\nclassifiers.append(GradientBoostingClassifier())\n\n# multilayer perceptron (NN)\nclassifiers.append(MLPClassifier())\n\n# SVM\nclassifiers.append(SVC())\n\n# store results from cross-validation\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier,train_clean_X,train_clean_Y,scoring=\"accuracy\",cv=10,n_jobs=-1))\n\n# compute average performance & std deviation\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())", "class": "Model Evaluation", "desc": "This snippet creates a list of classifiers (including logistic regression, LDA, QDA, KNN, decision trees, bagging, random forests, gradient boosting, neural networks, and SVM), performs cross-validation on each classifier to evaluate their accuracy on the training dataset, and then computes the average and standard deviation of the performance results.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.97773737}, "cluster": -1}, {"cell_id": 65, "code": "logistic.best_estimator_", "class": "Model Evaluation", "desc": "This snippet retrieves and displays the best logistic regression model identified by the grid search based on cross-validation performance.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.49770755}, "cluster": -1}, {"cell_id": 67, "code": "bagging.best_estimator_", "class": "Model Evaluation", "desc": "This snippet retrieves and displays the best bagging classifier model identified by the grid search based on cross-validation performance.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.51450545}, "cluster": -1}, {"cell_id": 69, "code": "forests.best_estimator_", "class": "Model Evaluation", "desc": "This snippet retrieves and displays the best Random Forest classifier model identified by the grid search based on cross-validation performance.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.48359537}, "cluster": -1}, {"cell_id": 71, "code": "boosting.best_estimator_", "class": "Model Evaluation", "desc": "This snippet retrieves and displays the best Gradient Boosting classifier model identified by the grid search based on cross-validation performance.", "testing": {"class": "Model_Train", "subclass": "find_best_score", "subclass_id": 1, "predicted_subclass_probability": 0.43086216}, "cluster": -1}, {"cell_id": 64, "code": "# grid parameters\ngrid = {\n    'max_iter' : [100,200,500,1000,2000]\n}\n\n# grid search\nlogistic = GridSearchCV(\n    linear_model.LogisticRegression(),\n    grid,\n    cv=10 \n)\n\n# hyperparameter tuning using crossvalidation\nlogistic = logistic.fit(train_clean_X,train_clean_Y)", "class": "Model Training", "desc": "This snippet defines a parameter grid for the maximum iterations of logistic regression, performs a grid search cross-validation to tune hyperparameters, and fits the logistic regression model to the training data using Scikit-learn's `GridSearchCV` method.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.9880946}, "cluster": 0}, {"cell_id": 66, "code": "# grid parameters\ngrid = {\n    'n_estimators' : [10,20,50,100,500,1000]\n}\n\n# grid search\nbagging = GridSearchCV(\n    BaggingClassifier(),\n    grid,\n    cv=10 \n)\n\n# hyperparameter tuning using crossvalidation\nbagging = bagging.fit(train_clean_X,train_clean_Y)", "class": "Model Training", "desc": "This snippet defines a parameter grid for the number of estimators in a bagging classifier, performs a grid search cross-validation to tune hyperparameters, and fits the bagging classifier model to the training data using Scikit-learn's `GridSearchCV` method.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.98861444}, "cluster": 0}, {"cell_id": 68, "code": "grid = {\n    'max_depth' : [3,5,7,10,15,20],\n}\n\n# grid search\nforests = GridSearchCV(\n    RandomForestClassifier(n_estimators=1000,n_jobs=-1),\n    grid,\n    cv=10,    \n    refit=True\n) \n\n# hyperparameter tuning using crossvalidation\nforests.fit(train_clean_X,train_clean_Y)", "class": "Model Training", "desc": "This snippet defines a parameter grid for the maximum depth of a Random Forest classifier, performs a grid search cross-validation to tune hyperparameters, and fits the Random Forest classifier model to the training data using Scikit-learn's `GridSearchCV` method.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.99105763}, "cluster": 0}, {"cell_id": 70, "code": "# grid parameters\ngrid = {'loss': ['deviance', 'exponential'],\n        'max_depth': [1, 3, 5, 10]\n}\n\n# grid search\nboosting = GridSearchCV(\n    GradientBoostingClassifier(n_estimators = 1000),\n    grid,\n    cv=10,    \n    refit=True,\n)\n\n# hyperparameter tuning using crossvalidation\nboosting.fit(train_clean_X,train_clean_Y)", "class": "Model Training", "desc": "This snippet defines a parameter grid for the loss function and maximum depth of a Gradient Boosting classifier, performs a grid search cross-validation to tune hyperparameters, and fits the Gradient Boosting classifier model to the training data using Scikit-learn's `GridSearchCV` method.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.9865433}, "cluster": 0}, {"cell_id": 72, "code": "# voting classifier\nvoting = VotingClassifier(estimators=[('logistic',logistic.best_estimator_),('lda',discriminant_analysis.LinearDiscriminantAnalysis()),\n('bagging',bagging.best_estimator_),('forests',forests.best_estimator_),('boosting',boosting.best_estimator_)],voting='hard',n_jobs=-1)\n\nvoting = voting.fit(train_clean_X,train_clean_Y)", "class": "Model Training", "desc": "This snippet creates and fits a Voting Classifier using the best estimators identified for logistic regression, LDA, bagging, Random Forest, and Gradient Boosting, utilizing hard voting and Scikit-learn's `VotingClassifier` method.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.99491066}, "cluster": 0}, {"cell_id": 73, "code": "# load test set\ntest_clean = pd.read_pickle('./test.clean.pkl')\n\n# get covariates\ntest_clean_X = test_clean.loc[:,['Pclass','Sex','Age','Fare','Embarked','Cabin_letter','Age_known','Ticket_number','Family_members']]\n\n# get dummies\ntest_clean_X = pd.get_dummies(test_clean_X, drop_first=True)\n\n# predict using test set \npredictions = voting.predict(test_clean_X)", "class": "Model Training", "desc": "This snippet loads the cleaned test dataset, extracts the covariates, converts categorical variables into dummy/indicator variables, and uses the trained Voting Classifier to make predictions on the test set.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.84637797}, "cluster": 0}, {"cell_id": 25, "code": "n_records = titanic_train.shape[0]\nn_survived = titanic_train[titanic_train.Survived==1].shape[0]\npercent_survivied = (n_survived*100)/n_records\n\nprint('Total number of passengers: {}'.format(n_records))\nprint('The number of survivors: {}'.format(n_survived))\nprint('The number of non-survivors: {}'.format(n_records - n_survived))\nprint('Survival probability: {:.2f}%'.format(percent_survivied))\n\nsns.countplot(x='Survived',label='Count',data=titanic_train)", "class": "Visualization", "desc": "This snippet calculates and prints the total number of passengers, number of survivors, number of non-survivors, and survival probability, and then visualizes the count of survivors and non-survivors using a Seaborn `countplot`.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.97286135}, "cluster": 0}, {"cell_id": 26, "code": "#SibSp\nplt.subplot(221)\ntitanic_train.boxplot(column = 'SibSp')\n#Parch\nplt.subplot(222)\ntitanic_train.boxplot(column = 'Parch')\n#Fare\nplt.subplot(223)\ntitanic_train.boxplot(column = 'Fare')", "class": "Visualization", "desc": "This snippet creates a series of box plots for the `SibSp`, `Parch`, and `Fare` columns in the dataset using Matplotlib's `boxplot` method in a subplot layout.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9977387}, "cluster": 0}, {"cell_id": 33, "code": "pd.crosstab(titanic_train.Survived,titanic_train.SibSp).plot(kind='bar')", "class": "Visualization", "desc": "This snippet creates a bar plot to visualize the crosstab of survival status against the number of siblings/spouses aboard (`SibSp`) using Pandas' `crosstab` and `plot` methods.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9948726}, "cluster": 0}, {"cell_id": 35, "code": "pd.crosstab(titanic_train.Survived,titanic_train.Parch).plot(kind='bar')", "class": "Visualization", "desc": "This snippet creates a bar plot to visualize the crosstab of survival status against the number of parents/children aboard (`Parch`) using Pandas' `crosstab` and `plot` methods.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.994934}, "cluster": 0}, {"cell_id": 37, "code": "titanic_train['Family_members'] = titanic_train['SibSp'] + titanic_train['Parch']\n\npd.crosstab(titanic_train.Survived,titanic_train.Family_members).plot(kind='bar')", "class": "Visualization", "desc": "This snippet creates a new variable `Family_members` by summing `SibSp` and `Parch`, and visualizes the crosstab of survival status against the total number of family members aboard using Pandas' `crosstab` and `plot` methods.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9977151}, "cluster": 0}, {"cell_id": 39, "code": "pd.crosstab(titanic_train.Embarked,titanic_train.Survived).plot(kind='bar')", "class": "Visualization", "desc": "This snippet creates a bar plot to visualize the crosstab of embarkation points (`Embarked`) against survival status using Pandas' `crosstab` and `plot` methods.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.98807496}, "cluster": 0}, {"cell_id": 41, "code": "pd.crosstab(titanic_train.Sex,titanic_train.Survived).plot(kind='bar')", "class": "Visualization", "desc": "This snippet creates a bar plot to visualize the crosstab of gender (`Sex`) against survival status using Pandas' `crosstab` and `plot` methods.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99690646}, "cluster": 0}, {"cell_id": 63, "code": "# create dataframe with results\ncv_res = pd.DataFrame({\"average performance\":cv_means,\"standard deviation\": cv_std,\"algorithm\":[\"logistic regression\",\"LDA\",\"QDA\",\n\"KNN\",\"decision trees\",\"bagging\",\"random forests\",\"gradient boosting\",\"NN\",\"SVM\"]})\n\ng = sns.barplot(\"average performance\",\"algorithm\",data = cv_res,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"average performance\")\ng = g.set_title(\"cross validation scores\")", "class": "Visualization", "desc": "This snippet creates a DataFrame to store the average performance and standard deviation of each classifier, and visualizes these cross-validation scores using a horizontal bar plot with Seaborn.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.8318433}, "cluster": 2}], "notebook_id": 9, "notebook_name": "data-cleansing-exploration-voting-classifier.ipynb", "user": "data-cleansing-exploration-voting-classifier.ipynb"}, {"cells": [{"cell_id": 25, "code": "# Format Output data & Submit it\n\ntest_data['Survived'] = [ round(x) for x in test_data['Survived']] \nsolution = test_data[['PassengerId', 'Survived']]\nsolution.to_csv(\"Neural_Network_Solution.csv\", index=False)", "class": "Data Export", "desc": "This code snippet rounds the predicted \"Survived\" values, formats them into a new DataFrame containing \"PassengerId\" and \"Survived\" columns, and exports this DataFrame to a CSV file named \"Neural_Network_Solution.csv\".", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99949336}, "cluster": -1}, {"cell_id": 1, "code": "# Importing Kaggle's Training Data\n\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.tail()", "class": "Data Extraction", "desc": "This code snippet imports the training data from a CSV file located in a specified directory using pandas and displays the last few rows of the dataset.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996389}, "cluster": -1}, {"cell_id": 2, "code": "# Importing Kaggle's Testing Data\n\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.tail()", "class": "Data Extraction", "desc": "This code snippet imports the testing data from a CSV file located in a specified directory using pandas and displays the last few rows of the dataset.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996618}, "cluster": -1}, {"cell_id": 3, "code": "# Cleaning and formatting data\n\n# Training Data\ny = train_data[\"Survived\"] #Answers\nfeatures = [\"Pclass\", \"Sex\",\"Embarked\"] # What Columns to include\nX = pd.get_dummies(train_data[features]) # Copying important columns & convert to numbers\n\n# Final Testing Data\nX_final = pd.get_dummies(test_data[features]) # Copying imporant columns & convert to numbers", "class": "Data Transform", "desc": "This code snippet cleans and formats the training and testing data by selecting specific columns, converting categorical variables into dummy/indicator variables using pandas, and separates the target variable for the training data.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.95256627}, "cluster": 3}, {"cell_id": 5, "code": "#Function to scale data from 0 - 1\n\ndef scale_data(X_train, X_test):\n    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n    \n    # Initialise a new scaling object for normalising input data\n    sc = MinMaxScaler()\n\n    # Set up the scaler just on the training set\n    sc.fit(X_train)\n\n    # Apply the scaler to the training and test sets\n    train_sc = sc.transform(X_train)\n    test_sc = sc.transform(X_test)\n    \n    return train_sc, test_sc", "class": "Data Transform", "desc": "This code snippet defines a function to scale the training and testing data between 0 and 1 using the MinMaxScaler from scikit-learn to normalize the input data based on the training set's minimum and maximum values.", "testing": {"class": "Data_Transform", "subclass": "normalization", "subclass_id": 18, "predicted_subclass_probability": 0.99454945}, "cluster": 2}, {"cell_id": 6, "code": "# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = 0.25, random_state=42)\n\n# Scale X data\nX_train_sc, X_test_sc = scale_data(X_train, X_test)", "class": "Data Transform", "desc": "This code snippet splits the data into training and testing sets using train_test_split from scikit-learn and then scales these sets using the previously defined scale_data function.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9981787}, "cluster": 2}, {"cell_id": 24, "code": "# Predict Final Data\ntest_data['Survived'] = model.predict(X_final)", "class": "Data Transform", "desc": "This code snippet predicts the \"Survived\" outcome for the test dataset using the final trained neural network model and stores these predictions in the \"Survived\" column of the test_data DataFrame.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99416625}, "cluster": 9}, {"cell_id": 0, "code": "# Importing needed libraries/modules\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# sklearn for pre-processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# TensorFlow sequential model\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam", "class": "Imports and Environment", "desc": "This code snippet imports various libraries and modules including matplotlib, numpy, pandas, scikit-learn, and TensorFlow for data manipulation, visualization, preprocessing, and building neural network models.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.9993278}, "cluster": -1}, {"cell_id": 7, "code": "#Function to calculate accuracy for troubleshooting / optimizing\n\ndef calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n    \"\"\"Calculate and print accuracy of trainign and test data fits\"\"\"    \n    \n    ### Get accuracy of fit to training data\n    probability = model.predict(X_train_sc)\n    y_pred_train = probability >= 0.5\n    y_pred_train = y_pred_train.flatten()\n    accuracy_train = np.mean(y_pred_train == y_train)\n    \n    ### Get accuracy of fit to test data\n    probability = model.predict(X_test_sc)\n    y_pred_test = probability >= 0.5\n    y_pred_test = y_pred_test.flatten()\n    accuracy_test = np.mean(y_pred_test == y_test)\n\n    # Show acuracy\n    print (f'Training accuracy {accuracy_train:0.3f}')\n    print (f'Test accuracy {accuracy_test:0.3f}')", "class": "Model Evaluation", "desc": "This code snippet defines a function to calculate and print the accuracy of the model's predictions on both the training and testing data, comparing predicted values with actual values using numpy and TensorFlow's Keras API.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.7046628}, "cluster": 0}, {"cell_id": 10, "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)", "class": "Model Evaluation", "desc": "This code snippet calculates and prints the accuracy of the neural network model on both the training and testing datasets using the previously defined calculate_accuracy function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99758875}, "cluster": 0}, {"cell_id": 13, "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)", "class": "Model Evaluation", "desc": "This code snippet calculates and prints the accuracy of the less complex neural network model on both the training and testing datasets using the previously defined calculate_accuracy function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99758875}, "cluster": 0}, {"cell_id": 16, "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)", "class": "Model Evaluation", "desc": "This code snippet calculates and prints the accuracy of the neural network model trained with reduced epochs on both the training and testing datasets using the previously defined calculate_accuracy function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99758875}, "cluster": 0}, {"cell_id": 19, "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)", "class": "Model Evaluation", "desc": "This code snippet calculates and prints the accuracy of the neural network model incorporating dropout on both the training and testing datasets using the previously defined calculate_accuracy function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99758875}, "cluster": 0}, {"cell_id": 22, "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)", "class": "Model Evaluation", "desc": "This code snippet calculates and prints the accuracy of the final neural network model on both the training and testing datasets using the previously defined calculate_accuracy function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99758875}, "cluster": 0}, {"cell_id": 4, "code": "#Function to make neural network\n\ndef make_net(number_features, \n             hidden_layers=3, \n             hidden_layer_neurones=128, \n             dropout=0.0, \n             learning_rate=0.001):\n    \n    \"\"\"Make TensorFlow neural net\"\"\"\n    \n    # Clear Tensorflow \n    K.clear_session()\n    \n    # Set up neural net\n    net = Sequential()\n    \n    # Add hidden hidden_layers using a loop\n    for i in range(hidden_layers):\n        # Add fully connected layer with ReLu activation\n        net.add(Dense(\n            hidden_layer_neurones, \n            input_dim=number_features,\n            activation='leaky_relu'))\n        # Add droput layer\n        net.add(Dropout(dropout))\n    \n    # Add final sigmoid activation output\n    net.add(Dense(1, activation='sigmoid'))    \n    \n    # Compiling model\n    opt = Adam(learning_rate=learning_rate)\n    \n    net.compile(loss='binary_crossentropy', \n                optimizer=opt, \n                metrics=['accuracy'])\n    \n    return net", "class": "Model Training", "desc": "This code snippet defines a function to create and compile a neural network using TensorFlow's Keras API, specifying the number of hidden layers, neurons per layer, dropout rate, and learning rate, and utilizes the Sequential model, Dense layers with leaky ReLU and sigmoid activations, and the Adam optimizer.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.99296665}, "cluster": 0}, {"cell_id": 9, "code": "# Base Neural Network with default Paramters\n\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)", "class": "Model Training", "desc": "This code snippet builds a neural network model with default parameters using the previously defined make_net function and trains it on the scaled training data for 250 epochs with a batch size of 64, while also validating against the scaled test data using TensorFlow's Keras API.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9984475}, "cluster": 0}, {"cell_id": 12, "code": "# Neural Network with less complexity (less hidden layer neurons)\n\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features,\n                hidden_layers=1, # changed from 3 layer default\n                hidden_layer_neurones=32) # changed from 128 neuron default\n\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)", "class": "Model Training", "desc": "This code snippet builds a less complex neural network model with one hidden layer and 32 neurons using the previously defined make_net function and trains it on the scaled training data for 250 epochs with a batch size of 64, while also validating against the scaled test data using TensorFlow's Keras API.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.99484044}, "cluster": 0}, {"cell_id": 15, "code": "# Neural Network with less training time (less epochs)\n\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=25,# changed from 250 \n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)", "class": "Model Training", "desc": "This code snippet builds a neural network model with the default complexity but trains it for only 25 epochs (instead of 250) on the scaled training data, while also validating against the scaled test data using TensorFlow's Keras API.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.99849296}, "cluster": 0}, {"cell_id": 18, "code": "# Neural Network with Dropout (only training % of neurons at once)\n\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features,\n                dropout=0.5)\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=20,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)", "class": "Model Training", "desc": "This code snippet builds a neural network model incorporating dropout with a rate of 0.5 for regularization using the previously defined make_net function and trains it on the scaled training data for 20 epochs with a batch size of 64, while also validating against the scaled test data using TensorFlow's Keras API.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9987827}, "cluster": 0}, {"cell_id": 21, "code": "#Final Neural Network \n\n# Define save checkpoint callback (only save if new best validation results)\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.h5', save_best_only=True)\n\n# Define early stopping callback\n# Stop when no validation improvement for 25 epochs\n# Restore weights to best validation accuracy\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=25, restore_best_weights=True)\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(\n    number_features,\n    hidden_layers=8,\n    hidden_layer_neurones=8,\n    dropout=0.2)\n\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=50,\n                    batch_size=32,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0, \n                    callbacks=[checkpoint_cb, early_stopping_cb])", "class": "Model Training", "desc": "This code snippet builds a final neural network model with 8 hidden layers, each containing 8 neurons and a dropout rate of 0.2, and implements checkpointing and early stopping callbacks during training on the scaled data for up to 50 epochs with a batch size of 32, using TensorFlow's Keras API.", "testing": {"class": "Model_Train", "subclass": "train_on_grid", "subclass_id": 6, "predicted_subclass_probability": 0.70758486}, "cluster": 0}, {"cell_id": 8, "code": "#Function to Visualize Accuracy\n\ndef plot_training(history_dict):\n    acc_values = history_dict['accuracy']\n    val_acc_values = history_dict['val_accuracy']\n    epochs = range(1, len(acc_values) + 1)\n\n    plt.plot(epochs, acc_values, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.show()", "class": "Visualization", "desc": "This code snippet defines a function to visualize the training and validation accuracy over epochs using matplotlib, plotting accuracy values stored in a history dictionary from a model training process.", "testing": {"class": "Visualization", "subclass": "learning_history", "subclass_id": 35, "predicted_subclass_probability": 0.9965463}, "cluster": -1}, {"cell_id": 11, "code": "# Plot accuracy\nplot_training(history.history)", "class": "Visualization", "desc": "This code snippet visualizes the training and validation accuracy over epochs by plotting the accuracy values from the model's training history using matplotlib and the previously defined plot_training function.", "testing": {"class": "Visualization", "subclass": "learning_history", "subclass_id": 35, "predicted_subclass_probability": 0.9935103}, "cluster": -1}, {"cell_id": 14, "code": "# Plot accuracy\nplot_training(history.history)", "class": "Visualization", "desc": "This code snippet visualizes the training and validation accuracy of the less complex neural network model over epochs by plotting the accuracy values from the model's training history using matplotlib and the previously defined plot_training function.", "testing": {"class": "Visualization", "subclass": "learning_history", "subclass_id": 35, "predicted_subclass_probability": 0.9935103}, "cluster": -1}, {"cell_id": 17, "code": "# Plot accuracy\nplot_training(history.history)", "class": "Visualization", "desc": "This code snippet visualizes the training and validation accuracy of the neural network model trained with reduced epochs over time by plotting the accuracy values from the model's training history using matplotlib and the previously defined plot_training function.", "testing": {"class": "Visualization", "subclass": "learning_history", "subclass_id": 35, "predicted_subclass_probability": 0.9935103}, "cluster": -1}, {"cell_id": 20, "code": "# Plot accuracy\nplot_training(history.history)", "class": "Visualization", "desc": "This code snippet visualizes the training and validation accuracy of the neural network model incorporating dropout over time by plotting the accuracy values from the model\u2019s training history using matplotlib and the previously defined plot_training function.", "testing": {"class": "Visualization", "subclass": "learning_history", "subclass_id": 35, "predicted_subclass_probability": 0.9935103}, "cluster": -1}, {"cell_id": 23, "code": "# Plot accuracy\nplot_training(history.history)", "class": "Visualization", "desc": "This code snippet visualizes the training and validation accuracy of the final neural network model over time by plotting the accuracy values from the model\u2019s training history using matplotlib and the previously defined plot_training function.", "testing": {"class": "Visualization", "subclass": "learning_history", "subclass_id": 35, "predicted_subclass_probability": 0.9935103}, "cluster": -1}], "notebook_id": 10, "notebook_name": "final-neural-network-titanic-survival-challenge.ipynb", "user": "final-neural-network-titanic-survival-challenge.ipynb"}, {"cells": [{"cell_id": 1, "code": "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()", "class": "Data Extraction", "desc": "This code snippet reads the Titanic training dataset from a CSV file using pandas and displays the first few rows of the dataframe.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.999642}, "cluster": -1}, {"cell_id": 2, "code": "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()", "class": "Data Extraction", "desc": "This code snippet reads the Titanic test dataset from a CSV file using pandas and displays the first few rows of the dataframe.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99966204}, "cluster": -1}, {"cell_id": 6, "code": "print(\"Before\", train_data.shape, test_data.shape)\n\ntrain_data = train_data.drop(['Ticket', 'Cabin'], axis=1)\ntest_data = test_data.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_data, test_data]\n\n\"After\", train_data.shape, test_data.shape", "class": "Data Transform", "desc": "This code snippet prints the shapes of the training and test datasets before and after dropping the 'Ticket' and 'Cabin' columns from both datasets.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.33762202}, "cluster": 6}, {"cell_id": 7, "code": "combine = [train_data, test_data]", "class": "Data Transform", "desc": "This code snippet creates a list containing both the training and test datasets for easier combined data manipulation.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.9982835}, "cluster": 3}, {"cell_id": 9, "code": "for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])", "class": "Data Transform", "desc": "This code snippet extracts titles from the 'Name' column of both training and test datasets and creates a new 'Title' column, then displays a cross-tabulation of the 'Title' and 'Sex' columns in the training dataset using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.94240606}, "cluster": 6}, {"cell_id": 10, "code": "for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()", "class": "Data Transform", "desc": "This code snippet standardizes the 'Title' column by replacing less common titles with 'Rare', and certain French titles with their English equivalents, then displays the average survival rate for each title group.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.6105874}, "cluster": 4}, {"cell_id": 11, "code": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()", "class": "Data Transform", "desc": "This code snippet maps the standardized titles in the 'Title' column to numerical values for both training and test datasets, filling any missing values with 0, and then displays the first few rows of the transformed training dataset.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99322236}, "cluster": 6}, {"cell_id": 12, "code": "train_data = train_data.drop(['Name', 'PassengerId'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.shape, test_data.shape", "class": "Data Transform", "desc": "This code snippet drops the 'Name' and 'PassengerId' columns from the training dataset and the 'Name' column from the test dataset, then prints the shapes of the modified datasets.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.5906634}, "cluster": 6}, {"cell_id": 13, "code": "#train_data['Sex'].fillNA(-1)\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_data.head()", "class": "Data Transform", "desc": "This code snippet maps the 'Sex' column to numerical values (1 for female and 0 for male) for both training and test datasets, then displays the first few rows of the transformed training dataset.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.97668195}, "cluster": 6}, {"cell_id": 14, "code": "guess_ages = np.zeros((2,3))\nguess_ages", "class": "Data Transform", "desc": "This code snippet initializes a 2x3 numpy array with zeros and prints it, likely to store guessed ages based on some criteria in later steps.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.4285954}, "cluster": 7}, {"cell_id": 15, "code": "for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_data.head()", "class": "Data Transform", "desc": "This code snippet imputes missing 'Age' values in both the training and test datasets by estimating ages based on the median age of passengers with the same 'Sex' and 'Pclass', then converts the 'Age' column to integers, and displays the first few rows of the transformed training dataset.", "testing": {"class": "Model_Train", "subclass": "find_best_params", "subclass_id": 2, "predicted_subclass_probability": 0.19565275}, "cluster": 6}, {"cell_id": 17, "code": "for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_data.head()", "class": "Data Transform", "desc": "This code snippet assigns numerical labels to different age groups based on previously defined age bands for both the training and test datasets, and then displays the first few rows of the transformed training dataset.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.998941}, "cluster": 3}, {"cell_id": 18, "code": "train_data = train_data.drop(['AgeBand'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()", "class": "Data Transform", "desc": "This code snippet drops the 'AgeBand' column from the training dataset and updates the combined datasets, then displays the first few rows of the transformed training dataset.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9964252}, "cluster": 3}, {"cell_id": 19, "code": "for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)", "class": "Data Transform", "desc": "This code snippet creates a new 'FamilySize' column in both the training and test datasets by combining the 'SibSp' and 'Parch' columns and adding 1, then calculates and displays the average survival rate for each family size group, sorted by survival rate.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9927946}, "cluster": 6}, {"cell_id": 20, "code": "for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()", "class": "Data Transform", "desc": "This code snippet creates a new 'IsAlone' column in both the training and test datasets to indicate if a passenger is alone (1) or not (0) based on the 'FamilySize', then calculates and displays the average survival rate for passengers who are alone versus those who are not.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9402816}, "cluster": -1}, {"cell_id": 23, "code": "train_data = train_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_data = test_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_data, test_data]\n\ntrain_data.head()", "class": "Data Transform", "desc": "This code snippet removes the 'Parch', 'SibSp', and 'FamilySize' columns from both the training and test datasets, updates the combined datasets, and displays the first few rows of the transformed training dataset.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.98982376}, "cluster": 6}, {"cell_id": 24, "code": "for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_data.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)", "class": "Data Transform", "desc": "This code snippet creates a new 'Age*Class' feature in both the training and test datasets by multiplying the 'Age' and 'Pclass' columns, then displays the first ten rows of the training dataset for these columns.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99864763}, "cluster": 6}, {"cell_id": 3, "code": "women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the survival rate of women in the Titanic training dataset by filtering based on the 'Sex' column and computing the mean survival rate.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.45923898}, "cluster": 1}, {"cell_id": 4, "code": "men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)", "class": "Exploratory Data Analysis", "desc": "This code snippet calculates and prints the survival rate of men in the Titanic training dataset by filtering based on the 'Sex' column and computing the mean survival rate.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.59011316}, "cluster": 1}, {"cell_id": 8, "code": "print(train_data.columns.values)", "class": "Exploratory Data Analysis", "desc": "This code snippet prints out the column names of the training dataset to provide an overview of the available features.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9982193}, "cluster": 0}, {"cell_id": 16, "code": "train_data['AgeBand'] = pd.cut(train_data['Age'], 5)\ntrain_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)", "class": "Exploratory Data Analysis", "desc": "This code snippet creates age bands by binning the 'Age' column into five equal-width intervals and then calculates the average survival rate for each age band, displaying the results sorted by age bands.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.881014}, "cluster": 1}, {"cell_id": 21, "code": "train_data.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates and displays descriptive statistics for the numerical columns in the training dataset, providing insights into the distribution, central tendency, and variability of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994442}, "cluster": -1}, {"cell_id": 22, "code": "train_data.shape", "class": "Exploratory Data Analysis", "desc": "This code snippet prints the shape of the training dataset, showing the number of rows and columns.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995814}, "cluster": 1}, {"cell_id": 25, "code": "train_data.describe()", "class": "Exploratory Data Analysis", "desc": "This code snippet generates and displays descriptive statistics for the numerical columns in the transformed training dataset, providing insights into the distribution, central tendency, and variability of the data.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994442}, "cluster": -1}, {"cell_id": 26, "code": "train_data.head()", "class": "Exploratory Data Analysis", "desc": "This code snippet displays the first few rows of the transformed training dataset to provide a preview of the data after all transformations have been applied.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997532}, "cluster": 0}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "This code snippet imports essential libraries like numpy for linear algebra and pandas for data processing, and lists all files available in the specified input directory on a Kaggle environment.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.99921954}, "cluster": -1}, {"cell_id": 5, "code": "from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Model Training", "desc": "This code snippet prepares feature matrices using one-hot encoding for both training and test datasets, trains a RandomForestClassifier from scikit-learn on the training data, makes predictions on the test data, and saves the results to a CSV file.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9991411}, "cluster": 0}, {"cell_id": 27, "code": "y = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Title\", \"IsAlone\", \"Age*Class\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Model Training", "desc": "This code snippet selects features, applies one-hot encoding, trains a RandomForestClassifier on the training data, makes predictions on the test data, and saves the results to a CSV file.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9991677}, "cluster": 0}, {"cell_id": 28, "code": "from sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n#model = SVC()\n#svc.fit(X,y)\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Title\", \"IsAlone\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n#model = DecisionTreeClassifier()\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Model Training", "desc": "This code snippet imports additional classifiers (SVC and DecisionTreeClassifier), prepares the data with selected features using one-hot encoding, trains a RandomForestClassifier on the training data, makes predictions on the test data, and saves the results to a CSV file.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9964826}, "cluster": 0}], "notebook_id": 11, "notebook_name": "notebookda32490206.ipynb", "user": "notebookda32490206.ipynb"}, {"cells": [{"cell_id": 29, "code": "predictions = rfc1.predict(test_data[features])\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission3.csv', index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Data Export", "desc": "The code predicts the target variable on the test_data using the trained RandomForestClassifier model, creates a DataFrame with 'PassengerId' and 'Survived' columns, and exports it to a CSV file named 'my_submission3.csv' without the index, using pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9990941}, "cluster": -1}, {"cell_id": 1, "code": "train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')", "class": "Data Extraction", "desc": "The code reads the Titanic dataset's train and test CSV files into pandas DataFrame objects named train_data and test_data, respectively.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997478}, "cluster": -1}, {"cell_id": 4, "code": "train_data['female'] = pd.get_dummies(train_data['Sex'])['female']\ntest_data['female'] = pd.get_dummies(test_data['Sex'])['female']", "class": "Data Transform", "desc": "The code creates a new binary 'female' column in both the train_data and test_data DataFrames by applying one-hot encoding to the 'Sex' column and extracting the 'female' category using pandas' get_dummies method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99779177}, "cluster": 9}, {"cell_id": 5, "code": "sum(train_data['Age'].isnull())\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())", "class": "Data Transform", "desc": "The code fills missing values in the 'Age' column of both train_data and test_data DataFrames with the mean age calculated from their respective 'Age' columns using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.97469246}, "cluster": 1}, {"cell_id": 10, "code": "train_data['class1'] = pd.get_dummies(train_data.Pclass)[1]\ntest_data['class1'] = pd.get_dummies(test_data.Pclass)[1]\ntrain_data['class2'] = pd.get_dummies(train_data.Pclass)[2]\ntest_data['class2'] = pd.get_dummies(test_data.Pclass)[2]", "class": "Data Transform", "desc": "The code creates new binary columns 'class1' and 'class2' in both the train_data and test_data DataFrames by applying one-hot encoding to the 'Pclass' column and extracting the respective class categories using pandas' get_dummies method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9973912}, "cluster": 9}, {"cell_id": 12, "code": "sibs = train_data.loc[train_data.SibSp <= 1]['Survived']\nprint(sum(sibs)/len(sibs))\ntrain_data['many_sibs'] = (train_data.SibSp > 1)*1\ntest_data['many_sibs'] = (test_data.SibSp > 1)*1", "class": "Data Transform", "desc": "The code calculates the survival rate for passengers with one or no siblings/spouses in the train_data DataFrame, and creates a new binary 'many_sibs' column in both train_data and test_data DataFrames indicating whether passengers have more than one sibling/spouse using a boolean transformation.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9980178}, "cluster": -1}, {"cell_id": 14, "code": "bins = [0.42, 15, 30, 50,80]\ntrain_data['bin_age'] = pd.cut(x=train_data.Age, bins=bins)\ntest_data['bin_age'] = pd.cut(x=test_data.Age, bins=bins)", "class": "Data Transform", "desc": "The code creates a new column 'bin_age' in both the train_data and test_data DataFrames by categorizing the 'Age' column into bins defined by the specified age ranges using pandas' cut method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.5303526}, "cluster": 9}, {"cell_id": 15, "code": "train_data['young'] = pd.get_dummies(train_data.bin_age).iloc[:,0]\ntest_data['young'] = pd.get_dummies(test_data.bin_age).iloc[:,0]\ntrain_data['senior'] = pd.get_dummies(train_data.bin_age).iloc[:,3]\ntest_data['senior'] = pd.get_dummies(test_data.bin_age).iloc[:,3]", "class": "Data Transform", "desc": "The code creates new binary columns 'young' and 'senior' in both the train_data and test_data DataFrames by applying one-hot encoding to the 'bin_age' column and extracting specific age categories using pandas' get_dummies method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99718374}, "cluster": 9}, {"cell_id": 18, "code": "X = train_data[features]\ny = train_data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)", "class": "Data Transform", "desc": "The code splits the selected feature set X and target variable y from the train_data DataFrame into training and test sets using sklearn's train_test_split function with a test size of 33% and a random state of 0 for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9982145}, "cluster": 2}, {"cell_id": 25, "code": "test_data.Fare = test_data.Fare.fillna(test_data.Fare.mean())", "class": "Data Transform", "desc": "The code fills missing values in the 'Fare' column of the test_data DataFrame with the mean fare calculated from the 'Fare' column using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.27125782}, "cluster": 1}, {"cell_id": 2, "code": "train_data.head()", "class": "Exploratory Data Analysis", "desc": "The code displays the first five rows of the train_data DataFrame to provide an initial look at the dataset.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997532}, "cluster": 0}, {"cell_id": 3, "code": "women = train_data.loc[train_data.Sex == 'female']['Survived']\nprint('Women survived',sum(women)/len(women))\n\nmen = train_data.loc[train_data.Sex == 'male']['Survived']\nprint('Men survived',sum(men)/len(men))", "class": "Exploratory Data Analysis", "desc": "The code calculates and prints the survival rates of women and men in the train_data DataFrame by filtering based on the 'Sex' column and calculating the mean of the 'Survived' column for each gender.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.29674977}, "cluster": 1}, {"cell_id": 7, "code": "high_fare = train_data.loc[train_data.Fare > 100]['Survived']\nprint('High fare survivors',sum(high_fare)/len(high_fare))\nlow_fare = train_data.loc[train_data.Fare < 32]['Survived']\nprint('High fare survivors',sum(low_fare)/len(low_fare))", "class": "Exploratory Data Analysis", "desc": "The code calculates and prints the survival rates of passengers with high fares (greater than 100) and low fares (less than 32) in the train_data DataFrame by filtering based on the 'Fare' column and calculating the mean of the 'Survived' column for each fare group.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.97302115}, "cluster": 1}, {"cell_id": 8, "code": "pclass1 = train_data.loc[train_data.Pclass == 1]['Survived']\nprint('Class1',sum(pclass1)/len(pclass1))\npclass2 = train_data.loc[train_data.Pclass == 2]['Survived']\nprint('Class2',sum(pclass2)/len(pclass2))\npclass3 = train_data.loc[train_data.Pclass == 3]['Survived']\nprint('Class3',sum(pclass3)/len(pclass3))", "class": "Exploratory Data Analysis", "desc": "The code calculates and prints the survival rates of passengers in each class (Pclass 1, 2, and 3) in the train_data DataFrame by filtering based on the 'Pclass' column and calculating the mean of the 'Survived' column for each class.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.92158645}, "cluster": 1}, {"cell_id": 9, "code": "sum(test_data.Pclass.isna())", "class": "Exploratory Data Analysis", "desc": "The code calculates and returns the number of missing values in the 'Pclass' column of the test_data DataFrame using pandas' isna and sum methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99908054}, "cluster": 1}, {"cell_id": 11, "code": "sum(test_data.SibSp.isna())", "class": "Exploratory Data Analysis", "desc": "The code calculates and returns the number of missing values in the 'SibSp' column of the test_data DataFrame using pandas' isna and sum methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9991391}, "cluster": 1}, {"cell_id": 13, "code": "young = train_data.loc[train_data.Age <= 15]['Survived']\nprint(sum(young)/len(young))\n\nold = train_data.loc[train_data.Age >=40]['Survived']\nprint(sum(old)/len(old))", "class": "Exploratory Data Analysis", "desc": "The code calculates and prints the survival rates of young passengers (age 15 or younger) and older passengers (age 40 or older) in the train_data DataFrame by filtering based on the 'Age' column and calculating the mean of the 'Survived' column for each age group.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.9821845}, "cluster": 1}, {"cell_id": 16, "code": "train_data.corr()['Survived']\n\nfeatures = ['Pclass', 'Fare', 'female', 'class1', 'class2', 'many_sibs', 'young', 'senior']", "class": "Exploratory Data Analysis", "desc": "The code calculates the correlation of all columns with 'Survived' in the train_data DataFrame and defines a list of selected features for further analysis.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99760026}, "cluster": 1}, {"cell_id": 0, "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session", "class": "Imports and Environment", "desc": "The code imports essential libraries such as numpy, pandas, matplotlib, and seaborn for data processing and visualization, and it lists all files available in the Kaggle input directory using the os module.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.999241}, "cluster": -1}, {"cell_id": 17, "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix", "class": "Imports and Environment", "desc": "The code imports train_test_split from sklearn.model_selection, LogisticRegression from sklearn.linear_model, and accuracy_score and confusion_matrix from sklearn.metrics for data splitting, logistic regression modeling, and model evaluation respectively.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.99930274}, "cluster": -1}, {"cell_id": 26, "code": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n#CV_rfc = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)\n#CV_rfc.fit(X_train, y_train)", "class": "Imports and Environment", "desc": "The code imports additional classes and functions for model evaluation and optimization, specifically cross_val_score and GridSearchCV from sklearn.model_selection, RandomForestClassifier from sklearn.ensemble, accuracy_score from sklearn.metrics, and SVC from sklearn.svm, and defines a parameter grid for a RandomForestClassifier to be used in a commented-out grid search.", "testing": {"class": "Model_Train", "subclass": "define_search_space", "subclass_id": 5, "predicted_subclass_probability": 0.85772324}, "cluster": -1}, {"cell_id": 20, "code": "accuracy_score(y_pred, y_test)", "class": "Model Evaluation", "desc": "The code calculates and returns the accuracy of the predictions (y_pred) against the actual test labels (y_test) using sklearn's accuracy_score function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99797374}, "cluster": -1}, {"cell_id": 21, "code": "confusion_matrix(y_pred, y_test)", "class": "Model Evaluation", "desc": "The code computes and returns the confusion matrix for the predictions (y_pred) compared to the true test labels (y_test) using sklearn's confusion_matrix function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9979062}, "cluster": -1}, {"cell_id": 23, "code": "accuracy_score(y_pred, y_test)", "class": "Model Evaluation", "desc": "The code calculates and returns the accuracy of the predictions (y_pred) against the actual test labels (y_test) using sklearn's accuracy_score function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99797374}, "cluster": -1}, {"cell_id": 24, "code": "confusion_matrix(y_pred, y_test)", "class": "Model Evaluation", "desc": "The code computes and returns the confusion matrix for the predictions (y_pred) compared to the true test labels (y_test) using sklearn's confusion_matrix function.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9979062}, "cluster": -1}, {"cell_id": 27, "code": "#CV_rfc.best_params_", "class": "Model Evaluation", "desc": "The commented-out code is intended to output the best parameters found during the grid search for the RandomForestClassifier, which was instantiated and fit using GridSearchCV.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.9968646}, "cluster": 0}, {"cell_id": 19, "code": "log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\ny_pred", "class": "Model Training", "desc": "The code initializes a LogisticRegression model, fits it to the training data (X_train and y_train), and predicts the target variable on the test data (X_test) using sklearn's LogisticRegression class.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.7683394}, "cluster": 0}, {"cell_id": 22, "code": "from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)", "class": "Model Training", "desc": "The code initializes a RandomForestClassifier with 100 estimators and a maximum depth of 5, fits it to the training data (X_train and y_train), and predicts the target variable on the test data (X_test) using sklearn's RandomForestClassifier class.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.977549}, "cluster": 0}, {"cell_id": 28, "code": "rfc1=RandomForestClassifier(random_state=42, max_features='log2', n_estimators= 200, max_depth=6, criterion='entropy')\nrfc1.fit(X_train, y_train)", "class": "Model Training", "desc": "The code initializes a RandomForestClassifier with specified hyperparameters (random_state=42, max_features='log2', n_estimators=200, max_depth=6, criterion='entropy'), and fits it to the training data (X_train and y_train) using sklearn's RandomForestClassifier class.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9996785}, "cluster": 1}, {"cell_id": 6, "code": "plt.subplot(1,2,1)\nsns.histplot(train_data.Age)\nplt.subplot(1,2,2)\nsns.histplot(test_data.Age)", "class": "Visualization", "desc": "The code creates two subplots to display histograms of the 'Age' distribution in the train_data and test_data DataFrames using Seaborn's histplot function.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99291444}, "cluster": 1}], "notebook_id": 12, "notebook_name": "notebookbac9aa939b.ipynb", "user": "notebookbac9aa939b.ipynb"}, {"cells": [{"cell_id": 22, "code": "data_target.to_csv('my_submission.csv',index=False)\nprint(\"Your submission was successfully saved!\")", "class": "Data Export", "desc": "This code exports the final DataFrame `data_target` to a CSV file named 'my_submission.csv' without including the index, and prints a confirmation message.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99907625}, "cluster": -1}, {"cell_id": 1, "code": "data=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndata.head(10)", "class": "Data Extraction", "desc": "This code reads a CSV file located at '/kaggle/input/titanic/train.csv' into a pandas DataFrame and displays the first 10 rows.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99953437}, "cluster": -1}, {"cell_id": 16, "code": "test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest.head(10)", "class": "Data Extraction", "desc": "This code reads a CSV file located at '/kaggle/input/titanic/test.csv' into a pandas DataFrame and displays the first 10 rows.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9996283}, "cluster": -1}, {"cell_id": 3, "code": "data.drop([\"PassengerId\",\"Cabin\",\"Name\",\"Ticket\"],inplace=True,axis=1)\ndata['Age']=data['Age'].fillna(data['Age'].median())\ndata['Embarked']=data['Embarked'].fillna(data['Embarked'].mode()[0])\ndata['Fare'][data['Fare']>400]", "class": "Data Transform", "desc": "This code drops the columns 'PassengerId', 'Cabin', 'Name', and 'Ticket', fills missing values in the 'Age' column with its median, fills missing values in the 'Embarked' column with its mode, and displays the 'Fare' values that are greater than 400.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.8541413}, "cluster": 5}, {"cell_id": 4, "code": "data[\"Sex\"]=data[\"Sex\"].map({\"female\":0,\"male\":1})\ndata=pd.get_dummies(data,drop_first=True)\ndata.head(10)", "class": "Data Transform", "desc": "This code converts the 'Sex' column to a binary numeric representation, transforms categorical variables into dummy/indicator variables using one-hot encoding with `pd.get_dummies`, and displays the first 10 rows of the transformed DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9988757}, "cluster": 2}, {"cell_id": 7, "code": "data.drop([\"Fare\",\"Embarked_Q\",\"SibSp\"],inplace=True,axis=1)\ndata.head()", "class": "Data Transform", "desc": "This code drops the 'Fare', 'Embarked_Q', and 'SibSp' columns from the DataFrame and displays the first few rows of the modified DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991449}, "cluster": 5}, {"cell_id": 8, "code": "mm_scale=MinMaxScaler()\ndata_scaled=pd.DataFrame(mm_scale.fit_transform(data),columns=data.columns)\ndata_scaled.head()", "class": "Data Transform", "desc": "This code applies Min-Max Scaling to normalize the dataset using scikit-learn's `MinMaxScaler`, creating a new DataFrame with the scaled values and displaying the first few rows.", "testing": {"class": "Data_Transform", "subclass": "normalization", "subclass_id": 18, "predicted_subclass_probability": 0.97623605}, "cluster": 2}, {"cell_id": 9, "code": "yScaled=data[\"Survived\"]\nxScaled=data_scaled.drop(\"Survived\",axis=1)\nxScaled.head()", "class": "Data Transform", "desc": "This code separates the 'Survived' column as the target variable `yScaled` and drops it from the scaled DataFrame to create the feature matrix `xScaled`, then displays the first few rows of `xScaled`.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.99836594}, "cluster": 2}, {"cell_id": 13, "code": "xc.drop(\"Parch\",inplace=True,axis=1)\nxc.drop(\"const\",inplace=True,axis=1)", "class": "Data Transform", "desc": "This code drops the 'Parch' and 'const' columns from the DataFrame `xc`.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991861}, "cluster": 6}, {"cell_id": 17, "code": "passengerId=test['PassengerId']\ntest.drop([\"PassengerId\",\"Name\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\"],inplace=True,axis=1)\ntest.head()", "class": "Data Transform", "desc": "This code extracts the 'PassengerId' column, then drops the 'PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Fare', and 'Cabin' columns from the test DataFrame, and displays the first few rows of the modified DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99917966}, "cluster": 5}, {"cell_id": 18, "code": "test['Sex']=test['Sex'].map({\"female\":0,\"male\":1})\ntest=pd.get_dummies(test,drop_first=True)\ntest.head()", "class": "Data Transform", "desc": "This code converts the 'Sex' column in the test dataset to a binary numeric representation, transforms categorical variables into dummy/indicator variables using one-hot encoding with `pd.get_dummies`, and displays the first few rows of the transformed test DataFrame.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9989386}, "cluster": 2}, {"cell_id": 19, "code": "test.drop(\"Embarked_Q\",inplace=True,axis=1)\ntest[\"Age\"]=test[\"Age\"].fillna(test[\"Age\"].median())\ntest_scaled=pd.DataFrame(mm_scale.fit_transform(test),columns=test.columns)\ntest_scaled.head()", "class": "Data Transform", "desc": "This code drops the 'Embarked_Q' column, fills missing values in the 'Age' column with its median, applies Min-Max Scaling to the test dataset using scikit-learn's `MinMaxScaler`, and displays the first few rows of the scaled test DataFrame.", "testing": {"class": "Data_Transform", "subclass": "normalization", "subclass_id": 18, "predicted_subclass_probability": 0.9365268}, "cluster": 2}, {"cell_id": 21, "code": "pred_data=pd.DataFrame(pred,columns=['Survived'])\ndata_target=pd.concat([passengerId,pred_data],axis=1)\ndata_target", "class": "Data Transform", "desc": "This code creates a DataFrame from the prediction results, concatenates it with the 'PassengerId' column, and forms the final DataFrame `data_target` containing both the IDs and the predicted 'Survived' column.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.99849606}, "cluster": 4}, {"cell_id": 2, "code": "data.isnull().mean()", "class": "Exploratory Data Analysis", "desc": "This code calculates the mean of null values for each column in the DataFrame. ", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9989335}, "cluster": 1}, {"cell_id": 0, "code": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom xgboost import XGBClassifier\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))", "class": "Imports and Environment", "desc": "This code imports various libraries including NumPy, pandas, Matplotlib, scikit-learn, statsmodels, XGBoost, and Seaborn, and lists filenames in the '/kaggle/input' directory.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.9992461}, "cluster": -1}, {"cell_id": 11, "code": "vif=pd.DataFrame()\nvif['features']=xScaled.columns\nvif['VIF']=[variance_inflation_factor(xScaled.values,i) for i in range(xScaled.shape[1])]\nvif['VIF']=round(vif['VIF'],3)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif", "class": "Model Evaluation", "desc": "This code calculates the Variance Inflation Factor (VIF) for each feature in the feature matrix `xScaled` to detect multicollinearity, and then sorts and displays the VIF values in descending order in a DataFrame.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.28141785}, "cluster": 1}, {"cell_id": 12, "code": "xc=sm.add_constant(xScaled)\nlm=sm.OLS(yScaled,xc).fit()\nprint(lm.summary())", "class": "Model Evaluation", "desc": "This code adds a constant term to the features, trains an Ordinary Least Squares (OLS) regression model using statsmodels, and prints the model's summary including statistical details of the regression.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.6782264}, "cluster": 1}, {"cell_id": 20, "code": "pred=model.predict(test_scaled)\npred", "class": "Model Evaluation", "desc": "This code uses the trained XGBoost classifier model to predict the target variable for the scaled test dataset and stores the predictions in the `pred` variable.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9937383}, "cluster": 0}, {"cell_id": 10, "code": "lm=LinearRegression()\nlm.fit(xScaled,yScaled)\nrfe=RFE(lm,4)\nrfe=rfe.fit(xScaled,yScaled)\nlist(zip(xScaled.columns,rfe.support_,rfe.ranking_))", "class": "Model Training", "desc": "This code trains a Linear Regression model using scikit-learn and then applies Recursive Feature Elimination (RFE) to select the top 4 features, displaying the feature names along with their support status and ranking.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.83992904}, "cluster": 0}, {"cell_id": 15, "code": "model=XGBClassifier(learning_rate=0.05, max_depth=4, n_estimators=81, nthread=-1, scale_pos_weight=1, random_state=14)\nmodel.fit(xc,yScaled)", "class": "Model Training", "desc": "This code initializes and trains an XGBoost classifier (`XGBClassifier`) with specified hyperparameters on the feature matrix `xc` and target variable `yScaled`.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.99963725}, "cluster": 2}, {"cell_id": 5, "code": "sns.pairplot(data,vars=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Sex\",\"Embarked_Q\",\"Embarked_S\"],kind=\"reg\",hue=\"Survived\")", "class": "Visualization", "desc": "This code generates a pair plot of the specified variables colored by the 'Survived' column, using Seaborn's `pairplot` function with regression lines.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99358493}, "cluster": 1}, {"cell_id": 6, "code": "correlation=data.corr()\nplt.figure(figsize=(14,12),dpi=80)\nsns.heatmap(correlation,annot=True)", "class": "Visualization", "desc": "This code calculates the correlation matrix of the DataFrame and then visualizes it as a heatmap with annotations using Seaborn's `heatmap` function.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9986854}, "cluster": 1}, {"cell_id": 14, "code": "sns.countplot(yScaled)", "class": "Visualization", "desc": "This code generates a count plot of the target variable `yScaled` using Seaborn's `countplot` function.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99077016}, "cluster": 1}], "notebook_id": 13, "notebook_name": "titanic-with-xgboost.ipynb", "user": "titanic-with-xgboost.ipynb"}, {"cells": [{"cell_id": 29, "code": "y_pred = svc.predict(test_df)\n\nsubmission = pd.DataFrame({ 'PassengerId': test_dfn['PassengerId'],\n\n                            'Survived': y_pred })\n\nsubmission.to_csv(\"submission.csv\", index=False)", "class": "Data Export", "desc": "The code generates predictions on the test set using the trained `SVC` model, creates a submission DataFrame with 'PassengerId' and predicted 'Survived' values, and exports it to a CSV file named \"submission.csv\" using pandas.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.99937505}, "cluster": -1}, {"cell_id": 30, "code": "submission.head()", "class": "Data Export", "desc": "The code displays the first few rows of the `submission` DataFrame using the `head` method of pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99974185}, "cluster": -1}, {"cell_id": 1, "code": "'''load training dataset'''\n\ntrain_df = pd.read_csv('../input/titanic/train.csv')\n\ntest_df = pd.read_csv('../input/titanic/test.csv')\n\nsubdf = pd.read_csv('../input/titanic/gender_submission.csv')", "class": "Data Extraction", "desc": "The code loads the training, testing, and submission datasets for the Titanic dataset into pandas DataFrames using the `read_csv` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9997547}, "cluster": -1}, {"cell_id": 2, "code": "sub_df = subdf.drop(['PassengerId'],axis =1)\n\ntest_df =pd.concat([test_df,sub_df],axis = 1)\n\ndf = pd.concat([train_df,test_df],axis =0)\n\ndf.head()", "class": "Data Transform", "desc": "The code drops the 'PassengerId' column from the submission DataFrame, merges it with the test DataFrame, combines the training and modified test DataFrames, and then displays the first few rows using pandas functions.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9992582}, "cluster": 4}, {"cell_id": 4, "code": "df = df[['PassengerId','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Survived']]\n\ndf.head()", "class": "Data Transform", "desc": "The code reorders the columns of the DataFrame to a specified order and displays the first few rows using pandas functions.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.78551346}, "cluster": 3}, {"cell_id": 11, "code": "lis=['Cabin','Name']\n\ndf= df.drop(lis ,axis=1)\n\ndf.head()", "class": "Data Transform", "desc": "The code drops the 'Cabin' and 'Name' columns from the DataFrame and displays the first few rows using pandas functions.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9992561}, "cluster": 4}, {"cell_id": 14, "code": "cleaning = df.drop(['Survived'],axis = 1)\n\nSurvived = df['Survived']\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnumeric_cols = cleaning.select_dtypes(include=numerics)\n\nnumeric_cols = numeric_cols.fillna(numeric_cols.mean())", "class": "Data Transform", "desc": "The code separates the 'Survived' column from the DataFrame, selects the numeric columns, and fills any missing values in these numeric columns with their respective mean values using pandas functions.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.6854556}, "cluster": 3}, {"cell_id": 15, "code": "categorical = ['object']\n\ncategorical_cols = cleaning.select_dtypes(include=categorical)\n\ncategorical_cols = categorical_cols.fillna('none')\n\ncategorical_cols = pd.get_dummies(categorical_cols )", "class": "Data Transform", "desc": "The code selects the categorical columns, fills any missing values in these categorical columns with 'none', and converts these columns into dummy/indicator variables using the `get_dummies` method in pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99940133}, "cluster": 2}, {"cell_id": 16, "code": "cleaned = pd.concat([numeric_cols,categorical_cols],axis= 1)\n\ndf = pd.concat([cleaned,Survived],axis = 1)\n\ndf.head()", "class": "Data Transform", "desc": "The code concatenates the numeric and dummy-encoded categorical columns, then adds the 'Survived' column back to form a cleaned DataFrame and displays the first few rows using pandas functions.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9990748}, "cluster": 3}, {"cell_id": 17, "code": "test_dfn = df.iloc[ 891 : ,:-1]\n\ntest_df = df.iloc[ 891 : ,:-1].values\n\ntest_dfn", "class": "Data Transform", "desc": "The code extracts the test set from the cleaned DataFrame (excluding the 'Survived' column) starting from the 891st row to the end, storing it as both a pandas DataFrame and a NumPy array.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9969296}, "cluster": 4}, {"cell_id": 18, "code": "test_df", "class": "Data Transform", "desc": "The code outputs the NumPy array `test_df`, which contains the test set data excluding the 'Survived' column, starting from the 891st row onwards.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.99976057}, "cluster": 4}, {"cell_id": 19, "code": "X = df.iloc[:,:-1].values\n\ny = df['Survived'].values", "class": "Data Transform", "desc": "The code extracts the feature matrix `X` (all columns except 'Survived') and the target vector `y` (the 'Survived' column) as NumPy arrays from the cleaned DataFrame using pandas `iloc` indexing.", "testing": {"class": "Data_Transform", "subclass": "prepare_x_and_y", "subclass_id": 21, "predicted_subclass_probability": 0.9993187}, "cluster": 0}, {"cell_id": 20, "code": "scl = MinMaxScaler(feature_range = (0, 1))\n\nX = scl.fit_transform(X) \n\ntest_df = scl.fit_transform(test_df) ", "class": "Data Transform", "desc": "The code scales the feature matrix `X` and the test data `test_df` to a range between 0 and 1 using the `MinMaxScaler` from sklearn.", "testing": {"class": "Data_Transform", "subclass": "normalization", "subclass_id": 18, "predicted_subclass_probability": 0.9974043}, "cluster": 2}, {"cell_id": 21, "code": "X_train ,X_test ,y_train ,y_test = train_test_split(X, y , test_size = 0.3, random_state = 44)", "class": "Data Transform", "desc": "The code splits the feature matrix `X` and target vector `y` into training and testing sets with 70% training data and 30% testing data using the `train_test_split` function from sklearn, with a fixed random state for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9983181}, "cluster": 2}, {"cell_id": 3, "code": "list(df.columns.values)", "class": "Exploratory Data Analysis", "desc": "The code lists all the column names of the combined DataFrame using the `columns.values` attribute of pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_columns", "subclass_id": 71, "predicted_subclass_probability": 0.9981401}, "cluster": 1}, {"cell_id": 5, "code": "df.shape", "class": "Exploratory Data Analysis", "desc": "The code outputs the dimensions of the DataFrame using the `shape` attribute of pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.9995491}, "cluster": 1}, {"cell_id": 6, "code": "df.describe()", "class": "Exploratory Data Analysis", "desc": "The code generates summary statistics for the DataFrame using the `describe` method of pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994438}, "cluster": 1}, {"cell_id": 7, "code": "print(df.isnull().sum())\n\nsns.heatmap(df.isnull(),cbar=False, cmap='viridis')", "class": "Exploratory Data Analysis", "desc": "The code prints the total number of missing values for each column using the `isnull().sum()` method of pandas and visualizes the missing values across the DataFrame using a heatmap from the Seaborn library.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.99889094}, "cluster": 1}, {"cell_id": 8, "code": "df['Survived'].value_counts()\n\nsns.countplot(df['Survived'])", "class": "Exploratory Data Analysis", "desc": "The code counts the occurrences of each unique value in the 'Survived' column using the `value_counts` method of pandas and visualizes the frequency of each survival outcome using a count plot from the Seaborn library.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9774469}, "cluster": 1}, {"cell_id": 9, "code": "corr_matrix = df.corr()\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True) \n\nsns.heatmap(corr_matrix, annot=None ,cmap=cmap)", "class": "Exploratory Data Analysis", "desc": "The code calculates the correlation matrix for the DataFrame using the `corr` method of pandas and visualizes it with a heatmap using a diverging color palette from the Seaborn library.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.99885654}, "cluster": 1}, {"cell_id": 10, "code": "corr_matrix.nlargest(5, 'Survived')['Survived'].index", "class": "Exploratory Data Analysis", "desc": "The code finds and lists the top five columns most positively correlated with the 'Survived' column using the `nlargest` method of the correlation matrix.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.5149967}, "cluster": 1}, {"cell_id": 0, "code": "import numpy as np \n\nimport pandas as pd\n\nimport seaborn as sns \n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.metrics import classification_report\n\nimport warnings \n\nwarnings.filterwarnings('ignore')", "class": "Imports and Environment", "desc": "The code imports various libraries and modules for data manipulation (NumPy, Pandas), visualization (Seaborn, Matplotlib), machine learning models and metrics (sklearn), and it suppresses warnings.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.9989849}, "cluster": -1}, {"cell_id": 27, "code": "svc = SVC()\n\nsvc = svc.fit(X_train,y_train)\n\ny_train_pred = svc.predict(X_train)\n\ny_test_pred = svc.predict(X_test)\n\nprint('train score :',accuracy_score(y_train ,y_train_pred ))\n\nprint('test score :',accuracy_score(y_test , y_test_pred))\n\nprint('con matrix :',confusion_matrix(y_test, y_test_pred))\n\nprint('report :',classification_report(y_test, y_test_pred ))\n\ncon = confusion_matrix(y_test,y_test_pred)\n\nhmap =sns.heatmap(con,annot=True,fmt=\"d\")\n\nprint ('Confusion Matrix',hmap)", "class": "Model Evaluation", "desc": "The code trains an `SVC` model on the training data, makes predictions on both the training and testing sets, evaluates the model's performance by printing accuracy scores, confusion matrix, and classification report, and visualizes the confusion matrix using a heatmap from Seaborn.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.37717277}, "cluster": -1}, {"cell_id": 28, "code": "labels = np.arange(2)\n\nclf_report = classification_report(y_test,y_test_pred,labels=labels,target_names=('died','survived'), output_dict=True)\n\nhmap1 = sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\n\nprint ('Classification Report',hmap1)", "class": "Model Evaluation", "desc": "The code generates a classification report for the `SVC` predictions on the test set with specific labels and target names, converts it to a pandas DataFrame, and visualizes it using a heatmap from Seaborn, while printing the heatmap with a label 'Classification Report'.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9841629}, "cluster": -1}, {"cell_id": 22, "code": "#lgm = LogisticRegression()\n\n#lgm = lgm.fit(X_train,y_train)\n\n#y_tpred = lgm.predict(X_train)\n\n#y_pred = lgm.predict(X_test)\n\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n\n#print('test score :',accuracy_score(y_test , y_pred))\n\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n\n#print('report :',classification_report(y_test, y_pred ))\n\n                                                                                      # SCORE : 0.85", "class": "Model Training", "desc": "The commented-out code appears to instantiate and train a `LogisticRegression` model with the training data, predict on both training and testing sets, and then evaluate the model's accuracy, confusion matrix, and classification report using sklearn methods.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.89561415}, "cluster": 0}, {"cell_id": 23, "code": "# rnc = RandomForestClassifier(n_estimators=100, max_depth=4,random_state=0)\n\n# rnc = rnc.fit(X_train,y_train)\n\n# y_tpred = rnc.predict(X_train)\n\n# y_pred = rnc.predict(X_test)\n\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n\n# print('test score :',accuracy_score(y_test , y_pred))\n\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n\n# print('report :',classification_report(y_test, y_pred ))\n\n                                                                                     # SCORE : 0.87", "class": "Model Training", "desc": "The commented-out code appears to instantiate and train a `RandomForestClassifier` with specified hyperparameters using the training data, predict on both training and testing sets, and then evaluate the model's accuracy, confusion matrix, and classification report using sklearn methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.8540831}, "cluster": 0}, {"cell_id": 24, "code": "# gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5,max_depth=3, random_state=140)\n\n# gbc = gbc.fit(X_train,y_train)\n\n# y_tpred = gbc.predict(X_train)\n\n# y_pred = gbc.predict(X_test)\n\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n\n# print('test score :',accuracy_score(y_test , y_pred))\n\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n\n# print('report :',classification_report(y_test, y_pred ))\n\n                                                                                   # SCORE : 0.85", "class": "Model Training", "desc": "The commented-out code appears to instantiate and train a `GradientBoostingClassifier` with specified hyperparameters using the training data, predict on both training and testing sets, and then evaluate the model's accuracy, confusion matrix, and classification report using sklearn methods.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.96510017}, "cluster": 0}, {"cell_id": 25, "code": "# svc = SVC()\n\n# svc = svc.fit(X_train,y_train)\n\n# y_tpred = svc.predict(X_train)\n\n# y_pred = svc.predict(X_test)\n\n# print('train score :',accuracy_score(y_train ,y_tpred))\n\n# print('test score :',accuracy_score(y_test , y_pred))\n\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n\n# print('report :',classification_report(y_test, y_pred ))\n\n                                                                                   # SCORE : 0.86", "class": "Model Training", "desc": "The commented-out code appears to instantiate and train an `SVC` (Support Vector Classifier) using the training data, predict on both training and testing sets, and then evaluate the model's accuracy, confusion matrix, and classification report using sklearn methods.", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.761462}, "cluster": 0}, {"cell_id": 26, "code": "#v1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=1 , C = 0.5 , tol = 0.001)\n\n#v2 = RandomForestClassifier(n_estimators=100, max_depth= 5,random_state=144)\n\n#v3 = SVC()\n\n#eclf = VotingClassifier(estimators=[('lr', v1), ('rf', v2), ('gnb', v3)],voting='hard')\n\n\n\n#for clf, label in zip([v1, v2, v3, eclf], ['Logistic Regression', 'Random Forest', 'SVC ', 'Ensemble ']): \n\n #   scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n\n  #  print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))", "class": "Model Training", "desc": "The commented-out code appears to define individual models (`LogisticRegression`, `RandomForestClassifier`, `SVC`) and an ensemble model (`VotingClassifier`), then calculates cross-validated accuracy scores for each model and the ensemble model using the `cross_val_score` method from sklearn.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "commented", "subclass_id": 76, "predicted_subclass_probability": 0.936291}, "cluster": 2}, {"cell_id": 12, "code": "sns.set_style('whitegrid')\n\nsns.countplot(x=df['Survived'],hue=df['Sex'],data=df)", "class": "Visualization", "desc": "The code sets the Seaborn plotting style to 'whitegrid' and creates a count plot to show the distribution of the 'Survived' column, with bars differentiated by the 'Sex' column.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99833965}, "cluster": 1}, {"cell_id": 13, "code": "for i in df[['Age' ,'Pclass' ,'Fare','Parch']] :\n\n    print(i,'&','Survived')\n\n    df.hist(column=i, by='Survived')", "class": "Visualization", "desc": "The code loops through the specified columns ('Age', 'Pclass', 'Fare', 'Parch') and creates histograms to visualize the distributions of these features segmented by the 'Survived' column using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9965384}, "cluster": 0}], "notebook_id": 14, "notebook_name": "titanic-disaster-88-simple-explanation.ipynb", "user": "titanic-disaster-88-simple-explanation.ipynb"}, {"cells": [{"cell_id": 29, "code": "adaSub = pd.DataFrame({'PassengerId': PassengerId, 'Survived':t_pred })\nadaSub.head()", "class": "Data Export", "desc": "This code creates a new DataFrame \"adaSub\" with columns 'PassengerId' and 'Survived', using the previously extracted 'PassengerId' and the predictions 't_pred', and displays the first few rows using the `head()` method.", "testing": {"class": "Data_Export", "subclass": "prepare_output", "subclass_id": 55, "predicted_subclass_probability": 0.9886446}, "cluster": -1}, {"cell_id": 30, "code": "adaSub.to_csv(\"1_Ada_Submission.csv\", index = False)", "class": "Data Export", "desc": "This code exports the \"adaSub\" DataFrame to a CSV file named \"1_Ada_Submission.csv\" without including the DataFrame's index.", "testing": {"class": "Data_Export", "subclass": "save_to_csv", "subclass_id": 25, "predicted_subclass_probability": 0.9992505}, "cluster": -1}, {"cell_id": 1, "code": "train = pd.read_csv(\"../input/train.csv\")\ntrain.head()", "class": "Data Extraction", "desc": "This code reads the training data from a CSV file named \"train.csv\" into a Pandas DataFrame and displays the first few rows using the `head()` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9871219}, "cluster": -1}, {"cell_id": 2, "code": "test = pd.read_csv(\"../input/test.csv\")\ntest.head()", "class": "Data Extraction", "desc": "This code reads the test data from a CSV file named \"test.csv\" into a Pandas DataFrame and displays the first few rows using the `head()` method.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.9992644}, "cluster": -1}, {"cell_id": 28, "code": "PassengerId = all_test['PassengerId']", "class": "Data Extraction", "desc": "This code extracts the 'PassengerId' column from the \"all_test\" DataFrame and stores it in the variable \"PassengerId\".", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.9945315}, "cluster": -1}, {"cell_id": 5, "code": "all = pd.concat([train, test], sort = False)\nall.info()", "class": "Data Transform", "desc": "This code concatenates the training and test DataFrames into a single DataFrame named \"all\" and provides a summary of this combined DataFrame using the `info()` method. ", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.60922176}, "cluster": 9}, {"cell_id": 6, "code": "#Fill Missing numbers with median\nall['Age'] = all['Age'].fillna(value=all['Age'].median())\nall['Fare'] = all['Fare'].fillna(value=all['Fare'].median())", "class": "Data Transform", "desc": "This code fills missing values in the 'Age' and 'Fare' columns of the combined DataFrame \"all\" with their respective median values.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.72388726}, "cluster": 1}, {"cell_id": 9, "code": "all['Embarked'] = all['Embarked'].fillna('S')\nall.info()", "class": "Data Transform", "desc": "This code fills missing values in the 'Embarked' column of the combined DataFrame \"all\" with the most frequent value 'S'.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9427343}, "cluster": 1}, {"cell_id": 10, "code": "#Age\nall.loc[ all['Age'] <= 16, 'Age'] = 0\nall.loc[(all['Age'] > 16) & (all['Age'] <= 32), 'Age'] = 1\nall.loc[(all['Age'] > 32) & (all['Age'] <= 48), 'Age'] = 2\nall.loc[(all['Age'] > 48) & (all['Age'] <= 64), 'Age'] = 3\nall.loc[ all['Age'] > 64, 'Age'] = 4 ", "class": "Data Transform", "desc": "This code categorizes the 'Age' column in the combined DataFrame \"all\" into five age groups by assigning numeric codes to each group using conditional updating with the `loc` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9980605}, "cluster": 4}, {"cell_id": 11, "code": "#Title\nimport re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+\\.)', name)\n    \n    if title_search:\n        return title_search.group(1)\n    return \"\"", "class": "Data Transform", "desc": "This code defines a function `get_title` that extracts titles from a given name using a regular expression search.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.961291}, "cluster": 7}, {"cell_id": 12, "code": "all['Title'] = all['Name'].apply(get_title)\nall['Title'].value_counts()", "class": "Data Transform", "desc": "This code creates a new column 'Title' in the combined DataFrame \"all\" by applying the `get_title` function to the 'Name' column, and then counts the occurrences of each unique title using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9991881}, "cluster": 9}, {"cell_id": 13, "code": "all['Title'] = all['Title'].replace(['Capt.', 'Dr.', 'Major.', 'Rev.'], 'Officer.')\nall['Title'] = all['Title'].replace(['Lady.', 'Countess.', 'Don.', 'Sir.', 'Jonkheer.', 'Dona.'], 'Royal.')\nall['Title'] = all['Title'].replace(['Mlle.', 'Ms.'], 'Miss.')\nall['Title'] = all['Title'].replace(['Mme.'], 'Mrs.')\nall['Title'].value_counts()", "class": "Data Transform", "desc": "This code standardizes certain titles in the 'Title' column of the combined DataFrame \"all\" by replacing variations of titles with broader categories using the `replace()` method, and then counts the occurrences of each unique standardized title using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.97918713}, "cluster": 9}, {"cell_id": 14, "code": "#Cabin\nall['Cabin'] = all['Cabin'].fillna('Missing')\nall['Cabin'] = all['Cabin'].str[0]\nall['Cabin'].value_counts()", "class": "Data Transform", "desc": "This code fills missing values in the 'Cabin' column of the combined DataFrame \"all\" with 'Missing', extracts the first character of the 'Cabin' values, and then counts the occurrences of each unique first character using the `value_counts()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9994407}, "cluster": 9}, {"cell_id": 15, "code": "#Family Size & Alone \nall['Family_Size'] = all['SibSp'] + all['Parch'] + 1\nall['IsAlone'] = 0\nall.loc[all['Family_Size']==1, 'IsAlone'] = 1\nall.head()", "class": "Data Transform", "desc": "This code creates new columns 'Family_Size' and 'IsAlone' in the combined DataFrame \"all\": 'Family_Size' is calculated as the sum of 'SibSp', 'Parch', and 1, while 'IsAlone' is set to 1 if 'Family_Size' equals 1, and otherwise set to 0, and then displays the first few rows using the `head()` method.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9981501}, "cluster": 5}, {"cell_id": 16, "code": "#Drop unwanted variables\nall_1 = all.drop(['Name', 'Ticket'], axis = 1)\nall_1.head()", "class": "Data Transform", "desc": "This code creates a new DataFrame \"all_1\" by dropping the 'Name' and 'Ticket' columns from the combined DataFrame \"all\" and then displays the first few rows using the `head()` method.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99922025}, "cluster": 9}, {"cell_id": 17, "code": "all_dummies = pd.get_dummies(all_1, drop_first = True)\nall_dummies.head()", "class": "Data Transform", "desc": "This code converts categorical variables in the DataFrame \"all_1\" into dummy/indicator variables using one-hot encoding, drops the first category for each feature to avoid multicollinearity, and then displays the first few rows of the resulting DataFrame \"all_dummies\" using the `head()` method.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99894685}, "cluster": 2}, {"cell_id": 18, "code": "all_train = all_dummies[all_dummies['Survived'].notna()]\nall_train.info()", "class": "Data Transform", "desc": "This code creates a new DataFrame \"all_train\" by filtering rows from \"all_dummies\" where the 'Survived' column is not null, and then provides a summary of the \"all_train\" DataFrame using the `info()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.7295958}, "cluster": 9}, {"cell_id": 19, "code": "all_test = all_dummies[all_dummies['Survived'].isna()]\nall_test.info()", "class": "Data Transform", "desc": "This code creates a new DataFrame \"all_test\" by filtering rows from \"all_dummies\" where the 'Survived' column is null, and then provides a summary of the \"all_test\" DataFrame using the `info()` method.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.7275288}, "cluster": 9}, {"cell_id": 20, "code": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(all_train.drop(['PassengerId','Survived'],axis=1), \n                                                    all_train['Survived'], test_size=0.30, \n                                                    random_state=101, stratify = all_train['Survived'])", "class": "Data Transform", "desc": "This code splits the \"all_train\" DataFrame into training and testing sets for features (X) and target (y), using the `train_test_split` function from scikit-learn with a 70-30 split ratio, stratification on 'Survived', and a random state of 101 for reproducibility.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.99786854}, "cluster": 2}, {"cell_id": 26, "code": "TestForPred = all_test.drop(['PassengerId', 'Survived'], axis = 1)", "class": "Data Transform", "desc": "This code creates a new DataFrame \"TestForPred\" by dropping the 'PassengerId' and 'Survived' columns from the \"all_test\" DataFrame.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99855393}, "cluster": 9}, {"cell_id": 3, "code": "train.info()", "class": "Exploratory Data Analysis", "desc": "This code provides a summary of the training DataFrame, including the data types, non-null counts, and memory usage of each column using the `info()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993617}, "cluster": 1}, {"cell_id": 4, "code": "test.info()", "class": "Exploratory Data Analysis", "desc": "This code provides a summary of the test DataFrame, including the data types, non-null counts, and memory usage of each column using the `info()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993563}, "cluster": 1}, {"cell_id": 7, "code": "all.info()", "class": "Exploratory Data Analysis", "desc": "This code provides a summary of the combined DataFrame \"all,\" including data types, non-null counts, and memory usage of each column using the `info()` method.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993235}, "cluster": 1}, {"cell_id": 0, "code": "import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))", "class": "Imports and Environment", "desc": "This code imports necessary libraries such as NumPy, Pandas, Matplotlib, and Seaborn, sets inline plotting for Matplotlib, and lists the files in the specified directory.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.999067}, "cluster": -1}, {"cell_id": 21, "code": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier", "class": "Imports and Environment", "desc": "This code imports the `AdaBoostClassifier` from the `sklearn.ensemble` module and the `DecisionTreeClassifier` from the `sklearn.tree` module.", "testing": {"class": "Imports_and_Environment", "subclass": "import_modules", "subclass_id": 22, "predicted_subclass_probability": 0.9992812}, "cluster": -1}, {"cell_id": 23, "code": "predictions = ada.predict(X_test)", "class": "Model Evaluation", "desc": "This code uses the trained AdaBoost classifier to generate predictions on the test feature set (X_test).", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.99481404}, "cluster": 0}, {"cell_id": 24, "code": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))", "class": "Model Evaluation", "desc": "This code imports the `classification_report` function from scikit-learn's metrics module and prints a detailed classification report of the model's performance, including precision, recall, and F1-score, by comparing the true labels (y_test) and the predicted labels (predictions).", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.9934075}, "cluster": 1}, {"cell_id": 25, "code": "print (f'Train Accuracy - : {ada.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {ada.score(X_test,y_test):.3f}')", "class": "Model Evaluation", "desc": "This code prints the training and testing accuracy of the AdaBoost classifier by scoring the model on the training set (X_train, y_train) and the test set (X_test, y_test).", "testing": {"class": "Model_Evaluation", "subclass": "compute_test_metric", "subclass_id": 49, "predicted_subclass_probability": 0.99685717}, "cluster": -1}, {"cell_id": 27, "code": "t_pred = ada.predict(TestForPred).astype(int)", "class": "Model Evaluation", "desc": "This code generates predictions for the \"TestForPred\" dataset using the trained AdaBoost classifier and converts the predicted values to integers.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.94950974}, "cluster": 0}, {"cell_id": 22, "code": "ada = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=100, random_state=0)\nada.fit(X_train,y_train)", "class": "Model Training", "desc": "This code creates an instance of the `AdaBoostClassifier` with a `DecisionTreeClassifier` as its base estimator, sets the number of estimators to 100, initializes it with a random state of 0, and fits the model to the training data (X_train, y_train).", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.9996433}, "cluster": 0}, {"cell_id": 8, "code": "sns.catplot(x = 'Embarked', kind = 'count', data = all) #or all['Embarked'].value_counts()", "class": "Visualization", "desc": "This code creates a categorical plot to display the count distribution of the 'Embarked' column in the combined DataFrame \"all\" using Seaborn's `catplot` function.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9461969}, "cluster": 1}], "notebook_id": 15, "notebook_name": "titanic-basic-solution-using-adaboost.ipynb", "user": "titanic-basic-solution-using-adaboost.ipynb"}, {"cells": [{"cell_id": 116, "code": "# Create submission dataframe\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_pred})", "class": "Data Export", "desc": "This code creates a new DataFrame called `output` containing the 'PassengerId' from the `test` dataset and the corresponding 'Survived' predictions from the `Y_pred` variable using pandas.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.91532314}, "cluster": -1}, {"cell_id": 117, "code": "# Create and save csv file \noutput.to_csv(\"submission_titanic.csv\", index = False)", "class": "Data Export", "desc": "This code saves the `output` DataFrame to a CSV file named \"submission_titanic.csv\" without including the index column using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.5049722}, "cluster": -1}, {"cell_id": 118, "code": "print(\"Your submission was successfully saved!\")", "class": "Data Export", "desc": "This code prints a confirmation message indicating that the submission file has been successfully saved.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.99812156}, "cluster": -1}, {"cell_id": 1, "code": "import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))", "class": "Data Extraction", "desc": "This code traverses the directory '/kaggle/input' and prints the full paths of all the files within it using the `os` module.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "list_files", "subclass_id": 88, "predicted_subclass_probability": 0.99936193}, "cluster": -1}, {"cell_id": 2, "code": "# import data\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest=pd.read_csv('/kaggle/input/titanic/test.csv')\nsub = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')", "class": "Data Extraction", "desc": "This code imports the Titanic dataset by reading CSV files 'train.csv', 'test.csv', and 'gender_submission.csv' from the '/kaggle/input/titanic' directory using pandas.", "testing": {"class": "Data_Extraction", "subclass": "load_from_csv", "subclass_id": 45, "predicted_subclass_probability": 0.99974436}, "cluster": -1}, {"cell_id": 11, "code": "def detect_outliers(df, n, features):\n\n    outlier_indices = [] \n    for col in features: \n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col) \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(key for key, value in outlier_indices.items() if value > n) \n    return multiple_outliers", "class": "Data Transform", "desc": "This code defines a function `detect_outliers` that identifies and returns indices of rows in a DataFrame that have multiple outliers based on the interquartile range (IQR) method, using numpy and collections.Counter.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9970662}, "cluster": 2}, {"cell_id": 12, "code": "outliers_to_drop = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\nprint(\"The {} indices for the outliers to drop are: \".format(len(outliers_to_drop)), outliers_to_drop)", "class": "Data Transform", "desc": "This code identifies and prints the indices of rows in the 'train' DataFrame that contain outliers in the 'Age', 'SibSp', 'Parch', and 'Fare' columns using the previously defined `detect_outliers` function.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9952669}, "cluster": 5}, {"cell_id": 14, "code": "# Drop outliers and reset index\nprint(\"Before: {} rows\".format(len(train)))\ntrain = train.drop(outliers_to_drop, axis = 0).reset_index(drop = True)\nprint(\"After: {} rows\".format(len(train)))", "class": "Data Transform", "desc": "This code removes the identified outlier rows from the 'train' DataFrame, resets the index, and prints the number of rows before and after the removal using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99861073}, "cluster": 4}, {"cell_id": 16, "code": "outliers_to_drop_to_test = detect_outliers(test, 2, ['Age', 'SibSp', 'Parch', 'Fare'])", "class": "Data Transform", "desc": "This code identifies the indices of rows in the 'test' DataFrame that contain outliers in the 'Age', 'SibSp', 'Parch', and 'Fare' columns using the previously defined `detect_outliers` function.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.6760485}, "cluster": 5}, {"cell_id": 44, "code": "# Drop ticket  feature from training and test set\ntrain = train.drop(['Ticket', 'Cabin'], axis = 1)\ntest = test.drop(['Ticket','Cabin'], axis = 1)", "class": "Data Transform", "desc": "This code removes the 'Ticket' and 'Cabin' columns from both the 'train' and 'test' DataFrames using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99949265}, "cluster": 8}, {"cell_id": 46, "code": "# Compute the most frequent value of Embarked in training set\nmode = train['Embarked'].dropna().mode()[0]\nmode", "class": "Data Transform", "desc": "This code calculates and returns the most frequent non-null value in the 'Embarked' column of the 'train' DataFrame using pandas.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.997647}, "cluster": 1}, {"cell_id": 47, "code": "# Fill missing value in Embarked with mode\ntrain['Embarked'].fillna(mode, inplace = True)", "class": "Data Transform", "desc": "This code fills the missing values in the 'Embarked' column of the 'train' DataFrame with the most frequent value (mode) using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.8942647}, "cluster": 1}, {"cell_id": 49, "code": "# Compute median of Fare in test set \nmedian = test['Fare'].dropna().median()\nmedian", "class": "Data Transform", "desc": "This code calculates and returns the median value of the 'Fare' column in the 'test' DataFrame using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.63533545}, "cluster": 1}, {"cell_id": 50, "code": "# Fill missing value in Fare with median\ntest['Fare'].fillna(median, inplace = True)", "class": "Data Transform", "desc": "This code fills the missing values in the 'Fare' column of the 'test' DataFrame with the median value using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9957889}, "cluster": 1}, {"cell_id": 51, "code": "# Combine training set and test set\ncombine = pd.concat([train, test], axis = 0).reset_index(drop = True)\ncombine.head()", "class": "Data Transform", "desc": "This code concatenates the 'train' and 'test' DataFrames into a single DataFrame called 'combine' and resets the index, displaying the first few rows of the combined DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99838865}, "cluster": 9}, {"cell_id": 53, "code": "# Convert Sex into numerical values where 0 = male and 1 = female\ncombine['Sex'] = combine['Sex'].map({'male': 0, 'female': 1})", "class": "Data Transform", "desc": "This code converts the categorical 'Sex' column into numerical values, mapping 'male' to 0 and 'female' to 1, using the `map` function in pandas.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.99478626}, "cluster": 8}, {"cell_id": 57, "code": "# Loop through list and impute missing ages\nfor index in age_nan_indices:\n    median_age = combine['Age'].median()\n    predict_age = combine['Age'][(combine['SibSp'] == combine.iloc[index]['SibSp']) \n                                 & (combine['Parch'] == combine.iloc[index]['Parch'])\n                                 & (combine['Pclass'] == combine.iloc[index][\"Pclass\"])].median()\n    if np.isnan(predict_age):\n        combine['Age'].iloc[index] = median_age\n    else:\n        combine['Age'].iloc[index] = predict_age", "class": "Data Transform", "desc": "This code imputes missing 'Age' values in the 'combine' DataFrame by predicting the median age based on similar 'SibSp', 'Parch', and 'Pclass' values or using the overall median age if no similar records are found, using pandas and numpy.", "testing": {"class": "Data_Transform", "subclass": "correct_missing_values", "subclass_id": 17, "predicted_subclass_probability": 0.52764386}, "cluster": 4}, {"cell_id": 60, "code": "# Apply log transformation to Fare column to reduce skewness\ntrain['Fare'] = train['Fare'].map(lambda x: np.log(x) if x > 0 else 0)", "class": "Data Transform", "desc": "This code applies a log transformation to the 'Fare' column in the 'train' DataFrame to reduce its skewness, retaining zero for non-positive values, using numpy and pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.99930644}, "cluster": 4}, {"cell_id": 63, "code": "# Get title from name\ncombine['Title'] = [name.split(',')[1].split('.')[0].strip() for name in combine['Name']]\ncombine[['Name', 'Title']].head()", "class": "Data Transform", "desc": "This code extracts titles from the 'Name' column in the 'combine' DataFrame, creates a new 'Title' column with these extracted titles, and displays the first few rows of the 'Name' and 'Title' columns using pandas.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.86563176}, "cluster": 6}, {"cell_id": 66, "code": "# Simplify title\ncombine['Title'] = combine['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Capt'], 'Officer')\ncombine['Title'] = combine['Title'].replace(['Lady', 'Jonkheer', 'Don','the Countess','Sir', 'Dona'], 'Royalty')\ncombine['Title'] = combine['Title'].replace(['Mlle', 'Miss'], 'Miss')\ncombine['Title'] = combine['Title'].replace(['Mme','Mrs','Ms'], 'Mrs')\ncombine['Title'] = combine['Title'].replace('Mr', 'Mr')\ncombine['Title'] = combine['Title'].replace('Master', 'Master')", "class": "Data Transform", "desc": "This code simplifies the titles in the 'Title' column of the 'combine' DataFrame by consolidating various titles into broader categories like 'Officer', 'Royalty', 'Miss', and 'Mrs' using the `replace` method in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9944529}, "cluster": 0}, {"cell_id": 70, "code": "# Drop name column\ncombine = combine.drop('Name', axis = 1)\ncombine.head()", "class": "Data Transform", "desc": "This code removes the 'Name' column from the 'combine' DataFrame and displays the first few rows of the modified DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99648714}, "cluster": 6}, {"cell_id": 71, "code": "# Calculate family size from SibSp and Parch\ncombine['Family_Size'] = combine['SibSp'] + combine['Parch'] + 1\ncombine[['SibSp', 'Parch', 'Family_Size']].head(10)", "class": "Data Transform", "desc": "This code calculates the 'Family_Size' by summing 'SibSp', 'Parch', and 1, adds it as a new column in the 'combine' DataFrame, and displays the first ten rows of the 'SibSp', 'Parch', and 'Family_Size' columns using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9995365}, "cluster": 5}, {"cell_id": 73, "code": "# Create Alone feature\ncombine['Alone'] = 0\ncombine.loc[combine['Family_Size'] == 1, 'Alone'] = 1", "class": "Data Transform", "desc": "This code creates a new binary 'Alone' column in the 'combine' DataFrame, setting it to 1 for passengers with 'Family_Size' equal to 1, and 0 otherwise using pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.6503439}, "cluster": 4}, {"cell_id": 75, "code": "# Drop SibSp, Parch and FamilySize features from combine dataframe\ncombine = combine.drop(['SibSp', 'Parch', 'Family_Size'], axis = 1)\ncombine.head()", "class": "Data Transform", "desc": "This code removes the 'SibSp', 'Parch', and 'Family_Size' columns from the 'combine' DataFrame and displays the first few rows of the modified DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9810821}, "cluster": 5}, {"cell_id": 76, "code": "combine['Minor'] = combine['Age'] <= 17\ncombine['Major'] = 1 - combine['Minor']  ", "class": "Data Transform", "desc": "This code creates two new binary columns, 'Minor' and 'Major', in the 'combine' DataFrame, where 'Minor' is set to True for ages 17 or below and 'Major' is set to the inverse of 'Minor', using pandas.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.9321478}, "cluster": 5}, {"cell_id": 78, "code": "combine.loc[(combine['Age'] <= 17), 'Major'] = 0\ncombine.loc[(combine['Age'] > 17), 'Major'] = 1", "class": "Data Transform", "desc": "This code ensures that the 'Major' column in the 'combine' DataFrame is correctly set to 0 for ages 17 or below and 1 for ages above 17, using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.99650925}, "cluster": 4}, {"cell_id": 80, "code": "# Drop Age and Minor from combine dataframe\ncombine = combine.drop(['Age', 'Minor'], axis = 1)\ncombine.head()", "class": "Data Transform", "desc": "This code removes the 'Age' and 'Minor' columns from the 'combine' DataFrame and displays the first few rows of the modified DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9985499}, "cluster": 6}, {"cell_id": 82, "code": "# Encode Title and Embarked feature\ncombine = pd.get_dummies(combine, columns = ['Title'])\ncombine = pd.get_dummies(combine, columns = ['Embarked'], prefix = 'Em')\ncombine.head()", "class": "Data Transform", "desc": "This code one-hot encodes the 'Title' and 'Embarked' columns in the 'combine' DataFrame, creating binary indicator columns for each unique value, and displays the first few rows of the modified DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99897325}, "cluster": 4}, {"cell_id": 83, "code": "# Divide Fare into four bands\ncombine['Fare_Band'] = pd.cut(combine['Fare'], 4)\ncombine[['Fare_Band', 'Survived']].groupby(['Fare_Band'], as_index=False).mean().sort_values(by = 'Fare_Band')", "class": "Data Transform", "desc": "This code divides the 'Fare' column in the 'combine' DataFrame into four equal-width bins using the `pd.cut` function, creates a new 'Fare_Band' column, and computes the mean survival rate for each fare band, displaying the results sorted by 'Fare_Band' using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9994172}, "cluster": 2}, {"cell_id": 84, "code": "# Assign ordinal to each fare band\ncombine.loc[combine['Fare'] <= 1.56, 'Fare'] = 0\ncombine.loc[(combine['Fare'] > 1.56) & (combine['Fare'] <= 3.119), 'Fare'] = 1\ncombine.loc[(combine['Fare'] > 3.119) & (combine['Fare'] <= 4.679), 'Fare'] = 2\ncombine.loc[combine['Fare'] > 4.679, 'Fare'] = 3", "class": "Data Transform", "desc": "This code assigns ordinal values to the 'Fare' column based on the predefined fare bands, converting the fare amounts into categorical values 0, 1, 2, or 3 within the 'combine' DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.94123465}, "cluster": 4}, {"cell_id": 85, "code": "# Convert Fare into integer\ncombine['Fare'] = combine['Fare'].astype('int')", "class": "Data Transform", "desc": "This code converts the 'Fare' column in the 'combine' DataFrame to integer data type using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9987791}, "cluster": 1}, {"cell_id": 86, "code": "# Drop FareBand feature\ncombine = combine.drop('Fare_Band', axis = 1)", "class": "Data Transform", "desc": "This code removes the 'Fare_Band' column from the 'combine' DataFrame using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997482}, "cluster": 1}, {"cell_id": 88, "code": "# Separate training and test set from the combined dataframe\ntrain = combine[:len(train)]\ntest = combine[len(train):]", "class": "Data Transform", "desc": "This code separates the 'combine' DataFrame back into 'train' and 'test' DataFrames based on the original lengths of the 'train' and 'test' datasets using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997482}, "cluster": 8}, {"cell_id": 89, "code": "# Drop passenger ID column from and training set\ntrain = train.drop('PassengerId', axis = 1)\ntrain.head()", "class": "Data Transform", "desc": "This code removes the 'PassengerId' column from the 'train' DataFrame and displays the first few rows of the modified DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "categorify", "subclass_id": 20, "predicted_subclass_probability": 0.9992955}, "cluster": 6}, {"cell_id": 90, "code": "# Convert survived back to integer in the training set\ntrain['Survived'] = train['Survived'].astype('int')\ntrain.head()", "class": "Data Transform", "desc": "This code converts the 'Survived' column in the 'train' DataFrame back to integer data type and displays the first few rows of the modified DataFrame using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.8972458}, "cluster": 4}, {"cell_id": 91, "code": "# Drop passenger survived column from test set\ntest = test.drop('Survived', axis = 1)\ntest.head()", "class": "Data Transform", "desc": "This code removes the 'Survived' column from the 'test' DataFrame and displays the first few rows of the modified DataFrame using pandas.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9982375}, "cluster": 6}, {"cell_id": 94, "code": "X_train = train.drop('Survived', axis = 1)\nY_train = train['Survived']\nX_test = test.drop('PassengerId', axis = 1).copy()\nprint(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"X_test shape: \", X_test.shape)", "class": "Data Transform", "desc": "This code splits the 'train' DataFrame into feature set `X_train` and target variable `Y_train`, and prepares `X_test` by dropping the 'PassengerId' column from the 'test' DataFrame, printing the shapes of the resulting datasets using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997482}, "cluster": 2}, {"cell_id": 3, "code": "train.head()", "class": "Exploratory Data Analysis", "desc": "This code displays the first few rows of the 'train' DataFrame to give an initial look at the data using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997507}, "cluster": 0}, {"cell_id": 4, "code": "print(\"Training set shape: \", train.shape)\nprint(\"Test set shape: \", test.shape)\nprint(\"Sample submission shape: \", sub.shape)", "class": "Exploratory Data Analysis", "desc": "This code prints the dimensions (number of rows and columns) of the 'train', 'test', and 'sub' DataFrames using the `shape` attribute in pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_shape", "subclass_id": 58, "predicted_subclass_probability": 0.72919387}, "cluster": 1}, {"cell_id": 5, "code": "# data types\ntrain.info()", "class": "Exploratory Data Analysis", "desc": "This code displays a concise summary of the 'train' DataFrame, including the data types of each column and the non-null count, using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99930036}, "cluster": 1}, {"cell_id": 6, "code": "test.info()", "class": "Exploratory Data Analysis", "desc": "This code displays a concise summary of the 'test' DataFrame, including the data types of each column and the non-null count, using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993563}, "cluster": 1}, {"cell_id": 7, "code": "# Missing data in training set \ntrain.isnull().sum().sort_values(ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the count of missing values in each column of the 'train' DataFrame, sorted in descending order using pandas.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9620355}, "cluster": 1}, {"cell_id": 8, "code": "# Missing data in testing set \ntest.isnull().sum().sort_values(ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the count of missing values in each column of the 'test' DataFrame, sorted in descending order using pandas.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.7282071}, "cluster": 1}, {"cell_id": 9, "code": "train.describe()", "class": "Exploratory Data Analysis", "desc": "This code generates descriptive statistics of the 'train' DataFrame, including measures such as mean, standard deviation, min, and max for numerical columns, using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99943346}, "cluster": 1}, {"cell_id": 10, "code": "train.describe(include=\"O\")", "class": "Exploratory Data Analysis", "desc": "This code generates descriptive statistics for the categorical (object) columns in the 'train' DataFrame using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9994286}, "cluster": 1}, {"cell_id": 13, "code": "# Outliers in numerical variables\ntrain.loc[outliers_to_drop, :]", "class": "Exploratory Data Analysis", "desc": "This code displays the rows in the 'train' DataFrame that are identified as outliers based on their indices from the `outliers_to_drop` list using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9984988}, "cluster": 1}, {"cell_id": 15, "code": "test.head()", "class": "Exploratory Data Analysis", "desc": "This code displays the first few rows of the 'test' DataFrame to give an initial look at the data using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.5586606}, "cluster": 0}, {"cell_id": 17, "code": "# Outliers in numerical variables\ntest.loc[outliers_to_drop_to_test, :]", "class": "Exploratory Data Analysis", "desc": "This code displays the rows in the 'test' DataFrame that are identified as outliers based on their indices from the `outliers_to_drop_to_test` list using pandas.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.9880804}, "cluster": 1}, {"cell_id": 19, "code": "# Value counts of the SibSp column \ntrain['SibSp'].value_counts(dropna = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the counts of unique values in the 'SibSp' column of the 'train' DataFrame, including NaN values, using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997483}, "cluster": 1}, {"cell_id": 20, "code": "# Mean of survival by SibSp\ntrain[['SibSp', 'Survived']].groupby('SibSp', as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by unique values in the 'SibSp' column of the 'train' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.35934398}, "cluster": 1}, {"cell_id": 22, "code": "# Value counts of the Parch column \ntrain['Parch'].value_counts(dropna = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the counts of unique values in the 'Parch' column of the 'train' DataFrame, including NaN values, using pandas.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9989446}, "cluster": 1}, {"cell_id": 23, "code": "# Mean of survival by Parch\ntrain[['Parch', 'Survived']].groupby('Parch', as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by unique values in the 'Parch' column of the 'train' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9994972}, "cluster": 1}, {"cell_id": 25, "code": "# Null values in Age column \ntrain['Age'].isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the number of missing (null) values in the 'Age' column of the 'train' DataFrame using pandas.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.9957302}, "cluster": 1}, {"cell_id": 29, "code": "# Null values of Fare column \ntrain['Fare'].isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the number of missing (null) values in the 'Fare' column of the 'train' DataFrame using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.9982951}, "cluster": 1}, {"cell_id": 31, "code": "# Value counts of the SibSp column \n\ntrain['Pclass'].value_counts(dropna = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the counts of unique values in the 'Pclass' column of the 'train' DataFrame, including NaN values, using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99736184}, "cluster": 1}, {"cell_id": 32, "code": "# Mean of survival by sex\ntrain[['Pclass', 'Survived']].groupby('Pclass', as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by unique values in the 'Pclass' column of the 'train' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.98468137}, "cluster": 1}, {"cell_id": 37, "code": "# Value counts of the sex column\ntrain['Sex'].value_counts(dropna = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the counts of unique values in the 'Sex' column of the 'train' DataFrame, including NaN values, using pandas.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.9959014}, "cluster": 1}, {"cell_id": 38, "code": "# Mean of survival by sex\ntrain[['Sex', 'Survived']].groupby('Sex', as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by unique values in the 'Sex' column of the 'train' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.99524397}, "cluster": 1}, {"cell_id": 40, "code": "# Value counts of the Embarked column \ntrain['Embarked'].value_counts(dropna = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the counts of unique values in the 'Embarked' column of the 'train' DataFrame, including NaN values, using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.99608815}, "cluster": 1}, {"cell_id": 41, "code": "# Mean of survival by point of embarkation\ntrain[['Embarked', 'Survived']].groupby(['Embarked'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by unique values in the 'Embarked' column of the 'train' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99947804}, "cluster": 1}, {"cell_id": 45, "code": "# Missing values in training set \ntrain.isnull().sum().sort_values(ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the count of missing values in each column of the 'train' DataFrame, sorted in descending order, using pandas.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9939486}, "cluster": 1}, {"cell_id": 48, "code": "# Missing values in test set\ntest.isnull().sum().sort_values(ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the count of missing values in each column of the 'test' DataFrame, sorted in descending order, using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9915372}, "cluster": 1}, {"cell_id": 52, "code": "# Missing values in the combined dataset\ncombine.isnull().sum().sort_values(ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the count of missing values in each column of the 'combine' DataFrame, sorted in descending order, using pandas.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9538963}, "cluster": 1}, {"cell_id": 56, "code": "# Check number of missing ages \nage_nan_indices = list(combine[combine['Age'].isnull()].index)\nlen(age_nan_indices)", "class": "Exploratory Data Analysis", "desc": "This code identifies and counts the number of missing values in the 'Age' column of the 'combine' DataFrame, returning the length of the list of indices that contain NaNs using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9202347}, "cluster": 1}, {"cell_id": 58, "code": "# Make sure there is no more missing ages \ncombine['Age'].isnull().sum()", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the number of missing values in the 'Age' column of the 'combine' DataFrame to ensure all missing values have been imputed, using pandas.", "testing": {"class": "Data_Transform", "subclass": "concatenate", "subclass_id": 11, "predicted_subclass_probability": 0.9994412}, "cluster": 1}, {"cell_id": 62, "code": "combine.head()", "class": "Exploratory Data Analysis", "desc": "This code displays the first few rows of the 'combine' DataFrame to give an initial look at the data using pandas.", "testing": {"class": "Visualization", "subclass": "heatmap", "subclass_id": 80, "predicted_subclass_probability": 0.9989661}, "cluster": 0}, {"cell_id": 64, "code": "# Value counts of Title\ncombine['Title'].value_counts()", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the counts of unique values in the 'Title' column of the 'combine' DataFrame using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "define_variables", "subclass_id": 77, "predicted_subclass_probability": 0.2736002}, "cluster": 1}, {"cell_id": 65, "code": "# Number of unique Title\ncombine['Title'].nunique()", "class": "Exploratory Data Analysis", "desc": "This code calculates and displays the number of unique values in the 'Title' column of the 'combine' DataFrame using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99797064}, "cluster": 1}, {"cell_id": 68, "code": "# Mean of survival by name title\ncombine[['Title', 'Survived']].groupby(['Title'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by unique values in the 'Title' column of the 'combine' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.7528276}, "cluster": 1}, {"cell_id": 72, "code": "# Mean of survival by family_size\ncombine[['Family_Size', 'Survived']].groupby('Family_Size', as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by unique values in the 'Family_Size' column of the 'combine' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_unique_values", "subclass_id": 54, "predicted_subclass_probability": 0.9727156}, "cluster": 1}, {"cell_id": 74, "code": "# Mean of survival by Alone\ncombine[['Alone', 'Survived']].groupby('Alone', as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by the 'Alone' column of the 'combine' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Visualization", "subclass": "distribution", "subclass_id": 33, "predicted_subclass_probability": 0.9955486}, "cluster": 1}, {"cell_id": 77, "code": "combine[['Major', 'Survived']].groupby('Major', as_index = False).mean().sort_values(by = 'Survived', ascending = False)", "class": "Exploratory Data Analysis", "desc": "This code computes and displays the mean survival rate grouped by the 'Major' column of the 'combine' DataFrame, sorted in descending order of the survival rate, using pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99916553}, "cluster": 1}, {"cell_id": 79, "code": "combine.head()", "class": "Exploratory Data Analysis", "desc": "This code displays the first few rows of the 'combine' DataFrame to give an initial look at the data using pandas.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9918848}, "cluster": 0}, {"cell_id": 81, "code": "combine.head()", "class": "Exploratory Data Analysis", "desc": "This code displays the first few rows of the 'combine' DataFrame to give an initial look at the data using pandas.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9802186}, "cluster": 0}, {"cell_id": 87, "code": "combine.head()", "class": "Exploratory Data Analysis", "desc": "This code displays the first few rows of the 'combine' DataFrame to give an initial look at the data using pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9991159}, "cluster": 0}, {"cell_id": 92, "code": "train.info()", "class": "Exploratory Data Analysis", "desc": "This code displays a concise summary of the 'train' DataFrame, including the data types of each column and the non-null count, using pandas.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.9838637}, "cluster": 1}, {"cell_id": 93, "code": "test.info()", "class": "Exploratory Data Analysis", "desc": "This code displays a concise summary of the 'test' DataFrame, including the data types of each column and the non-null count, using pandas.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99928004}, "cluster": 1}, {"cell_id": 0, "code": "# Data wrangling\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n# Data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Machine learning models\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Model evaluation\nfrom sklearn.model_selection import cross_val_score\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')", "class": "Imports and Environment", "desc": "This code imports various libraries and modules including pandas, numpy, seaborn, matplotlib, and several machine learning models and tools from scikit-learn, and also configures the environment to ignore warnings.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.9990087}, "cluster": -1}, {"cell_id": 104, "code": "models = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n                                 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', \n                                 'Linear SVC', 'Decision Tree'],\n                       'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron,\n                                 acc_sgd, acc_linear_svc, acc_decision_tree]})\n\nmodels.sort_values(by = 'Score', ascending = False, ignore_index = True)", "class": "Model Evaluation", "desc": "This code creates a DataFrame to compare the accuracy scores of various machine learning models, sorts the models by their accuracy scores in descending order, and returns the sorted DataFrame using pandas.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.5063414}, "cluster": 1}, {"cell_id": 106, "code": "# Create a list which contains cross validation results for each classifier\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy', cv = 9))", "class": "Model Evaluation", "desc": "This code calculates cross-validation accuracy scores for each classifier in the `classifiers` list using 9-fold cross-validation on the `X_train` and `Y_train` datasets, and stores the results in the `cv_results` list using scikit-learn's `cross_val_score`.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.5461685}, "cluster": -1}, {"cell_id": 107, "code": "# Mean and standard deviation of cross validation results for each classifier  \ncv_mean = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_mean.append(cv_result.mean())\n    cv_std.append(cv_result.std())", "class": "Model Evaluation", "desc": "This code computes and stores the mean and standard deviation of the cross-validation accuracy scores for each classifier in the `cv_mean` and `cv_std` lists respectively, using the results in `cv_results`.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.9476168}, "cluster": -1}, {"cell_id": 108, "code": "cv_res = pd.DataFrame({'Cross Validation Mean': cv_mean, 'Cross Validation Std': cv_std, 'Algorithm': ['Logistic Regression', 'Support Vector Machines', 'KNN', 'Gausian Naive Bayes', 'Perceptron', 'Linear SVC', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest']})\ncv_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True)", "class": "Model Evaluation", "desc": "This code creates a DataFrame called `cv_res` to compare the cross-validation mean and standard deviation of accuracy for various machine learning algorithms, sorts the DataFrame by the cross-validation mean in descending order, and returns the sorted DataFrame using pandas.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.4700903}, "cluster": 1}, {"cell_id": 111, "code": "print(\"Best parameters: \", grid.best_params_) \nprint(\"Best estimator: \", grid.best_estimator_)", "class": "Model Evaluation", "desc": "This code prints the best hyperparameters and best estimator found during the grid search for the Random Forest classifier using scikit-learn's `GridSearchCV`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.40337494}, "cluster": 0}, {"cell_id": 113, "code": "# Mean cross validation score\ncross_val_score(random_forest, X_train, Y_train, scoring = 'accuracy', cv = 10).mean()", "class": "Model Evaluation", "desc": "This code calculates and returns the mean cross-validation accuracy score of the previously trained Random Forest classifier using 10-fold cross-validation on the `X_train` and `Y_train` datasets with scikit-learn's `cross_val_score`.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.52557796}, "cluster": 1}, {"cell_id": 114, "code": "# Survival predictions by Random Forest classifier\nY_pred", "class": "Model Evaluation", "desc": "This code outputs the survival predictions made by the trained Random Forest classifier on the `X_test` dataset, stored in the `Y_pred` variable.", "testing": {"class": "Data_Transform", "subclass": "create_dataframe", "subclass_id": 12, "predicted_subclass_probability": 0.9969283}, "cluster": 0}, {"cell_id": 115, "code": "len(Y_pred)", "class": "Model Evaluation", "desc": "This code calculates and returns the number of survival predictions made by the trained Random Forest classifier on the `X_test` dataset, stored in the `Y_pred` variable.", "testing": {"class": "Model_Train", "subclass": "choose_model_class", "subclass_id": 4, "predicted_subclass_probability": 0.94283515}, "cluster": 0}, {"cell_id": 95, "code": "logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log", "class": "Model Training", "desc": "This code trains a Logistic Regression model using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "split", "subclass_id": 13, "predicted_subclass_probability": 0.9369069}, "cluster": 1}, {"cell_id": 96, "code": "svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc", "class": "Model Training", "desc": "This code trains a Support Vector Classifier (SVC) using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.9988219}, "cluster": 1}, {"cell_id": 97, "code": "knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn", "class": "Model Training", "desc": "This code trains a K-Nearest Neighbors (KNN) classifier with 5 neighbors using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.97265536}, "cluster": 1}, {"cell_id": 98, "code": "gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian", "class": "Model Training", "desc": "This code trains a Gaussian Naive Bayes classifier using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.99926084}, "cluster": 1}, {"cell_id": 99, "code": "perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron", "class": "Model Training", "desc": "This code trains a Perceptron classifier using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.9854525}, "cluster": 1}, {"cell_id": 100, "code": "linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc", "class": "Model Training", "desc": "This code trains a Linear Support Vector Classifier (Linear SVC) using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Data_Transform", "subclass": "data_type_conversions", "subclass_id": 16, "predicted_subclass_probability": 0.9859805}, "cluster": 1}, {"cell_id": 101, "code": "sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd", "class": "Model Training", "desc": "This code trains a Stochastic Gradient Descent (SGD) classifier using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993617}, "cluster": 1}, {"cell_id": 102, "code": "decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree", "class": "Model Training", "desc": "This code trains a Decision Tree classifier using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.9993563}, "cluster": 1}, {"cell_id": 103, "code": "random_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest", "class": "Model Training", "desc": "This code trains a Random Forest classifier with 100 estimators using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Model_Train", "subclass": "train_model", "subclass_id": 7, "predicted_subclass_probability": 0.50944585}, "cluster": 1}, {"cell_id": 105, "code": "# Create a list which contains classifiers \n\nclassifiers = []\nclassifiers.append(LogisticRegression())\nclassifiers.append(SVC())\nclassifiers.append(KNeighborsClassifier(n_neighbors = 5))\nclassifiers.append(GaussianNB())\nclassifiers.append(Perceptron())\nclassifiers.append(LinearSVC())\nclassifiers.append(SGDClassifier())\nclassifiers.append(DecisionTreeClassifier())\nclassifiers.append(RandomForestClassifier())\n\nlen(classifiers)", "class": "Model Training", "desc": "This code creates a list called `classifiers` containing instances of various machine learning classifiers from scikit-learn, and outputs the number of classifiers in the list.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.5250405}, "cluster": 0}, {"cell_id": 110, "code": "param_grid = { \n    'n_estimators': [200, 500],\n    'max_depth' : [4,5,6,7,8]\n}\n# Instantiate the grid search model\ngrid = GridSearchCV(RandomForestClassifier(), param_grid = param_grid, \n                          cv=5)\ngrid.fit(X_train, Y_train)", "class": "Model Training", "desc": "This code performs a grid search with cross-validation to find the optimal hyperparameters 'n_estimators' and 'max_depth' for a Random Forest classifier using the `GridSearchCV` function from scikit-learn, and fits the model using the `X_train` and `Y_train` datasets.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.75977}, "cluster": 1}, {"cell_id": 112, "code": "random_forest = RandomForestClassifier(n_estimators=200, max_depth=4)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest", "class": "Model Training", "desc": "This code trains a Random Forest classifier with the optimized hyperparameters `n_estimators=200` and `max_depth=4` using the `X_train` and `Y_train` datasets, makes predictions on the `X_test` dataset, and calculates the training accuracy of the model using scikit-learn.", "testing": {"class": "Model_Evaluation", "subclass": "predict_on_test", "subclass_id": 48, "predicted_subclass_probability": 0.6077191}, "cluster": 1}, {"cell_id": 18, "code": "sns.heatmap(train[['Survived', 'SibSp', 'Parch', 'Age', 'Fare', 'Pclass']].corr(), annot = True, fmt = '.2f', cmap = 'Reds')", "class": "Visualization", "desc": "This code creates and displays a heatmap to visualize the correlation matrix of selected features ('Survived', 'SibSp', 'Parch', 'Age', 'Fare', 'Pclass') in the 'train' DataFrame, using seaborn.", "testing": {"class": "Data_Transform", "subclass": "drop_column", "subclass_id": 10, "predicted_subclass_probability": 0.6817812}, "cluster": 0}, {"cell_id": 21, "code": "sns.barplot(x = 'SibSp', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by SibSp')", "class": "Visualization", "desc": "This code creates and displays a bar plot showing the relationship between 'SibSp' and 'Survived' columns, representing survival probability, using seaborn and matplotlib.", "testing": {"class": "Data_Transform", "subclass": "filter", "subclass_id": 14, "predicted_subclass_probability": 0.9842105}, "cluster": 2}, {"cell_id": 24, "code": "sns.barplot(x = 'Parch', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Parch')", "class": "Visualization", "desc": "This code creates and displays a bar plot showing the relationship between 'Parch' and 'Survived' columns, representing survival probability, using seaborn and matplotlib.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.98814243}, "cluster": 2}, {"cell_id": 26, "code": "# Passenger age distribution\nsns.distplot(train['Age'], label = 'Skewness: %.2f'%(train['Age'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Age Distribution')", "class": "Visualization", "desc": "This code creates and displays a distribution plot of the 'Age' column in the 'train' DataFrame, along with its skewness value, using seaborn and matplotlib.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.9995035}, "cluster": 2}, {"cell_id": 27, "code": "# Age distribution by survival\ng = sns.FacetGrid(train, col = 'Survived')\ng.map(sns.distplot, 'Age')", "class": "Visualization", "desc": "This code creates and displays a facet grid of distribution plots showing the 'Age' distribution separated by survival status ('Survived' column) using seaborn.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9777845}, "cluster": -1}, {"cell_id": 28, "code": "sns.kdeplot(train['Age'][train['Survived'] == 0], label = 'Died')\nsns.kdeplot(train['Age'][train['Survived'] == 1], label = 'Survived')\nplt.xlabel('Age')\nplt.title('Passenger Age Distribution by Survival')", "class": "Visualization", "desc": "This code creates and displays KDE plots comparing the 'Age' distributions of passengers who survived and those who did not using seaborn and matplotlib.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.9960498}, "cluster": 2}, {"cell_id": 30, "code": "# Passenger fare distribution\nsns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.ylabel('Passenger Pclass Distribution')", "class": "Visualization", "desc": "This code creates and displays a distribution plot of the 'Fare' column in the 'train' DataFrame, along with its skewness value, using seaborn and matplotlib.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.93026966}, "cluster": 2}, {"cell_id": 33, "code": "sns.barplot(x = 'Pclass', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Pclass')", "class": "Visualization", "desc": "This code creates and displays a bar plot showing the relationship between 'Pclass' and 'Survived' columns, representing survival probability, using seaborn and matplotlib.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_missing_values", "subclass_id": 39, "predicted_subclass_probability": 0.99873155}, "cluster": 2}, {"cell_id": 34, "code": "# Survival by gender and passenger class\n\ng = sns.factorplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = train, kind = 'bar')\ng.despine(left = True)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Sex and Passenger Class')", "class": "Visualization", "desc": "This code creates and displays a bar plot showing the relationship between 'Pclass', 'Sex', and 'Survived' columns, representing survival probability, using seaborn's `factorplot` and matplotlib.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99223536}, "cluster": 2}, {"cell_id": 35, "code": "# Passenger fare distribution\nsns.distplot(train['Pclass'], label = 'Skewness: %.2f'%(train['Pclass'].skew()))\nplt.legend(loc = 'best')\nplt.ylabel('Passenger Fare Distribution')", "class": "Visualization", "desc": "This code creates and displays a distribution plot of the 'Pclass' column in the 'train' DataFrame, along with its skewness value, using seaborn and matplotlib.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "count_values", "subclass_id": 72, "predicted_subclass_probability": 0.99950254}, "cluster": 2}, {"cell_id": 36, "code": "# Age distribution by survival\ng = sns.FacetGrid(train, col = 'Survived')\ng.map(sns.distplot, 'Pclass')", "class": "Visualization", "desc": "This code creates and displays a facet grid of distribution plots showing the 'Pclass' distribution separated by survival status ('Survived' column) using seaborn.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9796997}, "cluster": -1}, {"cell_id": 39, "code": "sns.barplot(x = 'Sex', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Gender')", "class": "Visualization", "desc": "This code creates and displays a bar plot showing the relationship between 'Sex' and 'Survived' columns, representing survival probability, using seaborn and matplotlib.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table_attributes", "subclass_id": 40, "predicted_subclass_probability": 0.99602973}, "cluster": 2}, {"cell_id": 42, "code": "sns.barplot(x = 'Embarked', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Point of Embarkation')", "class": "Visualization", "desc": "This code creates and displays a bar plot showing the relationship between 'Embarked' and 'Survived' columns, representing survival probability, using seaborn and matplotlib.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9917911}, "cluster": 2}, {"cell_id": 43, "code": "# Survival probability by all categorical variables\ngrid = sns.FacetGrid(train, row = 'Embarked', size = 2.2, aspect = 1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette = 'deep')\ngrid.add_legend()", "class": "Visualization", "desc": "This code creates and displays a facet grid of point plots showing the relationship between 'Pclass', 'Sex', and 'Survived' columns, with each row representing a different 'Embarked' value, using seaborn.", "testing": {"class": "Visualization", "subclass": "model_coefficients", "subclass_id": 79, "predicted_subclass_probability": 0.55584514}, "cluster": 2}, {"cell_id": 54, "code": "sns.factorplot(y = 'Age', x = 'Sex', hue = 'Pclass', kind = 'box', data = combine)\nsns.factorplot(y = 'Age', x = 'Parch', kind = 'box', data = combine)\nsns.factorplot(y = 'Age', x = 'SibSp', kind = 'box', data = combine)", "class": "Visualization", "desc": "This code creates and displays box plots showing the distribution of 'Age' against 'Sex' and 'Pclass', 'Parch', and 'SibSp' respectively, using seaborn's `factorplot` function.", "testing": {"class": "Imports_and_Environment", "subclass": "set_options", "subclass_id": 23, "predicted_subclass_probability": 0.24961525}, "cluster": 0}, {"cell_id": 55, "code": "sns.heatmap(train.drop(['Survived', 'Name', 'PassengerId', 'Fare'], axis = 1).corr(), annot = True, cmap = 'Reds')", "class": "Visualization", "desc": "This code creates and displays a heatmap to visualize the correlation matrix of selected features (excluding 'Survived', 'Name', 'PassengerId', and 'Fare') in the 'train' DataFrame, using seaborn.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.8276658}, "cluster": 0}, {"cell_id": 59, "code": "# Passenger fare distribution\nsns.distplot(combine['Fare'], label = 'Skewness: %.2f'%(combine['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Fare Distribution')", "class": "Visualization", "desc": "This code creates and displays a distribution plot of the 'Fare' column in the 'combine' DataFrame, along with its skewness value, using seaborn and matplotlib.", "testing": {"class": "Data_Transform", "subclass": "sort_values", "subclass_id": 9, "predicted_subclass_probability": 0.9728184}, "cluster": 2}, {"cell_id": 61, "code": "# Passenger fare distribution after log transformation\nsns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Fare Distribution After Log Transformation')", "class": "Visualization", "desc": "This code creates and displays a distribution plot of the 'Fare' column in the 'train' DataFrame after applying log transformation, along with its skewness value, using seaborn and matplotlib.", "testing": {"class": "Visualization", "subclass": "time_series", "subclass_id": 75, "predicted_subclass_probability": 0.9786926}, "cluster": 2}, {"cell_id": 67, "code": "figure = plt.figure(figsize=(25, 7))\nsns.countplot(combine['Title'])", "class": "Visualization", "desc": "This code creates and displays a count plot showing the frequency of each unique value in the 'Title' column of the 'combine' DataFrame, using seaborn and matplotlib.", "testing": {"class": "Data_Transform", "subclass": "feature_engineering", "subclass_id": 8, "predicted_subclass_probability": 0.9991322}, "cluster": 2}, {"cell_id": 69, "code": "sns.factorplot(x = 'Title', y = 'Survived', data = combine, kind = 'bar')\nplt.ylabel('Survival Probability')\nplt.title('Mean of survival by Title')", "class": "Visualization", "desc": "This code creates and displays a bar plot showing the relationship between 'Title' and 'Survived' columns, representing survival probability, using seaborn's `factorplot` and matplotlib.", "testing": {"class": "Exploratory_Data_Analysis", "subclass": "show_table", "subclass_id": 41, "predicted_subclass_probability": 0.9997482}, "cluster": 2}, {"cell_id": 109, "code": "sns.barplot('Cross Validation Mean', 'Algorithm', data = cv_res, order = cv_res.sort_values(by = 'Cross Validation Mean', ascending = False)['Algorithm'], palette = 'Set3', **{'xerr': cv_std})\nplt.ylabel('Algorithm')\nplt.title('Cross Validation Scores')", "class": "Visualization", "desc": "This code creates and displays a horizontal bar plot showing the cross-validation mean scores for various machine learning algorithms, ordered by their mean scores in descending order and including error bars representing the standard deviations, using seaborn and matplotlib.", "testing": {"class": "Model_Train", "subclass": "compute_train_metric", "subclass_id": 28, "predicted_subclass_probability": 0.91900784}, "cluster": 2}], "notebook_id": 16, "notebook_name": "titanic-com.ipynb", "user": "titanic-com.ipynb"}], "metadata": {"clusters": {"Data Transform": {"titles": {"0": "Pandas DataFrame Preprocessing and Encoding", "1": "Data Imputation Techniques Using Pandas", "2": "Data Imputation and Analysis with Pandas and Scikit-learn", "3": "Pandas for Data Cleaning and Encoding", "4": "Data Preprocessing for Titanic Dataset Using Pandas", "5": "DataFrame Preprocessing with Pandas and Sklearn", "6": "Data Preprocessing Using Pandas in Python", "7": "Data Cleaning with Pandas and Numpy", "8": "Pandas DataFrame Manipulation and Cleanup", "9": "Titanic Data Preprocessing with Pandas & Scikit-Learn", "-1": "Titanic Passenger Family Relationships Analysis"}, "accuracy": {"silhouette_score": 0.034126632594454344, "ch_index": 10.20957839328108, "db_index": 2.92208100231438}}, "Data Extraction": {"titles": {"-1": "Titanic CSV Processing using Pandas"}, "accuracy": {"silhouette_score": 0, "ch_index": 0, "db_index": 0}}, "Visualization": {"titles": {"0": "Seaborn and Matplotlib Data Visualizations", "1": "Seaborn and Matplotlib Visualization in Python", "2": "Data Visualization with Seaborn and Matplotlib", "-1": "Seaborn and Matplotlib Titanic Survival Visualizations"}, "accuracy": {"silhouette_score": 0.1391111901749793, "ch_index": 13.134626236352783, "db_index": 1.9404900139743047}}, "Model Training": {"titles": {"0": "Hyperparameter Tuning and Predictions using Scikit-learn, TensorFlow", "1": "Scikit-learn Classifiers Training and Evaluation", "2": "Sklearn Grid Search for Classifier Tuning", "-1": "Sklearn Model Initialization and Training"}, "accuracy": {"silhouette_score": 0.16352215231537823, "ch_index": 14.756615287493345, "db_index": 2.0745685916098355}}, "Model Evaluation": {"titles": {"0": "Machine Learning Performance Evaluation with scikit-learn", "1": "Model Comparison Using Scikit-learn & Pandas", "2": "Scikit-Learn Model Evaluation and Storage", "-1": "Model Evaluation with Scikit-learn and Pandas"}, "accuracy": {"silhouette_score": 0.06090917336671552, "ch_index": 4.5387704021911235, "db_index": 3.4758194765645976}}, "Imports and Environment": {"titles": {"-1": "Data Analysis and ML with Python Libraries"}, "accuracy": {"silhouette_score": 0, "ch_index": 0, "db_index": 0}}, "Data Export": {"titles": {"-1": "DataFrame Predictions Export Using Pandas"}, "accuracy": {"silhouette_score": 0, "ch_index": 0, "db_index": 0}}, "Exploratory Data Analysis": {"titles": {"0": "Inspect DataFrames Using Pandas Head", "1": "Data Analysis and Summary with Pandas", "-1": "Data Analysis with Pandas Crosstabs"}, "accuracy": {"silhouette_score": 0.08957485540521205, "ch_index": 10.044029333523234, "db_index": 2.0841614223486515}}}, "clustering_accuracy": 0.33067729083665337}}