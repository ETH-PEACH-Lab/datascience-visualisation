{
    "notebooks": [{
        "cells": [{
            "cell_id": 89,
            "code": "output = pd.DataFrame({'PassengerId': passenger_id, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Data Export",
            "desc": "The code snippet creates a DataFrame `output` containing 'PassengerId' and the model predictions 'Survived', saves it to a CSV file named 'submission.csv', and confirms the successful save with a print statement.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9988844
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntrain_df.head()",
            "class": "Data Extraction",
            "desc": "The code snippet reads the \"train.csv\" file into a pandas DataFrame and displays the first few rows to inspect the data.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99962866
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_df.head()",
            "class": "Data Extraction",
            "desc": "The code snippet reads the \"test.csv\" file into a pandas DataFrame and displays the first few rows to inspect the test data.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9996499
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "passenger_id = test_df['PassengerId']\npassenger_id",
            "class": "Data Extraction",
            "desc": "The code snippet extracts the 'PassengerId' column from the `test_df` DataFrame and outputs it to inspect the unique passenger IDs from the test data set.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9961324
            },
            "cluster": 1
        }, {
            "cell_id": 5,
            "code": "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n\ntrain_df = train_df[features + ['Survived']]\ntrain_df.head()",
            "class": "Data Transform",
            "desc": "The code snippet selects a subset of features along with the target variable 'Survived' from the `train_df` DataFrame to prepare the data for training.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99079293
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "test_df = test_df[features]\ntest_df.head()",
            "class": "Data Transform",
            "desc": "The code snippet selects a subset of features from the `test_df` DataFrame to ensure consistency with the features used in the training data.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9990376
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "all_df = pd.concat((train_df, test_df), axis=0)",
            "class": "Data Transform",
            "desc": "The code snippet concatenates the `train_df` and `test_df` DataFrames along the rows to create a combined dataset (`all_df`) for unified preprocessing.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.9995192
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()",
            "class": "Data Transform",
            "desc": "The code snippet splits the unified `all_df` DataFrame back into separate training and test datasets by indices, ensuring any transformations applied to `all_df` are reflected in both subsets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.89564276
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "print('Before encoding:\\n{}\\n'.format(all_df['Sex'].value_counts()))\nall_df['Sex'].replace({'male':0, 'female':1}, inplace=True)\nprint('After encoding:\\n{}'.format(all_df['Sex'].value_counts()))",
            "class": "Data Transform",
            "desc": "The code snippet replaces categorical values in the 'Sex' column of `all_df` with numeric values (0 for 'male' and 1 for 'female') to prepare the data for model training, and prints the value counts before and after encoding.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9664374
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()",
            "class": "Data Transform",
            "desc": "The code snippet re-splits the unified `all_df` DataFrame back into separate training and test datasets by indices after applying data transformations, ensuring consistency between the datasets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.89564276
            },
            "cluster": 0
        }, {
            "cell_id": 25,
            "code": "all_df['Embarked'].fillna('S', inplace=True)\nall_df['Embarked'].isnull().any()",
            "class": "Data Transform",
            "desc": "The code snippet fills missing values in the 'Embarked' column of `all_df` with 'S' and checks if there are any remaining null values in this column to ensure data completeness.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.993634
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "\"\"\"\n# We will encode `Embarked` with 'S' to 0, 'Q' to 1 and 'C' to 2\nprint('Before encoding:\\n{}\\n'.format(train_df['Embarked'].value_counts()))\ntrain_df['Embarked'].replace({'S':0, 'Q':1, 'C':2}, inplace=True)\nprint('After encoding:\\n{}'.format(train_df['Embarked'].value_counts()))\n\"\"\";",
            "class": "Data Transform",
            "desc": "The code snippet replaces categorical values in the 'Embarked' column of `train_df` with numeric values (0 for 'S', 1 for 'Q', and 2 for 'C') to prepare the data for model training, and prints the value counts before and after encoding.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.43857637
            },
            "cluster": 0
        }, {
            "cell_id": 28,
            "code": "embark_dummies = pd.get_dummies(data=all_df['Embarked'], prefix='Embarked')\nembark_dummies.head()",
            "class": "Data Transform",
            "desc": "The code snippet creates one-hot encoding for the 'Embarked' column of `all_df` using pandas' `get_dummies` method and displays the first few rows of the resultant dummy variables.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9989791
            },
            "cluster": 0
        }, {
            "cell_id": 29,
            "code": "all_df = pd.concat([all_df, embark_dummies], axis=1)\nall_df.drop(columns='Embarked', inplace=True)\nall_df.head()",
            "class": "Data Transform",
            "desc": "The code snippet concatenates the one-hot encoded dummy variables for the 'Embarked' column to `all_df`, drops the original 'Embarked' column, and displays the first few rows of the updated DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.99898034
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()",
            "class": "Data Transform",
            "desc": "The code snippet re-splits the updated `all_df` DataFrame back into separate training and test datasets by indices after applying the one-hot encoding transformation, ensuring consistency between the datasets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.89564276
            },
            "cluster": 0
        }, {
            "cell_id": 32,
            "code": "all_df['Cabin'].fillna('U', inplace=True)\nall_df['Cabin'].isnull().any()",
            "class": "Data Transform",
            "desc": "The code snippet fills missing values in the 'Cabin' column of `all_df` with 'U' and checks if there are any remaining null values in this column to ensure data completeness.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.96726316
            },
            "cluster": 0
        }, {
            "cell_id": 33,
            "code": "all_df['Cabin'] = all_df['Cabin'].apply(lambda x: x[0])\nall_df['Cabin'].value_counts()",
            "class": "Data Transform",
            "desc": "The code snippet transforms the 'Cabin' column of `all_df` to retain only the first letter of each cabin code and then uses `value_counts()` to display the frequency distribution of the transformed codes.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9995357
            },
            "cluster": 0
        }, {
            "cell_id": 36,
            "code": "cabin_dummies = pd.get_dummies(data=all_df['Cabin'], prefix='Cabin')\ncabin_dummies.head()",
            "class": "Data Transform",
            "desc": "The code snippet creates one-hot encoding for the 'Cabin' column of `all_df` using pandas' `get_dummies` method and displays the first few rows of the resultant dummy variables.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99897337
            },
            "cluster": 0
        }, {
            "cell_id": 37,
            "code": "all_df = pd.concat([all_df, cabin_dummies], axis=1)\nall_df.drop(columns='Cabin', inplace=True)\nall_df.head()",
            "class": "Data Transform",
            "desc": "The code snippet concatenates the one-hot encoded dummy variables for the 'Cabin' column to `all_df`, drops the original 'Cabin' column, and displays the first few rows of the updated DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.9989489
            },
            "cluster": 0
        }, {
            "cell_id": 38,
            "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()",
            "class": "Data Transform",
            "desc": "The code snippet re-splits the updated `all_df` DataFrame back into separate training and test datasets by indices after applying the one-hot encoding transformation for the 'Cabin' column, ensuring consistency between the datasets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.89564276
            },
            "cluster": 0
        }, {
            "cell_id": 43,
            "code": "\"\"\"\n# Choosing to impute with mean.\ntrain_df.groupby(by=['Pclass', 'Parch'])['Age'].transform('mean').isnull().sum()\ntrain_df['Age'].fillna(train_df.groupby(by=['Pclass', 'Sex', 'Parch'])['Age'].transform('mean'), inplace=True)\n\"\"\";",
            "class": "Data Transform",
            "desc": "The code snippet fills missing 'Age' values in `train_df` with the mean age calculated by grouping on 'Pclass', 'Sex', and 'Parch' using the `transform('mean')` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.84770185
            },
            "cluster": 0
        }, {
            "cell_id": 46,
            "code": "scaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(train_df)\nscaled_df = pd.DataFrame(data=scaled_df, columns=train_df.columns)\nscaled_df.head()",
            "class": "Data Transform",
            "desc": "The code snippet applies MinMax scaling to the features in `train_df` using `MinMaxScaler`, converts the scaled data back into a pandas DataFrame, and displays the first few rows of the scaled DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "normalization",
                "subclass_id": 18,
                "predicted_subclass_probability": 0.9299693
            },
            "cluster": 0
        }, {
            "cell_id": 47,
            "code": "imputer = KNNImputer(n_neighbors=3)\nimputed_df = imputer.fit_transform(scaled_df)\n#imputer_df = pd.DataFrame(data=imputed_df, columns=features)\nimputed_df = scaler.inverse_transform(imputed_df)\nimputed_df = pd.DataFrame(data=imputed_df, columns=train_df.columns)",
            "class": "Data Transform",
            "desc": "The code snippet applies K-Nearest Neighbors (KNN) imputation to fill missing values in the scaled DataFrame, then inversely transforms the imputed data back to the original scale, and converts it into a pandas DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.37421712
            },
            "cluster": 0
        }, {
            "cell_id": 49,
            "code": "train_df['Age'] = imputed_df['Age']",
            "class": "Data Transform",
            "desc": "The code snippet updates the 'Age' column in `train_df` with the imputed age values from `imputed_df` to incorporate the filled missing values into the original training data.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99938726
            },
            "cluster": -1
        }, {
            "cell_id": 50,
            "code": "scaled_df = scaler.transform(test_df)\nscaled_df = pd.DataFrame(data=scaled_df, columns=test_df.columns)\nscaled_df.head()",
            "class": "Data Transform",
            "desc": "The code snippet applies the previously fitted MinMax scaling transformation to `test_df`, converts the scaled data back into a pandas DataFrame, and displays the first few rows of the scaled test DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.26208633
            },
            "cluster": 0
        }, {
            "cell_id": 51,
            "code": "imputed_df = imputer.transform(scaled_df)\n#imputer_df = pd.DataFrame(data=imputed_df, columns=all_df[891:].columns)\nimputed_df = scaler.inverse_transform(imputed_df)\nimputed_df = pd.DataFrame(data=imputed_df, columns=all_df.iloc[891:].columns)\nimputed_df.head()",
            "class": "Data Transform",
            "desc": "The code snippet applies K-Nearest Neighbors (KNN) imputation to fill missing values in the scaled test DataFrame, then inversely transforms the imputed data back to the original scale, and converts it into a pandas DataFrame, displaying the first few rows of the imputed test DataFrame.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.39289653
            },
            "cluster": 0
        }, {
            "cell_id": 52,
            "code": "test_df['Age'] = imputed_df['Age']",
            "class": "Data Transform",
            "desc": "The code snippet updates the 'Age' column in `test_df` with the imputed age values from `imputed_df` to incorporate the filled missing values into the original test data.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9993936
            },
            "cluster": -1
        }, {
            "cell_id": 53,
            "code": "all_df.iloc[:891] = train_df.copy()\nall_df.iloc[891:] = test_df.copy()",
            "class": "Data Transform",
            "desc": "The code snippet updates `all_df` with the modified `train_df` and `test_df` by replacing the respective portions of `all_df` to maintain consistency across the unified dataset after transformations.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.49698666
            },
            "cluster": 0
        }, {
            "cell_id": 54,
            "code": "all_df['Age_band'] = all_df['Age'].apply(lambda x: math.ceil(x / 5))\nall_df.sample(10)",
            "class": "Data Transform",
            "desc": "The code snippet creates a new column 'Age_band' in `all_df` by applying a lambda function that divides each age by 5 and rounds up using the `math.ceil` function, and then displays a random sample of 10 rows from the DataFrame to inspect the new column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99901354
            },
            "cluster": 0
        }, {
            "cell_id": 55,
            "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()",
            "class": "Data Transform",
            "desc": "The code snippet re-splits the updated `all_df` DataFrame back into separate training and test datasets by indices to ensure consistency in both datasets after the creation of the 'Age_band' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.89564276
            },
            "cluster": 0
        }, {
            "cell_id": 60,
            "code": "all_df['Fare'].fillna(train_df.Fare.mean(), inplace=True)",
            "class": "Data Transform",
            "desc": "The code snippet fills missing values in the 'Fare' column of `all_df` with the mean fare from the training dataset to ensure data completeness.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.45188195
            },
            "cluster": 0
        }, {
            "cell_id": 61,
            "code": "def create_fare_class(x):\n    if x >= 100:\n        fare_class = 1\n    elif x >= 80 and x < 100:\n        fare_class = 2\n    elif x >= 60 and x < 80:\n        fare_class = 3\n    elif x >= 40 and x < 60:\n        fare_class = 4\n    elif x >= 20 and x < 40:\n        fare_class = 5\n    else:\n        fare_class = 6\n\n    return fare_class",
            "class": "Data Transform",
            "desc": "The code snippet defines a function `create_fare_class` that categorizes fare amounts into six classes based on specified ranges to facilitate fare-based feature engineering.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9775144
            },
            "cluster": 0
        }, {
            "cell_id": 62,
            "code": "all_df['Fare_class'] = all_df['Fare'].apply(create_fare_class)\nall_df.head()",
            "class": "Data Transform",
            "desc": "The code snippet applies the `create_fare_class` function to the 'Fare' column of `all_df` and creates a new column 'Fare_class', then displays the first few rows of the updated DataFrame to inspect the new feature.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99655885
            },
            "cluster": 0
        }, {
            "cell_id": 63,
            "code": "train_df = all_df.iloc[:891].copy()\ntest_df = all_df.iloc[891:].copy()",
            "class": "Data Transform",
            "desc": "The code snippet re-splits the updated `all_df` DataFrame back into separate training and test datasets by indices to ensure consistency in both datasets after the creation of the 'Fare_class' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.89564276
            },
            "cluster": 0
        }, {
            "cell_id": 69,
            "code": "X_train = train_df.drop(columns=['Age', 'Fare', 'Survived'])\nX_train.head()",
            "class": "Data Transform",
            "desc": "The code snippet creates the feature matrix `X_train` by dropping the 'Age', 'Fare', and 'Survived' columns from `train_df`, as these columns are not needed for training the model, and displays the first few rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99889225
            },
            "cluster": 0
        }, {
            "cell_id": 70,
            "code": "y_train = train_df['Survived'].astype('int')\ny_train.head()",
            "class": "Data Transform",
            "desc": "The code snippet creates the target vector `y_train` by extracting the 'Survived' column from `train_df` and converting its data type to integer, then displays the first few values to confirm the target labels.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.6704867
            },
            "cluster": 0
        }, {
            "cell_id": 72,
            "code": "X_test = test_df.drop(columns=['Age', 'Fare', 'Survived'])\nX_test.head()",
            "class": "Data Transform",
            "desc": "The code snippet creates the feature matrix `X_test` by dropping the 'Age', 'Fare', and 'Survived' columns from `test_df`, and displays the first few rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.999015
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "train_df.sample(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet randomly samples and displays 10 rows from the `train_df` DataFrame using pandas to get a better understanding of the data distribution.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99976045
            },
            "cluster": 1
        }, {
            "cell_id": 8,
            "code": "all_df.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first few rows of the concatenated DataFrame `all_df` to inspect the unified dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997588
            },
            "cluster": 1
        }, {
            "cell_id": 11,
            "code": "train_df.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet uses the `info()` method on `train_df` to display a concise summary of the DataFrame, including the data types, non-null count, and memory usage.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993624
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "train_df.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet uses the `describe()` method on `train_df` to generate descriptive statistics, including count, mean, standard deviation, and other summary statistics for numerical columns.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994492
            },
            "cluster": -1
        }, {
            "cell_id": 13,
            "code": "print(train_df['Survived'].value_counts())\nprint(train_df['Survived'].value_counts(normalize=True))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the counts and proportions of each class in the 'Survived' column to examine the distribution of the target variable in the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.99950135
            },
            "cluster": 7
        }, {
            "cell_id": 15,
            "code": "women = train_df.loc[train_df.Sex == 'female']['Survived']\nrate_women = sum(women)/len(women)\n\nprint('% of women who survived: {:.3f}'.format(rate_women))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and prints the survival rate of women in the training dataset by filtering the 'Survived' column for female passengers and computing the proportion of survivors.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.31332606
            },
            "cluster": 4
        }, {
            "cell_id": 16,
            "code": "men = train_df.loc[train_df.Sex == 'male']['Survived']\nrate_men = sum(men)/len(men)\n\nprint('% of men who survived: {:.3f}'.format(rate_men))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and prints the survival rate of men in the training dataset by filtering the 'Survived' column for male passengers and computing the proportion of survivors.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.35764214
            },
            "cluster": 4
        }, {
            "cell_id": 20,
            "code": "print(train_df['Pclass'].value_counts())\nprint(train_df['Pclass'].value_counts(normalize=True))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the counts and proportions of each class in the 'Pclass' column of `train_df` to examine the distribution of passenger classes in the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9995016
            },
            "cluster": 7
        }, {
            "cell_id": 23,
            "code": "train_df['Embarked'].value_counts()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet uses the `value_counts()` method on the 'Embarked' column of `train_df` to display the frequency distribution of the different embarkation points.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9995072
            },
            "cluster": 5
        }, {
            "cell_id": 24,
            "code": "print('Percentage of null values in Embarked column: {:.3g}%'.format(train_df['Embarked'].isnull().sum() / len(train_df) * 100))\nprint('Percentage of null values in Embarked column: {:.3g}%'.format(test_df['Embarked'].isnull().sum() / len(test_df) * 100))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and prints the percentage of null values in the 'Embarked' column for both `train_df` and `test_df` to assess the extent of missing data in this column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9974777
            },
            "cluster": 3
        }, {
            "cell_id": 31,
            "code": "print('Percentage of null values in Cabin column: {:.3g}%'.format(train_df['Cabin'].isnull().sum() / len(train_df) * 100))\nprint('Percentage of null values in Cabin column: {:.3g}%'.format(test_df['Cabin'].isnull().sum() / len(test_df) * 100))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and prints the percentage of null values in the 'Cabin' column for both `train_df` and `test_df` to assess the extent of missing data in this column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.98824114
            },
            "cluster": 3
        }, {
            "cell_id": 34,
            "code": "all_df['Cabin'].unique()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet uses the `unique()` method on the 'Cabin' column of `all_df` to display the unique cabin codes after transformation.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_unique_values",
                "subclass_id": 57,
                "predicted_subclass_probability": 0.99807227
            },
            "cluster": 5
        }, {
            "cell_id": 40,
            "code": "temp_df = train_df[train_df.Age.isnull()]\nprint(\"The number of null values in Age column is {}\".format(len(temp_df)))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet filters `train_df` to find rows with null values in the 'Age' column, stores them in `temp_df`, and prints the number of such rows to quantify missing data for the 'Age' column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.39663118
            },
            "cluster": 3
        }, {
            "cell_id": 45,
            "code": "train_df.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first few rows of the `train_df` DataFrame to inspect the current state of the data after the recent transformations and imputations.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997545
            },
            "cluster": 1
        }, {
            "cell_id": 48,
            "code": "imputed_df.isnull().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet checks for any remaining null values in the `imputed_df` DataFrame by summing the null values across its columns.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9989617
            },
            "cluster": 5
        }, {
            "cell_id": 56,
            "code": "train_df[train_df.Fare==0]",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet filters `train_df` to find and display rows where the 'Fare' column has a value of 0, likely to investigate cases of ticket prices being zero.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.97212875
            },
            "cluster": 4
        }, {
            "cell_id": 66,
            "code": "all_df.columns",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet lists all column names in the `all_df` DataFrame to review the features and their order in the dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.9983329
            },
            "cluster": 1
        }, {
            "cell_id": 68,
            "code": "train_df.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first few rows of the `train_df` DataFrame to inspect the current state of the data after the recent transformations and feature engineering.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997545
            },
            "cluster": 1
        }, {
            "cell_id": 71,
            "code": "test_df.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first few rows of the `test_df` DataFrame to inspect the test data after recent transformations and feature engineering.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997558
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "The code snippet imports essential libraries including numpy, pandas, math, and tqdm, and lists files in the input directory to familiarize with the data structure in the Kaggle environment.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.9992084
            },
            "cluster": -1
        }, {
            "cell_id": 10,
            "code": "import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline",
            "class": "Imports and Environment",
            "desc": "The code snippet imports the matplotlib.pyplot and seaborn libraries for creating visualizations and sets inline display for matplotlib plots in Jupyter notebooks.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9993888
            },
            "cluster": -1
        }, {
            "cell_id": 44,
            "code": "from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import KNNImputer",
            "class": "Imports and Environment",
            "desc": "The code snippet imports the MinMaxScaler and KNNImputer classes from the scikit-learn library for scaling features and imputing missing values, respectively.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.999292
            },
            "cluster": -1
        }, {
            "cell_id": 73,
            "code": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nrandom_state=2",
            "class": "Imports and Environment",
            "desc": "The code snippet imports various machine learning algorithms, including `LogisticRegression`, `RandomForestClassifier`, `AdaBoostClassifier`, `GradientBoostingClassifier` from scikit-learn, `XGBClassifier` from xgboost, and tools for cross-validation and hyperparameter tuning like `cross_val_score` and `GridSearchCV`, setting a random state for reproducibility.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.84664047
            },
            "cluster": -1
        }, {
            "cell_id": 74,
            "code": "cv_score_columns = ['logreg', 'rforest', 'adaboost', 'gboost', 'xg']\n# cv_score_columns = ['logreg', 'rforest', 'adaboost', 'gboost']\ncv_score_df = pd.DataFrame(columns=cv_score_columns)\ncv_score_df",
            "class": "Model Evaluation",
            "desc": "The code snippet initializes a DataFrame `cv_score_df` with columns named after different machine learning models ('logreg', 'rforest', 'adaboost', 'gboost', 'xg') to store cross-validation scores for evaluating model performance.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.99424237
            },
            "cluster": 0
        }, {
            "cell_id": 76,
            "code": "models = [logreg, rforest, adaboost, gboost, xg]\ncv_scores = []\nfor model in models:\n    cvs_mean = (cross_val_score(model, X_train, y_train, cv=5)).mean()\n    cv_scores.append(cvs_mean)",
            "class": "Model Evaluation",
            "desc": "The code snippet evaluates different machine learning models (`logreg`, `rforest`, `adaboost`, `gboost`, `xg`) using 5-fold cross-validation on the training data, calculates the mean cross-validation score for each model, and stores these scores in the `cv_scores` list.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.8604194
            },
            "cluster": 0
        }, {
            "cell_id": 77,
            "code": "cv_score_df.loc['base'] = cv_scores\ncv_score_df",
            "class": "Model Evaluation",
            "desc": "The code snippet adds the calculated mean cross-validation scores for the baseline models to the `cv_score_df` DataFrame as a new row labeled 'base' and displays the DataFrame to review the stored scores.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9977927
            },
            "cluster": 1
        }, {
            "cell_id": 80,
            "code": "best_model_df = pd.DataFrame(columns=cv_score_columns)\nbest_model_list = []\ncv_scores = []",
            "class": "Model Evaluation",
            "desc": "The code snippet initializes an empty DataFrame `best_model_df` with columns named after different machine learning models and two empty lists `best_model_list` and `cv_scores` to store the best models post-grid search and their corresponding cross-validation scores.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.99682707
            },
            "cluster": 0
        }, {
            "cell_id": 82,
            "code": "best_model_list",
            "class": "Model Evaluation",
            "desc": "The code snippet outputs the `best_model_list`, which contains dictionaries of each model's best estimator and its best hyperparameters obtained from the grid search process.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9995352
            },
            "cluster": 1
        }, {
            "cell_id": 83,
            "code": "cv_scores",
            "class": "Model Evaluation",
            "desc": "The code snippet outputs the `cv_scores` list, which contains the best cross-validation scores for each model obtained from the grid search process.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99977154
            },
            "cluster": 1
        }, {
            "cell_id": 84,
            "code": "best_model_df.loc['best'] = best_model_list\ncv_score_df.loc['best'] = cv_scores",
            "class": "Model Evaluation",
            "desc": "The code snippet updates `best_model_df` with the list of best models and parameters as a new row labeled 'best', and updates `cv_score_df` with the corresponding cross-validation scores as another new row labeled 'best', for comparison against the baseline models.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.8962994
            },
            "cluster": 0
        }, {
            "cell_id": 85,
            "code": "cv_score_df",
            "class": "Model Evaluation",
            "desc": "The code snippet displays the `cv_score_df` DataFrame, which contains the cross-validation scores for both the baseline and the best hyperparameter-tuned models for further analysis.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997156
            },
            "cluster": 1
        }, {
            "cell_id": 88,
            "code": "predictions = model_best.predict(X_test)",
            "class": "Model Evaluation",
            "desc": "The code snippet uses the trained best RandomForest model (`model_best`) to make predictions on the test data (`X_test`) and stores the results in the `predictions` variable.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9945082
            },
            "cluster": 0
        }, {
            "cell_id": 75,
            "code": "logreg = LogisticRegression(solver='liblinear', random_state=random_state)\nrforest = RandomForestClassifier(random_state=random_state)\nadaboost = AdaBoostClassifier(random_state=random_state)\ngboost = GradientBoostingClassifier(random_state=random_state)\nxg = xgb.XGBClassifier(use_label_encoder=False, random_state=random_state)",
            "class": "Model Training",
            "desc": "The code snippet initializes instances of various classifiers, including `LogisticRegression`, `RandomForestClassifier`, `AdaBoostClassifier`, `GradientBoostingClassifier` from scikit-learn, and `XGBClassifier` from xgboost, setting the `random_state` for reproducibility.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.97994995
            },
            "cluster": -1
        }, {
            "cell_id": 78,
            "code": "logreg = LogisticRegression(solver='liblinear', random_state=random_state)\nrforest = RandomForestClassifier(random_state=random_state)\nadaboost = AdaBoostClassifier(random_state=random_state)\ngboost = GradientBoostingClassifier(random_state=random_state)\nxg = xgb.XGBClassifier(use_label_encoder=False, random_state=random_state)",
            "class": "Model Training",
            "desc": "The code snippet re-initializes instances of various classifiers, including `LogisticRegression`, `RandomForestClassifier`, `AdaBoostClassifier`, `GradientBoostingClassifier` from scikit-learn, and `XGBClassifier` from xgboost, setting the `random_state` for reproducibility, potentially resetting the models for further training or tuning.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.97994995
            },
            "cluster": -1
        }, {
            "cell_id": 79,
            "code": "gridsearch_dict = {'model_list':models,\n                   'param_list':[{'penalty':('l1', 'l2'), 'C':(.001, .01, .1)},\n                                 {'n_estimators':(100, 200, 300),'criterion':('gini', 'entropy'), 'max_depth':(2, 3, 4, 5)},\n                                 {'n_estimators':(50, 100, 200, 300), 'learning_rate':(.001, .01, .1)},\n                                 {'loss':('deviance', 'exponential'), 'learning_rate':(.001, .01), 'n_estimators':(100, 200, 300)},\n                                 {'n_estimators':(100, 200, 300), 'learning_rate':(.001, .01, .1)}\n                                ]\n                   }",
            "class": "Model Training",
            "desc": "The code snippet defines a dictionary `gridsearch_dict` containing the list of models and the associated hyperparameters to be tuned for each model using grid search, readying the models for hyperparameter optimization.",
            "testing": {
                "class": "Model_Train",
                "subclass": "define_search_space",
                "subclass_id": 5,
                "predicted_subclass_probability": 0.9896735
            },
            "cluster": -1
        }, {
            "cell_id": 81,
            "code": "%%time\nfor m, p in tqdm(zip(gridsearch_dict['model_list'], gridsearch_dict['param_list'])):\n    clf = GridSearchCV(m, p)\n    clf.fit(X_train, y_train)\n    best_model_list.append({'best_model':clf.best_estimator_, 'best_param':clf.best_params_})\n    cv_scores.append(clf.best_score_)",
            "class": "Model Training",
            "desc": "The code snippet uses grid search (`GridSearchCV`) to find the best hyperparameters for each model in the list, fits the models on the training data, and stores the best models along with their parameters in `best_model_list` and their best cross-validation scores in `cv_scores`, displaying progress using `tqdm`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.99058175
            },
            "cluster": 1
        }, {
            "cell_id": 86,
            "code": "model_best = best_model_df.iloc[0]['rforest']['best_model']",
            "class": "Model Training",
            "desc": "The code snippet selects the best RandomForest model from the `best_model_df` DataFrame, storing it in the variable `model_best` for making predictions or further evaluation.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.913111
            },
            "cluster": 1
        }, {
            "cell_id": 87,
            "code": "model_best.fit(X_train, y_train)",
            "class": "Model Training",
            "desc": "The code snippet fits the selected best RandomForest model (`model_best`) on the training data (`X_train`, `y_train`) to train it for making predictions or further evaluation.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9997105
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "sns.countplot(x=train_df['Survived'])\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `countplot` to create and display a bar plot of the counts for each class in the 'Survived' column, visualizing the distribution of the target variable.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99577636
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "sns.countplot(x='Survived', data=train_df, hue='Sex' )\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `countplot` to create and display a bar plot of the counts for the 'Survived' column, with a hue based on 'Sex', to visualize survival distribution among different genders.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99815875
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "sns.countplot(x='Survived', data=train_df, hue='Pclass')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `countplot` to create and display a bar plot of the counts for the 'Survived' column, with a hue based on 'Pclass', to visualize survival distribution among different passenger classes.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99799454
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "sns.catplot(x='Pclass', data=train_df, hue='Sex',\n            col='Survived', kind='count')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `catplot` to create and display a count plot divided by survival status (`Survived`), passenger class (`Pclass`), and gender (`Sex`) to visualize the interaction between these variables in the training dataset.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9954602
            },
            "cluster": 0
        }, {
            "cell_id": 26,
            "code": "sns.countplot(x='Embarked', data=train_df, hue='Survived')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `countplot` to create and display a bar plot of the counts for the 'Embarked' column, with a hue based on 'Survived', to visualize the distribution of embarkation points and their relationship with survival rates.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9850413
            },
            "cluster": 0
        }, {
            "cell_id": 35,
            "code": "sns.countplot(x='Cabin', data=all_df.iloc[:891], hue='Survived')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `countplot` to create and display a bar plot of the counts for the 'Cabin' column in the training dataset, with a hue based on 'Survived', to visualize the distribution of cabin codes and their relationship with survival rates.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9963553
            },
            "cluster": 0
        }, {
            "cell_id": 39,
            "code": "plt.figure(figsize=(15, 4))\nplt.subplot(121)\nsns.histplot(x='Age', data=train_df, hue='Survived', multiple='stack')\nplt.subplot(122)\nsns.histplot(x='Age', data=train_df, hue='Survived', element='poly')\n#sns.histplot(x='Age', data=train_df, ax=ax2, color='g')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet creates two histograms using Seaborn's `histplot` to visualize the distribution of the 'Age' column in the training dataset, both stacked and overlaid, colored by survival status to understand age-related survival trends.",
            "testing": {
                "class": "Visualization",
                "subclass": "time_series",
                "subclass_id": 75,
                "predicted_subclass_probability": 0.8408765
            },
            "cluster": 0
        }, {
            "cell_id": 41,
            "code": "plt.figure(figsize=(18, 10))\nplt.subplot(231)\nsns.countplot(x='Pclass', data=temp_df)\nplt.subplot(232)\nsns.countplot(x='Sex', data=temp_df)\nplt.subplot(233)\nsns.countplot(x='SibSp', data=temp_df)\nplt.subplot(234)\nsns.countplot(x='Parch', data=temp_df)\nplt.subplot(235)\nsns.histplot(x='Fare', data=temp_df)\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet creates a series of count and histogram plots using Seaborn's `countplot` and `histplot` to visualize the distribution of various features ('Pclass', 'Sex', 'SibSp', 'Parch', and 'Fare') among the rows with missing 'Age' values.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99794
            },
            "cluster": 0
        }, {
            "cell_id": 42,
            "code": "\"\"\"\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.histplot(data=train_df.groupby(by=['Pclass', 'Parch', 'SibSp'])['Age'].transform('median'), ax=ax1, bins=20, kde=True, color='b')\nsns.histplot(data=train_df.Age, ax=ax1, kde=True, color='r')\n\nsns.histplot(data=train_df.groupby(by=['Pclass', 'Parch', 'Sex'])['Age'].transform('median'), ax=ax2, bins=20, kde=True, color='b')\nsns.histplot(data=train_df.Age, ax=ax2, kde=True, color='r')\n\nplt.show()\n\"\"\";",
            "class": "Visualization",
            "desc": "The code snippet creates two histograms using Seaborn's `histplot` to compare the distribution of the 'Age' column against median age values grouped by various features ('Pclass', 'Parch', 'SibSp' and 'Sex') to explore imputation strategies, visualizing the plots side-by-side with Kernel Density Estimate (KDE) curves.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9966239
            },
            "cluster": 0
        }, {
            "cell_id": 57,
            "code": "plt.figure(figsize=(18, 12))\nplt.subplot(211)\nsns.histplot(x='Fare', data=train_df, bins=50, hue='Survived', multiple='stack')\nplt.subplot(212)\nsns.histplot(x='Fare', data=train_df, bins=50, hue='Survived', element='poly')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet creates two histograms using Seaborn's `histplot` to visualize the distribution of the 'Fare' column in the training dataset, both stacked and overlaid, colored by survival status to understand the relationship between fare prices and survival rates.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9983432
            },
            "cluster": 0
        }, {
            "cell_id": 58,
            "code": "plt.figure(figsize=(18, 6))\nsns.scatterplot(x='Age', y='Fare', data=train_df, hue='Survived', size='Fare', sizes=(20, 150))\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `scatterplot` to create and display a scatter plot of 'Age' versus 'Fare', with points colored by survival status and sized by fare amount, to visualize the relationship between age, fare, and survival in the training dataset.",
            "testing": {
                "class": "Visualization",
                "subclass": "time_series",
                "subclass_id": 75,
                "predicted_subclass_probability": 0.97067386
            },
            "cluster": 0
        }, {
            "cell_id": 59,
            "code": "plt.figure(figsize=(18, 6))\nax = plt.subplot()\n\nax.scatter(train_df[train_df.Survived==1]['Age'], train_df[train_df.Survived==1]['Fare'],\n           c='g', s=train_df[train_df.Survived==1]['Fare'])\nax.scatter(train_df[train_df.Survived==0]['Age'], train_df[train_df.Survived==0]['Fare'],\n           c='r', s=train_df[train_df.Survived==0]['Fare'])\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet creates a scatter plot where points representing passengers who survived are colored green and points representing those who did not survive are colored red, with point sizes corresponding to 'Fare', to examine the relationship between age, fare, and survival.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9976399
            },
            "cluster": 0
        }, {
            "cell_id": 64,
            "code": "sns.countplot(x='Fare_class', data=all_df, hue='Survived')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `countplot` to create and display a bar plot of the counts for the 'Fare_class' column in `all_df`, with a hue based on 'Survived', to visualize the distribution of fare classes and their relationship with survival rates.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.4513631
            },
            "cluster": 0
        }, {
            "cell_id": 65,
            "code": "sns.countplot(x='SibSp', data=train_df, hue='Survived')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `countplot` to create and display a bar plot of the counts for the 'SibSp' column in `train_df`, with a hue based on 'Survived', to visualize the distribution of the number of siblings/spouses aboard and their relationship with survival rates.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99788505
            },
            "cluster": 0
        }, {
            "cell_id": 67,
            "code": "sns.countplot(x='Parch', data=train_df, hue='Survived')\nplt.show()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `countplot` to create and display a bar plot of the counts for the 'Parch' column in `train_df`, with a hue based on 'Survived', to visualize the distribution of the number of parents/children aboard and their relationship with survival rates.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99725753
            },
            "cluster": 0
        }],
        "notebook_id": 0,
        "notebook_name": "titanic-submissions.ipynb",
        "user": "titanic-submissions.ipynb"
    }, {
        "cells": [{
            "cell_id": 37,
            "code": "output = pd.DataFrame({'PassengerId':test_df.PassengerId, 'Survived':logistic_pred})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Data Export",
            "desc": "This code snippet creates a DataFrame with 'PassengerId' and predicted 'Survived' values, then saves it as a CSV file named 'submission.csv' without the index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99903464
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_df = pd.read_csv('../input/titanic/train.csv')\ntest_df = pd.read_csv('../input/titanic/test.csv')",
            "class": "Data Extraction",
            "desc": "This code snippet reads the training and test datasets for the Titanic dataset from CSV files into pandas DataFrames.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99975973
            },
            "cluster": 1
        }, {
            "cell_id": 35,
            "code": "test_df[features]",
            "class": "Data Extraction",
            "desc": "This code snippet extracts the specified feature columns ('Pclass', 'Sex', 'SibSp', 'Parch') from the test DataFrame and displays them.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99951756
            },
            "cluster": -1
        }, {
            "cell_id": 17,
            "code": "from sklearn.preprocessing import LabelEncoder\n\nclass FE:\n    def __init__(self, dataframes):\n        self.dataframes = dataframes\n        \n    def remove_cols(self):\n        for df in self.dataframes:\n            try:\n                df.drop(['Cabin', 'Name', 'Ticket'], axis=1, inplace=True)\n            except:\n                pass\n        \n    def convert_sex(self):\n        for df in self.dataframes:\n            le = LabelEncoder()\n            df.Sex = le.fit_transform(df.Sex)\n            ",
            "class": "Data Transform",
            "desc": "This code snippet defines a feature engineering class `FE` that includes methods to remove specific columns ('Cabin', 'Name', 'Ticket') and convert the 'Sex' column to numeric values using Label Encoding with scikit-learn's `LabelEncoder`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9992322
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "fe = FE([train_df, test_df])\nfe.remove_cols()\nfe.convert_sex()",
            "class": "Data Transform",
            "desc": "This code snippet initializes an instance of the `FE` class with the training and test DataFrames and calls the methods to remove specified columns and convert the 'Sex' column to numeric values.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.6193043
            },
            "cluster": 0
        }, {
            "cell_id": 20,
            "code": "features = ['Pclass', 'Sex', 'SibSp', 'Parch']\nX = train_df[features]\ny = train_df['Survived']",
            "class": "Data Transform",
            "desc": "This code snippet selects the 'Pclass', 'Sex', 'SibSp', and 'Parch' columns as features and the 'Survived' column as the target variable from the training DataFrame, assigning them to the variables `X` and `y` respectively.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.999345
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=2)",
            "class": "Data Transform",
            "desc": "This code snippet splits the features and target variable into training and test sets using a 75-25 split ratio and a random state of 2 with the `train_test_split` function from scikit-learn.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.99821234
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "print(train_df.shape, test_df.shape)",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet prints the dimensions (number of rows and columns) of the training and test datasets.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9990921
            },
            "cluster": 5
        }, {
            "cell_id": 3,
            "code": "train_df.info()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet provides a concise summary of the training DataFrame, including the data types and non-null values of each column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993624
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "train_df.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet generates descriptive statistics for the numerical columns in the training DataFrame using the pandas `describe()` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994492
            },
            "cluster": 1
        }, {
            "cell_id": 5,
            "code": "train_df.isna().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the total number of missing values for each column in the training DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9990694
            },
            "cluster": 7
        }, {
            "cell_id": 6,
            "code": "train_df.duplicated().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the number of duplicated rows in the training DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.8890608
            },
            "cluster": 7
        }, {
            "cell_id": 12,
            "code": "sexes = train_df.Sex.unique()\ncount = list()\n\nfor sex in sexes:\n    percentage = round(len(train_df[(train_df.Sex == sex) & (train_df.Survived == 1)]) / len(train_df[train_df.Sex == sex]), 3) * 100\n    count.append(percentage)\n    \nprint(count) ",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the survival percentage for each unique sex in the training DataFrame by dividing the number of survivors by the total count of each sex and multiplying by 100.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.86492133
            },
            "cluster": 8
        }, {
            "cell_id": 14,
            "code": "p_classes = train_df.Pclass.unique()\npcts = list()\n\nfor p in p_classes:\n    percentage = round(len(train_df[(train_df.Pclass == p) & (train_df.Survived == 1)]) / len(train_df[train_df.Pclass == p]),3) * 100\n    pcts.append(percentage)\n    \nprint(pcts) ",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the survival percentage for each unique passenger class in the training DataFrame by dividing the number of survivors by the total count for each class and multiplying by 100.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.8156362
            },
            "cluster": 8
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "This code snippet imports essential Python libraries (NumPy, pandas, Matplotlib, Seaborn, missingno, and warnings), sets up the environment for plotting inline, and lists the files in the Kaggle input directory.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.9989272
            },
            "cluster": -1
        }, {
            "cell_id": 19,
            "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report",
            "class": "Imports and Environment",
            "desc": "This code snippet imports the `train_test_split` and `cross_val_score` functions from scikit-learn's model_selection module, and the `accuracy_score` and `classification_report` functions from scikit-learn's metrics module.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99933124
            },
            "cluster": -1
        }, {
            "cell_id": 28,
            "code": "df_score = pd.DataFrame({'model':['Logistic Regression', 'KNN', 'Random Forest Classifier', 'Decision Tree Classifier', 'Support Vector Machine', 'Naive Bayes'],\n                        'accuracy':[lr_acc, knn_acc, rfc_acc, dtc_acc, clf_acc, gnb_acc]})",
            "class": "Model Evaluation",
            "desc": "This code snippet creates a pandas DataFrame to compare the accuracy scores of various trained models including Logistic Regression, KNN, Random Forest Classifier, Decision Tree Classifier, Support Vector Machine, and Naive Bayes.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.9986626
            },
            "cluster": 0
        }, {
            "cell_id": 29,
            "code": "df_score.sort_values('accuracy', ascending=False)",
            "class": "Model Evaluation",
            "desc": "This code snippet sorts the DataFrame of model accuracies in descending order based on the accuracy scores.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9950781
            },
            "cluster": -1
        }, {
            "cell_id": 30,
            "code": "df_score.accuracy.mean()",
            "class": "Model Evaluation",
            "desc": "This code snippet calculates and returns the mean accuracy score across all the evaluated models by accessing the 'accuracy' column of the DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.6630979
            },
            "cluster": 0
        }, {
            "cell_id": 31,
            "code": "model_names = ['Logistic Regression', 'KNN', 'Random Forest Classifier', 'Decision Tree Classifier', 'Support Vector Machine', 'Naive Bayes']\nmodel_selectors = [lr, knn, rfc, dtc, clf, gnb]\nscores_dict = {'model':[], 'accuracy':[]}\n\nfor label, selector in zip(model_names, model_selectors):\n    cv_mean_score = np.mean(cross_val_score(selector, X, y, cv=5))\n    scores_dict['model'].append(label)\n    scores_dict['accuracy'].append(cv_mean_score)    \n    \ndf_cv_score = pd.DataFrame(scores_dict)",
            "class": "Model Evaluation",
            "desc": "This code snippet performs 5-fold cross-validation for each of the specified models, calculates the mean cross-validation score for each model, and stores these scores in a pandas DataFrame (`df_cv_score`).",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.6588713
            },
            "cluster": 0
        }, {
            "cell_id": 32,
            "code": "df_cv_score.sort_values('accuracy', ascending=False)",
            "class": "Model Evaluation",
            "desc": "This code snippet sorts the DataFrame containing the cross-validation scores in descending order based on the accuracy.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9968382
            },
            "cluster": -1
        }, {
            "cell_id": 33,
            "code": "df_cv_score.accuracy.mean()",
            "class": "Model Evaluation",
            "desc": "This code snippet calculates and returns the mean cross-validation accuracy score across all the evaluated models by accessing the 'accuracy' column of the DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.6315649
            },
            "cluster": 0
        }, {
            "cell_id": 34,
            "code": "from sklearn.model_selection import GridSearchCV\n\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\nlogistic = GridSearchCV(estimator=model, param_grid={'solver':solvers, 'penalty':penalty, 'C':c_values}, cv=5)\nlogistic.fit(train_x, train_y)\n\ngrid_info = pd.DataFrame(logistic.cv_results_)\ngrid_info[['mean_test_score', 'param_solver', 'param_penalty', 'param_C']].sort_values('mean_test_score', ascending=False)",
            "class": "Model Evaluation",
            "desc": "This code snippet performs hyperparameter tuning for a Logistic Regression model using GridSearchCV to evaluate different combinations of solvers, penalties, and regularization strengths, fits the model to the training data, and creates a DataFrame to display and sort the results based on the mean test scores.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.98615915
            },
            "cluster": 0
        }, {
            "cell_id": 36,
            "code": "logistic_pred = logistic.predict(test_df[features])\nlogistic_pred",
            "class": "Model Evaluation",
            "desc": "This code snippet uses the best Logistic Regression model from the GridSearchCV to predict the target variable for the test DataFrame's feature set.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9945187
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(train_x, train_y)\n\nlr_pred = lr.predict(test_x)\n\nlr_acc = accuracy_score(lr_pred, test_y)",
            "class": "Model Training",
            "desc": "This code snippet initializes a Logistic Regression model using scikit-learn, trains it on the training data, makes predictions on the test data, and calculates the accuracy of the predictions.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.78042775
            },
            "cluster": -1
        }, {
            "cell_id": 23,
            "code": "from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=12)\nknn.fit(train_x, train_y)\n\nknn_pred = knn.predict(test_x)\n\nknn_acc = accuracy_score(knn_pred, test_y)",
            "class": "Model Training",
            "desc": "This code snippet initializes a K-Nearest Neighbors classifier with 12 neighbors using scikit-learn, trains it on the training data, makes predictions on the test data, and calculates the accuracy of the predictions.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.7452794
            },
            "cluster": -1
        }, {
            "cell_id": 24,
            "code": "from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(train_x, train_y)\n\nrfc_pred = rfc.predict(test_x)\n\nrfc_acc = accuracy_score(rfc_pred, test_y)",
            "class": "Model Training",
            "desc": "This code snippet initializes a Random Forest classifier with 100 estimators using scikit-learn, trains it on the training data, makes predictions on the test data, and calculates the accuracy of the predictions.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.8482741
            },
            "cluster": -1
        }, {
            "cell_id": 25,
            "code": "from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(train_x, train_y)\n\ndtc_pred = dtc.predict(test_x)\n\ndtc_acc = accuracy_score(dtc_pred, test_y)",
            "class": "Model Training",
            "desc": "This code snippet initializes a Decision Tree classifier using scikit-learn, trains it on the training data, makes predictions on the test data, and calculates the accuracy of the predictions.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.6479414
            },
            "cluster": -1
        }, {
            "cell_id": 26,
            "code": "from sklearn.svm import SVC\n\nclf = SVC(kernel='linear')\nclf.fit(train_x, train_y)\n\nclf_pred = clf.predict(test_x)\n\nclf_acc = accuracy_score(clf_pred, test_y)",
            "class": "Model Training",
            "desc": "This code snippet initializes a Support Vector Classifier with a linear kernel using scikit-learn, trains it on the training data, makes predictions on the test data, and calculates the accuracy of the predictions.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.64344877
            },
            "cluster": -1
        }, {
            "cell_id": 27,
            "code": "from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(train_x, train_y)\n\ngnb_pred = gnb.predict(test_x)\n\ngnb_acc = accuracy_score(gnb_pred, test_y)",
            "class": "Model Training",
            "desc": "This code snippet initializes a Gaussian Naive Bayes classifier using scikit-learn, trains it on the training data, makes predictions on the test data, and calculates the accuracy of the predictions.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.87931037
            },
            "cluster": -1
        }, {
            "cell_id": 7,
            "code": "msno.matrix(train_df)",
            "class": "Visualization",
            "desc": "This code snippet generates a visualization of the distribution and pattern of missing values in the training DataFrame using the `matrix` function from the missingno library.",
            "testing": {
                "class": "Visualization",
                "subclass": "missing_values",
                "subclass_id": 34,
                "predicted_subclass_probability": 0.9554358
            },
            "cluster": -1
        }, {
            "cell_id": 8,
            "code": "corr = train_df.corr()\nplt.figure(figsize=(15,8))\nsns.heatmap(corr, annot=True)",
            "class": "Visualization",
            "desc": "This code snippet calculates the correlation matrix for the training DataFrame and visualizes it as a heatmap with annotations using Seaborn.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.99775463
            },
            "cluster": 1
        }, {
            "cell_id": 9,
            "code": "for col in ['Age', 'SibSp', 'Parch', 'Fare']:\n    plt.title(col)\n    sns.boxplot(train_df[col])\n    plt.show()",
            "class": "Visualization",
            "desc": "This code snippet generates and displays boxplots for the 'Age', 'SibSp', 'Parch', and 'Fare' columns in the training DataFrame using Seaborn, with each plot being titled by its respective column name.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9927158
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "fig, axes = plt.subplots(3,3, figsize=(20,15))\n\nsns.countplot(ax=axes[0,0], data=train_df, x='Survived')\nsns.countplot(ax=axes[0,1], data=train_df, x='Sex')\nsns.countplot(ax=axes[0,2], data=train_df, x='Pclass')\nsns.histplot(ax=axes[1,0], data=train_df, x='Age')\nsns.histplot(ax=axes[1,1], data=train_df, x='Fare')\nsns.countplot(ax=axes[1,2], data=train_df, x='SibSp')\nsns.countplot(ax=axes[2,0], data=train_df, x='Parch')\nsns.countplot(ax=axes[2,1], data=train_df, x='Embarked')\n\nplt.tight_layout()\nplt.show()",
            "class": "Visualization",
            "desc": "This code snippet creates a 3x3 grid of subplots and fills it with count plots for categorical variables and histogram plots for continuous variables in the training DataFrame using Seaborn.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9985378
            },
            "cluster": 1
        }, {
            "cell_id": 11,
            "code": "sns.relplot(data=train_df, x='Age', y='Fare', hue='Sex', col='Survived') ",
            "class": "Visualization",
            "desc": "This code snippet creates a relational plot of 'Age' versus 'Fare' with points colored by 'Sex' and faceted by 'Survived' using Seaborn.",
            "testing": {
                "class": "Visualization",
                "subclass": "time_series",
                "subclass_id": 75,
                "predicted_subclass_probability": 0.96493864
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(17,6))\nfig.suptitle('Survival Rates Between Males and Females\\n', fontsize=20)\n\nsns.countplot(ax=ax1, data=train_df, x='Sex', hue='Survived', edgecolor='black')\nax1.set_xticklabels(['Male','Female'])\nax1.legend(labels=['False', 'True'], title='Survived\\n')\nax1.set_xlabel('')\nax1.set_ylabel('Count')\n\nplt.pie(count, labels=['Male', 'Female'], explode=[0, 0.2], shadow=True, autopct='%1.1f%%', wedgeprops={'edgecolor':'black'})\n\nplt.tight_layout()",
            "class": "Visualization",
            "desc": "This code snippet creates a figure with two subplots: one displaying a count plot of survival counts differentiated by gender using Seaborn, and another displaying a pie chart of the survival percentages for males and females using Matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99289554
            },
            "cluster": -1
        }, {
            "cell_id": 15,
            "code": "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(17,6))\nfig.suptitle('Survival Rates in Different Ship Classes\\n', fontsize=20)\n\nsns.countplot(ax=ax1, data=train_df, x='Pclass', hue='Survived', edgecolor='black')\nax1.set_xticklabels(['First','Second', 'Third'])\nax1.legend(labels=['False', 'True'], title='Survived\\n')\nax1.set_xlabel('')\nax1.set_ylabel('Count')\n\nplt.pie(pcts, labels=['Third', 'First', 'Second'], explode=[0, 0., 0.2], shadow=True, autopct='%1.1f%%', startangle=90, wedgeprops={'edgecolor':'black'})\n\nplt.tight_layout()",
            "class": "Visualization",
            "desc": "This code snippet creates a figure with two subplots: one displaying a count plot of survival counts differentiated by passenger class using Seaborn, and another displaying a pie chart of the survival percentages for each ship class using Matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.98514646
            },
            "cluster": -1
        }, {
            "cell_id": 16,
            "code": "plt.figure(figsize=(15,7))\nsns.histplot(data=train_df, x='Age', hue='Survived', multiple='stack')\nplt.show()",
            "class": "Visualization",
            "desc": "This code snippet generates a stacked histogram of the 'Age' column differentiated by survival status using Seaborn and displays it with a specified figure size using Matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99855167
            },
            "cluster": 1
        }],
        "notebook_id": 1,
        "notebook_name": "titanic-prediction.ipynb",
        "user": "titanic-prediction.ipynb"
    }, {
        "cells": [{
            "cell_id": 41,
            "code": "#take backup for later use\n\ntrain_copy = combined[:train_len]\n\ntest_copy = combined[train_len:].reset_index(drop=True)\n\ntest_copy.drop(columns=['Survived'],inplace=True)",
            "class": "Data Export",
            "desc": "The code creates backup copies of the combined DataFrame, splitting it back into the training (`train_copy`) and test (`test_copy`) sets, and drops the `Survived` column from the test set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.69678193
            },
            "cluster": -1
        }, {
            "cell_id": 46,
            "code": "train = combined[:train_len]\n\ntest = combined[train_len:]\n\ntest.drop(columns=['Survived'],inplace=True)",
            "class": "Data Export",
            "desc": "The code splits the transformed combined DataFrame back into the training (`train`) and test (`test`) sets, dropping the `Survived` column from the test set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.993755
            },
            "cluster": -1
        }, {
            "cell_id": 52,
            "code": "y_test_rfc = rfc.predict(test).astype(int)\n\ntest_out = pd.concat([test_copy['PassengerId'],pd.Series(y_test_rfc,name=\"Survived\")],axis=1)\n\ntest_out['Survived'] = test_out['Survived'].astype('int')\n\ntest_out.to_csv('submission.csv',index=False)",
            "class": "Data Export",
            "desc": "The code predicts survival on the test dataset using the trained Random Forest Classifier, creates a DataFrame with `PassengerId` and the predictions, and saves this DataFrame as a CSV file named 'submission.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9994411
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "train_df=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")",
            "class": "Data Extraction",
            "desc": "The code reads the Titanic dataset files (`train.csv` and `test.csv`) from the specified directory into Pandas DataFrames named `train_df` and `test_df`.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99974495
            },
            "cluster": -1
        }, {
            "cell_id": 4,
            "code": "train_len = len(train_df)\n\ncombined = train_df.append(test_df,ignore_index=True)\n\ncombined.fillna(np.nan)",
            "class": "Data Transform",
            "desc": "The code calculates the length of the training DataFrame, appends the test DataFrame to the training DataFrame to create a combined DataFrame, and fills any missing values with NaN using NumPy.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.99917126
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "combined['AgeGroup'] = 'adult'\n\ncombined.loc[combined['Name'].str.contains('Master'),'AgeGroup'] = \"child\"\n\ncombined.loc[combined['Age'] <= 14.0,'AgeGroup'] = \"child\"\n\ncombined.loc[(combined['Age'].isnull()) & (combined['Name'].str.contains('Miss')) & (combined['Parch'] != 0) ,'AgeGroup'] = \"child\"",
            "class": "Data Transform",
            "desc": "The code assigns the value 'adult' to a new column `AgeGroup` for all entries in the combined DataFrame, then updates this column to 'child' for entries based on specific conditions related to `Name`, `Age`, and `Parch` columns.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9986811
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "def Age(cols):\n\n    Age=cols[0]\n\n    Pclass=cols[1]\n\n    Sex=cols[2]\n\n    AgeGroup=cols[3]\n\n    if pd.isnull(Age):\n\n        if Pclass==1:\n\n            if Sex==\"male\":\n\n                if AgeGroup=='adult':\n\n                    return 42\n\n                else:\n\n                    return 7\n\n            elif Sex==\"female\":\n\n                if AgeGroup=='adult':\n\n                    return 37\n\n                else:\n\n                    return 8\n\n        elif Pclass==2:\n\n            if Sex==\"male\":\n\n                if AgeGroup=='adult':\n\n                    return 33\n\n                else:\n\n                    return 4\n\n            elif Sex==\"female\":\n\n                if AgeGroup=='adult':\n\n                    return 31\n\n                else:\n\n                    return 7\n\n        elif Pclass==3:\n\n            if Sex==\"male\":\n\n                if AgeGroup=='adult':\n\n                    return 29\n\n                else:\n\n                    return 7\n\n            elif Sex==\"female\":\n\n                if AgeGroup=='adult':\n\n                    return 27\n\n                else:\n\n                    return 5\n\n    else:\n\n        return Age\n\n    \n\ncombined[\"Age\"]=combined[[\"Age\",\"Pclass\",\"Sex\",\"AgeGroup\"]].apply(Age,axis=1)",
            "class": "Data Transform",
            "desc": "The code defines a function `Age` to impute missing age values based on the passenger's class (`Pclass`), sex (`Sex`), and `AgeGroup`, and applies this function to the `Age` column of the combined DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.980082
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "def AgeBand(col):\n\n    Age=col[0]\n\n    if Age <=7:\n\n        return \"0-7\"\n\n    elif Age <=14:\n\n        return \"8-14\"\n\n    elif Age <=21:\n\n        return \"15-21\"\n\n    elif Age <= 28:\n\n        return \"22-28\"\n\n    elif Age <= 35:\n\n        return \"29-35\"\n\n    elif Age <= 42:\n\n        return \"36-42\"\n\n    elif Age <= 49:\n\n        return \"43-49\"\n\n    elif Age <= 56:\n\n        return \"50-56\"\n\n    elif Age <= 63:\n\n        return \"57-63\"\n\n    else:\n\n        return \">=64\"\n\n\n\ncombined[\"AgeBand\"]=combined[[\"Age\"]].apply(AgeBand,axis=1)",
            "class": "Data Transform",
            "desc": "The code defines a function `AgeBand` to categorize age values into specific age bands and applies this function to create a new column `AgeBand` in the combined DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.98803115
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "combined[combined['Embarked'].isnull()]['Embarked'] = combined['Embarked'].mode()",
            "class": "Data Transform",
            "desc": "The code snippet fills in the missing values in the `Embarked` column of the combined DataFrame with the mode of the `Embarked` values.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99778533
            },
            "cluster": 0
        }, {
            "cell_id": 25,
            "code": "ticketCount = combined.groupby('Ticket')['PassengerId'].count().reset_index()\n\nticketCount.rename(columns={'PassengerId':'Count on Ticket'},inplace=True)\n\ncombined = combined.merge(ticketCount, on=\"Ticket\",how=\"left\")",
            "class": "Data Transform",
            "desc": "The code calculates the count of passengers sharing the same ticket in the combined DataFrame, renames the count column to 'Count on Ticket', and merges this information back into the combined DataFrame based on the `Ticket` column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "merge",
                "subclass_id": 32,
                "predicted_subclass_probability": 0.9985952
            },
            "cluster": 0
        }, {
            "cell_id": 26,
            "code": "combined['Diff'] = combined['FamilySize'] - combined['Count on Ticket']\n\ncombined['Family Status'] = combined.apply(lambda x:\"Has Family On Same Ticket\" if (x['FamilySize'] - x['Count on Ticket']) <= 0 else \"Family Not on same ticket\",axis=1)\n",
            "class": "Data Transform",
            "desc": "The code creates a new column `Diff` by subtracting the `Count on Ticket` from `FamilySize`, and then categorizes passengers into 'Has Family On Same Ticket' or 'Family Not on same ticket' based on whether they have family members on the same ticket or not, storing this information in a new column `Family Status`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99793893
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "combined['Family Status'] = combined.apply(lambda x:\"Is Alone\" if (x['FamilySize']==1) & (x['Count on Ticket']==1)  else x['Family Status'],axis=1)",
            "class": "Data Transform",
            "desc": "The code updates the `Family Status` column to classify passengers as 'Is Alone' if they have a `FamilySize` of 1 and are the only passenger with their `Ticket`, leaving other entries in `Family Status` unchanged.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.996369
            },
            "cluster": 0
        }, {
            "cell_id": 29,
            "code": "combined['Cabin Class'] = 'No Cabin'\n\n\n\ncombined['Cabin Class'] = combined.apply(lambda x: \"No Cabin\" if pd.isna(x[\"Cabin\"]) else x[\"Cabin\"][0] , axis=1)\n\n\n",
            "class": "Data Transform",
            "desc": "The code initializes the `Cabin Class` column to 'No Cabin' for all entries and then updates it to the first letter of the `Cabin` value for entries with non-null cabin information in the combined DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9755805
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "\n\ntickcab = combined[combined['Cabin Class'] != 'No Cabin'][['Ticket','Cabin Class']].drop_duplicates()\n\n\n\ntickcab = tickcab.rename(columns={'Cabin Class':'CabNam'})\n\ncombined = combined.merge(tickcab,how=\"left\",on=\"Ticket\")\n\n\n\ncombined['CabNam'].fillna('No Cabin')\n\n\n\ncombined['Cabin Class'] = combined.apply(lambda x:x['Cabin Class'] if x['Cabin Class'] != 'No Cabin' else x['CabNam'],axis=1)\n\ncombined.drop(columns=['CabNam'],inplace=True)\n\ncombined.drop_duplicates(inplace=True)\n",
            "class": "Data Transform",
            "desc": "The code creates a new DataFrame `tickcab` to capture unique ticket and cabin class combinations, merges this with the combined DataFrame, and then fills in missing `Cabin Class` values using this merged information before cleaning up temporary columns and duplicate entries.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "merge",
                "subclass_id": 32,
                "predicted_subclass_probability": 0.6754502
            },
            "cluster": 0
        }, {
            "cell_id": 33,
            "code": "combined[\"Fare\"] = combined[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\ncombined[\"Fare\"] = combined[\"Fare\"]/combined['Count on Ticket']",
            "class": "Data Transform",
            "desc": "The code applies a logarithmic transformation to the `Fare` values (setting values to 0 if they are non-positive), and then normalizes the `Fare` by dividing it by the count of passengers sharing the same ticket.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99918026
            },
            "cluster": 1
        }, {
            "cell_id": 36,
            "code": "companion = pd.pivot_table(combined, values='PassengerId',index=['Ticket'],columns=['AgeGroup'], aggfunc=\"count\").reset_index().fillna(0)\n\ncompanion.columns = ['Ticket','No. of Adult Companion', 'No. of Child Companion']\n\ncombined = combined.merge(companion, on='Ticket',how='left')",
            "class": "Data Transform",
            "desc": "The code creates a pivot table to count the number of adult and child companions per ticket in the combined DataFrame, renames the columns accordingly, and merges this information back into the combined DataFrame based on the `Ticket` column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "merge",
                "subclass_id": 32,
                "predicted_subclass_probability": 0.99741
            },
            "cluster": 0
        }, {
            "cell_id": 37,
            "code": "combined.loc[combined['AgeGroup']=='adult','No. of Adult Companion'] = combined.loc[combined['AgeGroup']=='adult','No. of Adult Companion'] - 1\n\ncombined.loc[combined['AgeGroup']=='child','No. of Child Companion'] = combined.loc[combined['AgeGroup']=='child','No. of Child Companion'] - 1\n\n\n\ncombined['Companion'] = 'Adult & Child Companion'\n\ncombined['Companion'] = combined.apply(lambda x:'Only Adult Companion' if (x['No. of Adult Companion'] > 0) & (x['No. of Child Companion']==0) else x['Companion'],axis=1)\n\ncombined['Companion'] = combined.apply(lambda x:'Only Child Companion' if (x['No. of Adult Companion'] == 0) & (x['No. of Child Companion']>0) else x['Companion'],axis=1)\n\ncombined['Companion'] = combined.apply(lambda x:'No Companion' if (x['No. of Adult Companion'] == 0) & (x['No. of Child Companion']==0) else x['Companion'],axis=1)",
            "class": "Data Transform",
            "desc": "The code adjusts the counts of adult and child companions by subtracting one if the current passenger is of that age group and then categorizes each passenger into one of four companion statuses (`'Adult & Child Companion'`, `'Only Adult Companion'`, `'Only Child Companion'`, or `'No Companion'`) based on the counts of companions.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.998919
            },
            "cluster": 0
        }, {
            "cell_id": 42,
            "code": "combined.drop(columns=['PassengerId','Name','Age', 'AgeGroup','SibSp','Parch','Ticket','Cabin','Count on Ticket','Diff','No. of Adult Companion','No. of Child Companion'],inplace=True)",
            "class": "Data Transform",
            "desc": "The code drops multiple columns such as `PassengerId`, `Name`, `Age`, `AgeGroup`, `SibSp`, `Parch`, `Ticket`, `Cabin`, `Count on Ticket`, `Diff`, `No. of Adult Companion`, and `No. of Child Companion` from the combined DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9992192
            },
            "cluster": 0
        }, {
            "cell_id": 45,
            "code": "combined = pd.get_dummies(combined, columns = [\"Sex\",\"Embarked\",\"AgeBand\",\"Family Status\",\"Cabin Class\",\"Companion\"],drop_first=True)",
            "class": "Data Transform",
            "desc": "The code converts categorical columns (`Sex`, `Embarked`, `AgeBand`, `Family Status`, `Cabin Class`, and `Companion`) into dummy/indicator variables using one-hot encoding and drops the first category to avoid multicollinearity.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99923813
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "combined.isnull().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet computes and returns the total number of missing values for each column in the combined DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99895346
            },
            "cluster": 7
        }, {
            "cell_id": 6,
            "code": "train_df.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "The code generates summary statistics of the training DataFrame using the `describe()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994492
            },
            "cluster": 1
        }, {
            "cell_id": 10,
            "code": "combined.groupby(['Pclass','Sex'])['Age'].mean()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates the mean age of passengers grouped by their class (`Pclass`) and sex (`Sex`) using the combined DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.99413943
            },
            "cluster": -1
        }, {
            "cell_id": 14,
            "code": "combined[combined['Age'].notnull()].groupby(['Pclass','Sex','AgeGroup'])['Age'].mean()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates the mean age of passengers with non-null ages, grouped by their class (`Pclass`), sex (`Sex`), and the newly created `AgeGroup` column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.9977055
            },
            "cluster": -1
        }, {
            "cell_id": 19,
            "code": "combined.groupby(['Pclass','Embarked'])['PassengerId'].count()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and returns the count of passengers for each combination of passenger class (`Pclass`) and embarkation point (`Embarked`) by grouping the combined DataFrame and counting the occurrences of `PassengerId`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.99464333
            },
            "cluster": -1
        }, {
            "cell_id": 35,
            "code": "combined.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first few rows of the combined DataFrame using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997458
            },
            "cluster": 2
        }, {
            "cell_id": 38,
            "code": "combined[combined[\"Survived\"].notnull()].groupby(['AgeGroup','Companion'])['PassengerId'].count()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and returns the count of passengers by grouping them based on `AgeGroup` and `Companion` status for entries with non-null survival values using the combined DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.9981712
            },
            "cluster": 4
        }, {
            "cell_id": 39,
            "code": "combined[combined[\"Survived\"].notnull()].groupby(['AgeGroup','Companion'])['Survived'].mean()",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and returns the mean survival rate of passengers by grouping them based on `AgeGroup` and `Companion` status for entries with non-null survival values using the combined DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.9978156
            },
            "cluster": 4
        }, {
            "cell_id": 40,
            "code": "combined.columns",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet lists all the column names of the combined DataFrame using the `columns` attribute in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.99846387
            },
            "cluster": 7
        }, {
            "cell_id": 43,
            "code": "combined.columns",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet lists all the remaining column names of the combined DataFrame using the `columns` attribute in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.99846387
            },
            "cluster": 7
        }, {
            "cell_id": 44,
            "code": "combined.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first few rows of the modified combined DataFrame using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997458
            },
            "cluster": 2
        }, {
            "cell_id": 48,
            "code": "X_train.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first few rows of the `X_train` dataset using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99975497
            },
            "cluster": 2
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# For example, here's several helpful packages to load\n\n\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "The code imports Python libraries such as NumPy and Pandas for data processing and iterates through the input directory to list all files using the `os` module.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.99921954
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "import pandas as pd\n\nimport numpy as np\n\nimport seaborn as sns\n\nimport time\n\nfrom collections import Counter\n\nfrom matplotlib import pyplot as plt\n\nfrom warnings import filterwarnings\n\nfrom sklearn import model_selection\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom scipy.stats import norm\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import cross_val_score,KFold\n\nimport plotly.express as px\n\nfrom plotly.offline import plot, iplot, init_notebook_mode\n\ninit_notebook_mode(connected=True)\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn import feature_selection",
            "class": "Imports and Environment",
            "desc": "The code snippet imports a variety of libraries for data manipulation (Pandas, NumPy), visualization (Seaborn, Matplotlib, Plotly), machine learning (scikit-learn, XGBoost), and statistical operations (SciPy), and initializes Plotly for offline mode in a Jupyter notebook.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9984848
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "import warnings\n\nwarnings.filterwarnings('ignore')",
            "class": "Imports and Environment",
            "desc": "The code snippet disables warnings by using the `warnings` library to filter out all warnings during execution.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.999143
            },
            "cluster": -1
        }, {
            "cell_id": 50,
            "code": "report=classification_report(y_test,pred)\n\nprint(\"Decision Tree report \\n\",report)",
            "class": "Model Evaluation",
            "desc": "The code generates and prints a classification report, including precision, recall, f1-score, and support, for the Decision Tree Classifier's predictions on the test dataset.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9974105
            },
            "cluster": 0
        }, {
            "cell_id": 47,
            "code": "X = train.iloc[:,1:]\n\ny = train.iloc[:,0]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)",
            "class": "Model Training",
            "desc": "The code separates the training set into features (`X`) and target (`y`), and then splits these into training and testing subsets using an 80-20 train-test split, with a fixed random state for reproducibility.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.99681467
            },
            "cluster": 1
        }, {
            "cell_id": 49,
            "code": "#Dtree\n\ndecision_tree = DecisionTreeClassifier()\n\ndecision_tree.fit(X_train, y_train)\n\npred_train = decision_tree.predict(X_train)\n\npred=decision_tree.predict(X_test)\n\npred_train_df=pd.DataFrame({\"Actual\":y_train,\"Pred\":pred_train})\n\npred_df=pd.DataFrame({\"Actual\":y_test,\"Pred\":pred})\n\ncm=confusion_matrix(y_test,pred)\n\ncm",
            "class": "Model Training",
            "desc": "The code initializes, trains, and predicts using a Decision Tree Classifier on both the training and test datasets, creates DataFrames to compare actual and predicted values, and computes the confusion matrix for the test predictions.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.25523284
            },
            "cluster": -1
        }, {
            "cell_id": 51,
            "code": "#RFC\n\n\n\nrfc=ensemble.RandomForestClassifier(max_depth=6,random_state=0,n_estimators=64)\n\nrfc.fit(X_train, y_train)\n\npred_train = rfc.predict(X_train)\n\npred=rfc.predict(X_test)\n\npred_train_df=pd.DataFrame({\"Actual\":y_train,\"Pred\":pred_train})\n\npred_df=pd.DataFrame({\"Actual\":y_test,\"Pred\":pred})\n\n\n\ncm=confusion_matrix(y_test,pred)\n\nprint(cm)\n\n\n\nreport=classification_report(y_test,pred)\n\nprint(\"Random Forest report \\n\",report)",
            "class": "Model Training",
            "desc": "The code initializes, trains, and predicts using a Random Forest Classifier with specific hyperparameters (`max_depth`, `random_state`, `n_estimators`) on both the training and test datasets, creates DataFrames to compare actual and predicted values, computes the confusion matrix, and prints the classification report for the test predictions.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.83314914
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "g = sns.barplot(x=\"Pclass\",y=\"Survived\",data=train_df)\n\ng.set_ylabel(\"Survival Probability\")",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Seaborn to show the survival probability based on the passenger class (`Pclass`) with the y-axis labeled as \"Survival Probability.\"",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9456676
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train_df)\n\ng.set_ylabel(\"Survival Probability\")",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Seaborn to show the survival probability based on the passenger's sex (`Sex`) with the y-axis labeled as \"Survival Probability.\"",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99116606
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "g = sns.barplot(x=\"Pclass\",y=\"Survived\",hue=\"Sex\",data=train_df)\n\ng.set_ylabel(\"Survival Probability\")",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Seaborn to show the survival probability based on the passenger class (`Pclass`) with an additional hue dimension for sex (`Sex`), and labels the y-axis as \"Survival Probability.\"",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.8091258
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "sns.distplot(train_df[\"Age\"])",
            "class": "Visualization",
            "desc": "The code creates a distribution plot of the `Age` variable from the training DataFrame using Seaborn's `distplot` function.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9981388
            },
            "cluster": -1
        }, {
            "cell_id": 12,
            "code": "g = sns.FacetGrid(train_df[train_df['Age'].notnull()], col='Sex',row='Pclass',hue=\"Survived\")\n\ng = g.map(sns.distplot, \"Age\",bins=10,hist_kws=dict(edgecolor=\"k\", linewidth=2),kde=False).add_legend()\n",
            "class": "Visualization",
            "desc": "The code creates a FacetGrid to display the distribution of the `Age` variable categorized by `Sex`, `Pclass`, and `Survived` status, and visualizes this using Seaborn's `distplot` function without the KDE plot.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9957695
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "g = sns.FacetGrid(combined[combined['Survived'].notnull()], col='Pclass')\n\ng = g.map(sns.barplot,\"AgeBand\",\"Survived\", order=[ \"0-7\",\"8-14\",\"15-21\",\"22-28\",\"29-35\",\"36-42\",\"43-49\",\"50-56\",\"57-63\",\">=64\"])\n\nfor axes in g.axes.flat:\n\n    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=70)\n\nplt.tight_layout()",
            "class": "Visualization",
            "desc": "The code creates a FacetGrid to display bar plots of survival probability based on the `AgeBand` column, grouped by passenger class (`Pclass`), and rotates the x-axis labels for better readability using Seaborn and Matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99827373
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "sns.barplot(\"Embarked\",\"Survived\",data=combined[combined['Survived'].notnull()])",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Seaborn to show the survival probability based on the embarkation point (`Embarked`) from the combined DataFrame, considering only the entries with non-null survival values.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9954508
            },
            "cluster": 0
        }, {
            "cell_id": 20,
            "code": "sns.barplot(\"Pclass\",\"Survived\",hue=\"Embarked\",data=combined[combined['Survived'].notnull()],ci=None)\n",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Seaborn to show the survival probability based on passenger class (`Pclass`), with an additional hue for embarkation point (`Embarked`), and sets the confidence interval (`ci`) to None for clarity, considering only the entries with non-null survival values.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9985398
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "sns.barplot(\"SibSp\",\"Survived\",data=train_df, ci = None)",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Seaborn to show the survival probability based on the number of siblings or spouses aboard (`SibSp`), using the training DataFrame and with the confidence interval (`ci`) set to None.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9982626
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "sns.barplot(\"Parch\",\"Survived\",data=train_df, ci = None)",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Seaborn to show the survival probability based on the number of parents or children aboard (`Parch`), using the training DataFrame and setting the confidence interval (`ci`) to None.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9982613
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "combined['FamilySize'] = combined['SibSp'] + combined['Parch'] + 1  # +1 is to include the passenger him/herself\n\nsns.barplot(\"FamilySize\",\"Survived\",data=combined[combined['Survived'].notnull()], ci = None)",
            "class": "Visualization",
            "desc": "The code creates a new column `FamilySize` in the combined DataFrame, calculated as the sum of siblings/spouses (`SibSp`) and parents/children (`Parch`) plus one, and then generates a bar plot using Seaborn to show the survival probability based on this `FamilySize` value, considering only the entries with non-null survival values.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99779296
            },
            "cluster": 0
        }, {
            "cell_id": 28,
            "code": "g = sns.FacetGrid(combined[combined['Survived'].notnull()], col='Pclass')\n\ng.map(sns.barplot,\"Family Status\",\"Survived\",ci=None)\n\nfor axes in g.axes.flat:\n\n    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=70)\n",
            "class": "Visualization",
            "desc": "The code creates a FacetGrid to display bar plots of the survival probability based on `Family Status`, grouped by passenger class (`Pclass`), and rotates the x-axis labels for better readability using Seaborn and Matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99821365
            },
            "cluster": 1
        }, {
            "cell_id": 31,
            "code": "sns.barplot(\"Cabin Class\",\"Survived\",data=combined[combined[\"Survived\"].notnull()],ci=None)",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Seaborn to show the survival probability based on the `Cabin Class` for entries with non-null survival values in the combined DataFrame, with the confidence interval (`ci`) set to None.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9981097
            },
            "cluster": 0
        }, {
            "cell_id": 32,
            "code": "g = sns.distplot(combined[\"Fare\"], color=\"m\",label=\"Skewness : %.2f\"%(combined[\"Fare\"].skew()))\n\ng = g.legend(loc=\"best\")",
            "class": "Visualization",
            "desc": "The code creates a distribution plot using Seaborn to visualize the `Fare` variable, and adds a legend indicating the skewness of the distribution.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99603015
            },
            "cluster": 0
        }, {
            "cell_id": 34,
            "code": "g = sns.distplot(combined[\"Fare\"], color=\"m\",label=\"Skewness : %.2f\"%(combined[\"Fare\"].skew()))\n\ng = g.legend(loc=\"best\")",
            "class": "Visualization",
            "desc": "The code creates a distribution plot using Seaborn to visualize the transformed `Fare` variable, and adds a legend indicating the skewness of the distribution.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99603015
            },
            "cluster": 0
        }],
        "notebook_id": 2,
        "notebook_name": "titanic-rfc-some-analysis-on-ticket.ipynb",
        "user": "titanic-rfc-some-analysis-on-ticket.ipynb"
    }, {
        "cells": [{
            "cell_id": 49,
            "code": "sample = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\nsubmission = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nsubmission['Survived'] = y_pred\nsubmission = submission[['PassengerId','Survived']]\ndisplay(submission)",
            "class": "Data Export",
            "desc": "This code reads a sample submission file and test data, creates a 'Survived' column in the test data using the predicted probabilities, and prepares a final submission DataFrame containing 'PassengerId' and 'Survived'.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99966645
            },
            "cluster": -1
        }, {
            "cell_id": 52,
            "code": "submission.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "This code saves the final submission DataFrame to a CSV file named 'submission.csv' without including the index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992442
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "# Save test data as dataframe and preview in order to get a sense of the what the data look like.\ntrain = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndisplay(train.head())",
            "class": "Data Extraction",
            "desc": "This code reads the training data from a CSV file into a Pandas DataFrame and displays the first few rows to preview the data.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99953103
            },
            "cluster": 0
        }, {
            "cell_id": 37,
            "code": "test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ndisplay(test.head())",
            "class": "Data Extraction",
            "desc": "This code reads the test data from a CSV file into a Pandas DataFrame and displays the first few rows to preview the data.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9995247
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "# It appears that all titles (i.e. 'Mr.', 'Mrs.',...) can be found between a space and a period, so we will create a set of unique titles by searching\n# for this pattern in every name\ntitles = set()\nfor i in train['Name']:\n    title = re.search('\\s([a-zA-Z]+)\\.', i)\n    titles.add(title.group(1))\nprint(titles)",
            "class": "Data Transform",
            "desc": "This code extracts unique titles (e.g., 'Mr.', 'Mrs.') from the 'Name' column in the training data by searching for patterns using regular expressions and adding each title to a set.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.97765005
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "# Create new columns with an indicator for each title and test if the difference in survival rates for that column is statistically significant\n# use a copy of the dataset so as to not modify the original data\ncopy = train.copy()\nfor title in titles:\n    copy[title] = np.where(copy['Name'].str.contains(title), 1, 0)\n    display(copy[['Survived',title]].groupby([title]).agg({'Survived':['count','sum','mean']}))\n    t, p = stats.ttest_ind(a = copy[copy[title] == 0]['Survived'], b = copy[copy[title] == 1]['Survived'])\n    display(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')",
            "class": "Data Transform",
            "desc": "This code creates new indicator columns for each title in the training data copy, calculates survival statistics for each title, and performs a t-test to assess the statistical significance of differences in survival rates between groups with and without each title.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.878474
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "# Many titles did not have sufficient frequency to use for predictions, but several may prove useful in our model\nfor title in titles.copy():\n    copy[title] = np.where(copy['Name'].str.contains(title), 1, 0)\n    t, p = stats.ttest_ind(a = copy[copy[title] == 0]['Survived'], b = copy[copy[title] == 1]['Survived'])\n    # After looking over the t-test results, we will keep all of the variables for which survival was significantly different amongs the two groups\n    if not p<0.05:\n        titles.remove(title)\ntitles = list(titles)\ndisplay(titles)\n# Add these new indicator columns to our actual dataset for these titles now that we have chosen them\nfor title in titles:\n    train[title] = np.where(copy['Name'].str.contains(title), 1, 0)",
            "class": "Data Transform",
            "desc": "This code filters out the titles that do not show a statistically significant difference in survival during t-tests and creates new indicator columns for the significant titles in the original training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99799144
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "# first we will try imputing the mean age into the null values\ntrain['Age_mean_imputed'] = np.where(train['Age'].isnull(),np.mean(train['Age']),train['Age'])\n# Now lets fit a bivariate regression model\nY, X = train['Survived'], train['Age_mean_imputed']\nage_model = sm.Logit(Y, X).fit()\ndisplay(age_model.summary().tables[1])",
            "class": "Data Transform",
            "desc": "This code imputes mean values into null 'Age' entries, creates a logistic regression model to predict 'Survived' based on the imputed 'Age' values using Statsmodels, and displays the model summary.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.35666126
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "# second method is to use a regression model to predict the missing ages\nsub_data = train[~train['Age'].isnull()].copy()\n# We will use the columns that intuitively seem like they could relate to age\nY, X = np.array(sub_data['Age']).reshape(-1, 1), np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']])\n# fit the model\nage_model = lm.LinearRegression().fit(X, Y)\n#predict on the missing values\nsub_data = train[train['Age'].isnull()].copy()\nage_predictions = age_model.predict(np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']]))\n#add the missing values back into the dataframe\ntrain['Age_lm'] = train['Age']\nage_predictions_indexed = pd.DataFrame(age_predictions,index = train[train['Age_lm'].isnull()].index)[0]\ntrain['Age_lm'].fillna(age_predictions_indexed, inplace = True)\n\n# Now we will see how this predictor does at predicting Survival\nY, X = train['Survived'], train['Age_lm']\nage_model = sm.Logit(Y, X).fit()\ndisplay(age_model.summary().tables[1])",
            "class": "Data Transform",
            "desc": "This code fits a linear regression model using relevant features to predict missing 'Age' values, imputes these predicted ages into the training dataset, and evaluates the impact of this new 'Age' variable on survival using a logistic regression model in Statsmodels, displaying the model summary.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.62230337
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "# many values of SibSp have a very low frequency. Let's try simple indicator for any siblings or spouses\ntrain['SibSp_ind'] = np.where(train['SibSp'] > 0, 1, 0)\ndisplay(train[['Survived','SibSp_ind']].groupby(['SibSp_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = train[train['SibSp_ind'] == 1]['Survived'], b = train[train['SibSp_ind'] == 0]['Survived'])\ndisplay(f't-stat: {t}    p-value: {p}    p<0.05: {p<0.05}')",
            "class": "Data Transform",
            "desc": "This code creates a binary indicator column for the presence of siblings or spouses ('SibSp_ind'), calculates survival statistics based on this new column, and performs a t-test to assess the significance of differences in survival rates between groups with and without siblings or spouses.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.92328453
            },
            "cluster": 1
        }, {
            "cell_id": 15,
            "code": "# Let's consider an indicator for Parch as well due to low frequency of high values\ntrain['Parch_ind'] = np.where(train['Parch'] > 0, 1, 0)\ndisplay(train[['Survived','Parch_ind']].groupby(['Parch_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = train[train['Parch_ind'] == 1]['Survived'], b = train[train['Parch_ind'] == 0]['Survived'])\ndisplay(f't-stat: {t}    p-value: {p}    p<0.05: {p<0.05}')",
            "class": "Data Transform",
            "desc": "This code creates a binary indicator column for the presence of parents or children ('Parch_ind') in the dataset, calculates survival statistics based on this column, and performs a t-test to evaluate the significance of differences in survival rates between groups with and without parents or children.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.953208
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "# Create new columns with an indicator for each prefix and test if the difference in survival rates for that column is statistically significant\n# use a copy of the dataset so as to not modify the original data\ncopy = train.copy()\nfor pref in prefixes:\n    # Make sure to check for a space at the end of the prefix so we don't accidentally capture substrings\n    copy[pref] = np.where(copy['Ticket'].str.contains(f'{pref} '), 1, 0)\n    display(copy[['Survived',pref]].groupby([pref]).agg({'Survived':['count','sum','mean']}))\n    t, p = stats.ttest_ind(a = copy[copy[pref] == 0]['Survived'], b = copy[copy[pref] == 1]['Survived'])\n    display(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')",
            "class": "Data Transform",
            "desc": "This code creates new indicator columns for each ticket prefix in the training data copy, calculates survival statistics for each prefix, and performs a t-test to evaluate the statistical significance of differences in survival rates between groups with and without each prefix.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.91176885
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "# There was a very low frequency of most prefixes. Let's compare prefixes to no prefix\ncopy = train.copy()\n# check if the ticket starts with any letter\ncopy['Ticket_prefix_ind'] = np.where(copy['Ticket'].str.startswith(tuple(prefixes)), 1, 0)\ndisplay(copy[['Survived','Ticket_prefix_ind']].groupby(['Ticket_prefix_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = copy[copy['Ticket_prefix_ind'] == 0]['Survived'], b = copy[copy['Ticket_prefix_ind'] == 1]['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')",
            "class": "Data Transform",
            "desc": "This code creates a binary indicator column 'Ticket_prefix_ind' to mark whether the ticket starts with any of the identified prefixes, calculates survival statistics based on this indicator, and performs a t-test to assess the significance of differences in survival rates between groups with and without ticket prefixes.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.98258317
            },
            "cluster": 1
        }, {
            "cell_id": 20,
            "code": "# as a whole, prefixes do not appear to be predictive. Let's just keep the two that were frequent enough and predictive on their own:\ncopy = train.copy()\n# check if the ticket starts with any letter\ncopy['Ticket_prefix_ind'] = np.where(copy['Ticket'].str.startswith(('C ','PC ')), 1, 0)\ndisplay(copy[['Survived','Ticket_prefix_ind']].groupby(['Ticket_prefix_ind']).agg({'Survived':['count','sum','mean']}))\nt, p = stats.ttest_ind(a = copy[copy['Ticket_prefix_ind'] == 0]['Survived'], b = copy[copy['Ticket_prefix_ind'] == 1]['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')",
            "class": "Data Transform",
            "desc": "This code creates a binary indicator column 'Ticket_prefix_ind' to mark tickets starting with the frequent and predictive prefixes 'C ' and 'PC ', calculates survival statistics based on this indicator, and performs a t-test to assess the significance of differences in survival rates between groups with and without these ticket prefixes.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.78813297
            },
            "cluster": 1
        }, {
            "cell_id": 21,
            "code": "# since these two prefixes are frequent enough and have sufficiently different survival rates, we'll keep them in for consideration\ntrain['Ticket_PC_C'] = np.where(train['Ticket'].str.startswith(('C ','PC ')), 1, 0)",
            "class": "Data Transform",
            "desc": "This code creates a new binary indicator column 'Ticket_PC_C' in the training dataset to mark tickets that start with the prefixes 'C ' and 'PC ', which have shown significant differences in survival rates.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99881774
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "train['Ticket_len5'] = train.Ticket.str.extract('(^\\d*)')\ntrain['Ticket_len5'] = np.where(train['Ticket_len5'].str.len() == 5, 1, 0)",
            "class": "Data Transform",
            "desc": "This code creates a new binary indicator column 'Ticket_len5' in the training dataset to mark tickets that have a numerical part of exactly 5 digits in length.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9989471
            },
            "cluster": 0
        }, {
            "cell_id": 25,
            "code": "# since the data are highly skewed with some outliers, let's bin the values into buckets using a decision tree and use that instead of the original column\nDT = tree.DecisionTreeClassifier(min_samples_leaf = 0.08, max_depth = 3)\nFare_tree = DT.fit(train['Fare'].to_frame(), train['Survived'])\ntrain['Fare_binned'] = Fare_tree.predict_proba(train['Fare'].to_frame())[:,1]\ndisplay(train[['Survived','Fare_binned','Fare']].groupby(['Fare_binned']).agg({'Survived':['count','sum','mean'],'Fare':['min','max','mean']}))",
            "class": "Data Transform",
            "desc": "This code bins the highly skewed 'Fare' values into buckets using a decision tree classifier method and creates a new 'Fare_binned' column to use instead of the original 'Fare' column, then calculates survival statistics for the binned fare values.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.41344222
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "# Let's merge all cabins that start with the same letter\ncopy = train.copy()\ncopy['Cabin_group'] = copy['Cabin'].str[:1]\ncopy['Cabin_group'].fillna('None', inplace = True)\ndisplay(copy[['Survived','Cabin_group']].groupby(['Cabin_group']).agg({'Survived':['count','sum','mean']}))",
            "class": "Data Transform",
            "desc": "This code creates a new column 'Cabin_group' in the training dataset by extracting the first letter of each 'Cabin' entry or filling it with 'None', and calculates survival statistics grouped by these cabin groups.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.3810074
            },
            "cluster": 0
        }, {
            "cell_id": 28,
            "code": "# Since there is still a low frequency in many groups and it's unclear how groups might be meaningfully merged, let's mean_encode this column\ntrain['Cabin_group'] = train['Cabin'].str[:1]\ntrain['Cabin_group'].fillna('None', inplace = True)\nmeans = train[['Survived','Cabin_group']].groupby(['Cabin_group']).mean()\nmeans.rename({'Survived':'Cabin_ME'}, axis = 1, inplace = True)\ntrain = train.join(means, how = 'left', on = ['Cabin_group'])",
            "class": "Data Transform",
            "desc": "This code mean-encodes the 'Cabin_group' by creating a new column 'Cabin_ME' that reflects the average survival rate for each 'Cabin_group' and joins this encoded column back into the training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "merge",
                "subclass_id": 32,
                "predicted_subclass_probability": 0.99588
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "# since there are only two null values for embarked, we will encode them with the mode\ntrain['Embarked'].fillna('S', inplace = True)\nprint(train['Embarked'].isnull().sum())\ndisplay(train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':['count','sum','mean']}))",
            "class": "Data Transform",
            "desc": "This code fills the two null values in the 'Embarked' column with the mode 'S' and verifies the imputation by checking for remaining null values and displaying survival statistics grouped by 'Embarked'.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.4329066
            },
            "cluster": 0
        }, {
            "cell_id": 31,
            "code": "# let's mean encode this column as well so that we can have all numerical columns\nmeans = train[['Survived','Embarked']].groupby(['Embarked']).mean()\nmeans.rename({'Survived':'Embarked_ME'}, axis = 1, inplace = True)\ntrain = train.join(means, how = 'left', on = ['Embarked'])",
            "class": "Data Transform",
            "desc": "This code mean-encodes the 'Embarked' column by creating a new column 'Embarked_ME' that reflects the average survival rate for each category of 'Embarked', and joins this encoded column back into the training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "merge",
                "subclass_id": 32,
                "predicted_subclass_probability": 0.9932888
            },
            "cluster": 0
        }, {
            "cell_id": 32,
            "code": "# create our final data set with just the columns we'll be keeping\ndata = train[['Survived','Pclass','Mrs', 'Mr', 'Master', 'Miss','Sex','Age_lm','SibSp_ind','Parch_ind','Ticket_PC_C','Fare_binned','Cabin_ME','Embarked_ME','Ticket_len5']].copy()\n# clean up the names\ndata.rename({'Age_lm':'Age','SibSp_ind':'SibSp','Parch_ind':'Parch','Ticket_PC_C':'Ticket','Fare_binned':'Fare','Cabin_ME':'Cabin','Embarked_ME':'Embarked'}, axis = 1, inplace = True)\ndata['Female'] = np.where(data['Sex'] == 'female', 1, 0)\ndata.drop('Sex', axis = 1, inplace = True)\ndisplay(data.head())\ndisplay(data.columns)",
            "class": "Data Transform",
            "desc": "This code creates a final dataset containing the selected columns with meaningful names, adds a binary indicator for female gender, and removes the original 'Sex' column, then displays the first few rows and column names of this final dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.85816985
            },
            "cluster": 0
        }, {
            "cell_id": 38,
            "code": "# Update names column\nfor title in ['Mrs', 'Mr', 'Miss', 'Master']:\n    test[title] = np.where(test['Name'].str.contains(title), 1, 0)\nlen(test[test['Name'].isnull()])",
            "class": "Data Transform",
            "desc": "This code updates the test dataset by creating new indicator columns for titles 'Mrs', 'Mr', 'Miss', and 'Master' based on the presence of these substrings in the 'Name' column, and checks for null values in the 'Name' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9953231
            },
            "cluster": 0
        }, {
            "cell_id": 39,
            "code": "#Update Sex columns\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\nlen(test[test['Sex'].isnull()])",
            "class": "Data Transform",
            "desc": "This code updates the test dataset by creating a new binary indicator column 'Female' based on the 'Sex' column and checks for null values in the 'Sex' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9913565
            },
            "cluster": 0
        }, {
            "cell_id": 40,
            "code": "#Update SibSp\ntest['SibSp_ind'] = np.where(test['SibSp'] > 0, 1, 0)\nlen(test[test['SibSp'].isnull()])\n# display(test.head())",
            "class": "Data Transform",
            "desc": "This code updates the test dataset by creating a new binary indicator column 'SibSp_ind' to mark whether there are any siblings or spouses, and checks for null values in the 'SibSp' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99579054
            },
            "cluster": 0
        }, {
            "cell_id": 41,
            "code": "#Update Parch\ntest['Parch_ind'] = np.where(test['Parch'] > 0, 1, 0)\nlen(test[test['Parch'].isnull()])\n# display(test.head())",
            "class": "Data Transform",
            "desc": "This code updates the test dataset by creating a new binary indicator column 'Parch_ind' to mark whether there are any parents or children, and checks for null values in the 'Parch' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99731416
            },
            "cluster": 0
        }, {
            "cell_id": 42,
            "code": "#Update ticket\ntest['Ticket_PC_C'] = np.where(test['Ticket'].str.startswith(('C ','PC ')), 1, 0)\nlen(test[test['Ticket_PC_C'].isnull()])\n\ntest['Ticket_len5'] = test.Ticket.str.extract('(^\\d*)')\ntest['Ticket_len5'] = np.where(test['Ticket_len5'].str.len() == 5, 1, 0)",
            "class": "Data Transform",
            "desc": "This code updates the test dataset by creating a new binary indicator column 'Ticket_PC_C' to mark tickets starting with 'C ' and 'PC ', checks for null values, and creates another binary indicator column 'Ticket_len5' for tickets with a numerical part of exactly 5 digits in length.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9972178
            },
            "cluster": 1
        }, {
            "cell_id": 43,
            "code": "#Update Fare\n#One null fare will be imputed with the median since outliers might skew the mean\ntest['Fare'].fillna(test['Fare'].median(), inplace = True)\n#Use the same decision tree we made before to impute the values\ntest['Fare_binned'] = Fare_tree.predict_proba(test['Fare'].to_frame())[:,1]\nlen(test[test['Fare'].isnull()])",
            "class": "Data Transform",
            "desc": "This code imputes the single null value in the 'Fare' column of the test dataset with the median fare, uses the previously trained decision tree model to create a 'Fare_binned' column, and checks for remaining null values in the 'Fare' column.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9609763
            },
            "cluster": 0
        }, {
            "cell_id": 44,
            "code": "#Update Cabin\ntest['Cabin_group'] = test['Cabin'].str[:1]\ntest['Cabin_group'].fillna('None', inplace = True)\n#Mean encode based on the original training data\nmeans = train[['Survived','Cabin_group']].groupby(['Cabin_group']).mean()\nmeans.rename({'Survived':'Cabin_ME'}, axis = 1, inplace = True)\ntest = test.join(means, how = 'left', on = ['Cabin_group'])",
            "class": "Data Transform",
            "desc": "This code creates a 'Cabin_group' column in the test dataset by extracting the first letter of each 'Cabin' entry or filling it with 'None', and mean-encodes this column based on the mean survival rates from the original training data, resulting in a new 'Cabin_ME' column being added to the test dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "merge",
                "subclass_id": 32,
                "predicted_subclass_probability": 0.9975425
            },
            "cluster": 0
        }, {
            "cell_id": 45,
            "code": "#Update Embarked\nlen(test[test['Embarked'].isnull()])\n# mean encode\nmeans = train[['Survived','Embarked']].groupby(['Embarked']).mean()\nmeans.rename({'Survived':'Embarked_ME'}, axis = 1, inplace = True)\ntest = test.join(means, how = 'left', on = ['Embarked'])",
            "class": "Data Transform",
            "desc": "This code checks for null values in the 'Embarked' column of the test dataset, and mean-encodes the 'Embarked' column based on mean survival rates from the original training data, resulting in a new 'Embarked_ME' column being added to the test dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "merge",
                "subclass_id": 32,
                "predicted_subclass_probability": 0.9983809
            },
            "cluster": 0
        }, {
            "cell_id": 46,
            "code": "#Update Age column\nsub_data = test[~test['Age'].isnull()].copy()\n# We will use the columns that intuitively seem like they could relate to age\nY, X = np.array(sub_data['Age']).reshape(-1, 1), np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']])\n# fit the model\nage_model = lm.LinearRegression().fit(X, Y)\n#predict on the missing values\nsub_data = test[test['Age'].isnull()].copy()\nage_predictions = age_model.predict(np.array(sub_data[['Fare','Mrs', 'Mr', 'Master', 'Miss','SibSp','Parch']]))\n#add the missing values back into the dataframe\ntest['Age_lm'] = test['Age']\nage_predictions_indexed = pd.DataFrame(age_predictions,index = test[test['Age_lm'].isnull()].index)[0]\ntest['Age_lm'].fillna(age_predictions_indexed, inplace = True)\ndisplay(test.head())",
            "class": "Data Transform",
            "desc": "This code fits a linear regression model using relevant features to predict missing 'Age' values in the test dataset, imputes these predicted ages into a new column 'Age_lm', and displays the first few rows of the updated test dataset.",
            "testing": {
                "class": "Data_Export",
                "subclass": "prepare_output",
                "subclass_id": 55,
                "predicted_subclass_probability": 0.436364
            },
            "cluster": 0
        }, {
            "cell_id": 47,
            "code": "# create our final data set with just the columns we'll be keeping\ntest = test[['Pclass','Mrs', 'Mr', 'Master', 'Miss','Sex','Age_lm','SibSp_ind','Parch_ind','Ticket_PC_C','Fare_binned','Cabin_ME','Embarked_ME','Ticket_len5']].copy()\n# clean up the names\ntest.rename({'Age_lm':'Age','SibSp_ind':'SibSp','Parch_ind':'Parch','Ticket_PC_C':'Ticket','Fare_binned':'Fare','Cabin_ME':'Cabin','Embarked_ME':'Embarked'}, axis = 1, inplace = True)\ntest['Female'] = np.where(test['Sex'] == 'female', 1, 0)\ntest.drop('Sex', axis = 1, inplace = True)\ndisplay(test.head())\ndisplay(test.columns)",
            "class": "Data Transform",
            "desc": "This code creates a final test dataset containing selected columns with meaningful names, adds a binary indicator for female gender, removes the original 'Sex' column, and displays the first few rows and column names of this final dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.96216625
            },
            "cluster": 0
        }, {
            "cell_id": 51,
            "code": "submission['Survived'] = np.where(submission['Survived'] < 0.573, 0, 1)\nprint(submission['Survived'].mean())\ndisplay(submission.head(20))",
            "class": "Data Transform",
            "desc": "This code updates the 'Survived' column in the submission DataFrame by applying a cutoff value of 0.573 to classify survival probabilities into binary outcomes, calculates the mean survival rate, and displays the first 20 rows of the updated submission DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.86286855
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "# Initial review shows that Pclass appears to be predictive\ndisplay(train[['Survived','Pclass']].groupby(['Pclass']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Pclass'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and displays the count, sum, and mean of the 'Survived' column grouped by 'Pclass' to check its predictive power, and computes the total and proportion of null values in the 'Pclass' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.9523748
            },
            "cluster": 4
        }, {
            "cell_id": 3,
            "code": "# The Names column contains too many unique values. We will investigate particular substrings.\npd.set_option(\"display.max_rows\", 20)\ndisplay(train['Name'])\nnulls = train['Name'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code sets the display option to show more rows of the 'Name' column, displays the 'Name' column of the training data, and calculates the number and proportion of null values in the 'Name' column to investigate particular substrings.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.6983381
            },
            "cluster": 4
        }, {
            "cell_id": 7,
            "code": "# Display unique values and how they survived\ndisplay(train[['Survived','Sex']].groupby(['Sex']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Sex'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')\n# Perform a t-test to check for statistically significant difference in survival rates\nt, p = stats.ttest_ind(a = train[train['Sex'] == 'female']['Survived'], b = train[train['Sex'] == 'male']['Survived'])\ndisplay(f't-stat: {t} \\n p-value: {p} \\t p<0.05: {p<0.05}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and displays survival statistics by 'Sex', checks for null values in the 'Sex' column, and performs a t-test to determine if the difference in survival rates between males and females is statistically significant.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "statistical_test",
                "subclass_id": 47,
                "predicted_subclass_probability": 0.93878275
            },
            "cluster": 4
        }, {
            "cell_id": 12,
            "code": "# Initial review shows that SibSp appears to be predictive\ndisplay(train[['Survived','SibSp']].groupby(['SibSp']).agg({'Survived':['count','sum','mean']}))\nnulls = train['SibSp'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and displays the count, sum, and mean of the 'Survived' column grouped by 'SibSp' to check its predictive power, and computes the total and proportion of null values in the 'SibSp' column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.88131183
            },
            "cluster": 4
        }, {
            "cell_id": 14,
            "code": "# Initial review shows that Parch appears to be predictive\ndisplay(train[['Survived','Parch']].groupby(['Parch']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Parch'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and displays the count, sum, and mean of the 'Survived' column grouped by 'Parch' to check its predictive power, and computes the total and proportion of null values in the 'Parch' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.9513562
            },
            "cluster": 4
        }, {
            "cell_id": 16,
            "code": "# The Ticket column contains too many unique values. We will investigate particular substrings.\n#pd.set_option(\"display.max_rows\", None)\ndisplay(train['Ticket'])\nnulls = train['Ticket'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the 'Ticket' column of the training data and calculates the number and proportion of null values in the 'Ticket' column to investigate particular substrings.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.91641235
            },
            "cluster": 4
        }, {
            "cell_id": 17,
            "code": "# let's look for patterns in the beginning letters of the ticket\nprefixes = set()\nfor i in train['Ticket']:\n    pref = re.search('([^\\s]+)\\s', i)\n    if pref is None: prefixes.add(\"None\")\n    else: prefixes.add(pref.group(1))\nprint(prefixes)",
            "class": "Exploratory Data Analysis",
            "desc": "This code identifies unique prefixes from the 'Ticket' column by searching for patterns using regular expressions and collects these prefixes in a set for further analysis.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.31170517
            },
            "cluster": 4
        }, {
            "cell_id": 22,
            "code": "copy['Ticket_num'] = copy.Ticket.str.extract('(^\\d*)')\ncopy['Ticket_num'] = copy['Ticket_num'].str.len()\ndisplay(copy[['Survived','Ticket_num']].groupby(['Ticket_num']).agg({'Survived':['count','sum','mean']}))",
            "class": "Exploratory Data Analysis",
            "desc": "This code extracts numerical parts from the 'Ticket' column to create a 'Ticket_num' column indicating the length of these numerical parts, and calculates survival statistics grouped by 'Ticket_num' to identify potential patterns.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.98412067
            },
            "cluster": 4
        }, {
            "cell_id": 26,
            "code": "# Initial review shows that Parch appears to be predictive\nnulls = train['Cabin'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates the number and proportion of null values in the 'Cabin' column to review its predictive potential.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.96462893
            },
            "cluster": -1
        }, {
            "cell_id": 29,
            "code": "# Initial review shows that Embarked appears to be predictive\ndisplay(train[['Survived','Embarked']].groupby(['Embarked']).agg({'Survived':['count','sum','mean']}))\nnulls = train['Embarked'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and displays the count, sum, and mean of the 'Survived' column grouped by 'Embarked' to check its predictive power, and computes the total and proportion of null values in the 'Embarked' column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.73904485
            },
            "cluster": 4
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport itertools as it # to avoid nested for-loops\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # to search for substrings using regular expressions\nimport scipy.stats as stats # for performing t-tests for statistically significant difference in mean values\nfrom sklearn.model_selection import KFold # for k-fold validation of models\nfrom sklearn import metrics as met # for model evaluation metrics\nimport sklearn.linear_model as lm # for linear models\nfrom sklearn import tree # decision tree used to discretize some continuous variables\nimport seaborn as sns # for vizualiations\nimport statsmodels.api as sm # Used to create logistic regression models to check for statistical significance of continuous variables. This package returns confidence intervals for predictors\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "This code imports multiple essential libraries such as NumPy, Pandas, Scipy, Scikit-Learn, and Seaborn, and lists available input data files in the Kaggle environment by traversing the directory structure using the os library.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99935275
            },
            "cluster": -1
        }, {
            "cell_id": 11,
            "code": "# Finally, let's use ROC AUC to compare which version is superior\nmodel = lm.LogisticRegression(random_state = 8292010, solver = 'lbfgs')\nROC_AUC_mean = []\nROC_AUC_lm = []\n# We will do 5-fold validation when calculating the ROC_AUC\nkfold = KFold(10, shuffle = True, random_state = 1)\nfor tr, te in kfold.split(train):\n    # capture metrics for the first variation of the age with mean imputing:\n    X, y = np.array(train['Age_mean_imputed']).reshape(-1,1)[tr], np.array(train['Survived']).reshape(-1,1)[tr]\n    model.fit(X, y.ravel())\n    y_hat = model.predict_proba(X = np.array(train['Age_mean_imputed']).reshape(-1,1))[:,1][te]\n    y_true = np.array(train['Survived']).reshape(-1,1)[te]\n    ROC_AUC_mean.append(met.roc_auc_score(y_true, y_score = y_hat))\n    \n    # capture metrics for the second variation of the age with linear regression:\n    X, y = np.array(train['Age_lm']).reshape(-1,1)[tr], np.array(train['Survived']).reshape(-1,1)[tr]\n    model.fit(X, y.ravel())\n    y_hat = model.predict_proba(X = np.array(train['Age_lm']).reshape(-1,1))[:,1][te]\n    y_true = np.array(train['Survived']).reshape(-1,1)[te]\n    ROC_AUC_lm.append(met.roc_auc_score(y_true, y_score = y_hat))\ndisplay(np.average(ROC_AUC_mean))\ndisplay(np.average(ROC_AUC_lm))",
            "class": "Model Evaluation",
            "desc": "This code performs 10-fold cross-validation to compare the ROC AUC scores of logistic regression models based on two different age imputation methods (mean imputation and linear regression imputation), and then calculates and displays the average ROC AUC scores for each method.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_model_class",
                "subclass_id": 3,
                "predicted_subclass_probability": 0.44388086
            },
            "cluster": 0
        }, {
            "cell_id": 35,
            "code": "np.max(df[7])",
            "class": "Model Evaluation",
            "desc": "This code calculates the maximum ROC AUC score for the decision tree model with `max_depth` set to 7 by accessing the specific column in the DataFrame and applying the `np.max` function.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9973348
            },
            "cluster": 1
        }, {
            "cell_id": 50,
            "code": "# In order to choose a cutoff value for survival, we will tune the cutoff on our original model to maximize the matthews correlation coefficient\nX = data[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']]\ny = data['Survived']\ncutoffs = {}\n# try cutoff values from 0.1 to 0.9\nfor i in np.arange(0.3,0.7,0.001):\n    MCC = []\n    for tr, te in kfold.split(data):\n        X_train, X_test = X.loc[tr], X.loc[te]\n        y_train, y_test = y.loc[tr], y.loc[te]\n        fit = model.fit(X = X_train, y = y_train)\n        y_pred = fit.predict_proba(X_test)[:,1]\n        y_pred = np.where(y_pred < i, 0, 1)\n        MCC.append(met.matthews_corrcoef(y_test, y_pred))\n    cutoffs[i] = np.mean(MCC)\ndisplay(max(cutoffs, key = cutoffs.get))",
            "class": "Model Evaluation",
            "desc": "This code performs a grid search to find the optimal cutoff value for classifying survival, aiming to maximize the Matthews correlation coefficient (MCC) by running the classifier on multiple cross-validation splits of the training dataset, and displays the cutoff value that yields the highest average MCC.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.6277582
            },
            "cluster": 0
        }, {
            "cell_id": 33,
            "code": "X, y = data[['Pclass', 'Mrs', 'Mr', 'Master', 'Miss', 'Age','SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Female','Ticket_len5']], data['Survived']\n# create lists of parameters to try to figure out the optimal parameters\n# I started with much bigger ranges and narrowed it down to these\nmin_leafs = range(32,45)\nmax_depth = range(5,15)\n# create an empty dataframe in which to store ROC AUC score\ndf = pd.DataFrame(columns = max_depth, index = min_leafs)\n\n# for each combination of parameter values, figure out the ROC AUC using 10 fold validation\nfor leaf, depth in it.product(min_leafs, max_depth):\n    dtc = tree.DecisionTreeClassifier(min_samples_leaf = leaf, max_depth = depth)\n    kfold = KFold(10, shuffle = True, random_state = 1)\n    ROC_AUC_scores = []\n    for tr, te in kfold.split(data):\n        X_train, X_test = X.loc[tr], X.loc[te]\n        y_train, y_test = y.loc[tr], y.loc[te]\n        # fit the data\n        dtc = dtc.fit(X_train, y_train)\n        # get predictions on the test set\n        y_pred = dtc.predict_proba(X_test)[:,1]\n        # get the ROC on the test set\n        ROC_AUC_scores.append(met.roc_auc_score(y_test, y_pred))\n    # average the ROC from all folds\n    ROC_AUC = np.mean(ROC_AUC_scores)\n    # store average ROC AUC in dataframe\n    df.loc[leaf][depth] = float(ROC_AUC)",
            "class": "Model Training",
            "desc": "This code performs a grid search over specified ranges of `min_samples_leaf` and `max_depth` parameters for a decision tree classifier, using 10-fold cross-validation to calculate the average ROC AUC scores for each parameter combination, and stores the results in a DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.8047089
            },
            "cluster": 1
        }, {
            "cell_id": 36,
            "code": "X, y = data[['Pclass', 'Mrs', 'Mr', 'Master', 'Miss', 'Age','SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Female','Ticket_len5']], data['Survived']\n# create an empty list in which to store ROC AUC score\nROC_AUC = []\npredictors = X.columns\nmodel = lm.LogisticRegression(random_state = 8292010, solver = 'liblinear')\nkfold = KFold(10, shuffle = True, random_state = 1)\nselected = []\nmax_AUC = {}\n\n# starting with one predictor, figure out the ROC AUC in a bivariate model, then continuously add one predictor each time to maximize ROC AUC\nfor i in range(0, len(predictors)):\n    next_best = {}\n    for pred in [p for p in predictors if p not in selected]:\n        ROC_AUC_scores = []\n        X = data[selected + [pred]]\n        for tr, te in kfold.split(data):\n            X_train, X_test = X.loc[tr], X.loc[te]\n            y_train, y_test = y.loc[tr], y.loc[te]\n            fit = model.fit(X = X_train, y = y_train)\n            y_pred = fit.predict_proba(X_test)[:,1]\n            ROC_AUC_scores.append(met.roc_auc_score(y_test, y_pred))\n        next_best[pred] = np.mean(ROC_AUC_scores)\n    selected.append(max(next_best, key = next_best.get))\n    max_AUC[', '.join(selected)] = next_best[max(next_best, key = next_best.get)]\nfor i in max_AUC.keys():\n    print(f'{i}: {max_AUC[i]}')",
            "class": "Model Training",
            "desc": "This code performs a stepwise feature selection process for logistic regression using 10-fold cross-validation, iteratively adding the predictor that maximizes the ROC AUC score, and maintains a record of the predictors and their corresponding AUC scores for each step.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.6774396
            },
            "cluster": 1
        }, {
            "cell_id": 48,
            "code": "model = lm.LogisticRegression(random_state = 8292010, solver = 'liblinear')\nX = data[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']]\ny = data['Survived']\nfit = model.fit(X = X, y = y)\n\ny_pred = fit.predict_proba(test[['Female', 'Pclass', 'Master', 'Embarked', 'Cabin', 'Parch', 'Age', 'Ticket_len5']])[:,1]",
            "class": "Model Training",
            "desc": "This code fits a logistic regression model using selected features from the training dataset and then uses the fitted model to predict the probabilities of survival for the test dataset.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.87436324
            },
            "cluster": -1
        }, {
            "cell_id": 8,
            "code": "# visualize distribution of ages\ntrain['Age'].hist()\n# check for null values\\\nnulls = train['Age'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Values: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Visualization",
            "desc": "This code visualizes the distribution of the 'Age' column using a histogram and checks for null values in the 'Age' column, displaying the total and proportion of null values.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9968941
            },
            "cluster": -1
        }, {
            "cell_id": 24,
            "code": "# visualize distribution of ages\ntrain['Fare'].hist(bins = 40)\n# check for null values\\\nnulls = train['Fare'].isnull().sum()\ntotal_rows = len(train)\ndisplay(f'Null Vales: {nulls} / {total_rows} = {nulls/total_rows}')",
            "class": "Visualization",
            "desc": "This code visualizes the distribution of the 'Fare' column using a histogram with 40 bins and checks for null values in the 'Fare' column, displaying the total and proportion of null values.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9981839
            },
            "cluster": -1
        }, {
            "cell_id": 34,
            "code": "# Looking at all of our parameter values, it looks like a max_depth = 7 and min_leaf_size = 38 are our best bets\ndf[df.columns] = df[df.columns].astype(float)\nsns.heatmap(df)",
            "class": "Visualization",
            "desc": "This code converts the DataFrame containing ROC AUC scores to floating-point types and visualizes the scores using a heatmap from the Seaborn library to identify the optimal `max_depth` and `min_samples_leaf` values.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.9982356
            },
            "cluster": 0
        }],
        "notebook_id": 3,
        "notebook_name": "titanic-logistic-regression-and-decision-tree.ipynb",
        "user": "titanic-logistic-regression-and-decision-tree.ipynb"
    }, {
        "cells": [{
            "cell_id": 30,
            "code": "#Creating Dataframe to store the Ids with Prediction\noutput=pd.DataFrame({'PassengerId':test_id,'Survived':pred})\nprint(output)",
            "class": "Data Export",
            "desc": "The code snippet creates a DataFrame using pandas to store the PassengerId along with the predicted 'Survived' outcomes, and then prints this DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.9984794
            },
            "cluster": -1
        }, {
            "cell_id": 31,
            "code": "output.to_csv(\"Titanic_Survival.csv\",index=False)\nprint(\"Completed\")",
            "class": "Data Export",
            "desc": "The code snippet saves the output DataFrame containing the PassengerId and predicted 'Survived' outcomes to a CSV file named \"Titanic_Survival.csv\" using pandas, and then prints \"Completed\".",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992403
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "#importing training data\ntrain_data=pd.read_csv(\"../input/titanic/train.csv\")\ntrain_data.head(10)",
            "class": "Data Extraction",
            "desc": "The code snippet imports the Titanic training dataset using pandas and displays the first ten rows.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9995395
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "#Importing testing data\ntest_data=pd.read_csv(\"../input/titanic/test.csv\")\ntest_data.head(10)",
            "class": "Data Extraction",
            "desc": "The code snippet imports the Titanic testing dataset using pandas and displays the first ten rows.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99959296
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "#Replacing Null Value with unknown as it is Categorical data \ntrain_data['Cabin']=train_data['Cabin'].fillna('Unknown')",
            "class": "Data Transform",
            "desc": "The code snippet replaces null values in the 'Cabin' column of the Titanic training dataset with 'Unknown' using the `fillna()` method from pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.72266513
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "#Replacing the null value with Median \ntrain_data['Age']=train_data['Age'].fillna(train_data['Age'].median())",
            "class": "Data Transform",
            "desc": "The code snippet replaces null values in the 'Age' column of the Titanic training dataset with the column's median value using the `fillna()` method from pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.89056116
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "train_data['Embarked']=train_data['Embarked'].fillna('Unknown')",
            "class": "Data Transform",
            "desc": "The code snippet replaces null values in the 'Embarked' column of the Titanic training dataset with 'Unknown' using the `fillna()` method from pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.8414222
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "#Dropping PassengerID, Name and Ticket because they will not have bigger impact predicting the survival \ntrain_data=train_data.drop(['PassengerId','Name','Ticket'],axis=1)",
            "class": "Data Transform",
            "desc": "The code snippet removes the 'PassengerId', 'Name', and 'Ticket' columns from the Titanic training dataset using the `drop()` method from pandas, considering them irrelevant for predicting survival.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9049642
            },
            "cluster": 0
        }, {
            "cell_id": 14,
            "code": "#Encoding Catergorical data\nencoder = LabelEncoder()\ntrain_data['Sex'] = encoder.fit_transform(train_data['Sex'])\ntrain_data['Cabin'] = encoder.fit_transform(train_data['Cabin'])\ntrain_data['Embarked']=encoder.fit_transform(train_data['Embarked'])\ntrain_data.head(10)",
            "class": "Data Transform",
            "desc": "The code snippet encodes categorical columns ('Sex', 'Cabin', 'Embarked') in the Titanic training dataset into numerical format using the `LabelEncoder` from scikit-learn, and displays the first ten rows of the transformed dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99937207
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "#Replacing the null value with Median \ntest_data['Age']=test_data['Age'].fillna(test_data['Age'].median())",
            "class": "Data Transform",
            "desc": "The code snippet replaces null values in the 'Age' column of the Titanic testing dataset with the column's median value using the `fillna()` method from pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9261921
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "#Replacing the null value with Median \ntest_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].median())",
            "class": "Data Transform",
            "desc": "The code snippet replaces null values in the 'Fare' column of the Titanic testing dataset with the column's median value using the `fillna()` method from pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9895257
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "#Replacing Null Value with unknown as it is Categorical data \ntest_data['Cabin']=test_data['Cabin'].fillna('Unknown')",
            "class": "Data Transform",
            "desc": "The code snippet replaces null values in the 'Cabin' column of the Titanic testing dataset with 'Unknown' using the `fillna()` method from pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.66284883
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "#Dropping Columns because bescause they were dropped in Training data\ntest_id=test_data['PassengerId']\ntest_data=test_data.drop(['PassengerId','Name','Ticket'],axis=1)",
            "class": "Data Transform",
            "desc": "The code snippet removes the 'PassengerId', 'Name', and 'Ticket' columns from the Titanic testing dataset, consistent with the preprocessing done on the training data, and preserves the 'PassengerId' column as `test_id`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9989949
            },
            "cluster": 0
        }, {
            "cell_id": 25,
            "code": "#Encoding Testing data\ntest_data['Sex'] = encoder.fit_transform(test_data['Sex'])\ntest_data['Cabin'] = encoder.fit_transform(test_data['Cabin'])\ntest_data['Embarked']=encoder.fit_transform(test_data['Embarked'])",
            "class": "Data Transform",
            "desc": "The code snippet encodes categorical columns ('Sex', 'Cabin', 'Embarked') in the Titanic testing dataset into numerical format using the `LabelEncoder` from scikit-learn.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99932957
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "#Allocating variable for testing data\nx_test=test_data",
            "class": "Data Transform",
            "desc": "The code snippet assigns the processed and encoded Titanic testing dataset to the variable `x_test` for further use.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.99759275
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "train_data.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays a concise summary of the Titanic training dataset using the `info()` method from pandas, including the data types and non-null counts of each column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99936634
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "train_data.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet generates descriptive statistics of the Titanic training dataset's numerical columns using the `describe()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994442
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "#Checking columns with missing data\nMiss_Percent=100*(train_data.isnull().sum()/len(train_data))\n\n#Creating a dataframe to show percentage of missing data and its respective data column in table\nDataFrame=pd.DataFrame(Miss_Percent)\nmiss_percent_table=DataFrame.rename(columns={0:'% of Missing Values'})\nMissPercent=miss_percent_table\n\n#Displaying Missing Value table\nMissPercent",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates the percentage of missing values for each column in the Titanic training dataset using pandas, creates a DataFrame to display this information, and outputs the resulting table.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.99713886
            },
            "cluster": 6
        }, {
            "cell_id": 5,
            "code": "#Checking Cabin column\ntrain_data['Cabin'].unique()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet retrieves and displays the unique values present in the 'Cabin' column of the Titanic training dataset using the `unique()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_unique_values",
                "subclass_id": 57,
                "predicted_subclass_probability": 0.99826425
            },
            "cluster": 5
        }, {
            "cell_id": 7,
            "code": "#Checking Age column\ntrain_data['Age'].unique()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet retrieves and displays the unique values present in the 'Age' column of the Titanic training dataset using the `unique()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_unique_values",
                "subclass_id": 57,
                "predicted_subclass_probability": 0.9981658
            },
            "cluster": 5
        }, {
            "cell_id": 9,
            "code": "#Checking Embarked Column\ntrain_data['Embarked'].unique()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet retrieves and displays the unique values present in the 'Embarked' column of the Titanic training dataset using the `unique()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_unique_values",
                "subclass_id": 57,
                "predicted_subclass_probability": 0.99811125
            },
            "cluster": 5
        }, {
            "cell_id": 17,
            "code": "test_data.isnull().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and displays the count of null values for each column in the Titanic testing dataset using the `isnull().sum()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9989555
            },
            "cluster": 5
        }, {
            "cell_id": 18,
            "code": "#Checking Age data column\ntest_data['Age'].unique()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet retrieves and displays the unique values present in the 'Age' column of the Titanic testing dataset using the `unique()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_unique_values",
                "subclass_id": 57,
                "predicted_subclass_probability": 0.99808854
            },
            "cluster": 5
        }, {
            "cell_id": 20,
            "code": "#Checking Fare data column\ntest_data['Fare'].unique()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet retrieves and displays the unique values present in the 'Fare' column of the Titanic testing dataset using the `unique()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_unique_values",
                "subclass_id": 57,
                "predicted_subclass_probability": 0.99812144
            },
            "cluster": 5
        }, {
            "cell_id": 22,
            "code": "#Checking Cabin Column\ntest_data['Cabin'].unique()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet retrieves and displays the unique values present in the 'Cabin' column of the Titanic testing dataset using the `unique()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_unique_values",
                "subclass_id": 57,
                "predicted_subclass_probability": 0.9981199
            },
            "cluster": 5
        }, {
            "cell_id": 26,
            "code": "#Checking the encoded test data\ntest_data.head(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first ten rows of the encoded Titanic testing dataset to verify the transformations.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99911183
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Libraries for Data Manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Libraries for Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\n%matplotlib inline\n\n#Library for modelling\nfrom sklearn.preprocessing import LabelEncoder #Encodes Categorical Data into Numerical\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "The code snippet imports essential libraries for data manipulation (NumPy, pandas), visualization (Seaborn, Matplotlib), and modeling (LabelEncoder, RandomForestClassifier from scikit-learn), and lists input data files from a specific directory.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.9978358
            },
            "cluster": -1
        }, {
            "cell_id": 29,
            "code": "#Testing the model with test data\npred = random_forest.predict(x_test)",
            "class": "Model Evaluation",
            "desc": "The code snippet uses the trained `RandomForestClassifier` to predict outcomes on the Titanic testing dataset (`x_test`).",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99370307
            },
            "cluster": -1
        }, {
            "cell_id": 15,
            "code": "#Setting training data into x_train and y_train\nx_train=train_data.drop('Survived',axis=1)\ny_train=train_data['Survived']\n\n#Shapes of x_train,y_train and test data\nx_train.shape, y_train.shape,",
            "class": "Model Training",
            "desc": "The code snippet splits the Titanic training dataset into features (`x_train`) and target (`y_train`) by separating the 'Survived' column, and then displays the shapes of both parts.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.9980792
            },
            "cluster": 1
        }, {
            "cell_id": 28,
            "code": "random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(x_train, y_train)\nrandom_forest.score(x_train, y_train)",
            "class": "Model Training",
            "desc": "The code snippet initializes a `RandomForestClassifier` with 100 estimators, fits it to the training data (`x_train` and `y_train`), and then calculates the accuracy score of the model on the training data using scikit-learn.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9990081
            },
            "cluster": 1
        }, {
            "cell_id": 12,
            "code": "#Barplot to show the total survivals based on Gender in each Passenger class\nplt.figure(figsize=(12,12))\nsns.barplot(x=\"Pclass\", y=\"Survived\",hue=\"Sex\", data=train_data)",
            "class": "Visualization",
            "desc": "The code snippet generates a bar plot using Seaborn to show the total number of survivors based on gender within each passenger class in the Titanic training dataset.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9939272
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "#Calculating Correlation \ncorrelation=train_data.corr()\n#Plotting the Correlation in HeatMap for the data columns which has correlation value more than 0.4\nplt.figure(figsize=(12,12))\nCorr_Heatmap=sns.heatmap(correlation,annot=True,cmap=\"GnBu\")",
            "class": "Visualization",
            "desc": "The code snippet calculates the correlation matrix for the Titanic training dataset using the `corr()` method from pandas and visualizes it in a heatmap using Seaborn for correlation values greater than 0.4.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.9982987
            },
            "cluster": 1
        }],
        "notebook_id": 4,
        "notebook_name": "titanic-survival-prediction-using-rfclassifier.ipynb",
        "user": "titanic-survival-prediction-using-rfclassifier.ipynb"
    }, {
        "cells": [{
            "cell_id": 46,
            "code": "# Get predictions for each model and create submission files\nfor model in best_models:\n    predictions = best_models[model].predict(test_X)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('submission_' + model + '.csv', index=False)",
            "class": "Data Export",
            "desc": "This code snippet generates predictions on the test data using each of the best models, creates corresponding submission files with 'PassengerId' and 'Survived' columns, and saves them as CSV files.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992563
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv')",
            "class": "Data Extraction",
            "desc": "This code snippet reads the training and test datasets from CSV files located in the '../input/titanic/' directory into Pandas DataFrames named `train_data` and `test_data`.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997619
            },
            "cluster": -1
        }, {
            "cell_id": 9,
            "code": "def clean_data(data):\n    # Too many missing values\n    data.drop(['Cabin'], axis=1, inplace=True)\n    \n    # Probably will not provide some useful information\n    data.drop(['Name', 'Ticket', 'Fare', 'Embarked'], axis=1, inplace=True)\n    \n    return data\n    \ntrain_data = clean_data(train_data)\ntest_data = clean_data(test_data)",
            "class": "Data Transform",
            "desc": "This code snippet defines and applies a function `clean_data` that removes the columns `Cabin`, `Name`, `Ticket`, `Fare`, and `Embarked` from both the `train_data` and `test_data` DataFrames due to excess missing values or perceived lack of useful information.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9989342
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "train_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\ntest_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\n\n# Merge two data to get the average Age and fill the column\nall_data = pd.concat([train_data, test_data])\naverage = all_data.Age.median()\nprint(\"Average Age: {0}\".format(average))\ntrain_data.fillna(value={'Age': average}, inplace=True)\ntest_data.fillna(value={'Age': average}, inplace=True)",
            "class": "Data Transform",
            "desc": "This code snippet replaces the categorical 'Sex' column with numerical values (0 for male, 1 for female) and fills missing 'Age' values with the median age calculated from the combined training and test datasets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.29412758
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "# Set X and y\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\ny = train_data['Survived']\ntest_X = test_data.drop(['PassengerId'], axis=1)",
            "class": "Data Transform",
            "desc": "This code snippet sets the feature matrix `X` by dropping 'Survived' and 'PassengerId' columns from `train_data`, sets the target vector `y` to the 'Survived' column, and prepares the test feature matrix `test_X` by dropping the 'PassengerId' column from `test_data`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.99934286
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "train_data",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet displays the entire training dataset stored in the `train_data` DataFrame for initial inspection.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997675
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "train_data.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet generates descriptive statistics of the numerical columns in the `train_data` DataFrame using the `describe()` method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994442
            },
            "cluster": 5
        }, {
            "cell_id": 4,
            "code": "print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet prints the list of column names in the `train_data` DataFrame using the `columns` attribute and `tolist()` method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.99567986
            },
            "cluster": 5
        }, {
            "cell_id": 5,
            "code": "missing_values = train_data.isna().any()\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet identifies and prints the column names in the `train_data` DataFrame that have missing values using the `isna()` and `any()` methods from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.92364883
            },
            "cluster": 3
        }, {
            "cell_id": 6,
            "code": "print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()/len(train_data))))",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the percentage of missing values in the `Age`, `Cabin`, and `Embarked` columns of the `train_data` DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99853253
            },
            "cluster": 3
        }, {
            "cell_id": 7,
            "code": "duplicates = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the number of duplicate rows in the `train_data` DataFrame using the `duplicated().sum()` method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.85191345
            },
            "cluster": 3
        }, {
            "cell_id": 8,
            "code": "categorical = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical))",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet counts the number of unique values for each column in the `train_data` DataFrame, sorts these counts in ascending order, and prints them to identify categorical variables.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.84571636
            },
            "cluster": 4
        }, {
            "cell_id": 10,
            "code": "train_data.tail()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet displays the last five rows of the cleaned `train_data` DataFrame using the `tail()` method from Pandas to verify the modifications.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997588
            },
            "cluster": 5
        }, {
            "cell_id": 12,
            "code": "train_data.tail()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet displays the last five rows of the modified `train_data` DataFrame using the `tail()` method from Pandas to confirm the recent changes.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997588
            },
            "cluster": 5
        }, {
            "cell_id": 15,
            "code": "print(\"Features: \\n{0} \".format(X.columns.tolist()))",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet prints the list of feature column names from the `X` DataFrame using the `columns` attribute and `tolist()` method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.93314546
            },
            "cluster": 5
        }, {
            "cell_id": 0,
            "code": "import numpy as np\nimport pandas as pd\n\n# Modelling\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\n\n# KNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Perceptron\nfrom sklearn.linear_model import Perceptron\n\n# Support Vector Machines\nfrom sklearn.svm import SVC\n\n# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\n# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\n# LightGBM\nfrom lightgbm import LGBMClassifier",
            "class": "Imports and Environment",
            "desc": "This code snippet imports various libraries for data manipulation (NumPy, Pandas) and a range of machine learning classifiers and utilities from Scikit-learn, XGBoost, and LightGBM for model training and evaluation.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993038
            },
            "cluster": -1
        }, {
            "cell_id": 17,
            "code": "evaluate_model(best_model_logistic.best_estimator_, 'logistic')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best logistic regression model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.7345922
            },
            "cluster": 1
        }, {
            "cell_id": 19,
            "code": "evaluate_model(best_model_gaussian_nb.best_estimator_, 'gaussian_nb')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Gaussian Naive Bayes model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.2824658
            },
            "cluster": 1
        }, {
            "cell_id": 21,
            "code": "evaluate_model(best_model_multinominal_nb.best_estimator_, 'multinominal_nb')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Multinomial Naive Bayes model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.20433934
            },
            "cluster": 1
        }, {
            "cell_id": 23,
            "code": "evaluate_model(best_model_complement_nb.best_estimator_, 'complement_nb')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Complement Naive Bayes model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.8224627
            },
            "cluster": 1
        }, {
            "cell_id": 25,
            "code": "evaluate_model(best_model_bernoulli_nb.best_estimator_, 'bernoulli_nb')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Bernoulli Naive Bayes model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.4510578
            },
            "cluster": 1
        }, {
            "cell_id": 27,
            "code": "evaluate_model(best_model_kneighbors.best_estimator_, 'kneighbors')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best K-Nearest Neighbors model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.7882486
            },
            "cluster": 1
        }, {
            "cell_id": 29,
            "code": "evaluate_model(best_model_perceptron.best_estimator_, 'perceptron')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Perceptron model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.6991431
            },
            "cluster": 1
        }, {
            "cell_id": 31,
            "code": "evaluate_model(best_model_svc.best_estimator_, 'svc')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Support Vector Classifier model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.73722243
            },
            "cluster": 1
        }, {
            "cell_id": 33,
            "code": "evaluate_model(best_model_sgd.best_estimator_, 'sgd')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Stochastic Gradient Descent model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.698045
            },
            "cluster": 1
        }, {
            "cell_id": 35,
            "code": "evaluate_model(best_model_gbc.best_estimator_, 'gbc')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Gradient Boosting Classifier model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.7379423
            },
            "cluster": 1
        }, {
            "cell_id": 37,
            "code": "evaluate_model(best_model_adaboost.best_estimator_, 'adaboost')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best AdaBoost Classifier model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.8056865
            },
            "cluster": 1
        }, {
            "cell_id": 39,
            "code": "evaluate_model(best_model_decision_tree.best_estimator_, 'decision_tree')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Decision Tree model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.8723382
            },
            "cluster": 1
        }, {
            "cell_id": 41,
            "code": "evaluate_model(best_model_random_forest.best_estimator_, 'random_forest')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best Random Forest model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.65981984
            },
            "cluster": 1
        }, {
            "cell_id": 43,
            "code": "evaluate_model(best_model_xgb.best_estimator_, 'xgb')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best XGBoost model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.74224275
            },
            "cluster": 1
        }, {
            "cell_id": 45,
            "code": "evaluate_model(best_model_lgbm.best_estimator_, 'lgbm')",
            "class": "Model Evaluation",
            "desc": "This code snippet evaluates the best LightGBM model obtained from grid search by calculating the accuracy score on the training data and stores the model in the `best_models` dictionary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.7132252
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "# To store models created\nbest_models = {}\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ndef print_best_parameters(hyperparameters, best_parameters):\n    value = \"Best parameters: \"\n    for key in hyperparameters:\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n    if hyperparameters:\n        print(value[:-2])\n\ndef get_best_model(estimator, hyperparameters, fit_params={}):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters, n_jobs=-1, cv=cv, scoring=\"accuracy\")\n    best_model = grid_search.fit(train_X, train_y, **fit_params)\n    best_parameters = best_model.best_estimator_.get_params()\n    print_best_parameters(hyperparameters, best_parameters)\n    return best_model\n\ndef evaluate_model(model, name):\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n    best_models[name] = model",
            "class": "Model Training",
            "desc": "This code snippet sets up the training and validation datasets, defines functions for hyperparameter tuning using GridSearchCV with RepeatedStratifiedKFold, stores and prints the best parameters, and evaluates models using accuracy score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.8947038
            },
            "cluster": 1
        }, {
            "cell_id": 16,
            "code": "# https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\nhyperparameters = {\n    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n    'penalty' : ['l2'],\n    'C'       : [100, 10, 1.0, 0.1, 0.01]\n}\nestimator = LogisticRegression(random_state=1)\nbest_model_logistic = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines hyperparameters for tuning a `LogisticRegression` model, initializes the model with a random state for reproducibility, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.99286205
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "# https://www.analyticsvidhya.com/blog/2021/01/gaussian-naive-bayes-with-hyperpameter-tuning/\nhyperparameters = {\n    'var_smoothing': np.logspace(0, -9, num=100)\n}\nestimator = GaussianNB()\nbest_model_gaussian_nb = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `GaussianNB` model, initializes the Gaussian Naive Bayes estimator, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.94975966
            },
            "cluster": 0
        }, {
            "cell_id": 20,
            "code": "# https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6\nhyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = MultinomialNB()\nbest_model_multinominal_nb = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `MultinomialNB` model, initializes the Multinomial Naive Bayes estimator, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.8767018
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n    'norm'      : [True, False]\n}\nestimator = ComplementNB()\nbest_model_complement_nb = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `ComplementNB` model, initializes the Complement Naive Bayes estimator, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.8994227
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = BernoulliNB()\nbest_model_bernoulli_nb = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `BernoulliNB` model, initializes the Bernoulli Naive Bayes estimator, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.6762601
            },
            "cluster": 0
        }, {
            "cell_id": 26,
            "code": "# https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f\nhyperparameters = {\n    'n_neighbors' : list(range(1,5)),\n    'weights'     : ['uniform', 'distance'],\n    'algorithm'   : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size'   : list(range(1,10)),\n    'p'           : [1,2]\n}\nestimator = KNeighborsClassifier()\nbest_model_kneighbors = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `KNeighborsClassifier` model, initializes the K-Nearest Neighbors estimator, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.76489675
            },
            "cluster": 0
        }, {
            "cell_id": 28,
            "code": "# https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/\n# https://machinelearningmastery.com/manually-optimize-hyperparameters/\nhyperparameters = {\n    'penalty'  : ['l1', 'l2', 'elasticnet'],\n    'eta0'     : [0.0001, 0.001, 0.01, 0.1, 1.0],\n    'max_iter' : list(range(50, 200, 50))\n}\nestimator = Perceptron(random_state=1)\nbest_model_perceptron = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `Perceptron` model, initializes the Perceptron estimator with a random state for reproducibility, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9484464
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "# https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n# https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167\nhyperparameters = {\n    'C'      : [0.1, 1, 10, 100],\n    'gamma'  : [0.0001, 0.001, 0.01, 0.1, 1],\n    'kernel' : ['rbf']\n}\nestimator = SVC(random_state=1)\nbest_model_svc = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `SVC` model with an RBF kernel, initializes the Support Vector Classifier estimator with a random state for reproducibility, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.8621029
            },
            "cluster": 0
        }, {
            "cell_id": 32,
            "code": "# https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4\n# https://www.knowledgehut.com/tutorials/machine-learning/hyperparameter-tuning-machine-learning\nhyperparameters = {\n    'loss'    : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n    'penalty' : ['l1', 'l2', 'elasticnet'],\n    'alpha'   : [0.01, 0.1, 1, 10]\n}\nestimator = SGDClassifier(random_state=1, early_stopping=True)\nbest_model_sgd = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `SGDClassifier` model, initializes the Stochastic Gradient Descent classifier with a random state for reproducibility and early stopping enabled, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9670545
            },
            "cluster": 0
        }, {
            "cell_id": 34,
            "code": "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\nhyperparameters = {\n    'loss'          : ['deviance', 'exponential'],\n    'learning_rate' : [0.01, 0.1, 0.2, 0.3],\n    'n_estimators'  : [50, 100, 200],\n    'subsample'     : [0.1, 0.2, 0.5, 1.0],\n    'max_depth'     : [2, 3, 4, 5]\n}\nestimator = GradientBoostingClassifier(random_state=1)\nbest_model_gbc = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `GradientBoostingClassifier` model, initializes the Gradient Boosting Classifier estimator with a random state for reproducibility, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.95942765
            },
            "cluster": 0
        }, {
            "cell_id": 36,
            "code": "# https://medium.com/@chaudhurysrijani/tuning-of-adaboost-with-computational-complexity-8727d01a9d20\nhyperparameters = {\n    'n_estimators'  : [10, 50, 100, 500],\n    'learning_rate' : [0.001, 0.01, 0.1, 1.0]\n}\nestimator = AdaBoostClassifier(random_state=1)\nbest_model_adaboost = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning an `AdaBoostClassifier` model, initializes the AdaBoost Classifier estimator with a random state for reproducibility, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9942344
            },
            "cluster": 0
        }, {
            "cell_id": 38,
            "code": "# https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n# https://www.kaggle.com/gauravduttakiit/hyperparameter-tuning-in-decision-trees\nhyperparameters = {\n    'criterion'         : ['gini', 'entropy'],\n    'splitter'          : ['best', 'random'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = DecisionTreeClassifier(random_state=1)\nbest_model_decision_tree = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `DecisionTreeClassifier` model, initializes the Decision Tree Classifier estimator with a random state for reproducibility, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.8945561
            },
            "cluster": 0
        }, {
            "cell_id": 40,
            "code": "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n# https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/\nhyperparameters = {\n    'n_estimators'      : list(range(10, 50, 10)),\n    'max_features'      : ['auto', 'sqrt', 'log2'],\n    'criterion'         : ['gini', 'entropy'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = RandomForestClassifier(random_state=1)\nbest_model_random_forest = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning a `RandomForestClassifier` model, initializes the Random Forest Classifier estimator with a random state for reproducibility, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "define_search_space",
                "subclass_id": 5,
                "predicted_subclass_probability": 0.47099417
            },
            "cluster": 0
        }, {
            "cell_id": 42,
            "code": "# https://towardsdatascience.com/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d\nhyperparameters = {\n    'learning_rate' : [0.3, 0.4, 0.5],\n    'gamma'         : [0, 0.4, 0.8],\n    'max_depth'     : [2, 3, 4],\n    'reg_lambda'    : [0, 0.1, 1],\n    'reg_alpha'     : [0.1, 1]\n}\nfit_params = {\n    'verbose'               : False,\n    'early_stopping_rounds' : 40,\n    'eval_metric'           : 'logloss',\n    'eval_set'              : [(val_X, val_y)]\n}\nestimator = XGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False)\nbest_model_xgb = get_best_model(estimator, hyperparameters, fit_params)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning an `XGBClassifier` model, initializes the XGBoost Classifier with GPU acceleration and additional parameters for early stopping, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters and fit parameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "init_hyperparams",
                "subclass_id": 59,
                "predicted_subclass_probability": 0.80115277
            },
            "cluster": 0
        }, {
            "cell_id": 44,
            "code": "# https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\nhyperparameters = {\n    'boosting_type' : ['gbdt', 'dart', 'goss'],\n    'num_leaves'    : [4, 8, 16, 32],\n    'learning_rate' : [0.01, 0.1, 1],\n    'n_estimators'  : [25, 50, 100],\n    'reg_alpha'     : [0, 0.1, 1],\n    'reg_lambda'    : [0, 0.1, 1],\n}\nestimator = LGBMClassifier(random_state=1, device='gpu')\nbest_model_lgbm = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code snippet defines the hyperparameters for tuning an `LGBMClassifier` model, initializes the LightGBM Classifier with GPU acceleration and a random state for reproducibility, and uses the `get_best_model` function to identify and store the best model based on the specified hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9681083
            },
            "cluster": 0
        }],
        "notebook_id": 5,
        "notebook_name": "titanic-hyperparameter-tuning-gridsearchcv.ipynb",
        "user": "titanic-hyperparameter-tuning-gridsearchcv.ipynb"
    }, {
        "cells": [{
            "cell_id": 29,
            "code": "predictions = rfc1.predict(test_data[features])\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Data Export",
            "desc": "This code generates predictions for the test dataset using the trained RandomForestClassifier, creates an output DataFrame with PassengerId and Survived columns, and saves the DataFrame as a CSV file named 'submission.csv' using pandas' `DataFrame` and `to_csv` methods.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99908555
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')",
            "class": "Data Extraction",
            "desc": "This code reads the training and test datasets for the Titanic machine learning task from specified file paths using the pandas library's `read_csv` method.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997478
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "train_data['female'] = pd.get_dummies(train_data['Sex'])['female']\ntest_data['female'] = pd.get_dummies(test_data['Sex'])['female']",
            "class": "Data Transform",
            "desc": "This code adds a new column 'female' to both the training and test datasets by converting the 'Sex' feature into a binary variable using pandas' `get_dummies` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99779177
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "sum(train_data['Age'].isnull())\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())",
            "class": "Data Transform",
            "desc": "This code fills missing values in the 'Age' column of both the training and test datasets with the mean age, using pandas' `fillna` method after calculating the number of missing values with `isnull`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.97469246
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "train_data['class1'] = pd.get_dummies(train_data.Pclass)[1]\ntest_data['class1'] = pd.get_dummies(test_data.Pclass)[1]\ntrain_data['class2'] = pd.get_dummies(train_data.Pclass)[2]\ntest_data['class2'] = pd.get_dummies(test_data.Pclass)[2]",
            "class": "Data Transform",
            "desc": "This code creates new columns 'class1' and 'class2' in both the training and test datasets by converting the 'Pclass' feature into binary variables using pandas' `get_dummies` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9973912
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "sibs = train_data.loc[train_data.SibSp <= 1]['Survived']\nprint(sum(sibs)/len(sibs))\ntrain_data['many_sibs'] = (train_data.SibSp > 1)*1\ntest_data['many_sibs'] = (test_data.SibSp > 1)*1",
            "class": "Data Transform",
            "desc": "This code calculates and prints the survival rate for passengers with one or fewer siblings/spouses and adds a new binary column 'many_sibs' to both the training and test datasets, indicating if a passenger has more than one sibling/spouse using basic pandas operations.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9980178
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "bins = [0.42, 15, 30, 50,80]\ntrain_data['bin_age'] = pd.cut(x=train_data.Age, bins=bins)\ntest_data['bin_age'] = pd.cut(x=test_data.Age, bins=bins)",
            "class": "Data Transform",
            "desc": "This code creates a new column 'bin_age' in both the training and test datasets by binning the 'Age' feature into specified age ranges using pandas' `cut` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.5303526
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "train_data['young'] = pd.get_dummies(train_data.bin_age).iloc[:,0]\ntest_data['young'] = pd.get_dummies(test_data.bin_age).iloc[:,0]\ntrain_data['senior'] = pd.get_dummies(train_data.bin_age).iloc[:,3]\ntest_data['senior'] = pd.get_dummies(test_data.bin_age).iloc[:,3]",
            "class": "Data Transform",
            "desc": "This code creates new binary columns 'young' and 'senior' in both the training and test datasets by converting the binned age ranges into dummy variables using pandas' `get_dummies` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99718374
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "X = train_data[features]\ny = train_data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)",
            "class": "Data Transform",
            "desc": "This code splits the training dataset into training and testing subsets for features and target variables (X_train, X_test, y_train, y_test) using scikit-learn's `train_test_split` method with a test size of 33% and a fixed random state.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9982145
            },
            "cluster": 0
        }, {
            "cell_id": 25,
            "code": "test_data.Fare = test_data.Fare.fillna(test_data.Fare.mean())",
            "class": "Data Transform",
            "desc": "This code fills missing values in the 'Fare' column of the test dataset with the mean fare using pandas' `fillna` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.27125782
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "train_data.head()",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first few rows of the training dataset using the pandas library's `head` method to get an initial look at the data.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997532
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "women = train_data.loc[train_data.Sex == 'female']['Survived']\nprint('Women survived',sum(women)/len(women))\n\nmen = train_data.loc[train_data.Sex == 'male']['Survived']\nprint('Men survived',sum(men)/len(men))",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and prints the survival rates for women and men in the Titanic training dataset using pandas to filter and aggregate the data.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.29674977
            },
            "cluster": 8
        }, {
            "cell_id": 7,
            "code": "high_fare = train_data.loc[train_data.Fare > 100]['Survived']\nprint('High fare survivors',sum(high_fare)/len(high_fare))\nlow_fare = train_data.loc[train_data.Fare < 32]['Survived']\nprint('High fare survivors',sum(low_fare)/len(low_fare))",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and prints the survival rates for passengers who paid high fares (over 100) and low fares (under 32) in the Titanic training dataset using pandas to filter and aggregate the data.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.97302115
            },
            "cluster": 8
        }, {
            "cell_id": 8,
            "code": "pclass1 = train_data.loc[train_data.Pclass == 1]['Survived']\nprint('Class1',sum(pclass1)/len(pclass1))\npclass2 = train_data.loc[train_data.Pclass == 2]['Survived']\nprint('Class2',sum(pclass2)/len(pclass2))\npclass3 = train_data.loc[train_data.Pclass == 3]['Survived']\nprint('Class3',sum(pclass3)/len(pclass3))",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and prints the survival rates for passengers in each passenger class (Pclass) in the Titanic training dataset using pandas to filter and aggregate the data.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.92158645
            },
            "cluster": 8
        }, {
            "cell_id": 9,
            "code": "sum(test_data.Pclass.isna())",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and returns the total number of missing values in the 'Pclass' column of the test dataset using pandas' `isna` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99908054
            },
            "cluster": 7
        }, {
            "cell_id": 11,
            "code": "sum(test_data.SibSp.isna())",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and returns the total number of missing values in the 'SibSp' column of the test dataset using pandas' `isna` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9991391
            },
            "cluster": 7
        }, {
            "cell_id": 13,
            "code": "young = train_data.loc[train_data.Age <= 15]['Survived']\nprint(sum(young)/len(young))\n\nold = train_data.loc[train_data.Age >=40]['Survived']\nprint(sum(old)/len(old))",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and prints the survival rates for young passengers (aged 15 or below) and older passengers (aged 40 or above) in the Titanic training dataset using pandas to filter and aggregate the data.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.9821845
            },
            "cluster": 8
        }, {
            "cell_id": 16,
            "code": "train_data.corr()['Survived']\n\nfeatures = ['Pclass', 'Fare', 'female', 'class1', 'class2', 'many_sibs', 'young', 'senior']",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and displays the correlation of each feature with the 'Survived' column in the training dataset and defines a list of selected feature names relevant to survival prediction using pandas' `corr` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99760026
            },
            "cluster": 4
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "This code sets up the environment for a machine learning task by importing essential libraries such as NumPy, pandas, Matplotlib, Seaborn, and using `os` to list files in the input directory.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.999241
            },
            "cluster": -1
        }, {
            "cell_id": 17,
            "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix",
            "class": "Imports and Environment",
            "desc": "This code imports `train_test_split` from `sklearn.model_selection`, `LogisticRegression` from `sklearn.linear_model`, and evaluation metrics `accuracy_score` and `confusion_matrix` from `sklearn.metrics` to facilitate model training and evaluation.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99930274
            },
            "cluster": -1
        }, {
            "cell_id": 26,
            "code": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n#CV_rfc = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)\n#CV_rfc.fit(X_train, y_train)",
            "class": "Imports and Environment",
            "desc": "This code imports additional machine learning utility functions and classes (`cross_val_score`, `GridSearchCV`, `RandomForestClassifier`, `accuracy_score`, `SVC`) from scikit-learn and defines a parameter grid for hyperparameter tuning via grid search.",
            "testing": {
                "class": "Model_Train",
                "subclass": "define_search_space",
                "subclass_id": 5,
                "predicted_subclass_probability": 0.85772324
            },
            "cluster": -1
        }, {
            "cell_id": 20,
            "code": "accuracy_score(y_pred, y_test)",
            "class": "Model Evaluation",
            "desc": "This code calculates and returns the accuracy score of the Logistic Regression model's predictions on the test dataset using scikit-learn's `accuracy_score` method.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99797374
            },
            "cluster": -1
        }, {
            "cell_id": 21,
            "code": "confusion_matrix(y_pred, y_test)",
            "class": "Model Evaluation",
            "desc": "This code generates and returns the confusion matrix for the Logistic Regression model's predictions on the test dataset using scikit-learn's `confusion_matrix` method.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9979062
            },
            "cluster": -1
        }, {
            "cell_id": 23,
            "code": "accuracy_score(y_pred, y_test)",
            "class": "Model Evaluation",
            "desc": "This code calculates and returns the accuracy score of the RandomForestClassifier model's predictions on the test dataset using scikit-learn's `accuracy_score` method.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99797374
            },
            "cluster": -1
        }, {
            "cell_id": 24,
            "code": "confusion_matrix(y_pred, y_test)",
            "class": "Model Evaluation",
            "desc": "This code generates and returns the confusion matrix for the RandomForestClassifier model's predictions on the test dataset using scikit-learn's `confusion_matrix` method.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9979062
            },
            "cluster": -1
        }, {
            "cell_id": 19,
            "code": "log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\ny_pred",
            "class": "Model Training",
            "desc": "This code trains a Logistic Regression model on the training dataset and then predicts the target variable on the test dataset using scikit-learn's `LogisticRegression` class and its `fit` and `predict` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.7683394
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)",
            "class": "Model Training",
            "desc": "This code initializes a RandomForestClassifier with specified hyperparameters, trains it on the training dataset, and predicts the target variable on the test dataset using scikit-learn's `RandomForestClassifier` class and its `fit` and `predict` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.977549
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "#CV_rfc.best_params_",
            "class": "Model Training",
            "desc": "This commented-out code snippet is intended to retrieve the best hyperparameters found by the GridSearchCV for the RandomForestClassifier using its `best_params_` attribute.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.9968646
            },
            "cluster": 1
        }, {
            "cell_id": 28,
            "code": "rfc1=RandomForestClassifier(random_state=42, max_features='log2', n_estimators= 200, max_depth=6, criterion='entropy')\nrfc1.fit(X_train, y_train)",
            "class": "Model Training",
            "desc": "This code initializes a RandomForestClassifier with specified hyperparameters and trains it on the training dataset using scikit-learn's `RandomForestClassifier` class and its `fit` method.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996785
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "plt.subplot(1,2,1)\nsns.histplot(train_data.Age)\nplt.subplot(1,2,2)\nsns.histplot(test_data.Age)",
            "class": "Visualization",
            "desc": "This code creates side-by-side histograms of the 'Age' distribution in the training and test datasets using Seaborn's `histplot` method and Matplotlib's `subplot` for layout.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99291444
            },
            "cluster": 0
        }],
        "notebook_id": 6,
        "notebook_name": "titanic-tutorial-vmindel.ipynb",
        "user": "titanic-tutorial-vmindel.ipynb"
    }, {
        "cells": [{
            "cell_id": 16,
            "code": "submit = pd.DataFrame({'PassengerId': final_test_data_id, 'Survived': prediction})",
            "class": "Data Export",
            "desc": "The code creates a DataFrame with 'PassengerId' and the corresponding 'Survived' predictions to prepare for submission using Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.9983236
            },
            "cluster": -1
        }, {
            "cell_id": 17,
            "code": "submit.to_csv(\"svm_titanic_sandorabad_7.csv\", index=False)",
            "class": "Data Export",
            "desc": "The code exports the submission DataFrame to a CSV file named \"svm_titanic_sandorabad_7.csv\" without including the index, using Pandas' `to_csv` method.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.999323
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "#Reading Data\ndata = pd.read_csv(\"../input/titanic/train.csv\") \nfinal_test_data = pd.read_csv(\"../input/titanic/test.csv\")\nfinal_test_data_id = np.array(final_test_data['PassengerId']) # we use it for our submission",
            "class": "Data Extraction",
            "desc": "The code reads the training and test data of the Titanic dataset from CSV files using Pandas and extracts the 'PassengerId' column from the test data as a NumPy array for future use in the submission.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997137
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "# we are droppin less relevant columns.\ndata.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'],axis=1, inplace=True)\ndata",
            "class": "Data Transform",
            "desc": "The code removes less relevant columns ('PassengerId', 'Name', 'Ticket', 'Cabin') from the dataset using Pandas' `drop` method to focus on the more pertinent features for the analysis.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9991437
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "mean_age = np.mean(data['Age'])\ndata['Age'] = data['Age'].fillna(mean_age)\ndata",
            "class": "Data Transform",
            "desc": "The code fills missing values in the 'Age' column with the mean age using NumPy's `mean` function and Pandas' `fillna` method to handle incomplete data.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99761415
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "# We need to transform the 'Sex' data, 'male' = 1 'female' = 0\ndata[\"Sex\"].replace('male', 1, inplace=True)\ndata[\"Sex\"].replace('female', 0, inplace=True)",
            "class": "Data Transform",
            "desc": "The code replaces the categorical 'Sex' data with numerical values, assigning 'male' a value of 1 and 'female' a value of 0, using Pandas' `replace` method to facilitate numerical processing in machine learning models.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9895449
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "# C = Cherbourg, Q = Queenstown, S = Southampton, after reading: \"https://www.kaggle.com/brendan45774/titanic-top-solution/notebook\" we now that this feature is important\n\ndata['Cherbourg'] = data['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\ndata['Queenstown'] = data['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\ndata['Southampton'] = data['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\ndata.drop('Embarked', axis=1, inplace=True)",
            "class": "Data Transform",
            "desc": "The code creates three new binary columns ('Cherbourg', 'Queenstown', 'Southampton') from the 'Embarked' feature and then drops the original 'Embarked' column using Pandas' `apply` method and `drop` method to encode the categorical data for machine learning tasks.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9882433
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "X = np.array(data.drop(['Survived'], axis=1))\ny = np.array(data['Survived'])",
            "class": "Data Transform",
            "desc": "The code separates the features (X) from the target variable (y) by converting the modified dataset into NumPy arrays, dropping the 'Survived' column from the features matrix using Pandas' `drop` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.9993857
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "# We create the train, test splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)",
            "class": "Data Transform",
            "desc": "The code splits the dataset into training and testing sets using Scikit-learn's `train_test_split` function, allocating 33% of the data to the test set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9983494
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "# we apply everything that we used with the \"train.csv\" dataset to the \"test.csv\" dataset\nfinal_test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\nfinal_test_data['Age'].fillna(np.mean(final_test_data['Age']), inplace=True)\nfinal_test_data['Fare'].fillna(np.mean(final_test_data['Fare']), inplace=True)\nfinal_test_data[\"Sex\"].replace('male', 1, inplace=True)\nfinal_test_data[\"Sex\"].replace('female', 0, inplace=True)\nfinal_test_data['Cherbourg'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\nfinal_test_data['Queenstown'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\nfinal_test_data['Southampton'] = final_test_data['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\nfinal_test_data.drop('Embarked', axis=1, inplace=True)\n\n\nX_final_test = np.array(final_test_data)",
            "class": "Data Transform",
            "desc": "The code processes the test dataset by applying the same transformations used on the training data, including dropping less relevant columns, filling missing values, replacing categorical 'Sex' data with numerical values, creating binary columns for 'Embarked' locations, and finally converting the processed data into a NumPy array.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99385875
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "data.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first few rows of the Titanic dataset using the `head` method of Pandas to give an initial view of the data's structure and content.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99974936
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "# We have missing values for the column age, we are going to meke those values outliers so we can still use the data\nprint(data.info(), data.describe())",
            "class": "Exploratory Data Analysis",
            "desc": "The code provides an overview of the dataset's structure, including data types and non-null counts using Pandas' `info` method, and displays summary statistics using the `describe` method to understand the distribution and central tendencies of the features.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994295
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "final_test_data",
            "class": "Exploratory Data Analysis",
            "desc": "The code outputs the final test data to visually inspect the applied transformations and confirm the data's readiness for model prediction.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997675
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "X_final_test",
            "class": "Exploratory Data Analysis",
            "desc": "The code outputs the processed test feature matrix `X_final_test` to verify the transformed and numerical data prepared for model prediction.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99973935
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "import numpy as np\nfrom sklearn import preprocessing, svm\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
            "class": "Imports and Environment",
            "desc": "The code imports essential libraries: NumPy for numerical operations, Scikit-learn for machine learning functionalities including preprocessing, support vector machines, and data splitting, and Pandas for data manipulation.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99935955
            },
            "cluster": -1
        }, {
            "cell_id": 18,
            "code": "# I based this nootebook on the tutorials of the youtuber: sentdex, \n# Specially on this series: \"https://pythonprogramming.net/machine-learning-tutorial-python-introduction/\" if someone is just starting as I am i really recomend it",
            "class": "Imports and Environment",
            "desc": "The code includes comments indicating that the notebook is based on Sentdex's tutorials, specifically from the \"machine-learning-tutorial-python-introduction\" series, providing a reference for those new to machine learning.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.445148
            },
            "cluster": -1
        }, {
            "cell_id": 11,
            "code": "# we test the model\nmodel_clf.fit(X_train, y_train)\naccuracy = model_clf.score(X_test, y_test)\nprint(accuracy)",
            "class": "Model Evaluation",
            "desc": "The code trains the Support Vector Classifier on the training data using the `fit` method, then evaluates its accuracy on the test set using the `score` method, and prints the accuracy value.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99956805
            },
            "cluster": 1
        }, {
            "cell_id": 15,
            "code": "prediction = model_clf.predict(X_final_test)",
            "class": "Model Evaluation",
            "desc": "The code generates predictions for the test dataset by applying the trained Support Vector Classifier model using the `predict` method.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99418724
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "# We create the model (classifier)\nmodel_clf = svm.SVC(kernel='linear')",
            "class": "Model Training",
            "desc": "The code initializes a Support Vector Classifier (SVC) with a linear kernel using Scikit-learn's `svm.SVC` to prepare for training the model on the dataset.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.83985466
            },
            "cluster": 1
        }],
        "notebook_id": 7,
        "notebook_name": "titanic-07.ipynb",
        "user": "titanic-07.ipynb"
    }, {
        "cells": [{
            "cell_id": 8,
            "code": "submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n\nsubmission[TARGET] = (preds > THRESHOLD).astype(int)\n\nsubmission.to_csv(SUBMISSION_PATH,index=False)\n\nsubmission.head()",
            "class": "Data Export",
            "desc": "This code updates the submission DataFrame with the predicted target values, applies a threshold to convert probabilities to binary outcomes, and saves the results to a CSV file.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99948066
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "# data \n\nTRAIN_PATH = \"../input/titanic/train.csv\"\n\nTEST_PATH = \"../input/titanic/test.csv\"\n\nSAMPLE_SUBMISSION_PATH = \"../input/titanic/gender_submission.csv\"\n\nSUBMISSION_PATH = \"submission.csv\"\n\nTARGET = \"Survived\"\n\nTHRESHOLD = 0.5\n\n\n\n# mljar \n\nMODE = \"Optuna\"\n\nEVAL_METRIC = \"auc\"\n\nALGORHYTHMS = [\"LightGBM\", \"Xgboost\", \"Extra Trees\"]\n\nOPTUNA_TIME_BUDGET = 30 \n\nTOTAL_TIME_LIMIT = 48 * OPTUNA_TIME_BUDGET\n\nFEATURES_SELECTION = False",
            "class": "Data Extraction",
            "desc": "This code snippet defines file paths for training, testing, and submission data, along with various machine learning parameters like target variable, evaluation metric, algorithms to use, and time constraints for MLJAR.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.999017
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "train = pd.read_csv(TRAIN_PATH)\n\ntest = pd.read_csv(TEST_PATH)",
            "class": "Data Extraction",
            "desc": "This code reads the training and testing datasets from the specified file paths into pandas DataFrame objects.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99966717
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "#1. delete unnecessary columns\n\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp','Parch']\n\ntrain = train.drop(drop_elements, axis = 1)\n\ntest = test.drop(drop_elements, axis = 1)\n\n\n\n#2.find null data and fill new data \n\ndef checkNull_fillData(df):\n\n    for col in df.columns:\n\n        if len(df.loc[df[col].isnull() == True]) != 0:\n\n            if df[col].dtype == \"float64\" or df[col].dtype == \"int64\":\n\n                df.loc[df[col].isnull() == True,col] = df[col].mean()\n\n            else:\n\n                df.loc[df[col].isnull() == True,col] = df[col].mode()[0]\n\n                \n\ncheckNull_fillData(train)\n\ncheckNull_fillData(test)\n\n\n\n#3.one hot encoding \n\nstr_list = [] \n\nnum_list = []\n\nfor colname, colvalue in train.iteritems():\n\n    if type(colvalue[1]) == str:\n\n        str_list.append(colname)\n\n    else:\n\n        num_list.append(colname)\n\n        \n\ntrain = pd.get_dummies(train, columns=str_list)\n\ntest = pd.get_dummies(test, columns=str_list)",
            "class": "Data Transform",
            "desc": "This code removes unnecessary columns, fills missing data using the mean for numerical columns and the mode for categorical columns, and applies one-hot encoding to transform categorical features into numerical format using pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.8790232
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "y = train[TARGET]\n\nX = train.drop([TARGET], axis=1)\n\nX_test = test",
            "class": "Data Transform",
            "desc": "This code separates the target variable from the training dataset and assigns it to `y`, while the remaining features are assigned to `X`, and the test dataset is assigned to `X_test`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.9993081
            },
            "cluster": -1
        }, {
            "cell_id": 0,
            "code": "from IPython.display import clear_output\n\n!pip install mljar-supervised --user\n\nclear_output()",
            "class": "Imports and Environment",
            "desc": "This code installs the `mljar-supervised` library using pip and clears the output in an IPython environment.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.95891166
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "import numpy as np\n\nimport pandas as pd\n\nfrom supervised import AutoML",
            "class": "Imports and Environment",
            "desc": "This code imports the numpy and pandas libraries, as well as the AutoML class from the supervised library.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993268
            },
            "cluster": -1
        }, {
            "cell_id": 7,
            "code": "preds = automl.predict(X_test)",
            "class": "Model Evaluation",
            "desc": "This code generates predictions for the test data `X_test` using the trained AutoML model.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9945115
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "automl = AutoML(mode=MODE, \n\n                eval_metric=EVAL_METRIC,\n\n                algorithms=ALGORHYTHMS,\n\n                optuna_time_budget=OPTUNA_TIME_BUDGET,   # tune each algorithm \n\n                total_time_limit=TOTAL_TIME_LIMIT,  # total time limit, set large enough to have time to compute all steps\n\n                features_selection=FEATURES_SELECTION)\n\nautoml.fit(X, y)",
            "class": "Model Training",
            "desc": "This code initializes the AutoML instance with specified parameters and fits it to the training data `X` and target `y` using the `mljar-supervised` library.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99916506
            },
            "cluster": 1
        }],
        "notebook_id": 8,
        "notebook_name": "automl-mljar-optuna-titanic.ipynb",
        "user": "automl-mljar-optuna-titanic.ipynb"
    }, {
        "cells": [{
            "cell_id": 25,
            "code": "# Get predictions for each model and create submission files\n\nfor model in best_models:\n\n    predictions = best_models[model].predict(test_X)\n\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n\n    output.to_csv('submission_' + model + '.csv', index=False)",
            "class": "Data Export",
            "desc": "This code generates predictions for each model in the `best_models` dictionary on the test dataset, creates a DataFrame containing `PassengerId` and predicted `Survived` values, and exports these to CSV files named `submission_<model>.csv` using the `to_csv()` method.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992563
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_data = pd.read_csv('../input/titanic/train.csv')\n\ntest_data = pd.read_csv('../input/titanic/test.csv')",
            "class": "Data Extraction",
            "desc": "This code reads the training and test datasets from CSV files into pandas DataFrames using `pd.read_csv()`.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997619
            },
            "cluster": 1
        }, {
            "cell_id": 9,
            "code": "for data in [train_data, test_data]:\n\n    # Too many missing values\n\n    data.drop(['Cabin'], axis=1, inplace=True)\n\n    # Probably will not provide some useful information\n\n    data.drop(['Ticket', 'Fare'], axis=1, inplace=True)",
            "class": "Data Transform",
            "desc": "This code drops the `Cabin` column due to many missing values and the `Ticket` and `Fare` columns considering them not useful for analysis from both `train_data` and `test_data` DataFrames using the `drop()` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99880826
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "# Find the women and boys\n\nfor data in [train_data, test_data]:\n\n    data['Title'] = data.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\n\n    data['Woman_Or_Boy'] = (data.Title == 'Master') | (data.Sex == 'female')\n\n    data.drop('Title', axis=1, inplace=True)\n\n    data.drop('Name', axis=1, inplace=True)",
            "class": "Data Transform",
            "desc": "This code extracts titles from the `Name` column to determine if passengers are women or boys, adds a new `Woman_Or_Boy` column based on this information and the `Sex` column, and then drops both `Title` and `Name` columns from both `train_data` and `test_data` DataFrames using string splitting and the `drop()` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.793178
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "# Encode 'Sex' and 'Woman_Or_Boy' columns\n\nlabel_encoder = LabelEncoder()\n\nfor data in [train_data, test_data]:\n\n    data['Sex'] = label_encoder.fit_transform(data['Sex'])\n\n    data['Woman_Or_Boy'] = label_encoder.fit_transform(data['Woman_Or_Boy'])",
            "class": "Data Transform",
            "desc": "This code encodes the `Sex` and `Woman_Or_Boy` columns in the `train_data` and `test_data` DataFrames into numerical format using the `LabelEncoder` from sklearn.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9993673
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "# Merge two data to get the average Age and fill the column\n\nall_data = pd.concat([train_data, test_data])\n\naverage = all_data.Age.median()\n\nprint(\"Average Age: {0}\".format(average))\n\nfor data in [train_data, test_data]:\n\n    data.fillna(value={'Age': average}, inplace=True)",
            "class": "Data Transform",
            "desc": "This code concatenates the `train_data` and `test_data` DataFrames to calculate the median age and then fills missing `Age` values in both DataFrames with this median using the `fillna()` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.35051963
            },
            "cluster": 0
        }, {
            "cell_id": 14,
            "code": "# Get the most common Embark and fill the column\n\nmost_common = all_data.Embarked.mode()\n\nprint(\"Most common Embarked value: {0}\".format(most_common[0]))\n\nfor data in [train_data, test_data]:\n\n    data.fillna(value={'Embarked': most_common[0]}, inplace=True)",
            "class": "Data Transform",
            "desc": "This code determines the most common value in the `Embarked` column from the concatenated dataset `all_data` using the `mode()` method and fills missing `Embarked` values in both `train_data` and `test_data` DataFrames with this mode value using the `fillna()` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.59436584
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "# Create categorical variable for traveling alone\n\n# Credits to https://www.kaggle.com/vaishvik25/titanic-eda-fe-3-model-decision-tree-viz\n\nfor data in [train_data, test_data]:\n\n    data['TravelAlone'] = np.where(data[\"SibSp\"] + data[\"Parch\"] > 0, 0, 1)\n\n    data.drop('SibSp', axis=1, inplace=True)\n\n    data.drop('Parch', axis=1, inplace=True)",
            "class": "Data Transform",
            "desc": "This code creates a new binary column `TravelAlone` in both `train_data` and `test_data` DataFrames indicating whether a passenger is traveling alone, based on the sum of `SibSp` and `Parch`, and then drops the `SibSp` and `Parch` columns using the `drop()` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9941294
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "# Encode 'Embarked' column\n\none_hot_encoder = OneHotEncoder(sparse=False)\n\ndef encode_embarked(data):\n\n    encoded = pd.DataFrame(one_hot_encoder.fit_transform(data[['Embarked']]))\n\n    encoded.columns = one_hot_encoder.get_feature_names(['Embarked'])\n\n    data.drop(['Embarked'], axis=1, inplace=True)\n\n    data = data.join(encoded)\n\n    return data\n\ntrain_data = encode_embarked(train_data)\n\ntest_data = encode_embarked(test_data)",
            "class": "Data Transform",
            "desc": "This code encodes the `Embarked` column in both `train_data` and `test_data` DataFrames using one-hot encoding with the `OneHotEncoder` from sklearn, replaces the original `Embarked` column with the new encoded columns, and returns the updated DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99923754
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "# Set X and y\n\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\n\ny = train_data['Survived']\n\ntest_X = test_data.drop(['PassengerId'], axis=1)",
            "class": "Data Transform",
            "desc": "This code separates the feature variables (`X`) and target variable (`y`) for model training from the `train_data` DataFrame by dropping the `Survived` and `PassengerId` columns and prepares the features (`test_X`) from the `test_data` DataFrame by dropping the `PassengerId` column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.99934286
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "train_data",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the contents of the `train_data` DataFrame to the console for initial inspection.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997675
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "train_data.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "This code generates summary statistics for the numerical columns in the `train_data` DataFrame using the `describe()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994442
            },
            "cluster": 5
        }, {
            "cell_id": 4,
            "code": "print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))",
            "class": "Exploratory Data Analysis",
            "desc": "This code prints a list of column names in the `train_data` DataFrame using the `columns` attribute and the `tolist()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.99567986
            },
            "cluster": 5
        }, {
            "cell_id": 5,
            "code": "missing_values = train_data.isna().any()\n\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))",
            "class": "Exploratory Data Analysis",
            "desc": "This code identifies columns in the `train_data` DataFrame that contain missing values by using the `isna().any()` method and then prints the names of those columns.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.92364883
            },
            "cluster": 3
        }, {
            "cell_id": 6,
            "code": "print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()/len(train_data))))\n\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()/len(train_data))))\n\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()/len(train_data))))",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and prints the percentage of missing values in the `Age`, `Cabin`, and `Embarked` columns of the `train_data` DataFrame using the `isna().sum()` method and length of `train_data`.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99853253
            },
            "cluster": 3
        }, {
            "cell_id": 7,
            "code": "duplicates = train_data.duplicated().sum()\n\nprint('Duplicates in train data: {0}'.format(duplicates))",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates the number of duplicate rows in the `train_data` DataFrame using the `duplicated().sum()` method and then prints the result.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.85191345
            },
            "cluster": 3
        }, {
            "cell_id": 8,
            "code": "categorical = train_data.nunique().sort_values(ascending=True)\n\nprint('Categorical variables in train data: \\n{0}'.format(categorical))",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates the number of unique values for each column in the `train_data` DataFrame using the `nunique()` method, sorts them in ascending order, and prints the result to identify categorical variables.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.84571636
            },
            "cluster": 4
        }, {
            "cell_id": 10,
            "code": "train_data.tail()",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the last few rows of the `train_data` DataFrame using the `tail()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997588
            },
            "cluster": 5
        }, {
            "cell_id": 17,
            "code": "train_data.tail()",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the last few rows of the `train_data` DataFrame using the `tail()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997588
            },
            "cluster": 5
        }, {
            "cell_id": 20,
            "code": "print(\"Features: \\n{0} \".format(X.columns.tolist()))",
            "class": "Exploratory Data Analysis",
            "desc": "This code prints a list of feature column names in the `X` DataFrame using the `columns` attribute and the `tolist()` method from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.93314546
            },
            "cluster": 5
        }, {
            "cell_id": 0,
            "code": "import numpy as np\n\nimport pandas as pd\n\n\n\n# Encoders\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\n# Modelling\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\n\n\n\n# Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n# XGBoost\n\nfrom xgboost import XGBClassifier\n\n\n\n# LightGBM\n\nfrom lightgbm import LGBMClassifier\n\n\n\n# Voting Classifier\n\nfrom sklearn.ensemble import VotingClassifier",
            "class": "Imports and Environment",
            "desc": "This code imports various libraries and modules necessary for data manipulation (NumPy, pandas), data encoding (OneHotEncoder, LabelEncoder from sklearn), model training and evaluation (RepeatedStratifiedKFold, train_test_split, GridSearchCV, accuracy_score from sklearn), and machine learning models (RandomForestClassifier from sklearn, XGBClassifier from xgboost, LGBMClassifier from lightgbm, VotingClassifier from sklearn).",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99930537
            },
            "cluster": -1
        }, {
            "cell_id": 24,
            "code": "evaluate_model(best_model_voting.best_estimator_, 'voting')",
            "class": "Model Evaluation",
            "desc": "This code evaluates the best estimator from the `VotingClassifier` by computing its accuracy score on the training data and stores it in the dictionary of best models using the `evaluate_model` function.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.86052555
            },
            "cluster": 1
        }, {
            "cell_id": 19,
            "code": "# To store models created\n\nbest_models = {}\n\n\n\n# Split data\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n\n\ndef print_best_parameters(hyperparameters, best_parameters):\n\n    value = \"Best parameters: \"\n\n    for key in hyperparameters:\n\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n\n    if hyperparameters:\n\n        print(value[:-2])\n\n\n\ndef get_best_model(estimator, hyperparameters):\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters,\n\n                               n_jobs=-1, cv=cv, scoring=\"accuracy\")\n\n    best_model = grid_search.fit(train_X, train_y)\n\n    best_parameters = best_model.best_estimator_.get_params()\n\n    print_best_parameters(hyperparameters, best_parameters)\n\n    return best_model\n\n\n\ndef evaluate_model(model, name):\n\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n\n    best_models[name] = model",
            "class": "Model Training",
            "desc": "This code initializes a dictionary to store the best models, splits the training data into training and validation sets using `train_test_split`, and defines three functions: `print_best_parameters` to print selected hyperparameters, `get_best_model` to perform grid search hyperparameter tuning using `GridSearchCV`, and `evaluate_model` to assess and store the model's accuracy using `accuracy_score`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.8173711
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "# I couldn't find a way to set fit_params of XGBClasssifier through GridSearchCV, so did a little trick.\n\n# https://stackoverflow.com/questions/35545733/how-do-you-use-fit-params-for-randomizedsearch-with-votingclassifier-in-sklearn\n\nclass MyXGBClassifier(XGBClassifier):\n\n    def fit(self, X, y=None):\n\n        return super(XGBClassifier, self).fit(X, y,\n\n                                              verbose=False,\n\n                                              early_stopping_rounds=40,\n\n                                              eval_metric='logloss',\n\n                                              eval_set=[(val_X, val_y)])",
            "class": "Model Training",
            "desc": "This code defines a custom `MyXGBClassifier` class that extends `XGBClassifier` to include specific `fit` method parameters such as `verbose`, `early_stopping_rounds`, `eval_metric`, and `eval_set` for better model training control during fitting.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9982059
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "# Models from, https://www.kaggle.com/sfktrkl/titanic-hyperparameter-tuning-gridsearchcv\n\nrandomForest = RandomForestClassifier(random_state=1, n_estimators=20, max_features='auto',\n\n                                      criterion='gini', max_depth=4, min_samples_split=2,\n\n                                      min_samples_leaf=3)\n\nxgbClassifier = MyXGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor',\n\n                                use_label_encoder=False, learning_rate=0.4, gamma=0.4,\n\n                                max_depth=4, reg_lambda=0, reg_alpha=0.1)\n\nlgbmClassifier = LGBMClassifier(random_state=1, device='gpu', boosting_type='dart',\n\n                                num_leaves=8, learning_rate=0.1, n_estimators=100,\n\n                                reg_alpha=1, reg_lambda=1)\n\n\n\nclassifiers = [\n\n    ('randomForest', randomForest),\n\n    ('xgbClassifier', xgbClassifier),\n\n    ('lgbmClassifier', lgbmClassifier)\n\n]",
            "class": "Model Training",
            "desc": "This code defines three machine learning models (RandomForestClassifier, MyXGBClassifier, and LGBMClassifier) with specific hyperparameters and stores them in a list of tuples named `classifiers` for later use.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9895577
            },
            "cluster": -1
        }, {
            "cell_id": 23,
            "code": "hyperparameters = {\n\n    'n_jobs'  : [-1],\n\n    'voting'  : ['hard', 'soft'],\n\n    'weights' : [(1, 1, 1),\n\n                (2, 1, 1), (1, 2, 1), (1, 1, 2),\n\n                (2, 2, 1), (1, 2, 2), (2, 1, 2),\n\n                (3, 2, 1), (1, 3, 2), (2, 1, 3), (3, 1, 2)]\n\n}\n\nestimator = VotingClassifier(estimators=classifiers)\n\nbest_model_voting = get_best_model(estimator, hyperparameters)",
            "class": "Model Training",
            "desc": "This code defines a set of hyperparameters for a `VotingClassifier`, creates the `VotingClassifier` using the previously defined classifiers, and then finds the best model using the `get_best_model` function with these hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.98003674
            },
            "cluster": 0
        }],
        "notebook_id": 9,
        "notebook_name": "titanic-votingclassifier-with-gridsearchcv.ipynb",
        "user": "titanic-votingclassifier-with-gridsearchcv.ipynb"
    }, {
        "cells": [{
            "cell_id": 45,
            "code": "# Save file\ntitanic_train.to_pickle('./train.clean.pkl')",
            "class": "Data Export",
            "desc": "This code saves the cleaned Titanic training dataset to a pickle file named 'train.clean.pkl' using Pandas' `to_pickle` method.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9738529
            },
            "cluster": -1
        }, {
            "cell_id": 58,
            "code": "titanic_test.to_pickle('./test.clean.pkl')",
            "class": "Data Export",
            "desc": "This code saves the cleaned Titanic test dataset to a pickle file named 'test.clean.pkl' using Pandas' `to_pickle` method.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9730775
            },
            "cluster": -1
        }, {
            "cell_id": 74,
            "code": "submission = pd.DataFrame({'Survived': predictions}, index = test_clean.PassengerId)\nsubmission.to_csv('./submission.csv')",
            "class": "Data Export",
            "desc": "This code creates a DataFrame for the survival predictions, indexed by the `PassengerId`, and saves it to a CSV file named 'submission.csv' using Pandas' `DataFrame` and `to_csv` methods.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99883705
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "# load the train dataset.\ntitanic_train = pd.read_csv(\"../input/titanic/train.csv\")\n\n# summary\nprint('This dataset has {} observations with {} variables each.' .format(titanic_train.shape[0], titanic_train.shape[1]))\n\n# print first 10 observations\ntitanic_train.head(10)",
            "class": "Data Extraction",
            "desc": "This code loads the Titanic training dataset using Pandas' `read_csv` method, prints the dataset's dimensions, and displays the first 10 observations.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99967563
            },
            "cluster": 1
        }, {
            "cell_id": 46,
            "code": "titanic_test = pd.read_csv('../input/titanic/test.csv')",
            "class": "Data Extraction",
            "desc": "This code loads the Titanic test dataset using Pandas' `read_csv` method.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997478
            },
            "cluster": 1
        }, {
            "cell_id": 60,
            "code": "# load clean trainset\ntrain_clean = pd.read_pickle('./train.clean.pkl')\n\n# obtain covariates and dependent variable\ntrain_clean_X = train_clean.loc[:,['Pclass','Sex','Age','Fare','Embarked','Cabin_letter','Age_known','Ticket_number','Family_members']]\ntrain_clean_Y = train_clean.loc[:,'Survived']\n\n# get dummies\ntrain_clean_X = pd.get_dummies(train_clean_X, drop_first=True)",
            "class": "Data Extraction",
            "desc": "This code loads the cleaned Titanic training dataset from a pickle file, separates it into covariates (`train_clean_X`) and the dependent variable (`train_clean_Y`), and converts categorical variables in `train_clean_X` to dummy/indicator variables using Pandas' `get_dummies` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.87908113
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "# Generate a new variable (Cabin_known) which equals one if the cabin of the passenger is known.\ntitanic_train[\"Cabin_known\"] = np.where(titanic_train[\"Cabin\"].isna() == False, 1, 0)\nsurvived_cabinknown = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Cabin_known\"], margins = True)\nsurvived_cabinknown.columns = [\"Unkown\",\"Known\",\"Row total\"]\nsurvived_cabinknown.index= [\"Died\",\"Survived\",\"Column total\"]\nsurvived_cabinknown/survived_cabinknown.loc[\"Column total\"] #gives us relative figures",
            "class": "Data Transform",
            "desc": "This code creates a new binary variable `Cabin_known` in the Titanic training dataset to indicate whether a passenger's cabin number is known, and then generates a cross-tabulation table to show the relationship between passenger survival and whether their cabin was known, using NumPy's `where` and Pandas' `crosstab` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.7482262
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "titanic_train[\"Cabin_letter\"] = titanic_train[\"Cabin\"].str.slice(start=0,stop=1) # Gets the cabin letter\nsurvived_cabinletter = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Cabin_letter\"], margins = True)\nsurvived_cabinletter.columns = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\", \"Row total\"]\nsurvived_cabinletter.index = [\"Died\", \"Survived\", \"Column total\"]\nsurvived_cabinletter # We need the absolute frequencies to see whether the differences are statistically significant",
            "class": "Data Transform",
            "desc": "This code extracts the first letter of the cabin number as a new variable `Cabin_letter` in the Titanic training dataset and creates a cross-tabulation table to show the relationship between passenger survival and their cabin letter, using Pandas' `str.slice` and `crosstab` methods.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.98749954
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "survived_cabinletter/survived_cabinletter.loc[\"Column total\"]",
            "class": "Data Transform",
            "desc": "This code calculates and displays the relative frequencies of passenger survival by cabin letter by normalizing the previously computed cross-tabulation table using the total counts, leveraging Pandas' division operations.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9450264
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "titanic_train.loc[titanic_train['Cabin_letter'].isin([\"A\",\"C\",\"F\",\"G\",\"T\"]), 'Cabin_letter'] = 'known low'\ntitanic_train.loc[titanic_train['Cabin_letter'].isin([\"B\",\"D\",\"E\"]), 'Cabin_letter'] = 'known high'\ntitanic_train.loc[titanic_train['Cabin_letter'].isna(), 'Cabin_letter'] = 'unkown'",
            "class": "Data Transform",
            "desc": "This code groups and renames the cabin letters in the Titanic training dataset into three categories: 'known low', 'known high', and 'unknown' based on specific criteria, using Pandas' `loc` and `isin` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.98631454
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "# drop Cabin_known\ntitanic_train.drop(columns='Cabin', inplace = True)\n# drop Cabin_letter\ntitanic_train.drop(columns='Cabin_known', inplace = True)",
            "class": "Data Transform",
            "desc": "This code removes the `Cabin` and `Cabin_known` columns from the Titanic training dataset using the Pandas `drop` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99904495
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "titanic_train[\"Age_known\"] = np.where(titanic_train[\"Age\"].isna() == False, 1, 0)\nsurvived_ageknown = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Age_known\"])\nsurvived_ageknown",
            "class": "Data Transform",
            "desc": "This code creates a new binary variable `Age_known` in the Titanic training dataset to indicate whether a passenger's age is known, and then generates a cross-tabulation table to show the relationship between passenger survival and whether their age is known, using NumPy's `where` and Pandas' `crosstab` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.72090036
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "# titanic_train[\"Age\"].max() # check max age (80)\nbins = [0, 5, 12, 18, 35, 55, 100]\ntitanic_train[\"Age_group\"] = pd.cut(titanic_train[\"Age\"], bins)\nsurvived_agegroup = pd.crosstab(index = titanic_train[\"Survived\"], \n                           columns = titanic_train[\"Age_group\"])\nsurvived_agegroup",
            "class": "Data Transform",
            "desc": "This code creates an `Age_group` variable in the Titanic training dataset by binning the `Age` variable into specified age ranges, and then generates a cross-tabulation table to show the relationship between passenger survival and their age group, using Pandas' `cut` and `crosstab` methods.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9908936
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "titanic_train[\"Age\"].fillna(round(titanic_train[\"Age\"].mean()), inplace=True)\n\n# drop Age_group\ntitanic_train.drop(columns='Age_group', inplace = True)",
            "class": "Data Transform",
            "desc": "This code fills the missing values in the `Age` column of the Titanic training dataset with the rounded mean age and then removes the `Age_group` column using Pandas' `fillna`, `mean`, `round`, and `drop` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9941328
            },
            "cluster": 0
        }, {
            "cell_id": 14,
            "code": "titanic_train.loc[titanic_train['Embarked'].isna(), 'Embarked'] = 'C'",
            "class": "Data Transform",
            "desc": "This code fills the missing values in the `Embarked` column of the Titanic training dataset with 'C' using Pandas' `loc` and `isna` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9946273
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "# drop 'Name'\ntitanic_train.drop(['Name'], axis=1, inplace=True)\n\n# 'Ticket_letter' = 1 if 'Ticket' starts with a letter\ntitanic_train['Ticket_letter'] = np.where(titanic_train['Ticket'].str.isnumeric() == False, 1, 0)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_letter\"])\nsurvived_Ticketletter",
            "class": "Data Transform",
            "desc": "This code removes the `Name` column, creates a new binary variable `Ticket_letter` to indicate whether the ticket starts with a letter, and generates a cross-tabulation table to show the relationship between passenger survival and whether their ticket includes a letter, using Pandas' `drop`, `str.isnumeric`, and `crosstab` methods, along with NumPy's `where` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.32447293
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "# extract first digit from 'Ticket'\ntitanic_train.loc[titanic_train['Ticket_letter'] == 0, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_train.loc[titanic_train['Ticket_letter'] == 1, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_number\"])\nsurvived_Ticketletter",
            "class": "Data Transform",
            "desc": "This code extracts and categorizes the first digit of the `Ticket` number based on whether the ticket starts with a letter or a digit, stores this information in the `Ticket_number` column, and creates a cross-tabulation table to show the relationship between passenger survival and their ticket's first digit, using Pandas' `loc`, `str.split`, `str.slice`, and `crosstab` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9830366
            },
            "cluster": 0
        }, {
            "cell_id": 20,
            "code": "titanic_train.loc[titanic_train['Ticket'].str.split().str.len() == 3, 'Ticket_number'] = titanic_train['Ticket'][titanic_train['Ticket'].str.split().str.len() == 3].str.split().str[2].str.slice(start=0,stop=1)\n\n# check survival rate\nsurvived_Ticketletter = pd.crosstab(index = titanic_train[\"Survived\"], \n                                columns = titanic_train[\"Ticket_number\"])\nsurvived_Ticketletter",
            "class": "Data Transform",
            "desc": "This code updates the `Ticket_number` column for tickets with three components by extracting the first digit of the third component and creates a cross-tabulation table to show the relationship between passenger survival and the updated ticket numbers, using Pandas' `loc`, `str.split`, `str.len`, and `crosstab` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.6221314
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "# convert 'Ticket_number' into numeric format \ntitanic_train['Ticket_number'] = pd.to_numeric(titanic_train['Ticket_number'])\n\n# check whether 'Ticket_number' == 'Pclass'\ntitanic_train['Pclass_Ticket_equal'] = np.where(titanic_train['Pclass'] == titanic_train['Ticket_number'], 1, 0)\n\nsum(titanic_train['Pclass_Ticket_equal'])/titanic_train.shape[0]",
            "class": "Data Transform",
            "desc": "This code converts the `Ticket_number` column into numeric format, creates a new binary variable `Pclass_Ticket_equal` to indicate whether `Pclass` is equal to `Ticket_number`, and calculates the proportion of rows where `Pclass` is equal to `Ticket_number`, using Pandas' `to_numeric` and `shape` methods, and NumPy's `where` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.60356903
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "# drop 'Ticket', Ticket_letter' and 'Pclass_Ticket_equal'\ntitanic_train.drop(['Ticket','Ticket_letter','Pclass_Ticket_equal'], axis=1, inplace=True)",
            "class": "Data Transform",
            "desc": "This code removes the `Ticket`, `Ticket_letter`, and `Pclass_Ticket_equal` columns from the Titanic training dataset using Pandas' `drop` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9992398
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "sum(titanic_train['Ticket_number'].isna()) # 4 missing values\n\ntitanic_train.loc[np.isnan(titanic_train['Ticket_number']),'Ticket_number'] = 4 ",
            "class": "Data Transform",
            "desc": "This code identifies the number of missing values in the `Ticket_number` column and then fills these missing values with 4 using NumPy's `isnan` function and Pandas' `loc` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.92739946
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "# divide 'Fare' into different groups\nbins = [-1, 10, 30, 70, 550]  # the interval starts in -1 to include passengers with a fare of 0 (workers?)   \ntitanic_train[\"Fare_group\"] = pd.cut(titanic_train[\"Fare\"], bins)\nsurvived_faregroup = pd.crosstab(index=titanic_train[\"Survived\"], \n                           columns=titanic_train[\"Fare_group\"], margins = True)\nsurvived_faregroup",
            "class": "Data Transform",
            "desc": "This code categorizes the `Fare` variable into different fare groups by binning, and then creates a cross-tabulation table to show the relationship between passenger survival and their fare group, using Pandas' `cut` and `crosstab` methods.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9928417
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "titanic_train.drop([\"Fare_group\"], axis = 1, inplace = True)",
            "class": "Data Transform",
            "desc": "This code removes the `Fare_group` column from the Titanic training dataset using Pandas' `drop` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99926406
            },
            "cluster": 0
        }, {
            "cell_id": 38,
            "code": "titanic_train[\"Family_members\"] = titanic_train[\"Family_members\"].map({0:'none', 1:'few', 2:'few', 3:'few', 4:'many', 5:'many', 6:'many', 7:'many', 10:'many'})\n\ntitanic_train.drop(columns=['SibSp','Parch'], inplace = True)",
            "class": "Data Transform",
            "desc": "This code categorizes the `Family_members` variable into 'none', 'few', and 'many', then removes the `SibSp` and `Parch` columns from the Titanic training dataset using Pandas' `map` and `drop` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99915195
            },
            "cluster": 0
        }, {
            "cell_id": 43,
            "code": "titanic_train.Pclass = pd.Categorical(titanic_train.Pclass)\ntitanic_train.Sex = pd.Categorical(titanic_train.Sex)\ntitanic_train.Embarked = pd.Categorical(titanic_train.Embarked)\ntitanic_train.Cabin_letter = pd.Categorical(titanic_train.Cabin_letter)\ntitanic_train.Age_known = pd.Categorical(titanic_train.Age_known)\ntitanic_train.Family_members = pd.Categorical(titanic_train.Family_members)",
            "class": "Data Transform",
            "desc": "This code converts the `Pclass`, `Sex`, `Embarked`, `Cabin_letter`, `Age_known`, and `Family_members` columns to categorical data types using Pandas' `Categorical` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.38450193
            },
            "cluster": 0
        }, {
            "cell_id": 48,
            "code": "titanic_test[\"Cabin_letter\"] = titanic_test[\"Cabin\"].str.slice(start=0,stop=1) # Gets the cabin letter\ntitanic_test.loc[titanic_test['Cabin_letter'].isin([\"A\",\"C\",\"F\",\"G\",\"T\"]), 'Cabin_letter'] = 'known low'\ntitanic_test.loc[titanic_test['Cabin_letter'].isin([\"B\",\"D\",\"E\"]), 'Cabin_letter'] = 'known high'\ntitanic_test.loc[titanic_test['Cabin_letter'].isna(), 'Cabin_letter'] = 'unkown'\n\n# drop Cabin\ntitanic_test.drop(columns='Cabin', inplace = True)",
            "class": "Data Transform",
            "desc": "This code extracts the first letter of the cabin number as a new variable `Cabin_letter`, categorizes this variable into three groups, 'known low', 'known high', and 'unknown', and then removes the `Cabin` column from the Titanic test dataset using Pandas' `str.slice`, `loc`, `isin`, and `drop` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9957533
            },
            "cluster": 0
        }, {
            "cell_id": 49,
            "code": "titanic_test[\"Age_known\"] = np.where(titanic_test[\"Age\"].isna() == False, 1, 0)\n\ntitanic_test[\"Age\"].fillna(round(titanic_test[\"Age\"].mean()), inplace=True)",
            "class": "Data Transform",
            "desc": "This code creates a new binary variable `Age_known` to indicate whether a passenger's age is known, and fills missing values in the `Age` column with the rounded mean age in the Titanic test dataset using NumPy's `where` method and Pandas' `fillna`, `mean`, and `round` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99483806
            },
            "cluster": 0
        }, {
            "cell_id": 50,
            "code": "titanic_test.loc[titanic_test['Embarked'].isna(), 'Embarked'] = 'C'",
            "class": "Data Transform",
            "desc": "This code fills the missing values in the `Embarked` column of the Titanic test dataset with 'C' using Pandas' `loc` and `isna` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9909113
            },
            "cluster": 0
        }, {
            "cell_id": 51,
            "code": "# 'Ticket_letter' = 1 if 'Ticket' starts with a letter\ntitanic_test['Ticket_letter'] = np.where(titanic_test['Ticket'].str.isnumeric() == False, 1, 0)\n\n# extract first digit from 'Ticket'\ntitanic_test.loc[titanic_test['Ticket_letter'] == 0, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_test.loc[titanic_test['Ticket_letter'] == 1, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# extract first digit from 'Ticket'\ntitanic_test.loc[titanic_test['Ticket_letter'] == 0, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 0].str.slice(start=0,stop=1)\ntitanic_test.loc[titanic_test['Ticket_letter'] == 1, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket_letter'] == 1].str.split().str[1].str.slice(start=0,stop=1)\n\n# extract first digit from 'Ticket' in case of three parts ticket number \ntitanic_test.loc[titanic_test['Ticket'].str.split().str.len() == 3, 'Ticket_number'] = titanic_test['Ticket'][titanic_test['Ticket'].str.split().str.len() == 3].str.split().str[2].str.slice(start=0,stop=1)\n\n# convert 'Ticket_number' into numeric format \ntitanic_test['Ticket_number'] = pd.to_numeric(titanic_test['Ticket_number'])",
            "class": "Data Transform",
            "desc": "This code creates a new binary variable `Ticket_letter` to indicate whether the ticket starts with a letter, extracts the first digit of the ticket number based on specific conditions, and converts the `Ticket_number` into numeric format in the Titanic test dataset using NumPy's `where` and Pandas' `str.isnumeric`, `str.slice`, `str.split`, `loc`, and `to_numeric` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99679714
            },
            "cluster": 0
        }, {
            "cell_id": 52,
            "code": "titanic_test.drop(['Name','Ticket','Ticket_letter'],1,inplace=True)",
            "class": "Data Transform",
            "desc": "This code removes the `Name`, `Ticket`, and `Ticket_letter` columns from the Titanic test dataset using Pandas' `drop` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99920505
            },
            "cluster": 0
        }, {
            "cell_id": 54,
            "code": "titanic_test[\"Family_members\"] = titanic_test[\"Family_members\"].map({0:'none', 1:'few', 2:'few', 3:'few', 4:'many', 5:'many', 6:'many', 7:'many', 10:'many'})\n\n# drop 'Parch' and 'SibSp'\ntitanic_test.drop(columns=['SibSp','Parch'], inplace = True)",
            "class": "Data Transform",
            "desc": "This code categorizes the `Family_members` variable into 'none', 'few', and 'many', then removes the `SibSp` and `Parch` columns from the Titanic test dataset using Pandas' `map` and `drop` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99894744
            },
            "cluster": 0
        }, {
            "cell_id": 56,
            "code": "titanic_test.loc[titanic_test['Fare'].isna(),'Fare'] = titanic_test[\"Fare\"].median()",
            "class": "Data Transform",
            "desc": "This code fills the missing values in the `Fare` column of the Titanic test dataset with the median fare using Pandas' `loc`, `isna`, and `median` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9961583
            },
            "cluster": 0
        }, {
            "cell_id": 57,
            "code": "titanic_test.Pclass = pd.Categorical(titanic_test.Pclass)\ntitanic_test.Sex = pd.Categorical(titanic_test.Sex)\ntitanic_test.Embarked = pd.Categorical(titanic_test.Embarked)\ntitanic_test.Cabin_letter = pd.Categorical(titanic_test.Cabin_letter)\ntitanic_test.Age_known = pd.Categorical(titanic_test.Age_known)\ntitanic_test.Family_members = pd.Categorical(titanic_test.Family_members)",
            "class": "Data Transform",
            "desc": "This code converts the `Pclass`, `Sex`, `Embarked`, `Cabin_letter`, `Age_known`, and `Family_members` columns to categorical data types using Pandas' `Categorical` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.7381678
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "#Get the number of missing observations for each variable\ntitanic_train.isna().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "This code computes and displays the number of missing observations for each variable in the Titanic training dataset using Pandas' `isna` and `sum` methods.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9988305
            },
            "cluster": 5
        }, {
            "cell_id": 10,
            "code": "round(titanic_train[\"Age\"].mean())",
            "class": "Exploratory Data Analysis",
            "desc": "This code computes and rounds the mean age of passengers in the Titanic training dataset using Pandas' `mean` and `round` methods.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9984812
            },
            "cluster": 5
        }, {
            "cell_id": 12,
            "code": "titanic_train[titanic_train['Embarked'].isna()]",
            "class": "Exploratory Data Analysis",
            "desc": "This code filters and displays the rows in the Titanic training dataset where the `Embarked` column contains missing values, using Pandas' `isna` method for identification and direct indexing for filtering.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.65819347
            },
            "cluster": 5
        }, {
            "cell_id": 13,
            "code": "survived_embarked = pd.crosstab(index = titanic_train[\"Survived\"], \n                           columns = titanic_train[\"Embarked\"])\nsurvived_embarked",
            "class": "Exploratory Data Analysis",
            "desc": "This code creates a cross-tabulation table to show the relationship between passenger survival and their embarkation port using Pandas' `crosstab` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.98169005
            },
            "cluster": -1
        }, {
            "cell_id": 15,
            "code": "# Have a look at the first rows.\ntitanic_train.head(10)",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first 10 rows of the Titanic training dataset using Pandas' `head` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9996778
            },
            "cluster": 2
        }, {
            "cell_id": 16,
            "code": "#Statistical description of numerical varibles\nprint('\\nStatistical description of dataset:')\ntitanic_train.describe() # It will only show up numerical variables",
            "class": "Exploratory Data Analysis",
            "desc": "This code provides and prints a statistical summary of the numerical variables in the Titanic training dataset using Pandas' `describe` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994048
            },
            "cluster": 1
        }, {
            "cell_id": 19,
            "code": "titanic_train.loc[titanic_train['Ticket_number']=='B']",
            "class": "Exploratory Data Analysis",
            "desc": "This code filters and displays the rows in the Titanic training dataset where the `Ticket_number` is 'B', using Pandas' `loc` method for filtering.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.99537355
            },
            "cluster": 5
        }, {
            "cell_id": 21,
            "code": "titanic_train.head()",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first five rows of the Titanic training dataset using Pandas' `head` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99975556
            },
            "cluster": 2
        }, {
            "cell_id": 25,
            "code": "n_records = titanic_train.shape[0]\nn_survived = titanic_train[titanic_train.Survived==1].shape[0]\npercent_survivied = (n_survived*100)/n_records\n\nprint('Total number of passengers: {}'.format(n_records))\nprint('The number of survivors: {}'.format(n_survived))\nprint('The number of non-survivors: {}'.format(n_records - n_survived))\nprint('Survival probability: {:.2f}%'.format(percent_survivied))\n\nsns.countplot(x='Survived',label='Count',data=titanic_train)",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and prints the total number of passengers, the number of survivors and non-survivors, and the survival probability, and then visualizes the survivor count using Seaborn's `countplot` method.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.97286135
            },
            "cluster": 4
        }, {
            "cell_id": 28,
            "code": "pclass_faregroup = pd.crosstab(index = titanic_train[\"Pclass\"], columns = titanic_train[\"Fare_group\"], margins = True)\npclass_faregroup",
            "class": "Exploratory Data Analysis",
            "desc": "This code creates a cross-tabulation table to show the relationship between passenger class (`Pclass`) and their fare group using Pandas' `crosstab` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.786694
            },
            "cluster": -1
        }, {
            "cell_id": 29,
            "code": "survived_pclass = pd.crosstab(index = titanic_train[\"Survived\"], columns = titanic_train[\"Pclass\"], margins = True)\nsurvived_pclass",
            "class": "Exploratory Data Analysis",
            "desc": "This code creates a cross-tabulation table to show the relationship between passenger survival and their passenger class (`Pclass`) using Pandas' `crosstab` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9923334
            },
            "cluster": -1
        }, {
            "cell_id": 31,
            "code": "titanic_train.groupby('Survived').mean()",
            "class": "Exploratory Data Analysis",
            "desc": "This code computes and displays the mean values of the numerical features in the Titanic training dataset grouped by passenger survival status using Pandas' `groupby` and `mean` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.9952874
            },
            "cluster": 5
        }, {
            "cell_id": 32,
            "code": "titanic_train.drop(['PassengerId'], axis=1, inplace=False).corr()",
            "class": "Exploratory Data Analysis",
            "desc": "This code computes and displays the correlation matrix of the features in the Titanic training dataset, excluding the `PassengerId` column, using Pandas' `drop` and `corr` methods.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.81973666
            },
            "cluster": 5
        }, {
            "cell_id": 34,
            "code": "pd.crosstab(titanic_train.Survived,titanic_train.SibSp)",
            "class": "Exploratory Data Analysis",
            "desc": "This code creates a cross-tabulation table to show the relationship between passenger survival and the number of siblings/spouses aboard (`SibSp`) using Pandas' `crosstab` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99778974
            },
            "cluster": -1
        }, {
            "cell_id": 36,
            "code": "pd.crosstab(titanic_train.Survived,titanic_train.Parch)",
            "class": "Exploratory Data Analysis",
            "desc": "This code creates a cross-tabulation table to show the relationship between passenger survival and the number of parents/children aboard (`Parch`) using Pandas' `crosstab` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9976615
            },
            "cluster": -1
        }, {
            "cell_id": 40,
            "code": "pd.crosstab(titanic_train.Embarked,titanic_train.Survived)",
            "class": "Exploratory Data Analysis",
            "desc": "This code creates a cross-tabulation table to show the relationship between embarkation port (`Embarked`) and passenger survival using Pandas' `crosstab` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99811065
            },
            "cluster": -1
        }, {
            "cell_id": 42,
            "code": "pd.crosstab(titanic_train.Sex,titanic_train.Survived)",
            "class": "Exploratory Data Analysis",
            "desc": "This code creates a cross-tabulation table to show the relationship between passenger sex (`Sex`) and survival using Pandas' `crosstab` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9979779
            },
            "cluster": -1
        }, {
            "cell_id": 44,
            "code": "titanic_train.info()",
            "class": "Exploratory Data Analysis",
            "desc": "This code provides a concise summary of the Titanic training dataset, including the number of non-null entries and data types of each column, using Pandas' `info` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99935824
            },
            "cluster": 0
        }, {
            "cell_id": 47,
            "code": "titanic_test.head()",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first five rows of the Titanic test dataset using Pandas' `head` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997546
            },
            "cluster": 2
        }, {
            "cell_id": 53,
            "code": "titanic_test['Family_members'] = titanic_test['SibSp'] + titanic_test['Parch']\n\ntitanic_test['Family_members'].value_counts()",
            "class": "Exploratory Data Analysis",
            "desc": "This code creates a new variable `Family_members` by summing `SibSp` and `Parch`, and then displays the count of unique values in the `Family_members` column using Pandas' `value_counts` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9993037
            },
            "cluster": 3
        }, {
            "cell_id": 55,
            "code": "titanic_test.isna().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "This code computes and displays the number of missing observations for each variable in the Titanic test dataset using Pandas' `isna` and `sum` methods.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9990232
            },
            "cluster": 5
        }, {
            "cell_id": 61,
            "code": "train_clean_X.head()",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first five rows of the covariates dataset (`train_clean_X`) using Pandas' `head` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997551
            },
            "cluster": 2
        }, {
            "cell_id": 75,
            "code": "submission.value_counts()",
            "class": "Exploratory Data Analysis",
            "desc": "This code counts and displays the occurrences of each unique value in the `submission` DataFrame using Pandas' `value_counts` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.99947196
            },
            "cluster": 5
        }, {
            "cell_id": 0,
            "code": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')",
            "class": "Imports and Environment",
            "desc": "This code imports necessary libraries such as NumPy, Pandas, Seaborn, and Matplotlib for data manipulation, visualization, and suppresses warnings using the warnings library.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9993629
            },
            "cluster": -1
        }, {
            "cell_id": 59,
            "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# LPM and logistic regression\nfrom sklearn import linear_model\n\n# lDA and QDA\nfrom sklearn import discriminant_analysis\n\n# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# grid search crossvalidation\nfrom sklearn.model_selection import GridSearchCV\n\n# decision trees\nfrom sklearn.tree import DecisionTreeClassifier\n\n# bagging\nfrom sklearn.ensemble import BaggingClassifier\n\n# random forests\nfrom sklearn.ensemble import RandomForestClassifier\n\n# gradient boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# neural networks\nfrom sklearn.neural_network import MLPClassifier\n\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# SVM\nfrom sklearn.svm import SVC\n\n# model selection\nfrom sklearn.model_selection import cross_val_score\n\n# voting classifier\nfrom sklearn.ensemble import VotingClassifier",
            "class": "Imports and Environment",
            "desc": "This code imports various libraries and modules required for machine learning tasks such as data preprocessing, model selection, training, and evaluation, including libraries for linear regression, discriminant analysis, KNN, decision trees, bagging, random forests, gradient boosting, neural networks, SVM, and ensemble methods using Scikit-learn.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.999298
            },
            "cluster": -1
        }, {
            "cell_id": 62,
            "code": "# create vector containing classifiers\n\nclassifiers = []\n\n# logistic regression\nclassifiers.append(linear_model.LogisticRegression())\n\n# linear discriminant analysis (LDA)\nclassifiers.append(discriminant_analysis.LinearDiscriminantAnalysis())\n\n# quadratic discriminant analysis (QDA)\nclassifiers.append(discriminant_analysis.QuadraticDiscriminantAnalysis())\n\n# KNN\nclassifiers.append(KNeighborsClassifier())\n\n# decision trees\nclassifiers.append(DecisionTreeClassifier())\n\n# bagging\nclassifiers.append(BaggingClassifier())\n\n# random forests\nclassifiers.append(RandomForestClassifier())\n\n# gradient boosting\nclassifiers.append(GradientBoostingClassifier())\n\n# multilayer perceptron (NN)\nclassifiers.append(MLPClassifier())\n\n# SVM\nclassifiers.append(SVC())\n\n# store results from cross-validation\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier,train_clean_X,train_clean_Y,scoring=\"accuracy\",cv=10,n_jobs=-1))\n\n# compute average performance & std deviation\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())",
            "class": "Model Evaluation",
            "desc": "This code creates a list of classifiers including logistic regression, LDA, QDA, KNN, decision trees, bagging, random forests, gradient boosting, MLP, and SVM, performs cross-validation on each classifier using Scikit-learn's `cross_val_score` method, and stores the mean and standard deviation of accuracy scores for each classifier.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.97773737
            },
            "cluster": 0
        }, {
            "cell_id": 65,
            "code": "logistic.best_estimator_",
            "class": "Model Evaluation",
            "desc": "This code retrieves and displays the best logistic regression model identified during the grid search using Scikit-learn's `best_estimator_` attribute.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.49770755
            },
            "cluster": -1
        }, {
            "cell_id": 67,
            "code": "bagging.best_estimator_",
            "class": "Model Evaluation",
            "desc": "This code retrieves and displays the best Bagging classifier model identified during the grid search using Scikit-learn's `best_estimator_` attribute.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.51450545
            },
            "cluster": -1
        }, {
            "cell_id": 69,
            "code": "forests.best_estimator_",
            "class": "Model Evaluation",
            "desc": "This code retrieves and displays the best Random Forest classifier model identified during the grid search using Scikit-learn's `best_estimator_` attribute.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.48359537
            },
            "cluster": -1
        }, {
            "cell_id": 71,
            "code": "boosting.best_estimator_",
            "class": "Model Evaluation",
            "desc": "This code retrieves and displays the best Gradient Boosting classifier model identified during the grid search using Scikit-learn's `best_estimator_` attribute.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_score",
                "subclass_id": 1,
                "predicted_subclass_probability": 0.43086216
            },
            "cluster": -1
        }, {
            "cell_id": 73,
            "code": "# load test set\ntest_clean = pd.read_pickle('./test.clean.pkl')\n\n# get covariates\ntest_clean_X = test_clean.loc[:,['Pclass','Sex','Age','Fare','Embarked','Cabin_letter','Age_known','Ticket_number','Family_members']]\n\n# get dummies\ntest_clean_X = pd.get_dummies(test_clean_X, drop_first=True)\n\n# predict using test set \npredictions = voting.predict(test_clean_X)",
            "class": "Model Evaluation",
            "desc": "This code loads the cleaned Titanic test dataset, selects the covariates, converts categorical variables to dummy variables, and generates survival predictions using the voting classifier model.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.84637797
            },
            "cluster": 0
        }, {
            "cell_id": 64,
            "code": "# grid parameters\ngrid = {\n    'max_iter' : [100,200,500,1000,2000]\n}\n\n# grid search\nlogistic = GridSearchCV(\n    linear_model.LogisticRegression(),\n    grid,\n    cv=10 \n)\n\n# hyperparameter tuning using crossvalidation\nlogistic = logistic.fit(train_clean_X,train_clean_Y)",
            "class": "Model Training",
            "desc": "This code sets up a grid of parameters for logistic regression, performs a grid search for hyperparameter tuning using cross-validation, and fits the logistic regression model to the Titanic training dataset using Scikit-learn's `GridSearchCV` and `fit` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.9880946
            },
            "cluster": 1
        }, {
            "cell_id": 66,
            "code": "# grid parameters\ngrid = {\n    'n_estimators' : [10,20,50,100,500,1000]\n}\n\n# grid search\nbagging = GridSearchCV(\n    BaggingClassifier(),\n    grid,\n    cv=10 \n)\n\n# hyperparameter tuning using crossvalidation\nbagging = bagging.fit(train_clean_X,train_clean_Y)",
            "class": "Model Training",
            "desc": "This code sets up a grid of parameters for a Bagging classifier, performs a grid search for hyperparameter tuning using cross-validation, and fits the Bagging classifier model to the Titanic training dataset using Scikit-learn's `GridSearchCV` and `fit` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.98861444
            },
            "cluster": 1
        }, {
            "cell_id": 68,
            "code": "grid = {\n    'max_depth' : [3,5,7,10,15,20],\n}\n\n# grid search\nforests = GridSearchCV(\n    RandomForestClassifier(n_estimators=1000,n_jobs=-1),\n    grid,\n    cv=10,    \n    refit=True\n) \n\n# hyperparameter tuning using crossvalidation\nforests.fit(train_clean_X,train_clean_Y)",
            "class": "Model Training",
            "desc": "This code sets up a grid of parameters for a Random Forest classifier, performs a grid search for hyperparameter tuning using cross-validation, and fits the Random Forest classifier model to the Titanic training dataset using Scikit-learn's `GridSearchCV` and `fit` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.99105763
            },
            "cluster": 1
        }, {
            "cell_id": 70,
            "code": "# grid parameters\ngrid = {'loss': ['deviance', 'exponential'],\n        'max_depth': [1, 3, 5, 10]\n}\n\n# grid search\nboosting = GridSearchCV(\n    GradientBoostingClassifier(n_estimators = 1000),\n    grid,\n    cv=10,    \n    refit=True,\n)\n\n# hyperparameter tuning using crossvalidation\nboosting.fit(train_clean_X,train_clean_Y)",
            "class": "Model Training",
            "desc": "This code sets up a grid of parameters for a Gradient Boosting classifier, performs a grid search for hyperparameter tuning using cross-validation, and fits the Gradient Boosting classifier model to the Titanic training dataset using Scikit-learn's `GridSearchCV` and `fit` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.9865433
            },
            "cluster": 1
        }, {
            "cell_id": 72,
            "code": "# voting classifier\nvoting = VotingClassifier(estimators=[('logistic',logistic.best_estimator_),('lda',discriminant_analysis.LinearDiscriminantAnalysis()),\n('bagging',bagging.best_estimator_),('forests',forests.best_estimator_),('boosting',boosting.best_estimator_)],voting='hard',n_jobs=-1)\n\nvoting = voting.fit(train_clean_X,train_clean_Y)",
            "class": "Model Training",
            "desc": "This code creates and trains a voting classifier ensemble using the best models identified for logistic regression, LDA, bagging, random forests, and gradient boosting, and fits it to the Titanic training dataset using Scikit-learn's `VotingClassifier` and `fit` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99491066
            },
            "cluster": 1
        }, {
            "cell_id": 26,
            "code": "#SibSp\nplt.subplot(221)\ntitanic_train.boxplot(column = 'SibSp')\n#Parch\nplt.subplot(222)\ntitanic_train.boxplot(column = 'Parch')\n#Fare\nplt.subplot(223)\ntitanic_train.boxplot(column = 'Fare')",
            "class": "Visualization",
            "desc": "This code generates a series of box plots to visualize the distributions of `SibSp`, `Parch`, and `Fare` variables in the Titanic training dataset using Matplotlib's `subplot` and `boxplot` methods.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9977387
            },
            "cluster": 0
        }, {
            "cell_id": 33,
            "code": "pd.crosstab(titanic_train.Survived,titanic_train.SibSp).plot(kind='bar')",
            "class": "Visualization",
            "desc": "This code generates a bar plot to visualize the relationship between passenger survival and the number of siblings/spouses aboard (`SibSp`) using Pandas' `crosstab` method and the `plot` function with the kind parameter set to 'bar'.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9948726
            },
            "cluster": 0
        }, {
            "cell_id": 35,
            "code": "pd.crosstab(titanic_train.Survived,titanic_train.Parch).plot(kind='bar')",
            "class": "Visualization",
            "desc": "This code generates a bar plot to visualize the relationship between passenger survival and the number of parents/children aboard (`Parch`) using Pandas' `crosstab` method and the `plot` function with the `kind` parameter set to 'bar'.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.994934
            },
            "cluster": 0
        }, {
            "cell_id": 37,
            "code": "titanic_train['Family_members'] = titanic_train['SibSp'] + titanic_train['Parch']\n\npd.crosstab(titanic_train.Survived,titanic_train.Family_members).plot(kind='bar')",
            "class": "Visualization",
            "desc": "This code creates a new variable `Family_members` by summing `SibSp` and `Parch`, and then generates a bar plot to visualize the relationship between passenger survival and the number of family members aboard using Pandas' `crosstab` method and the `plot` function with the `kind` parameter set to 'bar'.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9977151
            },
            "cluster": 0
        }, {
            "cell_id": 39,
            "code": "pd.crosstab(titanic_train.Embarked,titanic_train.Survived).plot(kind='bar')",
            "class": "Visualization",
            "desc": "This code generates a bar plot to visualize the relationship between the embarkation port (`Embarked`) and passenger survival using Pandas' `crosstab` method and the `plot` function with the `kind` parameter set to 'bar'.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.98807496
            },
            "cluster": 0
        }, {
            "cell_id": 41,
            "code": "pd.crosstab(titanic_train.Sex,titanic_train.Survived).plot(kind='bar')",
            "class": "Visualization",
            "desc": "This code generates a bar plot to visualize the relationship between passenger sex (`Sex`) and survival using Pandas' `crosstab` method and the `plot` function with the `kind` parameter set to 'bar'.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99690646
            },
            "cluster": 0
        }, {
            "cell_id": 63,
            "code": "# create dataframe with results\ncv_res = pd.DataFrame({\"average performance\":cv_means,\"standard deviation\": cv_std,\"algorithm\":[\"logistic regression\",\"LDA\",\"QDA\",\n\"KNN\",\"decision trees\",\"bagging\",\"random forests\",\"gradient boosting\",\"NN\",\"SVM\"]})\n\ng = sns.barplot(\"average performance\",\"algorithm\",data = cv_res,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"average performance\")\ng = g.set_title(\"cross validation scores\")",
            "class": "Visualization",
            "desc": "This code creates a dataframe containing the average performance and standard deviation of accuracy scores from cross-validation for various machine learning algorithms, and then visualizes the results using a horizontal bar plot with error bars using Seaborn's `barplot` method.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.8318433
            },
            "cluster": 1
        }],
        "notebook_id": 10,
        "notebook_name": "data-cleansing-exploration-voting-classifier.ipynb",
        "user": "data-cleansing-exploration-voting-classifier.ipynb"
    }, {
        "cells": [{
            "cell_id": 25,
            "code": "# Format Output data & Submit it\n\ntest_data['Survived'] = [ round(x) for x in test_data['Survived']] \nsolution = test_data[['PassengerId', 'Survived']]\nsolution.to_csv(\"Neural_Network_Solution.csv\", index=False)",
            "class": "Data Export",
            "desc": "This code snippet formats the 'Survived' predictions in the test data by rounding them, selects the 'PassengerId' and 'Survived' columns, and exports the result to a CSV file named \"Neural_Network_Solution.csv\".",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99949336
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "# Importing Kaggle's Training Data\n\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.tail()",
            "class": "Data Extraction",
            "desc": "This code snippet imports the training dataset for the Titanic problem from a CSV file using Pandas and displays the last few rows of the dataset.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9996389
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "# Importing Kaggle's Testing Data\n\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.tail()",
            "class": "Data Extraction",
            "desc": "This code snippet imports the testing dataset for the Titanic problem from a CSV file using Pandas and displays the last few rows of the dataset.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9996618
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "# Cleaning and formatting data\n\n# Training Data\ny = train_data[\"Survived\"] #Answers\nfeatures = [\"Pclass\", \"Sex\",\"Embarked\"] # What Columns to include\nX = pd.get_dummies(train_data[features]) # Copying important columns & convert to numbers\n\n# Final Testing Data\nX_final = pd.get_dummies(test_data[features]) # Copying imporant columns & convert to numbers",
            "class": "Data Transform",
            "desc": "This code snippet cleans and formats the training and testing data by selecting specific features, encoding categorical variables using one-hot encoding with Pandas' get_dummies method, and separating the target variable for training.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.95256627
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "#Function to scale data from 0 - 1\n\ndef scale_data(X_train, X_test):\n    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n    \n    # Initialise a new scaling object for normalising input data\n    sc = MinMaxScaler()\n\n    # Set up the scaler just on the training set\n    sc.fit(X_train)\n\n    # Apply the scaler to the training and test sets\n    train_sc = sc.transform(X_train)\n    test_sc = sc.transform(X_test)\n    \n    return train_sc, test_sc",
            "class": "Data Transform",
            "desc": "This code snippet defines a function to scale data between 0 and 1 using the MinMaxScaler from scikit-learn, fitting the scaler on the training data and then applying it to both the training and testing datasets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "normalization",
                "subclass_id": 18,
                "predicted_subclass_probability": 0.99454945
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = 0.25, random_state=42)\n\n# Scale X data\nX_train_sc, X_test_sc = scale_data(X_train, X_test)",
            "class": "Data Transform",
            "desc": "This code snippet splits the data into training and testing sets using scikit-learn's train_test_split function and then scales the feature data of both sets using the previously defined scale_data function.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9981787
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "# Predict Final Data\ntest_data['Survived'] = model.predict(X_final)",
            "class": "Data Transform",
            "desc": "This code snippet predicts the survival probability for the final test data using the trained model and adds a new column 'Survived' to the test_data DataFrame with these predictions.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99416625
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "# Importing needed libraries/modules\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# sklearn for pre-processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# TensorFlow sequential model\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam",
            "class": "Imports and Environment",
            "desc": "This code snippet imports necessary libraries and modules, including Matplotlib, NumPy, Pandas, scikit-learn, and TensorFlow/Keras, for data manipulation, preprocessing, visualization, and neural network modeling.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993278
            },
            "cluster": -1
        }, {
            "cell_id": 7,
            "code": "#Function to calculate accuracy for troubleshooting / optimizing\n\ndef calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n    \"\"\"Calculate and print accuracy of trainign and test data fits\"\"\"    \n    \n    ### Get accuracy of fit to training data\n    probability = model.predict(X_train_sc)\n    y_pred_train = probability >= 0.5\n    y_pred_train = y_pred_train.flatten()\n    accuracy_train = np.mean(y_pred_train == y_train)\n    \n    ### Get accuracy of fit to test data\n    probability = model.predict(X_test_sc)\n    y_pred_test = probability >= 0.5\n    y_pred_test = y_pred_test.flatten()\n    accuracy_test = np.mean(y_pred_test == y_test)\n\n    # Show acuracy\n    print (f'Training accuracy {accuracy_train:0.3f}')\n    print (f'Test accuracy {accuracy_test:0.3f}')",
            "class": "Model Evaluation",
            "desc": "This code snippet defines a function to calculate and print the accuracy of a given neural network model on both training and testing data by predicting probabilities, converting them to binary outcomes, and comparing them to the actual target values.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.7046628
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)",
            "class": "Model Evaluation",
            "desc": "This code snippet calculates and prints the accuracy of the neural network model on both the training and testing data using the previously defined calculate_accuracy function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99758875
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)",
            "class": "Model Evaluation",
            "desc": "This code snippet calculates and prints the accuracy of the less complex neural network model on both the training and testing data using the previously defined calculate_accuracy function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99758875
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)",
            "class": "Model Evaluation",
            "desc": "This code snippet calculates and prints the accuracy of the neural network model, which was trained for fewer epochs, on both the training and testing data using the previously defined calculate_accuracy function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99758875
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)",
            "class": "Model Evaluation",
            "desc": "This code snippet calculates and prints the accuracy of the neural network model with dropout on both the training and testing data using the previously defined calculate_accuracy function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99758875
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)",
            "class": "Model Evaluation",
            "desc": "This code snippet calculates and prints the accuracy of the final neural network model, trained with specified hyperparameters and callbacks, on both the training and testing data using the previously defined calculate_accuracy function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99758875
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "#Function to make neural network\n\ndef make_net(number_features, \n             hidden_layers=3, \n             hidden_layer_neurones=128, \n             dropout=0.0, \n             learning_rate=0.001):\n    \n    \"\"\"Make TensorFlow neural net\"\"\"\n    \n    # Clear Tensorflow \n    K.clear_session()\n    \n    # Set up neural net\n    net = Sequential()\n    \n    # Add hidden hidden_layers using a loop\n    for i in range(hidden_layers):\n        # Add fully connected layer with ReLu activation\n        net.add(Dense(\n            hidden_layer_neurones, \n            input_dim=number_features,\n            activation='leaky_relu'))\n        # Add droput layer\n        net.add(Dropout(dropout))\n    \n    # Add final sigmoid activation output\n    net.add(Dense(1, activation='sigmoid'))    \n    \n    # Compiling model\n    opt = Adam(learning_rate=learning_rate)\n    \n    net.compile(loss='binary_crossentropy', \n                optimizer=opt, \n                metrics=['accuracy'])\n    \n    return net",
            "class": "Model Training",
            "desc": "This code snippet defines a function to create and compile a TensorFlow Sequential neural network with a specified number of hidden layers, neurons per layer, dropout rate, and learning rate, utilizing ReLU activations, dropout regularization, and an Adam optimizer.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.99296665
            },
            "cluster": 1
        }, {
            "cell_id": 9,
            "code": "# Base Neural Network with default Paramters\n\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)",
            "class": "Model Training",
            "desc": "This code snippet creates a neural network with default parameters using the make_net function, and then trains it on the scaled training data for 250 epochs with a batch size of 64, also validating on the scaled testing data.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9984475
            },
            "cluster": 1
        }, {
            "cell_id": 12,
            "code": "# Neural Network with less complexity (less hidden layer neurons)\n\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features,\n                hidden_layers=1, # changed from 3 layer default\n                hidden_layer_neurones=32) # changed from 128 neuron default\n\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)",
            "class": "Model Training",
            "desc": "This code snippet creates a neural network with reduced complexity\u2014having only one hidden layer with 32 neurons\u2014and trains it on the scaled training data for 250 epochs with a batch size of 64, also validating on the scaled testing data.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99484044
            },
            "cluster": 1
        }, {
            "cell_id": 15,
            "code": "# Neural Network with less training time (less epochs)\n\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=25,# changed from 250 \n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)",
            "class": "Model Training",
            "desc": "This code snippet creates a neural network with the original parameters and trains it on the scaled training data for only 25 epochs\u2014instead of the previous 250 epochs\u2014with a batch size of 64, also validating on the scaled testing data.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99849296
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "# Neural Network with Dropout (only training % of neurons at once)\n\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features,\n                dropout=0.5)\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=20,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)",
            "class": "Model Training",
            "desc": "This code snippet creates a neural network with dropout implemented at a rate of 50%, which drops 50% of the neurons during training, and trains it on the scaled training data for 20 epochs with a batch size of 64, also validating on the scaled testing data.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9987827
            },
            "cluster": 1
        }, {
            "cell_id": 21,
            "code": "#Final Neural Network \n\n# Define save checkpoint callback (only save if new best validation results)\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.h5', save_best_only=True)\n\n# Define early stopping callback\n# Stop when no validation improvement for 25 epochs\n# Restore weights to best validation accuracy\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=25, restore_best_weights=True)\n\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(\n    number_features,\n    hidden_layers=8,\n    hidden_layer_neurones=8,\n    dropout=0.2)\n\n\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=50,\n                    batch_size=32,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0, \n                    callbacks=[checkpoint_cb, early_stopping_cb])",
            "class": "Model Training",
            "desc": "This code snippet creates a neural network with 8 hidden layers, each with 8 neurons and 20% dropout, and trains it on the scaled training data for up to 50 epochs with a batch size of 32, utilizing callbacks for model checkpointing and early stopping based on validation performance.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.70758486
            },
            "cluster": 1
        }, {
            "cell_id": 8,
            "code": "#Function to Visualize Accuracy\n\ndef plot_training(history_dict):\n    acc_values = history_dict['accuracy']\n    val_acc_values = history_dict['val_accuracy']\n    epochs = range(1, len(acc_values) + 1)\n\n    plt.plot(epochs, acc_values, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.show()",
            "class": "Visualization",
            "desc": "This code snippet defines a function to visualize the training and validation accuracy of a model over epochs using Matplotlib, plotting the accuracy values against the number of epochs.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9965463
            },
            "cluster": -1
        }, {
            "cell_id": 11,
            "code": "# Plot accuracy\nplot_training(history.history)",
            "class": "Visualization",
            "desc": "This code snippet plots the training and validation accuracy over epochs using the history object of the trained model and the previously defined plot_training function.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9935103
            },
            "cluster": -1
        }, {
            "cell_id": 14,
            "code": "# Plot accuracy\nplot_training(history.history)",
            "class": "Visualization",
            "desc": "This code snippet plots the training and validation accuracy over epochs for the less complex neural network using the history object of the trained model and the previously defined plot_training function.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9935103
            },
            "cluster": -1
        }, {
            "cell_id": 17,
            "code": "# Plot accuracy\nplot_training(history.history)",
            "class": "Visualization",
            "desc": "This code snippet plots the training and validation accuracy over epochs for the neural network trained with fewer epochs using the history object of the trained model and the previously defined plot_training function.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9935103
            },
            "cluster": -1
        }, {
            "cell_id": 20,
            "code": "# Plot accuracy\nplot_training(history.history)",
            "class": "Visualization",
            "desc": "This code snippet plots the training and validation accuracy over epochs for the neural network with dropout using the history object of the trained model and the previously defined plot_training function.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9935103
            },
            "cluster": -1
        }, {
            "cell_id": 23,
            "code": "# Plot accuracy\nplot_training(history.history)",
            "class": "Visualization",
            "desc": "This code snippet plots the training and validation accuracy over epochs for the final neural network using the history object of the trained model and the previously defined plot_training function.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9935103
            },
            "cluster": -1
        }],
        "notebook_id": 11,
        "notebook_name": "final-neural-network-titanic-survival-challenge.ipynb",
        "user": "final-neural-network-titanic-survival-challenge.ipynb"
    }, {
        "cells": [{
            "cell_id": 1,
            "code": "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()",
            "class": "Data Extraction",
            "desc": "This code snippet reads the train dataset from a CSV file located in the specified path using Pandas and displays the first few rows of the dataset.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.999642
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()",
            "class": "Data Extraction",
            "desc": "This code snippet reads the test dataset from a CSV file located in the specified path using Pandas and displays the first few rows of the dataset.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99966204
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "print(\"Before\", train_data.shape, test_data.shape)\n\ntrain_data = train_data.drop(['Ticket', 'Cabin'], axis=1)\ntest_data = test_data.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_data, test_data]\n\n\"After\", train_data.shape, test_data.shape",
            "class": "Data Transform",
            "desc": "This code snippet drops the 'Ticket' and 'Cabin' columns from both the training and test datasets, and prints the shapes of the datasets before and after this operation.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.33762202
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "combine = [train_data, test_data]",
            "class": "Data Transform",
            "desc": "This code snippet creates a list containing both the training and test datasets for combined operations.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9982835
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])",
            "class": "Data Transform",
            "desc": "This code snippet extracts titles from the 'Name' column for each dataset in the combined list and creates a new 'Title' column, followed by generating a cross-tabulation of 'Title' against 'Sex' using Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.94240606
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()",
            "class": "Data Transform",
            "desc": "This code snippet standardizes and consolidates rare titles into a single 'Rare' category, renames some titles to more common equivalents, and then calculates the mean survival rate for each title group in the training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.6105874
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()",
            "class": "Data Transform",
            "desc": "This code snippet maps the titles to numerical values using a predefined dictionary and fills any missing values in the 'Title' column with 0, then displays the first few rows of the training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99322236
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "train_data = train_data.drop(['Name', 'PassengerId'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.shape, test_data.shape",
            "class": "Data Transform",
            "desc": "This code snippet drops the 'Name' and 'PassengerId' columns from the training dataset and the 'Name' column from the test dataset, then updates the combined list with the modified datasets and displays their shapes.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.5906634
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "#train_data['Sex'].fillNA(-1)\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_data.head()",
            "class": "Data Transform",
            "desc": "This code snippet maps the 'Sex' column to numerical values, where 'female' is mapped to 1 and 'male' to 0, for both datasets in the combined list and displays the first few rows of the training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.97668195
            },
            "cluster": 0
        }, {
            "cell_id": 14,
            "code": "guess_ages = np.zeros((2,3))\nguess_ages",
            "class": "Data Transform",
            "desc": "This code snippet initializes a 2x3 NumPy array filled with zeros, presumably to be used for storing guessed ages later on.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.4285954
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_data.head()",
            "class": "Data Transform",
            "desc": "This code snippet estimates missing 'Age' values by calculating the median age for different combinations of 'Sex' and 'Pclass', fills the missing values with these estimates, and converts the 'Age' column to integers, followed by displaying the first few rows of the training dataset.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.19565275
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_data.head()",
            "class": "Data Transform",
            "desc": "This code snippet converts the 'Age' column into ordinal categories based on predefined age ranges and updates these values for both the training and test datasets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.998941
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "train_data = train_data.drop(['AgeBand'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()",
            "class": "Data Transform",
            "desc": "This code snippet removes the 'AgeBand' column from the training dataset, updates the combined list with the modified dataset, and displays the first few rows of the updated training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9964252
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
            "class": "Data Transform",
            "desc": "This code snippet creates a new 'FamilySize' column by summing the 'SibSp' and 'Parch' columns and adding 1 for each dataset in the combined list, then calculates and displays the mean survival rate for each family size.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9927946
            },
            "cluster": 0
        }, {
            "cell_id": 20,
            "code": "for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()",
            "class": "Data Transform",
            "desc": "This code snippet creates a new 'IsAlone' column indicating whether a passenger is traveling alone and then calculates and displays the mean survival rate based on this column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9402816
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "train_data = train_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_data = test_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_data, test_data]\n\ntrain_data.head()",
            "class": "Data Transform",
            "desc": "This code snippet drops the 'Parch', 'SibSp', and 'FamilySize' columns from both the training and test datasets, updates the combined list with these modified datasets, and displays the first few rows of the updated training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.98982376
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_data.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)",
            "class": "Data Transform",
            "desc": "This code snippet creates a new feature 'Age*Class' by multiplying the 'Age' and 'Pclass' columns for each dataset in the combined list and displays the first 10 rows of these columns in the training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99864763
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the survival rate of female passengers in the training dataset by filtering rows where the 'Sex' is 'female' and computing the ratio of survived females.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.45923898
            },
            "cluster": 4
        }, {
            "cell_id": 4,
            "code": "men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the survival rate of male passengers in the training dataset by filtering rows where the 'Sex' is 'male' and computing the ratio of survived males.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.59011316
            },
            "cluster": 4
        }, {
            "cell_id": 8,
            "code": "print(train_data.columns.values)",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet prints the column names of the training dataset to provide an overview of the features available.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.9982193
            },
            "cluster": 1
        }, {
            "cell_id": 16,
            "code": "train_data['AgeBand'] = pd.cut(train_data['Age'], 5)\ntrain_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet creates an 'AgeBand' categorical column by dividing 'Age' into five equal intervals and then calculates and displays the mean survival rate for each age band.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.881014
            },
            "cluster": 4
        }, {
            "cell_id": 21,
            "code": "train_data.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet generates and displays summary statistics for the numerical features in the training dataset using the Pandas `describe` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994442
            },
            "cluster": 1
        }, {
            "cell_id": 22,
            "code": "train_data.shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet retrieves and displays the dimensions (number of rows and columns) of the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9995814
            },
            "cluster": 5
        }, {
            "cell_id": 25,
            "code": "train_data.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet generates and displays summary statistics for the numerical features in the training dataset using the Pandas `describe` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994442
            },
            "cluster": 1
        }, {
            "cell_id": 26,
            "code": "train_data.head()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet displays the first few rows of the training dataset to provide an overview of its structure and contents.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997532
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "This code snippet imports essential Python libraries, such as NumPy for linear algebra and Pandas for data processing, and lists all files in the Kaggle input directory using the os module.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.99921954
            },
            "cluster": -1
        }, {
            "cell_id": 5,
            "code": "from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Model Training",
            "desc": "This code snippet prepares the dataset by encoding categorical features using Pandas' get_dummies, trains a RandomForestClassifier from scikit-learn on the training data, makes predictions on the test data, and saves the results to a CSV file.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9991411
            },
            "cluster": -1
        }, {
            "cell_id": 27,
            "code": "y = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Title\", \"IsAlone\", \"Age*Class\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Model Training",
            "desc": "This code snippet selects relevant features, applies one-hot encoding using Pandas' get_dummies, trains a RandomForestClassifier from scikit-learn on the training data, makes predictions on the test data, and saves the results to a CSV file.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9991677
            },
            "cluster": -1
        }, {
            "cell_id": 28,
            "code": "from sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n#model = SVC()\n#svc.fit(X,y)\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Title\", \"IsAlone\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n#model = DecisionTreeClassifier()\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Model Training",
            "desc": "This code snippet selects relevant features, applies one-hot encoding using Pandas' get_dummies, trains a RandomForestClassifier from scikit-learn on the training data, makes predictions on the test data, and saves the results to a CSV file, with placeholders for SVM and DecisionTreeClassifier models as alternatives.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9964826
            },
            "cluster": -1
        }],
        "notebook_id": 12,
        "notebook_name": "notebookda32490206.ipynb",
        "user": "notebookda32490206.ipynb"
    }, {
        "cells": [{
            "cell_id": 29,
            "code": "predictions = rfc1.predict(test_data[features])\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission3.csv', index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Data Export",
            "desc": "The code snippet predicts the 'Survived' column for the test dataset using the trained `RandomForestClassifier`, creates a DataFrame with 'PassengerId' and predicted 'Survived' values, and then exports this DataFrame to a CSV file named 'my_submission3.csv' using pandas' `to_csv` method.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9990941
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')",
            "class": "Data Extraction",
            "desc": "The code snippet reads the training and testing datasets from CSV files into pandas DataFrames.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997478
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "train_data['female'] = pd.get_dummies(train_data['Sex'])['female']\ntest_data['female'] = pd.get_dummies(test_data['Sex'])['female']",
            "class": "Data Transform",
            "desc": "The code snippet creates a new column 'female' in both the training and testing datasets, encoding the 'Sex' column using one-hot encoding from the pandas `get_dummies` method to indicate whether a passenger is female.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99779177
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "sum(train_data['Age'].isnull())\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())",
            "class": "Data Transform",
            "desc": "The code snippet calculates the number of missing values in the 'Age' column of the training dataset and then fills the missing 'Age' values in both training and testing datasets with the mean age using pandas' `fillna` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.97469246
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "sum(test_data.Pclass.isna())",
            "class": "Data Transform",
            "desc": "The code snippet calculates the number of missing values in the 'Pclass' column of the testing dataset using the `isna` and `sum` methods from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99908054
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "train_data['class1'] = pd.get_dummies(train_data.Pclass)[1]\ntest_data['class1'] = pd.get_dummies(test_data.Pclass)[1]\ntrain_data['class2'] = pd.get_dummies(train_data.Pclass)[2]\ntest_data['class2'] = pd.get_dummies(test_data.Pclass)[2]",
            "class": "Data Transform",
            "desc": "The code snippet creates new columns 'class1' and 'class2' in both the training and testing datasets by applying one-hot encoding on the 'Pclass' column using pandas' `get_dummies` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9973912
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "sum(test_data.SibSp.isna())",
            "class": "Data Transform",
            "desc": "The code snippet calculates the number of missing values in the 'SibSp' column of the testing dataset using the `isna` and `sum` methods from pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9991391
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "sibs = train_data.loc[train_data.SibSp <= 1]['Survived']\nprint(sum(sibs)/len(sibs))\ntrain_data['many_sibs'] = (train_data.SibSp > 1)*1\ntest_data['many_sibs'] = (test_data.SibSp > 1)*1",
            "class": "Data Transform",
            "desc": "The code snippet calculates and prints the survival rate of passengers with one or fewer siblings/spouses (`SibSp`), and creates a new binary column 'many_sibs' in both the training and testing datasets to indicate whether a passenger has more than one sibling/spouse on board.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9980178
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "bins = [0.42, 15, 30, 50,80]\ntrain_data['bin_age'] = pd.cut(x=train_data.Age, bins=bins)\ntest_data['bin_age'] = pd.cut(x=test_data.Age, bins=bins)",
            "class": "Data Transform",
            "desc": "The code snippet creates a new column 'bin_age' in both the training and testing datasets by binning the 'Age' column into specified intervals using the `cut` function from pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.5303526
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "train_data['young'] = pd.get_dummies(train_data.bin_age).iloc[:,0]\ntest_data['young'] = pd.get_dummies(test_data.bin_age).iloc[:,0]\ntrain_data['senior'] = pd.get_dummies(train_data.bin_age).iloc[:,3]\ntest_data['senior'] = pd.get_dummies(test_data.bin_age).iloc[:,3]",
            "class": "Data Transform",
            "desc": "The code snippet creates new binary columns 'young' and 'senior' in both the training and testing datasets by performing one-hot encoding on the 'bin_age' column using pandas' `get_dummies` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99718374
            },
            "cluster": 0
        }, {
            "cell_id": 25,
            "code": "test_data.Fare = test_data.Fare.fillna(test_data.Fare.mean())",
            "class": "Data Transform",
            "desc": "The code snippet fills the missing values in the 'Fare' column of the testing dataset with the mean fare using pandas' `fillna` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.27125782
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "train_data.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet displays the first five rows of the training dataset using the `head()` method from the pandas library.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997532
            },
            "cluster": 2
        }, {
            "cell_id": 3,
            "code": "women = train_data.loc[train_data.Sex == 'female']['Survived']\nprint('Women survived',sum(women)/len(women))\n\nmen = train_data.loc[train_data.Sex == 'male']['Survived']\nprint('Men survived',sum(men)/len(men))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and prints the survival rate of women and men in the training dataset by filtering based on the 'Sex' column and computing the mean of the 'Survived' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.29674977
            },
            "cluster": 4
        }, {
            "cell_id": 7,
            "code": "high_fare = train_data.loc[train_data.Fare > 100]['Survived']\nprint('High fare survivors',sum(high_fare)/len(high_fare))\nlow_fare = train_data.loc[train_data.Fare < 32]['Survived']\nprint('High fare survivors',sum(low_fare)/len(low_fare))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and prints the survival rate of passengers with high fares (greater than 100) and low fares (less than 32) in the training dataset by filtering based on the 'Fare' column and computing the mean of the 'Survived' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.97302115
            },
            "cluster": 4
        }, {
            "cell_id": 8,
            "code": "pclass1 = train_data.loc[train_data.Pclass == 1]['Survived']\nprint('Class1',sum(pclass1)/len(pclass1))\npclass2 = train_data.loc[train_data.Pclass == 2]['Survived']\nprint('Class2',sum(pclass2)/len(pclass2))\npclass3 = train_data.loc[train_data.Pclass == 3]['Survived']\nprint('Class3',sum(pclass3)/len(pclass3))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and prints the survival rates for passengers in the first, second, and third classes in the training dataset by filtering based on the 'Pclass' column and computing the mean of the 'Survived' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.92158645
            },
            "cluster": 4
        }, {
            "cell_id": 13,
            "code": "young = train_data.loc[train_data.Age <= 15]['Survived']\nprint(sum(young)/len(young))\n\nold = train_data.loc[train_data.Age >=40]['Survived']\nprint(sum(old)/len(old))",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and prints the survival rates for young passengers (aged 15 or below) and older passengers (aged 40 or above) in the training dataset by filtering based on the 'Age' column and computing the mean of the 'Survived' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.9821845
            },
            "cluster": 4
        }, {
            "cell_id": 16,
            "code": "train_data.corr()['Survived']\n\nfeatures = ['Pclass', 'Fare', 'female', 'class1', 'class2', 'many_sibs', 'young', 'senior']",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet calculates and displays the correlation of all columns with the 'Survived' column in the training dataset using the `corr` method from pandas, and then defines a list of selected feature columns for further analysis or model training.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99760026
            },
            "cluster": 4
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "The code snippet imports essential libraries such as NumPy, pandas, Matplotlib, and seaborn for data manipulation and visualization, and lists all files in the input directory using the os module.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.999241
            },
            "cluster": -1
        }, {
            "cell_id": 17,
            "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix",
            "class": "Imports and Environment",
            "desc": "The code snippet imports the `train_test_split` function from `sklearn.model_selection`, the `LogisticRegression` class from `sklearn.linear_model`, and the `accuracy_score` and `confusion_matrix` functions from `sklearn.metrics` to be used for model training and evaluation.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99930274
            },
            "cluster": -1
        }, {
            "cell_id": 20,
            "code": "accuracy_score(y_pred, y_test)",
            "class": "Model Evaluation",
            "desc": "The code snippet computes and returns the accuracy score of the logistic regression model's predictions by comparing `y_pred` with the actual test labels `y_test` using scikit-learn's `accuracy_score` function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99797374
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "confusion_matrix(y_pred, y_test)",
            "class": "Model Evaluation",
            "desc": "The code snippet computes and returns the confusion matrix of the logistic regression model's predictions, comparing `y_pred` with the actual test labels `y_test` using scikit-learn's `confusion_matrix` function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9979062
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "accuracy_score(y_pred, y_test)",
            "class": "Model Evaluation",
            "desc": "The code snippet computes and returns the accuracy score of the RandomForest classifier's predictions by comparing `y_pred` with the actual test labels `y_test` using scikit-learn's `accuracy_score` function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99797374
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "confusion_matrix(y_pred, y_test)",
            "class": "Model Evaluation",
            "desc": "The code snippet computes and returns the confusion matrix of the RandomForest classifier's predictions, comparing `y_pred` with the actual test labels `y_test` using scikit-learn's `confusion_matrix` function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9979062
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "X = train_data[features]\ny = train_data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)",
            "class": "Model Training",
            "desc": "The code snippet splits the training dataset into training and testing sets using the `train_test_split` function from scikit-learn, with a test size of 33% and a random state of 0 for reproducibility.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9982145
            },
            "cluster": 1
        }, {
            "cell_id": 19,
            "code": "log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\ny_pred",
            "class": "Model Training",
            "desc": "The code snippet creates an instance of the `LogisticRegression` model, fits it to the training data (`X_train` and `y_train`), and then predicts the target variable (`y_pred`) for the test data (`X_test`) using scikit-learn's `fit` and `predict` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.7683394
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)",
            "class": "Model Training",
            "desc": "The code snippet creates an instance of the `RandomForestClassifier` with specified hyperparameters, fits it on the training data (`X_train` and `y_train`), and then predicts the target variable (`y_pred`) for the test data (`X_test`) using scikit-learn's `fit` and `predict` methods.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.977549
            },
            "cluster": 0
        }, {
            "cell_id": 26,
            "code": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n#CV_rfc = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)\n#CV_rfc.fit(X_train, y_train)",
            "class": "Model Training",
            "desc": "The code snippet sets up hyperparameter tuning for a `RandomForestClassifier` using `GridSearchCV` with a specified parameter grid and cross-validation, though the actual fitting is commented out.",
            "testing": {
                "class": "Model_Train",
                "subclass": "define_search_space",
                "subclass_id": 5,
                "predicted_subclass_probability": 0.85772324
            },
            "cluster": 1
        }, {
            "cell_id": 27,
            "code": "#CV_rfc.best_params_",
            "class": "Model Training",
            "desc": "The code snippet, if uncommented, would display the best hyperparameters found for the `RandomForestClassifier` using `GridSearchCV` after fitting the model.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.9968646
            },
            "cluster": 1
        }, {
            "cell_id": 28,
            "code": "rfc1=RandomForestClassifier(random_state=42, max_features='log2', n_estimators= 200, max_depth=6, criterion='entropy')\nrfc1.fit(X_train, y_train)",
            "class": "Model Training",
            "desc": "The code snippet creates an instance of the `RandomForestClassifier` with specific hyperparameters (`random_state=42`, `max_features='log2'`, `n_estimators=200`, `max_depth=6`, `criterion='entropy'`) and fits it on the training data (`X_train` and `y_train`) using scikit-learn's `fit` method.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996785
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "plt.subplot(1,2,1)\nsns.histplot(train_data.Age)\nplt.subplot(1,2,2)\nsns.histplot(test_data.Age)",
            "class": "Visualization",
            "desc": "The code snippet creates two side-by-side histogram plots using seaborn's `histplot` function to visualize the distribution of ages in the training and testing datasets.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99291444
            },
            "cluster": 0
        }],
        "notebook_id": 13,
        "notebook_name": "notebookbac9aa939b.ipynb",
        "user": "notebookbac9aa939b.ipynb"
    }, {
        "cells": [{
            "cell_id": 22,
            "code": "data_target.to_csv('my_submission.csv',index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Data Export",
            "desc": "This code saves the combined DataFrame containing the 'PassengerId' and predicted 'Survived' values to a CSV file named 'my_submission.csv' and prints a confirmation message.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99907625
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "data=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndata.head(10)",
            "class": "Data Extraction",
            "desc": "This code reads the Titanic dataset from a CSV file using pandas and displays the first 10 rows of the data.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99953437
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest.head(10)",
            "class": "Data Extraction",
            "desc": "This code reads the test dataset from a CSV file using pandas and displays the first 10 rows of the data.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9996283
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "data.drop([\"PassengerId\",\"Cabin\",\"Name\",\"Ticket\"],inplace=True,axis=1)\ndata['Age']=data['Age'].fillna(data['Age'].median())\ndata['Embarked']=data['Embarked'].fillna(data['Embarked'].mode()[0])\ndata['Fare'][data['Fare']>400]",
            "class": "Data Transform",
            "desc": "This code drops specified columns ('PassengerId', 'Cabin', 'Name', 'Ticket'), fills missing 'Age' values with the median, fills missing 'Embarked' values with the mode, and selects 'Fare' values greater than 400 using pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.8541413
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "data[\"Sex\"]=data[\"Sex\"].map({\"female\":0,\"male\":1})\ndata=pd.get_dummies(data,drop_first=True)\ndata.head(10)",
            "class": "Data Transform",
            "desc": "This code maps the 'Sex' column to numerical values and converts categorical variables into dummy/indicator variables with pandas, then displays the first 10 rows of the transformed dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9988757
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "data.drop([\"Fare\",\"Embarked_Q\",\"SibSp\"],inplace=True,axis=1)\ndata.head()",
            "class": "Data Transform",
            "desc": "This code removes the specified columns ('Fare', 'Embarked_Q', 'SibSp') from the dataset using pandas and then displays the first few rows of the modified dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9991449
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "mm_scale=MinMaxScaler()\ndata_scaled=pd.DataFrame(mm_scale.fit_transform(data),columns=data.columns)\ndata_scaled.head()",
            "class": "Data Transform",
            "desc": "This code uses the MinMaxScaler from sklearn to scale all features in the dataset to a range of 0 to 1 and converts the scaled data back into a pandas DataFrame with the original column names, then displays the first few rows.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "normalization",
                "subclass_id": 18,
                "predicted_subclass_probability": 0.97623605
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "yScaled=data[\"Survived\"]\nxScaled=data_scaled.drop(\"Survived\",axis=1)\nxScaled.head()",
            "class": "Data Transform",
            "desc": "This code separates the target variable 'Survived' from the feature variables in the scaled dataset and displays the first few rows of the feature variables.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.99836594
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "xc.drop(\"Parch\",inplace=True,axis=1)\nxc.drop(\"const\",inplace=True,axis=1)",
            "class": "Data Transform",
            "desc": "This code removes the 'Parch' and 'const' columns from the dataset using pandas' drop method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9991861
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "passengerId=test['PassengerId']\ntest.drop([\"PassengerId\",\"Name\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\"],inplace=True,axis=1)\ntest.head()",
            "class": "Data Transform",
            "desc": "This code removes the specified columns ('PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin') from the test dataset using pandas and then displays the first few rows of the modified dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99917966
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "test['Sex']=test['Sex'].map({\"female\":0,\"male\":1})\ntest=pd.get_dummies(test,drop_first=True)\ntest.head()",
            "class": "Data Transform",
            "desc": "This code maps the 'Sex' column to numerical values and converts categorical variables into dummy/indicator variables with pandas, then displays the first few rows of the transformed test dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9989386
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "test.drop(\"Embarked_Q\",inplace=True,axis=1)\ntest[\"Age\"]=test[\"Age\"].fillna(test[\"Age\"].median())\ntest_scaled=pd.DataFrame(mm_scale.fit_transform(test),columns=test.columns)\ntest_scaled.head()",
            "class": "Data Transform",
            "desc": "This code removes the 'Embarked_Q' column, fills missing 'Age' values with the median, and scales all features of the test set to a range of 0 to 1 using MinMaxScaler, then displays the first few rows of the scaled test dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "normalization",
                "subclass_id": 18,
                "predicted_subclass_probability": 0.9365268
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "pred_data=pd.DataFrame(pred,columns=['Survived'])\ndata_target=pd.concat([passengerId,pred_data],axis=1)\ndata_target",
            "class": "Data Transform",
            "desc": "This code creates a DataFrame with the predicted 'Survived' values and the 'PassengerId' from the test dataset, then concatenates these into a single DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.99849606
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "data.isnull().mean()",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates the mean of missing values for each column in the dataset using pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9989335
            },
            "cluster": 7
        }, {
            "cell_id": 11,
            "code": "vif=pd.DataFrame()\nvif['features']=xScaled.columns\nvif['VIF']=[variance_inflation_factor(xScaled.values,i) for i in range(xScaled.shape[1])]\nvif['VIF']=round(vif['VIF'],3)\nvif=vif.sort_values(by=\"VIF\",ascending=False)\nvif",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates the Variance Inflation Factor (VIF) for each feature in the scaled dataset using statsmodels to check for multicollinearity and displays the results sorted in descending order.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.28141785
            },
            "cluster": 8
        }, {
            "cell_id": 0,
            "code": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom xgboost import XGBClassifier\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))",
            "class": "Imports and Environment",
            "desc": "This code imports necessary libraries such as numpy, pandas, matplotlib, sklearn, statsmodels, xgboost, and seaborn, and walks through input directory files using os to set up the environment.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.9992461
            },
            "cluster": -1
        }, {
            "cell_id": 12,
            "code": "xc=sm.add_constant(xScaled)\nlm=sm.OLS(yScaled,xc).fit()\nprint(lm.summary())",
            "class": "Model Evaluation",
            "desc": "This code adds a constant term to the feature variables, fits an Ordinary Least Squares (OLS) regression model using statsmodels, and prints the summary report of the model's performance and statistics.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.6782264
            },
            "cluster": 0
        }, {
            "cell_id": 20,
            "code": "pred=model.predict(test_scaled)\npred",
            "class": "Model Evaluation",
            "desc": "This code uses the previously trained XGBoost Classifier to predict the target variable 'Survived' for the test dataset.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9937383
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "lm=LinearRegression()\nlm.fit(xScaled,yScaled)\nrfe=RFE(lm,4)\nrfe=rfe.fit(xScaled,yScaled)\nlist(zip(xScaled.columns,rfe.support_,rfe.ranking_))",
            "class": "Model Training",
            "desc": "This code initializes a Linear Regression model using sklearn, fits it to the scaled feature and target variables, applies Recursive Feature Elimination (RFE) to select the top 4 features, and outputs the support and ranking of each feature.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.83992904
            },
            "cluster": -1
        }, {
            "cell_id": 15,
            "code": "model=XGBClassifier(learning_rate=0.05, max_depth=4, n_estimators=81, nthread=-1, scale_pos_weight=1, random_state=14)\nmodel.fit(xc,yScaled)",
            "class": "Model Training",
            "desc": "This code initializes an XGBoost Classifier with specified hyperparameters and fits it to the feature variables and target variable 'Survived'.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99963725
            },
            "cluster": 2
        }, {
            "cell_id": 5,
            "code": "sns.pairplot(data,vars=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Sex\",\"Embarked_Q\",\"Embarked_S\"],kind=\"reg\",hue=\"Survived\")",
            "class": "Visualization",
            "desc": "This code creates a pairplot using seaborn to visualize the relationships between specified variables in the dataset, colored by the 'Survived' column.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99358493
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "correlation=data.corr()\nplt.figure(figsize=(14,12),dpi=80)\nsns.heatmap(correlation,annot=True)",
            "class": "Visualization",
            "desc": "This code computes the correlation matrix of the dataset and plots a heatmap using seaborn and matplotlib to visualize the correlations between variables.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.9986854
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "sns.countplot(yScaled)",
            "class": "Visualization",
            "desc": "This code generates a count plot using seaborn to visualize the distribution of the target variable 'Survived'.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99077016
            },
            "cluster": 0
        }],
        "notebook_id": 14,
        "notebook_name": "titanic-with-xgboost.ipynb",
        "user": "titanic-with-xgboost.ipynb"
    }, {
        "cells": [{
            "cell_id": 29,
            "code": "y_pred = svc.predict(test_df)\n\nsubmission = pd.DataFrame({ 'PassengerId': test_dfn['PassengerId'],\n\n                            'Survived': y_pred })\n\nsubmission.to_csv(\"submission.csv\", index=False)",
            "class": "Data Export",
            "desc": "This code uses the trained SVC model to make predictions on the test data, creates a submission dataframe with 'PassengerId' and predicted 'Survived' values, and exports it to a CSV file named \"submission.csv\" using pandas' `to_csv` method.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99937505
            },
            "cluster": -1
        }, {
            "cell_id": 30,
            "code": "submission.head()",
            "class": "Data Export",
            "desc": "This code displays the first few rows of the submission dataframe using pandas\u2019 `head` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99974185
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "'''load training dataset'''\n\ntrain_df = pd.read_csv('../input/titanic/train.csv')\n\ntest_df = pd.read_csv('../input/titanic/test.csv')\n\nsubdf = pd.read_csv('../input/titanic/gender_submission.csv')",
            "class": "Data Extraction",
            "desc": "This code loads the training, test, and submission datasets from CSV files using pandas' `read_csv` method. ",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997547
            },
            "cluster": 1
        }, {
            "cell_id": 2,
            "code": "sub_df = subdf.drop(['PassengerId'],axis =1)\n\ntest_df =pd.concat([test_df,sub_df],axis = 1)\n\ndf = pd.concat([train_df,test_df],axis =0)\n\ndf.head()",
            "class": "Data Transform",
            "desc": "This code merges the submission dataframe (after dropping 'PassengerId') with the test dataframe, concatenates the training and modified test dataframes into a single dataframe, and displays the first few rows using pandas' `head` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.9992582
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "df = df[['PassengerId','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Survived']]\n\ndf.head()",
            "class": "Data Transform",
            "desc": "This code reorders the dataframe columns to a specified order and displays the first few rows using pandas' `head` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.78551346
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "lis=['Cabin','Name']\n\ndf= df.drop(lis ,axis=1)\n\ndf.head()",
            "class": "Data Transform",
            "desc": "This code drops the 'Cabin' and 'Name' columns from the dataframe and displays the first few rows using pandas' `head` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9992561
            },
            "cluster": 0
        }, {
            "cell_id": 14,
            "code": "cleaning = df.drop(['Survived'],axis = 1)\n\nSurvived = df['Survived']\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnumeric_cols = cleaning.select_dtypes(include=numerics)\n\nnumeric_cols = numeric_cols.fillna(numeric_cols.mean())",
            "class": "Data Transform",
            "desc": "This code separates the 'Survived' column from the dataframe, selects only the numeric columns, and fills any missing values in those numeric columns with their respective mean values.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.6854556
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "categorical = ['object']\n\ncategorical_cols = cleaning.select_dtypes(include=categorical)\n\ncategorical_cols = categorical_cols.fillna('none')\n\ncategorical_cols = pd.get_dummies(categorical_cols )",
            "class": "Data Transform",
            "desc": "This code selects the categorical columns from the dataframe, fills any missing values with 'none', and converts the categorical columns into dummy/indicator variables using one-hot encoding with pandas' `get_dummies` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99940133
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "cleaned = pd.concat([numeric_cols,categorical_cols],axis= 1)\n\ndf = pd.concat([cleaned,Survived],axis = 1)\n\ndf.head()",
            "class": "Data Transform",
            "desc": "This code concatenates the numeric and one-hot encoded categorical columns into a single dataframe, then adds the 'Survived' column back to this dataframe, and displays the first few rows using pandas' `head` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.9990748
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "test_dfn = df.iloc[ 891 : ,:-1]\n\ntest_df = df.iloc[ 891 : ,:-1].values\n\ntest_dfn",
            "class": "Data Transform",
            "desc": "This code extracts the test data (excluding the 'Survived' column) from the dataframe for rows after the 891st index, storing it in `test_dfn` as a dataframe and `test_df` as a numpy array.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9969296
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "test_df",
            "class": "Data Transform",
            "desc": "This code displays the numpy array representation of the test data extracted previously, which excludes the 'Survived' column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99976057
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "X = df.iloc[:,:-1].values\n\ny = df['Survived'].values",
            "class": "Data Transform",
            "desc": "This code separates the feature variables (X) and the target variable (y) from the dataframe, with X containing all columns except 'Survived' and y containing the 'Survived' column values.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.9993187
            },
            "cluster": 0
        }, {
            "cell_id": 20,
            "code": "scl = MinMaxScaler(feature_range = (0, 1))\n\nX = scl.fit_transform(X) \n\ntest_df = scl.fit_transform(test_df) ",
            "class": "Data Transform",
            "desc": "This code normalizes the feature variables (X) and test data (test_df) to a range between 0 and 1 using Scikit-Learn's `MinMaxScaler`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "normalization",
                "subclass_id": 18,
                "predicted_subclass_probability": 0.9974043
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "X_train ,X_test ,y_train ,y_test = train_test_split(X, y , test_size = 0.3, random_state = 44)",
            "class": "Data Transform",
            "desc": "This code splits the feature variables (X) and target variable (y) into training and testing sets, with 30% of the data allocated for testing, using Scikit-Learn's `train_test_split` function.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9983181
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "list(df.columns.values)",
            "class": "Exploratory Data Analysis",
            "desc": "This code lists the column names of the dataframe using the pandas' `columns.values` attribute.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.9981401
            },
            "cluster": 7
        }, {
            "cell_id": 5,
            "code": "df.shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code retrieves the dimensions of the dataframe (number of rows and columns) using pandas' `shape` attribute.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9995491
            },
            "cluster": 5
        }, {
            "cell_id": 6,
            "code": "df.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "This code generates descriptive statistics for the dataframe using the pandas' `describe` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994438
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "print(df.isnull().sum())\n\nsns.heatmap(df.isnull(),cbar=False, cmap='viridis')",
            "class": "Exploratory Data Analysis",
            "desc": "This code prints the count of missing values for each column in the dataframe and visualizes the locations of these missing values using a heatmap with Seaborn.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.99889094
            },
            "cluster": 6
        }, {
            "cell_id": 8,
            "code": "df['Survived'].value_counts()\n\nsns.countplot(df['Survived'])",
            "class": "Exploratory Data Analysis",
            "desc": "This code computes the count of each unique value in the 'Survived' column and visualizes the counts using a Seaborn count plot.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9774469
            },
            "cluster": 4
        }, {
            "cell_id": 9,
            "code": "corr_matrix = df.corr()\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True) \n\nsns.heatmap(corr_matrix, annot=None ,cmap=cmap)",
            "class": "Exploratory Data Analysis",
            "desc": "This code computes the correlation matrix of the dataframe and visualizes it using a diverging palette heatmap with Seaborn.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.99885654
            },
            "cluster": 6
        }, {
            "cell_id": 10,
            "code": "corr_matrix.nlargest(5, 'Survived')['Survived'].index",
            "class": "Exploratory Data Analysis",
            "desc": "This code identifies the top five features that have the highest positive correlation with the 'Survived' column in the correlation matrix.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.5149967
            },
            "cluster": -1
        }, {
            "cell_id": 13,
            "code": "for i in df[['Age' ,'Pclass' ,'Fare','Parch']] :\n\n    print(i,'&','Survived')\n\n    df.hist(column=i, by='Survived')",
            "class": "Exploratory Data Analysis",
            "desc": "This code generates histograms for the 'Age', 'Pclass', 'Fare', and 'Parch' columns, grouped by the 'Survived' column.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9965384
            },
            "cluster": 4
        }, {
            "cell_id": 0,
            "code": "import numpy as np \n\nimport pandas as pd\n\nimport seaborn as sns \n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.metrics import classification_report\n\nimport warnings \n\nwarnings.filterwarnings('ignore')",
            "class": "Imports and Environment",
            "desc": "This code imports various libraries and modules such as NumPy, Pandas, Seaborn, Matplotlib, and several Scikit-Learn utilities for preprocessing, model training, and evaluation, and also suppresses warnings.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9989849
            },
            "cluster": -1
        }, {
            "cell_id": 26,
            "code": "#v1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=1 , C = 0.5 , tol = 0.001)\n\n#v2 = RandomForestClassifier(n_estimators=100, max_depth= 5,random_state=144)\n\n#v3 = SVC()\n\n#eclf = VotingClassifier(estimators=[('lr', v1), ('rf', v2), ('gnb', v3)],voting='hard')\n\n\n\n#for clf, label in zip([v1, v2, v3, eclf], ['Logistic Regression', 'Random Forest', 'SVC ', 'Ensemble ']): \n\n #   scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n\n  #  print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))",
            "class": "Model Evaluation",
            "desc": "This code (commented out) sets up individual classifiers (Logistic Regression, Random Forest, and SVC) as well as an ensemble Voting Classifier, and evaluates their accuracy using 5-fold cross-validation, printing the mean and standard deviation of the accuracy scores for each classifier.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.936291
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "svc = SVC()\n\nsvc = svc.fit(X_train,y_train)\n\ny_train_pred = svc.predict(X_train)\n\ny_test_pred = svc.predict(X_test)\n\nprint('train score :',accuracy_score(y_train ,y_train_pred ))\n\nprint('test score :',accuracy_score(y_test , y_test_pred))\n\nprint('con matrix :',confusion_matrix(y_test, y_test_pred))\n\nprint('report :',classification_report(y_test, y_test_pred ))\n\ncon = confusion_matrix(y_test,y_test_pred)\n\nhmap =sns.heatmap(con,annot=True,fmt=\"d\")\n\nprint ('Confusion Matrix',hmap)",
            "class": "Model Evaluation",
            "desc": "This code initializes a Support Vector Classifier (SVC), trains it on the training data, makes predictions on both the training and testing sets, and then prints the accuracy scores, confusion matrix, and classification report for the predictions, along with visualizing the confusion matrix using Seaborn.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.37717277
            },
            "cluster": 0
        }, {
            "cell_id": 28,
            "code": "labels = np.arange(2)\n\nclf_report = classification_report(y_test,y_test_pred,labels=labels,target_names=('died','survived'), output_dict=True)\n\nhmap1 = sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\n\nprint ('Classification Report',hmap1)",
            "class": "Model Evaluation",
            "desc": "This code generates a detailed classification report for the SVC predictions, formats it as a dictionary with labels, converts it into a DataFrame, and visualizes it using Seaborn's heatmap.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.9841629
            },
            "cluster": -1
        }, {
            "cell_id": 22,
            "code": "#lgm = LogisticRegression()\n\n#lgm = lgm.fit(X_train,y_train)\n\n#y_tpred = lgm.predict(X_train)\n\n#y_pred = lgm.predict(X_test)\n\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n\n#print('test score :',accuracy_score(y_test , y_pred))\n\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n\n#print('report :',classification_report(y_test, y_pred ))\n\n                                                                                      # SCORE : 0.85",
            "class": "Model Training",
            "desc": "This code (commented out) initializes a Logistic Regression model, trains it on the training data, makes predictions on both the training and testing sets, and then prints the accuracy score, confusion matrix, and classification report for the predictions.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.89561415
            },
            "cluster": -1
        }, {
            "cell_id": 23,
            "code": "# rnc = RandomForestClassifier(n_estimators=100, max_depth=4,random_state=0)\n\n# rnc = rnc.fit(X_train,y_train)\n\n# y_tpred = rnc.predict(X_train)\n\n# y_pred = rnc.predict(X_test)\n\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n\n# print('test score :',accuracy_score(y_test , y_pred))\n\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n\n# print('report :',classification_report(y_test, y_pred ))\n\n                                                                                     # SCORE : 0.87",
            "class": "Model Training",
            "desc": "This code (commented out) initializes a Random Forest Classifier with 100 estimators and a maximum depth of 4, trains it on the training data, makes predictions on both the training and testing sets, and then prints the accuracy score, confusion matrix, and classification report for the predictions.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.8540831
            },
            "cluster": -1
        }, {
            "cell_id": 24,
            "code": "# gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5,max_depth=3, random_state=140)\n\n# gbc = gbc.fit(X_train,y_train)\n\n# y_tpred = gbc.predict(X_train)\n\n# y_pred = gbc.predict(X_test)\n\n# print('train score :',accuracy_score(y_train ,y_tpred ))\n\n# print('test score :',accuracy_score(y_test , y_pred))\n\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n\n# print('report :',classification_report(y_test, y_pred ))\n\n                                                                                   # SCORE : 0.85",
            "class": "Model Training",
            "desc": "This code (commented out) initializes a Gradient Boosting Classifier with 100 estimators, a learning rate of 0.5, and a maximum depth of 3, trains it on the training data, makes predictions on both the training and testing sets, and then prints the accuracy score, confusion matrix, and classification report for the predictions.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.96510017
            },
            "cluster": -1
        }, {
            "cell_id": 25,
            "code": "# svc = SVC()\n\n# svc = svc.fit(X_train,y_train)\n\n# y_tpred = svc.predict(X_train)\n\n# y_pred = svc.predict(X_test)\n\n# print('train score :',accuracy_score(y_train ,y_tpred))\n\n# print('test score :',accuracy_score(y_test , y_pred))\n\n# print('con matrix :',confusion_matrix(y_test, y_pred))\n\n# print('report :',classification_report(y_test, y_pred ))\n\n                                                                                   # SCORE : 0.86",
            "class": "Model Training",
            "desc": "This code (commented out) initializes a Support Vector Classifier (SVC), trains it on the training data, makes predictions on both the training and testing sets, and then prints the accuracy score, confusion matrix, and classification report for the predictions.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.761462
            },
            "cluster": -1
        }, {
            "cell_id": 12,
            "code": "sns.set_style('whitegrid')\n\nsns.countplot(x=df['Survived'],hue=df['Sex'],data=df)",
            "class": "Visualization",
            "desc": "This code sets the Seaborn plot style to 'whitegrid' and generates a count plot of the 'Survived' column with a hue based on 'Sex'.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99833965
            },
            "cluster": 0
        }],
        "notebook_id": 15,
        "notebook_name": "titanic-disaster-88-simple-explanation.ipynb",
        "user": "titanic-disaster-88-simple-explanation.ipynb"
    }, {
        "cells": [{
            "cell_id": 29,
            "code": "adaSub = pd.DataFrame({'PassengerId': PassengerId, 'Survived':t_pred })\nadaSub.head()",
            "class": "Data Export",
            "desc": "The code snippet creates a new DataFrame named `adaSub` with 'PassengerId' and the predicted 'Survived' values, and displays the first few rows of this DataFrame.",
            "testing": {
                "class": "Data_Export",
                "subclass": "prepare_output",
                "subclass_id": 55,
                "predicted_subclass_probability": 0.9886446
            },
            "cluster": -1
        }, {
            "cell_id": 30,
            "code": "adaSub.to_csv(\"1_Ada_Submission.csv\", index = False)",
            "class": "Data Export",
            "desc": "The code snippet exports the `adaSub` DataFrame to a CSV file named \"1_Ada_Submission.csv\" without including the DataFrame's index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992505
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train = pd.read_csv(\"../input/train.csv\")\ntrain.head()",
            "class": "Data Extraction",
            "desc": "The code snippet reads a CSV file named 'train.csv' from the specified directory into a pandas DataFrame and displays the first few rows.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9871219
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "test = pd.read_csv(\"../input/test.csv\")\ntest.head()",
            "class": "Data Extraction",
            "desc": "The code snippet reads a CSV file named 'test.csv' from the specified directory into a pandas DataFrame and displays the first few rows.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9992644
            },
            "cluster": 0
        }, {
            "cell_id": 28,
            "code": "PassengerId = all_test['PassengerId']",
            "class": "Data Extraction",
            "desc": "The code snippet extracts the 'PassengerId' column from the 'all_test' DataFrame into the `PassengerId` variable.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9945315
            },
            "cluster": -1
        }, {
            "cell_id": 5,
            "code": "all = pd.concat([train, test], sort = False)\nall.info()",
            "class": "Data Transform",
            "desc": "The code snippet concatenates the 'train' and 'test' DataFrames into a single DataFrame named 'all' and displays a concise summary of this combined DataFrame using the `info()` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.60922176
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "#Fill Missing numbers with median\nall['Age'] = all['Age'].fillna(value=all['Age'].median())\nall['Fare'] = all['Fare'].fillna(value=all['Fare'].median())",
            "class": "Data Transform",
            "desc": "The code snippet fills missing values in the 'Age' and 'Fare' columns of the 'all' DataFrame with their respective median values using the `fillna()` method of pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.72388726
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "all['Embarked'] = all['Embarked'].fillna('S')\nall.info()",
            "class": "Data Transform",
            "desc": "The code snippet fills missing values in the 'Embarked' column of the 'all' DataFrame with the value 'S' and displays a concise summary of this updated DataFrame using the `info()` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9427343
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "#Age\nall.loc[ all['Age'] <= 16, 'Age'] = 0\nall.loc[(all['Age'] > 16) & (all['Age'] <= 32), 'Age'] = 1\nall.loc[(all['Age'] > 32) & (all['Age'] <= 48), 'Age'] = 2\nall.loc[(all['Age'] > 48) & (all['Age'] <= 64), 'Age'] = 3\nall.loc[ all['Age'] > 64, 'Age'] = 4 ",
            "class": "Data Transform",
            "desc": "The code snippet categorizes the 'Age' column in the 'all' DataFrame into discrete bins by assigning integer labels to different age ranges using the `loc` method of pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9980605
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "#Title\nimport re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+\\.)', name)\n    \n    if title_search:\n        return title_search.group(1)\n    return \"\"",
            "class": "Data Transform",
            "desc": "The code snippet defines a function `get_title` that uses regular expressions to search and extract titles (e.g., 'Mr.', 'Mrs.') from the 'name' attribute in a string.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.961291
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "all['Title'] = all['Name'].apply(get_title)\nall['Title'].value_counts()",
            "class": "Data Transform",
            "desc": "The code snippet creates a new column 'Title' in the 'all' DataFrame by applying the `get_title` function to the 'Name' column, and then displays the count of each unique title using the `value_counts()` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9991881
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "all['Title'] = all['Title'].replace(['Capt.', 'Dr.', 'Major.', 'Rev.'], 'Officer.')\nall['Title'] = all['Title'].replace(['Lady.', 'Countess.', 'Don.', 'Sir.', 'Jonkheer.', 'Dona.'], 'Royal.')\nall['Title'] = all['Title'].replace(['Mlle.', 'Ms.'], 'Miss.')\nall['Title'] = all['Title'].replace(['Mme.'], 'Mrs.')\nall['Title'].value_counts()",
            "class": "Data Transform",
            "desc": "The code snippet replaces various specific titles in the 'Title' column of the 'all' DataFrame with more generalized group titles (e.g., 'Officer.', 'Royal.', 'Miss.', 'Mrs.') and then displays the count of each unique generalized title using the `value_counts()` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.97918713
            },
            "cluster": 0
        }, {
            "cell_id": 14,
            "code": "#Cabin\nall['Cabin'] = all['Cabin'].fillna('Missing')\nall['Cabin'] = all['Cabin'].str[0]\nall['Cabin'].value_counts()",
            "class": "Data Transform",
            "desc": "The code snippet fills missing values in the 'Cabin' column with the string 'Missing', extracts the first character from each cabin string, and then displays the count of each unique character using the `value_counts()` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9994407
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "#Family Size & Alone \nall['Family_Size'] = all['SibSp'] + all['Parch'] + 1\nall['IsAlone'] = 0\nall.loc[all['Family_Size']==1, 'IsAlone'] = 1\nall.head()",
            "class": "Data Transform",
            "desc": "The code snippet creates two new columns in the 'all' DataFrame: 'Family_Size' calculated by summing the 'SibSp' and 'Parch' columns plus one, and 'IsAlone' which is set to 1 if 'Family_Size' equals one, and displays the first few rows of the DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9981501
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "#Drop unwanted variables\nall_1 = all.drop(['Name', 'Ticket'], axis = 1)\nall_1.head()",
            "class": "Data Transform",
            "desc": "The code snippet drops the 'Name' and 'Ticket' columns from the 'all' DataFrame to create a new DataFrame named 'all_1' and displays the first few rows of this new DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99922025
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "all_dummies = pd.get_dummies(all_1, drop_first = True)\nall_dummies.head()",
            "class": "Data Transform",
            "desc": "The code snippet converts categorical variables in the 'all_1' DataFrame into dummy/indicator variables using one-hot encoding with the `get_dummies()` method and drops the first category level to avoid multicollinearity, creating a new DataFrame named 'all_dummies' and displaying its first few rows.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99894685
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "all_train = all_dummies[all_dummies['Survived'].notna()]\nall_train.info()",
            "class": "Data Transform",
            "desc": "The code snippet filters the 'all_dummies' DataFrame to create 'all_train', containing only rows with non-null values in the 'Survived' column, and displays a concise summary of this new DataFrame using the `info()` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.7295958
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "all_test = all_dummies[all_dummies['Survived'].isna()]\nall_test.info()",
            "class": "Data Transform",
            "desc": "The code snippet filters the 'all_dummies' DataFrame to create 'all_test', containing only rows with null values in the 'Survived' column, and displays a concise summary of this new DataFrame using the `info()` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.7275288
            },
            "cluster": 0
        }, {
            "cell_id": 26,
            "code": "TestForPred = all_test.drop(['PassengerId', 'Survived'], axis = 1)",
            "class": "Data Transform",
            "desc": "The code snippet creates a new DataFrame `TestForPred` by dropping the 'PassengerId' and 'Survived' columns from the 'all_test' DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99855393
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "train.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet uses the `info()` method of the pandas DataFrame to display a concise summary of the 'train' dataset, including the number of non-null entries, column names, data types, and memory usage.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993617
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "test.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet uses the `info()` method of the pandas DataFrame to display a concise summary of the 'test' dataset, including the number of non-null entries, column names, data types, and memory usage.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993563
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "all.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code snippet uses the `info()` method of the pandas DataFrame to display a concise summary of the 'all' dataset after filling in missing values, including the number of non-null entries, column names, data types, and memory usage.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993235
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))",
            "class": "Imports and Environment",
            "desc": "The code snippet imports essential libraries including NumPy, pandas, Matplotlib, and Seaborn for data manipulation, visualization, and plotting, and lists the files in the specified directory.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.999067
            },
            "cluster": -1
        }, {
            "cell_id": 21,
            "code": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier",
            "class": "Imports and Environment",
            "desc": "The code snippet imports the `AdaBoostClassifier` from scikit-learn's ensemble module and the `DecisionTreeClassifier` from scikit-learn's tree module for use in building and evaluating a machine learning model.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9992812
            },
            "cluster": -1
        }, {
            "cell_id": 23,
            "code": "predictions = ada.predict(X_test)",
            "class": "Model Evaluation",
            "desc": "The code snippet generates predictions for the testing dataset `X_test` using the previously trained AdaBoost classifier (`ada`).",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99481404
            },
            "cluster": -1
        }, {
            "cell_id": 24,
            "code": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))",
            "class": "Model Evaluation",
            "desc": "The code snippet imports the `classification_report` function from scikit-learn's metrics module and prints a classification report, comparing the true labels `y_test` with the predicted labels `predictions`.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9934075
            },
            "cluster": 0
        }, {
            "cell_id": 25,
            "code": "print (f'Train Accuracy - : {ada.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {ada.score(X_test,y_test):.3f}')",
            "class": "Model Evaluation",
            "desc": "The code snippet prints the training and testing accuracy of the AdaBoost classifier (`ada`) by evaluating its performance on `X_train`/`y_train` and `X_test`/`y_test` respectively, using the `score` method.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99685717
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "t_pred = ada.predict(TestForPred).astype(int)",
            "class": "Model Evaluation",
            "desc": "The code snippet generates predictions for the `TestForPred` dataset using the trained AdaBoost classifier (`ada`) and converts the predicted values to integers.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.94950974
            },
            "cluster": -1
        }, {
            "cell_id": 20,
            "code": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(all_train.drop(['PassengerId','Survived'],axis=1), \n                                                    all_train['Survived'], test_size=0.30, \n                                                    random_state=101, stratify = all_train['Survived'])",
            "class": "Model Training",
            "desc": "The code snippet uses scikit-learn's `train_test_split` function to split the 'all_train' DataFrame into training and testing datasets, extracting features by dropping 'PassengerId' and 'Survived' columns for `X` and using the 'Survived' column as `y`, with 30% of the data allocated for testing and ensuring stratified sampling based on the 'Survived' column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.99786854
            },
            "cluster": 1
        }, {
            "cell_id": 22,
            "code": "ada = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=100, random_state=0)\nada.fit(X_train,y_train)",
            "class": "Model Training",
            "desc": "The code snippet initializes an AdaBoost classifier with a DecisionTreeClassifier as the base estimator, setting `n_estimators` to 100 and `random_state` to 0, and fits the model to the training data `X_train` and `y_train`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996433
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "sns.catplot(x = 'Embarked', kind = 'count', data = all) #or all['Embarked'].value_counts()",
            "class": "Visualization",
            "desc": "The code snippet uses Seaborn's `catplot` function to create a count plot showing the number of entries for each unique value in the 'Embarked' column of the 'all' DataFrame.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9461969
            },
            "cluster": 0
        }],
        "notebook_id": 16,
        "notebook_name": "titanic-basic-solution-using-adaboost.ipynb",
        "user": "titanic-basic-solution-using-adaboost.ipynb"
    }, {
        "cells": [{
            "cell_id": 8,
            "code": "submission = pd.read_csv(\"../input/titanic/gender_submission.csv\")\n\nsubmission.Survived = voting_clf.predict(X_test)\n\nsubmission.to_csv(\"submission.csv\",index=False)\n\nsubmission.head()",
            "class": "Data Export",
            "desc": "The code predicts the target variable \"Survived\" using the trained voting classifier on the test dataset, updates the submission DataFrame, and exports the results to a CSV file named \"submission.csv\" using pandas.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992285
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train = pd.read_csv(\"../input/titanic/train.csv\")\n\ntest = pd.read_csv('../input/titanic/test.csv')",
            "class": "Data Extraction",
            "desc": "The code reads in the train and test datasets for the Titanic dataset from CSV files using pandas' `read_csv()` function.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997502
            },
            "cluster": 1
        }, {
            "cell_id": 2,
            "code": "#1. delete unnecessary columns\n\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp','Parch']\n\ntrain = train.drop(drop_elements, axis = 1)\n\ntest = test.drop(drop_elements, axis = 1)\n\n\n\n#2.find null data and fill new data \n\ndef checkNull_fillData(df):\n\n    for col in df.columns:\n\n        if len(df.loc[df[col].isnull() == True]) != 0:\n\n            if df[col].dtype == \"float64\" or df[col].dtype == \"int64\":\n\n                df.loc[df[col].isnull() == True,col] = df[col].mean()\n\n            else:\n\n                df.loc[df[col].isnull() == True,col] = df[col].mode()[0]\n\n                \n\ncheckNull_fillData(train)\n\ncheckNull_fillData(test)\n\n\n\n#3.one hot encoding \n\nstr_list = [] \n\nnum_list = []\n\nfor colname, colvalue in train.iteritems():\n\n    if type(colvalue[1]) == str:\n\n        str_list.append(colname)\n\n    else:\n\n        num_list.append(colname)\n\n        \n\ntrain = pd.get_dummies(train, columns=str_list)\n\ntest = pd.get_dummies(test, columns=str_list)",
            "class": "Data Transform",
            "desc": "The code performs data cleaning and transformation by first dropping unnecessary columns, filling null values with mean or mode depending on their type, and then converting categorical variables into dummy/indicator variables using one-hot encoding with pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.8790232
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "y = train[\"Survived\"]\n\nX = train.drop([\"Survived\"], axis=1)\n\nX_test = test",
            "class": "Data Transform",
            "desc": "The code separates the target variable \"Survived\" from the feature set in the training dataset and sets the test dataset as `X_test`, keeping it ready for further modeling steps using pandas operations.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.9993255
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "import joblib\n\nimport numpy as np\n\nimport pandas as pd\n\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import cross_validate, GridSearchCV\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom xgboost import XGBClassifier\n\nfrom catboost import CatBoostClassifier\n\nimport seaborn as sns",
            "class": "Imports and Environment",
            "desc": "The code imports various libraries and modules such as joblib, numpy, pandas, seaborn, and multiple machine learning models and utilities from scikit-learn, LightGBM, XGBoost, and CatBoost.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99931014
            },
            "cluster": -1
        }, {
            "cell_id": 5,
            "code": "base_models(X, y)",
            "class": "Model Evaluation",
            "desc": "The code evaluates the performance of several base models on the given features and target variable using cross-validation, outputting the ROC AUC score for each model.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.7267511
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "# Base Models\n\n\n\ndef base_models(X, y, scoring=\"roc_auc\"):\n\n    print(\"Base Models....\")\n\n    classifiers = [\n\n#                     ('LR', LogisticRegression()),\n\n#                     ('KNN', KNeighborsClassifier()),\n\n#                     (\"SVC\", SVC()),\n\n#                     (\"CART\", DecisionTreeClassifier()),\n\n                    (\"RF\", RandomForestClassifier()),\n\n                    ('Adaboost', AdaBoostClassifier()),\n\n                    ('GBM', GradientBoostingClassifier()),\n\n                    ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n\n                    ('LightGBM', LGBMClassifier()),\n\n                    ('CatBoost', CatBoostClassifier(verbose=False))\n\n                   ]\n\n\n\n    for name, classifier in classifiers:\n\n        cv_results = cross_validate(classifier, X, y, cv=3, scoring=scoring)\n\n        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")\n\n        \n\n\n\n# Manually Specified Subset of the Hyperparameter Space for learning algorithms\n\n\n\nknn_params = {\"n_neighbors\": range(2, 50)}\n\n\n\ncart_params = {'max_depth': range(1, 20),\n\n               \"min_samples_split\": range(2, 30)}\n\n\n\nrf_params = {\"max_depth\": [8, 15, None],\n\n             \"max_features\": [5, 7, \"auto\"],\n\n             \"min_samples_split\": [15, 20],\n\n             \"n_estimators\": [200, 300]}\n\n\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01],\n\n                  \"max_depth\": [5, 8],\n\n                  \"n_estimators\": [100, 200],\n\n                  \"colsample_bytree\": [0.5, 1]}\n\n\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1],\n\n                   \"n_estimators\": [300, 500],\n\n                   \"colsample_bytree\": [0.7, 1]}\n\n\n\nclassifiers = [('KNN', KNeighborsClassifier(), knn_params),\n\n               (\"CART\", DecisionTreeClassifier(), cart_params),\n\n               (\"RF\", RandomForestClassifier(), rf_params),\n\n               ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params),\n\n               ('LightGBM', LGBMClassifier(), lightgbm_params)]\n\n\n\n# Hyperparameter Optimization\n\n\n\ndef hyperparameter_optimization(X, y, cv=3, scoring=\"roc_auc\"):\n\n    print(\"Hyperparameter Optimization....\")\n\n    best_models = {}\n\n    for name, classifier, params in classifiers:\n\n        print(f\"########## {name} ##########\")\n\n        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)\n\n        print(f\"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}\")\n\n\n\n        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)\n\n        final_model = classifier.set_params(**gs_best.best_params_)\n\n\n\n        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)\n\n        print(f\"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}\")\n\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n        \n\n        best_models[name] = final_model\n\n    \n\n    return best_models\n\n\n\n# Stacking & Ensemble Learning\n\n\n\ndef voting_classifier(best_models, X, y):\n\n    print(\"Voting Classifier...\")\n\n    voting_clf = VotingClassifier(estimators=[('KNN', best_models[\"KNN\"]), ('RF', best_models[\"RF\"]),\n\n                                              ('LightGBM', best_models[\"LightGBM\"])],\n\n                                  voting='soft').fit(X, y)\n\n    cv_results = cross_validate(voting_clf, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n\n    print(f\"Accuracy: {cv_results['test_accuracy'].mean()}\")\n\n    print(f\"F1Score: {cv_results['test_f1'].mean()}\")\n\n    print(f\"ROC_AUC: {cv_results['test_roc_auc'].mean()}\")\n\n    return voting_clf",
            "class": "Model Training",
            "desc": "The code defines functions for evaluating base models, performing hyperparameter optimization, and creating a voting classifier ensemble, using various classifiers from scikit-learn, XGBoost, and LightGBM, with cross-validation and grid search techniques.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.70493513
            },
            "cluster": 1
        }, {
            "cell_id": 6,
            "code": "best_models = hyperparameter_optimization(X, y)",
            "class": "Model Training",
            "desc": "The code performs hyperparameter optimization on predefined classifiers using GridSearchCV and cross-validation, storing the best models and their configurations.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.7329651
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "voting_clf = voting_classifier(best_models, X, y)",
            "class": "Model Training",
            "desc": "The code creates and trains a VotingClassifier ensemble using the best models from hyperparameter optimization and evaluates its performance using cross-validation metrics like accuracy, F1 score, and ROC AUC.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.8512902
            },
            "cluster": 1
        }],
        "notebook_id": 17,
        "notebook_name": "automl-search-best-titanic-model.ipynb",
        "user": "automl-search-best-titanic-model.ipynb"
    }, {
        "cells": [{
            "cell_id": 116,
            "code": "# Create submission dataframe\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_pred})",
            "class": "Data Export",
            "desc": "The code creates a new DataFrame named output containing the 'PassengerId' from the test DataFrame and the predicted 'Survived' values (Y_pred) to prepare it for submission.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.91532314
            },
            "cluster": -1
        }, {
            "cell_id": 117,
            "code": "# Create and save csv file \noutput.to_csv(\"submission_titanic.csv\", index = False)",
            "class": "Data Export",
            "desc": "The code saves the output DataFrame to a CSV file named \"submission_titanic.csv\" without including the index using the to_csv method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.5049722
            },
            "cluster": -1
        }, {
            "cell_id": 118,
            "code": "print(\"Your submission was successfully saved!\")",
            "class": "Data Export",
            "desc": "The code prints a confirmation message indicating that the submission file was successfully saved.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.99812156
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))",
            "class": "Data Extraction",
            "desc": "The code traverses through the directory '/kaggle/input' and prints the full paths of all files located within it using the os.walk method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.99936193
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "# import data\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest=pd.read_csv('/kaggle/input/titanic/test.csv')\nsub = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')",
            "class": "Data Extraction",
            "desc": "The code reads three CSV files ('train.csv', 'test.csv', 'gender_submission.csv') from the '/kaggle/input/titanic' directory into Pandas DataFrames named train, test, and sub, respectively.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99974436
            },
            "cluster": -1
        }, {
            "cell_id": 11,
            "code": "def detect_outliers(df, n, features):\n\n    outlier_indices = [] \n    for col in features: \n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col) \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(key for key, value in outlier_indices.items() if value > n) \n    return multiple_outliers",
            "class": "Data Transform",
            "desc": "The code defines a function named detect_outliers that identifies indices of outlier data points in a DataFrame based on the interquartile range (IQR) method for specified features, and returns a list of indices where the outliers appear in more than 'n' features.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9970662
            },
            "cluster": 1
        }, {
            "cell_id": 12,
            "code": "outliers_to_drop = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\nprint(\"The {} indices for the outliers to drop are: \".format(len(outliers_to_drop)), outliers_to_drop)",
            "class": "Data Transform",
            "desc": "The code identifies and prints the indices of rows in the train DataFrame that are considered outliers in more than two of the specified columns ('Age', 'SibSp', 'Parch', 'Fare') using the previously defined detect_outliers function.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9952669
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "# Outliers in numerical variables\ntrain.loc[outliers_to_drop, :]",
            "class": "Data Transform",
            "desc": "The code displays the rows from the train DataFrame that have been identified as outliers (based on the previously computed outliers_to_drop indices) for further inspection.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9984988
            },
            "cluster": -1
        }, {
            "cell_id": 14,
            "code": "# Drop outliers and reset index\nprint(\"Before: {} rows\".format(len(train)))\ntrain = train.drop(outliers_to_drop, axis = 0).reset_index(drop = True)\nprint(\"After: {} rows\".format(len(train)))",
            "class": "Data Transform",
            "desc": "The code removes the identified outlier rows from the train DataFrame and resets the index, then prints the number of rows before and after the removal of outliers.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99861073
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "outliers_to_drop_to_test = detect_outliers(test, 2, ['Age', 'SibSp', 'Parch', 'Fare'])",
            "class": "Data Transform",
            "desc": "The code identifies the indices of rows in the test DataFrame that are considered outliers in more than two of the specified columns ('Age', 'SibSp', 'Parch', 'Fare') using the previously defined detect_outliers function.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.6760485
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "# Outliers in numerical variables\ntest.loc[outliers_to_drop_to_test, :]",
            "class": "Data Transform",
            "desc": "The code displays the rows from the test DataFrame that have been identified as outliers (based on the previously computed outliers_to_drop_to_test indices) for further inspection.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.9880804
            },
            "cluster": -1
        }, {
            "cell_id": 44,
            "code": "# Drop ticket  feature from training and test set\ntrain = train.drop(['Ticket', 'Cabin'], axis = 1)\ntest = test.drop(['Ticket','Cabin'], axis = 1)",
            "class": "Data Transform",
            "desc": "The code removes the 'Ticket' and 'Cabin' columns from both the train and test DataFrames using the drop method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.99949265
            },
            "cluster": 0
        }, {
            "cell_id": 46,
            "code": "# Compute the most frequent value of Embarked in training set\nmode = train['Embarked'].dropna().mode()[0]\nmode",
            "class": "Data Transform",
            "desc": "The code computes and displays the most frequent value (mode) in the 'Embarked' column of the train DataFrame after dropping missing values, using the dropna and mode methods from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.997647
            },
            "cluster": 0
        }, {
            "cell_id": 47,
            "code": "# Fill missing value in Embarked with mode\ntrain['Embarked'].fillna(mode, inplace = True)",
            "class": "Data Transform",
            "desc": "The code fills the missing values in the 'Embarked' column of the train DataFrame with the most frequent value (mode) using the fillna method from Pandas with the inplace parameter set to True.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.8942647
            },
            "cluster": 0
        }, {
            "cell_id": 49,
            "code": "# Compute median of Fare in test set \nmedian = test['Fare'].dropna().median()\nmedian",
            "class": "Data Transform",
            "desc": "The code computes and displays the median value of the 'Fare' column in the test DataFrame after dropping missing values, using the dropna and median methods from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.63533545
            },
            "cluster": 0
        }, {
            "cell_id": 50,
            "code": "# Fill missing value in Fare with median\ntest['Fare'].fillna(median, inplace = True)",
            "class": "Data Transform",
            "desc": "The code fills the missing values in the 'Fare' column of the test DataFrame with the column\u2019s median value using the fillna method from Pandas with the inplace parameter set to True.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9957889
            },
            "cluster": 0
        }, {
            "cell_id": 51,
            "code": "# Combine training set and test set\ncombine = pd.concat([train, test], axis = 0).reset_index(drop = True)\ncombine.head()",
            "class": "Data Transform",
            "desc": "The code concatenates the train and test DataFrames into a single DataFrame named combine, resets the index, and displays the first five rows using the concat and reset_index methods from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99838865
            },
            "cluster": 0
        }, {
            "cell_id": 53,
            "code": "# Convert Sex into numerical values where 0 = male and 1 = female\ncombine['Sex'] = combine['Sex'].map({'male': 0, 'female': 1})",
            "class": "Data Transform",
            "desc": "The code converts the 'Sex' column in the combined DataFrame into numerical values, mapping 'male' to 0 and 'female' to 1 using the map method from Pandas.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.99478626
            },
            "cluster": 0
        }, {
            "cell_id": 57,
            "code": "# Loop through list and impute missing ages\nfor index in age_nan_indices:\n    median_age = combine['Age'].median()\n    predict_age = combine['Age'][(combine['SibSp'] == combine.iloc[index]['SibSp']) \n                                 & (combine['Parch'] == combine.iloc[index]['Parch'])\n                                 & (combine['Pclass'] == combine.iloc[index][\"Pclass\"])].median()\n    if np.isnan(predict_age):\n        combine['Age'].iloc[index] = median_age\n    else:\n        combine['Age'].iloc[index] = predict_age",
            "class": "Data Transform",
            "desc": "The code imputes missing values in the 'Age' column of the combined DataFrame by predicting the age based on the median age of passengers with matching 'SibSp', 'Parch', and 'Pclass' values, and uses the overall median age if no matching values exist, updating the DataFrame in place.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "correct_missing_values",
                "subclass_id": 17,
                "predicted_subclass_probability": 0.52764386
            },
            "cluster": 0
        }, {
            "cell_id": 60,
            "code": "# Apply log transformation to Fare column to reduce skewness\ntrain['Fare'] = train['Fare'].map(lambda x: np.log(x) if x > 0 else 0)",
            "class": "Data Transform",
            "desc": "The code applies a logarithmic transformation to the 'Fare' column in the train DataFrame to reduce skewness, replacing each fare value with its natural logarithm if it is greater than 0, or 0 if it is not, using the map and lambda functions from Pandas and NumPy.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99930644
            },
            "cluster": 1
        }, {
            "cell_id": 63,
            "code": "# Get title from name\ncombine['Title'] = [name.split(',')[1].split('.')[0].strip() for name in combine['Name']]\ncombine[['Name', 'Title']].head()",
            "class": "Data Transform",
            "desc": "The code extracts titles from the 'Name' column and creates a new column 'Title' in the combined DataFrame, using list comprehension and string manipulation, and displays the first five rows of the resulting DataFrame with only 'Name' and 'Title' columns.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.86563176
            },
            "cluster": 0
        }, {
            "cell_id": 66,
            "code": "# Simplify title\ncombine['Title'] = combine['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Capt'], 'Officer')\ncombine['Title'] = combine['Title'].replace(['Lady', 'Jonkheer', 'Don','the Countess','Sir', 'Dona'], 'Royalty')\ncombine['Title'] = combine['Title'].replace(['Mlle', 'Miss'], 'Miss')\ncombine['Title'] = combine['Title'].replace(['Mme','Mrs','Ms'], 'Mrs')\ncombine['Title'] = combine['Title'].replace('Mr', 'Mr')\ncombine['Title'] = combine['Title'].replace('Master', 'Master')",
            "class": "Data Transform",
            "desc": "The code consolidates various titles into simplified categories within the 'Title' column of the combined DataFrame using the replace method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9944529
            },
            "cluster": 0
        }, {
            "cell_id": 70,
            "code": "# Drop name column\ncombine = combine.drop('Name', axis = 1)\ncombine.head()",
            "class": "Data Transform",
            "desc": "The code removes the 'Name' column from the combined DataFrame using the drop method from Pandas, and displays the first five rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99648714
            },
            "cluster": 0
        }, {
            "cell_id": 71,
            "code": "# Calculate family size from SibSp and Parch\ncombine['Family_Size'] = combine['SibSp'] + combine['Parch'] + 1\ncombine[['SibSp', 'Parch', 'Family_Size']].head(10)",
            "class": "Data Transform",
            "desc": "The code creates a new column 'Family_Size' in the combined DataFrame by summing the 'SibSp' and 'Parch' columns and adding 1, and displays the first ten rows of the resulting DataFrame with 'SibSp', 'Parch', and 'Family_Size' columns.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9995365
            },
            "cluster": 0
        }, {
            "cell_id": 73,
            "code": "# Create Alone feature\ncombine['Alone'] = 0\ncombine.loc[combine['Family_Size'] == 1, 'Alone'] = 1",
            "class": "Data Transform",
            "desc": "The code creates a new column 'Alone' in the combined DataFrame, initially setting all values to 0, and then updates this column to 1 for rows where 'Family_Size' equals 1, indicating passengers who were alone.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.6503439
            },
            "cluster": 0
        }, {
            "cell_id": 75,
            "code": "# Drop SibSp, Parch and FamilySize features from combine dataframe\ncombine = combine.drop(['SibSp', 'Parch', 'Family_Size'], axis = 1)\ncombine.head()",
            "class": "Data Transform",
            "desc": "The code removes the 'SibSp', 'Parch', and 'Family_Size' columns from the combined DataFrame using the drop method from Pandas, and displays the first five rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9810821
            },
            "cluster": 0
        }, {
            "cell_id": 76,
            "code": "combine['Minor'] = combine['Age'] <= 17\ncombine['Major'] = 1 - combine['Minor']  ",
            "class": "Data Transform",
            "desc": "The code creates two new columns in the combined DataFrame: 'Minor', which indicates if a passenger is 17 years old or younger, and 'Major', which is the opposite value (1 if not a minor, 0 if a minor), using boolean expressions and arithmetic operations.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9321478
            },
            "cluster": 0
        }, {
            "cell_id": 78,
            "code": "combine.loc[(combine['Age'] <= 17), 'Major'] = 0\ncombine.loc[(combine['Age'] > 17), 'Major'] = 1",
            "class": "Data Transform",
            "desc": "The code updates the 'Major' column in the combined DataFrame, setting it to 0 for passengers aged 17 or younger and to 1 for passengers older than 17 using conditional indexing with the loc method from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99650925
            },
            "cluster": 0
        }, {
            "cell_id": 80,
            "code": "# Drop Age and Minor from combine dataframe\ncombine = combine.drop(['Age', 'Minor'], axis = 1)\ncombine.head()",
            "class": "Data Transform",
            "desc": "The code removes the 'Age' and 'Minor' columns from the combined DataFrame using the drop method from Pandas and displays the first five rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9985499
            },
            "cluster": 0
        }, {
            "cell_id": 82,
            "code": "# Encode Title and Embarked feature\ncombine = pd.get_dummies(combine, columns = ['Title'])\ncombine = pd.get_dummies(combine, columns = ['Embarked'], prefix = 'Em')\ncombine.head()",
            "class": "Data Transform",
            "desc": "The code converts the 'Title' and 'Embarked' categorical columns into one-hot encoded columns in the combined DataFrame using the get_dummies method from Pandas, and displays the first five rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99897325
            },
            "cluster": 0
        }, {
            "cell_id": 83,
            "code": "# Divide Fare into four bands\ncombine['Fare_Band'] = pd.cut(combine['Fare'], 4)\ncombine[['Fare_Band', 'Survived']].groupby(['Fare_Band'], as_index=False).mean().sort_values(by = 'Fare_Band')",
            "class": "Data Transform",
            "desc": "The code creates a new column 'Fare_Band' in the combined DataFrame by dividing the 'Fare' column into four equal-width intervals using the cut method from Pandas, and then calculates and displays the mean survival rate for each fare band.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9994172
            },
            "cluster": 0
        }, {
            "cell_id": 84,
            "code": "# Assign ordinal to each fare band\ncombine.loc[combine['Fare'] <= 1.56, 'Fare'] = 0\ncombine.loc[(combine['Fare'] > 1.56) & (combine['Fare'] <= 3.119), 'Fare'] = 1\ncombine.loc[(combine['Fare'] > 3.119) & (combine['Fare'] <= 4.679), 'Fare'] = 2\ncombine.loc[combine['Fare'] > 4.679, 'Fare'] = 3",
            "class": "Data Transform",
            "desc": "The code assigns ordinal values to each fare band by updating the 'Fare' column in the combined DataFrame based on specific ranges using conditional indexing with the loc method from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.94123465
            },
            "cluster": 0
        }, {
            "cell_id": 85,
            "code": "# Convert Fare into integer\ncombine['Fare'] = combine['Fare'].astype('int')",
            "class": "Data Transform",
            "desc": "The code converts the 'Fare' column in the combined DataFrame to integer type using the astype method from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9987791
            },
            "cluster": 0
        }, {
            "cell_id": 86,
            "code": "# Drop FareBand feature\ncombine = combine.drop('Fare_Band', axis = 1)",
            "class": "Data Transform",
            "desc": "The code removes the 'Fare_Band' column from the combined DataFrame using the drop method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997482
            },
            "cluster": 0
        }, {
            "cell_id": 88,
            "code": "# Separate training and test set from the combined dataframe\ntrain = combine[:len(train)]\ntest = combine[len(train):]",
            "class": "Data Transform",
            "desc": "The code separates the combined DataFrame back into the train and test DataFrames by slicing the first len(train) rows for training data and the remaining rows for test data.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997482
            },
            "cluster": 0
        }, {
            "cell_id": 89,
            "code": "# Drop passenger ID column from and training set\ntrain = train.drop('PassengerId', axis = 1)\ntrain.head()",
            "class": "Data Transform",
            "desc": "The code removes the 'PassengerId' column from the train DataFrame using the drop method from Pandas and displays the first five rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9992955
            },
            "cluster": 0
        }, {
            "cell_id": 90,
            "code": "# Convert survived back to integer in the training set\ntrain['Survived'] = train['Survived'].astype('int')\ntrain.head()",
            "class": "Data Transform",
            "desc": "The code converts the 'Survived' column in the train DataFrame back to integer type using the astype method from Pandas and displays the first five rows of the resulting DataFrame.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.8972458
            },
            "cluster": 0
        }, {
            "cell_id": 91,
            "code": "# Drop passenger survived column from test set\ntest = test.drop('Survived', axis = 1)\ntest.head()",
            "class": "Data Transform",
            "desc": "The code removes the 'Survived' column from the test DataFrame using the drop method from Pandas and displays the first five rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9982375
            },
            "cluster": 0
        }, {
            "cell_id": 94,
            "code": "X_train = train.drop('Survived', axis = 1)\nY_train = train['Survived']\nX_test = test.drop('PassengerId', axis = 1).copy()\nprint(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"X_test shape: \", X_test.shape)",
            "class": "Data Transform",
            "desc": "The code splits the train DataFrame into features (X_train) by dropping the 'Survived' column and labels (Y_train) by selecting the 'Survived' column, and prepares the X_test DataFrame by dropping the 'PassengerId' column, then prints the shapes of X_train, Y_train, and X_test.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997482
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "train.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first five rows of the train DataFrame using the head method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997507
            },
            "cluster": 2
        }, {
            "cell_id": 4,
            "code": "print(\"Training set shape: \", train.shape)\nprint(\"Test set shape: \", test.shape)\nprint(\"Sample submission shape: \", sub.shape)",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the dimensions (number of rows and columns) of the train, test, and sub DataFrames using the shape attribute of each DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.72919387
            },
            "cluster": 5
        }, {
            "cell_id": 5,
            "code": "# data types\ntrain.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code provides a summary of the train DataFrame, displaying the data types, non-null counts, and memory usage of each column using the info method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99930036
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "test.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code provides a summary of the test DataFrame, displaying the data types, non-null counts, and memory usage of each column using the info method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993563
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "# Missing data in training set \ntrain.isnull().sum().sort_values(ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates the number of missing values for each column in the train DataFrame, sorts the counts in descending order, and displays the result using the isnull and sum methods from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9620355
            },
            "cluster": 6
        }, {
            "cell_id": 8,
            "code": "# Missing data in testing set \ntest.isnull().sum().sort_values(ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates the number of missing values for each column in the test DataFrame, sorts the counts in descending order, and displays the result using the isnull and sum methods from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.7282071
            },
            "cluster": 6
        }, {
            "cell_id": 9,
            "code": "train.describe()",
            "class": "Exploratory Data Analysis",
            "desc": "The code generates descriptive statistics for the numerical columns in the train DataFrame, including measures like mean, standard deviation, min, and max, using the describe method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99943346
            },
            "cluster": 8
        }, {
            "cell_id": 10,
            "code": "train.describe(include=\"O\")",
            "class": "Exploratory Data Analysis",
            "desc": "The code generates descriptive statistics for the categorical (object-type) columns in the train DataFrame, including measures like count, unique, top (most frequent), and freq (frequency of the top), using the describe method from Pandas with include=\"O\".",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9994286
            },
            "cluster": 8
        }, {
            "cell_id": 15,
            "code": "test.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first five rows of the test DataFrame using the head method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.5586606
            },
            "cluster": 2
        }, {
            "cell_id": 19,
            "code": "# Value counts of the SibSp column \ntrain['SibSp'].value_counts(dropna = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code computes and displays the frequency counts of each unique value in the 'SibSp' column of the train DataFrame using the value_counts method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997483
            },
            "cluster": 6
        }, {
            "cell_id": 20,
            "code": "# Mean of survival by SibSp\ntrain[['SibSp', 'Survived']].groupby('SibSp', as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'SibSp' column in the train DataFrame, sorts the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.35934398
            },
            "cluster": 8
        }, {
            "cell_id": 22,
            "code": "# Value counts of the Parch column \ntrain['Parch'].value_counts(dropna = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code computes and displays the frequency counts of each unique value in the 'Parch' column of the train DataFrame using the value_counts method from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.9989446
            },
            "cluster": 6
        }, {
            "cell_id": 23,
            "code": "# Mean of survival by Parch\ntrain[['Parch', 'Survived']].groupby('Parch', as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'Parch' column in the train DataFrame, sorting the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9994972
            },
            "cluster": 8
        }, {
            "cell_id": 25,
            "code": "# Null values in Age column \ntrain['Age'].isnull().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the total number of missing values in the 'Age' column of the train DataFrame using the isnull and sum methods from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9957302
            },
            "cluster": 6
        }, {
            "cell_id": 29,
            "code": "# Null values of Fare column \ntrain['Fare'].isnull().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the total number of missing values in the 'Fare' column of the train DataFrame using the isnull and sum methods from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9982951
            },
            "cluster": 6
        }, {
            "cell_id": 31,
            "code": "# Value counts of the SibSp column \n\ntrain['Pclass'].value_counts(dropna = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code computes and displays the frequency counts of each unique value in the 'Pclass' column of the train DataFrame using the value_counts method from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99736184
            },
            "cluster": 6
        }, {
            "cell_id": 32,
            "code": "# Mean of survival by sex\ntrain[['Pclass', 'Survived']].groupby('Pclass', as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'Pclass' column in the train DataFrame, sorting the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.98468137
            },
            "cluster": 8
        }, {
            "cell_id": 37,
            "code": "# Value counts of the sex column\ntrain['Sex'].value_counts(dropna = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code computes and displays the frequency counts of each unique value in the 'Sex' column of the train DataFrame using the value_counts method from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9959014
            },
            "cluster": 6
        }, {
            "cell_id": 38,
            "code": "# Mean of survival by sex\ntrain[['Sex', 'Survived']].groupby('Sex', as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'Sex' column in the train DataFrame, sorting the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.99524397
            },
            "cluster": 8
        }, {
            "cell_id": 40,
            "code": "# Value counts of the Embarked column \ntrain['Embarked'].value_counts(dropna = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code computes and displays the frequency counts of each unique value in the 'Embarked' column of the train DataFrame using the value_counts method from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99608815
            },
            "cluster": 6
        }, {
            "cell_id": 41,
            "code": "# Mean of survival by point of embarkation\ntrain[['Embarked', 'Survived']].groupby(['Embarked'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'Embarked' column in the train DataFrame, sorting the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.99947804
            },
            "cluster": 8
        }, {
            "cell_id": 45,
            "code": "# Missing values in training set \ntrain.isnull().sum().sort_values(ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates the number of missing values for each column in the train DataFrame, sorts the counts in descending order, and displays the result using the isnull and sum methods from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9939486
            },
            "cluster": 6
        }, {
            "cell_id": 48,
            "code": "# Missing values in test set\ntest.isnull().sum().sort_values(ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates the number of missing values for each column in the test DataFrame, sorts the counts in descending order, and displays the result using the isnull and sum methods from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9915372
            },
            "cluster": 6
        }, {
            "cell_id": 52,
            "code": "# Missing values in the combined dataset\ncombine.isnull().sum().sort_values(ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates the number of missing values for each column in the combined DataFrame, sorts the counts in descending order, and displays the result using the isnull and sum methods from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9538963
            },
            "cluster": 6
        }, {
            "cell_id": 56,
            "code": "# Check number of missing ages \nage_nan_indices = list(combine[combine['Age'].isnull()].index)\nlen(age_nan_indices)",
            "class": "Exploratory Data Analysis",
            "desc": "The code identifies and counts the number of missing values in the 'Age' column of the combined DataFrame, and stores their indices in the list age_nan_indices.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9202347
            },
            "cluster": 4
        }, {
            "cell_id": 58,
            "code": "# Make sure there is no more missing ages \ncombine['Age'].isnull().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "The code verifies that there are no missing values left in the 'Age' column of the combined DataFrame by calculating the total number of missing values using the isnull and sum methods from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.9994412
            },
            "cluster": 6
        }, {
            "cell_id": 62,
            "code": "combine.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first five rows of the combined DataFrame using the head method from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.9989661
            },
            "cluster": 2
        }, {
            "cell_id": 64,
            "code": "# Value counts of Title\ncombine['Title'].value_counts()",
            "class": "Exploratory Data Analysis",
            "desc": "The code computes and displays the frequency counts of each unique value in the 'Title' column of the combined DataFrame using the value_counts method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.2736002
            },
            "cluster": 6
        }, {
            "cell_id": 65,
            "code": "# Number of unique Title\ncombine['Title'].nunique()",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the number of unique titles in the 'Title' column of the combined DataFrame using the nunique method from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99797064
            },
            "cluster": 6
        }, {
            "cell_id": 68,
            "code": "# Mean of survival by name title\ncombine[['Title', 'Survived']].groupby(['Title'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'Title' column in the combined DataFrame, sorting the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.7528276
            },
            "cluster": 8
        }, {
            "cell_id": 72,
            "code": "# Mean of survival by family_size\ncombine[['Family_Size', 'Survived']].groupby('Family_Size', as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'Family_Size' column in the combined DataFrame, sorting the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.9727156
            },
            "cluster": 8
        }, {
            "cell_id": 74,
            "code": "# Mean of survival by Alone\ncombine[['Alone', 'Survived']].groupby('Alone', as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'Alone' column in the combined DataFrame, sorting the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9955486
            },
            "cluster": 8
        }, {
            "cell_id": 77,
            "code": "combine[['Major', 'Survived']].groupby('Major', as_index = False).mean().sort_values(by = 'Survived', ascending = False)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the mean survival rate grouped by the 'Major' column in the combined DataFrame, sorting the results by the survival rate in descending order, using the groupby and mean methods from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99916553
            },
            "cluster": 8
        }, {
            "cell_id": 79,
            "code": "combine.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first five rows of the combined DataFrame using the head method from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9918848
            },
            "cluster": 2
        }, {
            "cell_id": 81,
            "code": "combine.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first five rows of the combined DataFrame using the head method from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9802186
            },
            "cluster": 2
        }, {
            "cell_id": 87,
            "code": "combine.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first five rows of the combined DataFrame using the head method from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9991159
            },
            "cluster": 2
        }, {
            "cell_id": 92,
            "code": "train.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code provides a summary of the train DataFrame, displaying the data types, non-null counts, and memory usage of each column using the info method from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.9838637
            },
            "cluster": 0
        }, {
            "cell_id": 93,
            "code": "test.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code provides a summary of the test DataFrame, displaying the data types, non-null counts, and memory usage of each column using the info method from Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99928004
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "# Data wrangling\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n# Data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Machine learning models\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Model evaluation\nfrom sklearn.model_selection import cross_val_score\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')",
            "class": "Imports and Environment",
            "desc": "The code imports essential libraries and packages for data manipulation (Pandas, NumPy), visualization (Seaborn, Matplotlib), machine learning models (from scikit-learn), model evaluation (cross_val_score), and hyperparameter tuning (GridSearchCV), and it also suppresses warning messages using the warnings library.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9990087
            },
            "cluster": -1
        }, {
            "cell_id": 104,
            "code": "models = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n                                 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', \n                                 'Linear SVC', 'Decision Tree'],\n                       'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron,\n                                 acc_sgd, acc_linear_svc, acc_decision_tree]})\n\nmodels.sort_values(by = 'Score', ascending = False, ignore_index = True)",
            "class": "Model Evaluation",
            "desc": "The code creates a DataFrame containing the model names and their corresponding accuracy scores, then sorts this DataFrame by the 'Score' column in descending order, using the sort_values method from Pandas with ignore_index set to True.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.5063414
            },
            "cluster": 0
        }, {
            "cell_id": 106,
            "code": "# Create a list which contains cross validation results for each classifier\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy', cv = 9))",
            "class": "Model Evaluation",
            "desc": "The code creates a list named cv_results that contains cross-validation accuracy scores for each classifier in the classifiers list by performing 9-fold cross-validation on the training data (X_train and Y_train) using the cross_val_score method from scikit-learn.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.5461685
            },
            "cluster": 0
        }, {
            "cell_id": 107,
            "code": "# Mean and standard deviation of cross validation results for each classifier  \ncv_mean = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_mean.append(cv_result.mean())\n    cv_std.append(cv_result.std())",
            "class": "Model Evaluation",
            "desc": "The code calculates the mean and standard deviation of the cross-validation accuracy scores for each classifier, appending these values to the cv_mean and cv_std lists, respectively.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9476168
            },
            "cluster": 0
        }, {
            "cell_id": 108,
            "code": "cv_res = pd.DataFrame({'Cross Validation Mean': cv_mean, 'Cross Validation Std': cv_std, 'Algorithm': ['Logistic Regression', 'Support Vector Machines', 'KNN', 'Gausian Naive Bayes', 'Perceptron', 'Linear SVC', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest']})\ncv_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True)",
            "class": "Model Evaluation",
            "desc": "The code creates a DataFrame named cv_res that contains the cross-validation mean scores, standard deviations, and algorithm names, and then sorts this DataFrame by the 'Cross Validation Mean' column in descending order, using the sort_values method from Pandas with ignore_index set to True.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.4700903
            },
            "cluster": 0
        }, {
            "cell_id": 113,
            "code": "# Mean cross validation score\ncross_val_score(random_forest, X_train, Y_train, scoring = 'accuracy', cv = 10).mean()",
            "class": "Model Evaluation",
            "desc": "The code calculates and displays the mean cross-validation accuracy score for the Random Forest classifier with the specified parameters (n_estimators=200, max_depth=4) on the training data (X_train and Y_train) using 10-fold cross-validation with the cross_val_score method from scikit-learn.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.52557796
            },
            "cluster": 0
        }, {
            "cell_id": 114,
            "code": "# Survival predictions by Random Forest classifier\nY_pred",
            "class": "Model Evaluation",
            "desc": "The code returns the survival predictions made by the Random Forest classifier for the test set (X_test), which were previously stored in the Y_pred variable.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.9969283
            },
            "cluster": 0
        }, {
            "cell_id": 115,
            "code": "len(Y_pred)",
            "class": "Model Evaluation",
            "desc": "The code calculates and displays the number of survival predictions made by the Random Forest classifier for the test set by getting the length of the Y_pred array using the len function.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.94283515
            },
            "cluster": 0
        }, {
            "cell_id": 95,
            "code": "logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log",
            "class": "Model Training",
            "desc": "The code creates and trains a Logistic Regression model using the training data (X_train and Y_train) and then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9369069
            },
            "cluster": 2
        }, {
            "cell_id": 96,
            "code": "svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc",
            "class": "Model Training",
            "desc": "The code creates and trains a Support Vector Classifier (SVC) using the training data (X_train and Y_train) and then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9988219
            },
            "cluster": 2
        }, {
            "cell_id": 97,
            "code": "knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn",
            "class": "Model Training",
            "desc": "The code creates and trains a K-Nearest Neighbors (KNN) classifier with 5 neighbors using the training data (X_train and Y_train) and then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.97265536
            },
            "cluster": 2
        }, {
            "cell_id": 98,
            "code": "gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian",
            "class": "Model Training",
            "desc": "The code creates and trains a Gaussian Naive Bayes classifier using the training data (X_train and Y_train) and then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99926084
            },
            "cluster": 2
        }, {
            "cell_id": 99,
            "code": "perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron",
            "class": "Model Training",
            "desc": "The code creates and trains a Perceptron model using the training data (X_train and Y_train) and then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.9854525
            },
            "cluster": 2
        }, {
            "cell_id": 100,
            "code": "linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc",
            "class": "Model Training",
            "desc": "The code creates and trains a Linear Support Vector Classifier (LinearSVC) using the training data (X_train and Y_train) and then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.9859805
            },
            "cluster": 2
        }, {
            "cell_id": 101,
            "code": "sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd",
            "class": "Model Training",
            "desc": "The code creates and trains a Stochastic Gradient Descent (SGD) classifier using the training data (X_train and Y_train) and then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993617
            },
            "cluster": 2
        }, {
            "cell_id": 102,
            "code": "decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree",
            "class": "Model Training",
            "desc": "The code creates and trains a Decision Tree classifier using the training data (X_train and Y_train) and then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993563
            },
            "cluster": 2
        }, {
            "cell_id": 103,
            "code": "random_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest",
            "class": "Model Training",
            "desc": "The code creates and trains a Random Forest classifier with 100 estimators using the training data (X_train and Y_train), then predicts the labels for the test set (X_test) and finally calculates and stores the accuracy of the model on the training set.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.50944585
            },
            "cluster": 2
        }, {
            "cell_id": 105,
            "code": "# Create a list which contains classifiers \n\nclassifiers = []\nclassifiers.append(LogisticRegression())\nclassifiers.append(SVC())\nclassifiers.append(KNeighborsClassifier(n_neighbors = 5))\nclassifiers.append(GaussianNB())\nclassifiers.append(Perceptron())\nclassifiers.append(LinearSVC())\nclassifiers.append(SGDClassifier())\nclassifiers.append(DecisionTreeClassifier())\nclassifiers.append(RandomForestClassifier())\n\nlen(classifiers)",
            "class": "Model Training",
            "desc": "The code creates a list named classifiers and appends various classifier objects (Logistic Regression, SVC, KNN, GaussianNB, Perceptron, LinearSVC, SGDClassifier, DecisionTreeClassifier, RandomForestClassifier) to it, then calculates and displays the number of classifiers in the list.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.5250405
            },
            "cluster": -1
        }, {
            "cell_id": 110,
            "code": "param_grid = { \n    'n_estimators': [200, 500],\n    'max_depth' : [4,5,6,7,8]\n}\n# Instantiate the grid search model\ngrid = GridSearchCV(RandomForestClassifier(), param_grid = param_grid, \n                          cv=5)\ngrid.fit(X_train, Y_train)",
            "class": "Model Training",
            "desc": "The code defines a parameter grid for a Random Forest classifier, consisting of different choices for 'n_estimators' and 'max_depth', and then performs a grid search with 5-fold cross-validation using GridSearchCV from scikit-learn to find the best combination of parameters, fitting the model on the training data (X_train and Y_train).",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.75977
            },
            "cluster": 1
        }, {
            "cell_id": 111,
            "code": "print(\"Best parameters: \", grid.best_params_) \nprint(\"Best estimator: \", grid.best_estimator_)",
            "class": "Model Training",
            "desc": "The code prints the best parameters and the best estimator found by the grid search using the attributes best_params_ and best_estimator_ of the GridSearchCV object, respectively.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.40337494
            },
            "cluster": 0
        }, {
            "cell_id": 112,
            "code": "random_forest = RandomForestClassifier(n_estimators=200, max_depth=4)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest",
            "class": "Model Training",
            "desc": "The code creates and trains a Random Forest classifier with the best parameters (n_estimators=200, max_depth=4) using the training data (X_train and Y_train), then predicts the labels for the test set (X_test), finally calculating and storing the accuracy of the model on the training set.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.6077191
            },
            "cluster": 2
        }, {
            "cell_id": 18,
            "code": "sns.heatmap(train[['Survived', 'SibSp', 'Parch', 'Age', 'Fare', 'Pclass']].corr(), annot = True, fmt = '.2f', cmap = 'Reds')",
            "class": "Visualization",
            "desc": "The code creates a heatmap to visualize the correlation matrix of the selected variables ('Survived', 'SibSp', 'Parch', 'Age', 'Fare', 'Pclass') in the train DataFrame, with correlation coefficients displayed using Seaborn's heatmap function.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.6817812
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "sns.barplot(x = 'SibSp', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by SibSp')",
            "class": "Visualization",
            "desc": "The code creates a bar plot to visualize the survival probability by the number of siblings/spouses aboard ('SibSp') in the train DataFrame using Seaborn's barplot function, and labels the y-axis and title using Matplotlib's pyplot.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.9842105
            },
            "cluster": 1
        }, {
            "cell_id": 24,
            "code": "sns.barplot(x = 'Parch', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Parch')",
            "class": "Visualization",
            "desc": "The code creates a bar plot to visualize the survival probability by the number of parents/children aboard ('Parch') in the train DataFrame using Seaborn's barplot function, and labels the y-axis and title using Matplotlib's pyplot.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.98814243
            },
            "cluster": 1
        }, {
            "cell_id": 26,
            "code": "# Passenger age distribution\nsns.distplot(train['Age'], label = 'Skewness: %.2f'%(train['Age'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Age Distribution')",
            "class": "Visualization",
            "desc": "The code creates a distribution plot to visualize the age distribution of passengers in the train DataFrame using Seaborn's distplot function, adds a legend showing the skewness value, and sets the plot title using Matplotlib's pyplot.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9995035
            },
            "cluster": 1
        }, {
            "cell_id": 27,
            "code": "# Age distribution by survival\ng = sns.FacetGrid(train, col = 'Survived')\ng.map(sns.distplot, 'Age')",
            "class": "Visualization",
            "desc": "The code creates a facet grid to visualize the age distribution of passengers separated by survival status using Seaborn's FacetGrid and distplot functions.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9777845
            },
            "cluster": -1
        }, {
            "cell_id": 28,
            "code": "sns.kdeplot(train['Age'][train['Survived'] == 0], label = 'Died')\nsns.kdeplot(train['Age'][train['Survived'] == 1], label = 'Survived')\nplt.xlabel('Age')\nplt.title('Passenger Age Distribution by Survival')",
            "class": "Visualization",
            "desc": "The code creates a Kernel Density Estimate (KDE) plot to visualize the age distribution of passengers based on their survival status (died or survived) using Seaborn's kdeplot function, and labels the x-axis and plot title using Matplotlib's pyplot.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9960498
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "# Passenger fare distribution\nsns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.ylabel('Passenger Pclass Distribution')",
            "class": "Visualization",
            "desc": "The code creates a distribution plot to visualize the fare distribution of passengers in the train DataFrame, adds a legend showing the skewness value, and labels the y-axis using Seaborn's distplot function and Matplotlib's pyplot.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.93026966
            },
            "cluster": 1
        }, {
            "cell_id": 33,
            "code": "sns.barplot(x = 'Pclass', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Pclass')",
            "class": "Visualization",
            "desc": "The code creates a bar plot to visualize the survival probability by passenger class ('Pclass') in the train DataFrame using Seaborn's barplot function, and labels the y-axis and title using Matplotlib's pyplot.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99873155
            },
            "cluster": 1
        }, {
            "cell_id": 34,
            "code": "# Survival by gender and passenger class\n\ng = sns.factorplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = train, kind = 'bar')\ng.despine(left = True)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Sex and Passenger Class')",
            "class": "Visualization",
            "desc": "The code creates a bar plot to visualize the survival probability by passenger class ('Pclass') and gender ('Sex') in the train DataFrame using Seaborn's factorplot function, and labels the y-axis and title using Matplotlib's pyplot.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99223536
            },
            "cluster": 1
        }, {
            "cell_id": 35,
            "code": "# Passenger fare distribution\nsns.distplot(train['Pclass'], label = 'Skewness: %.2f'%(train['Pclass'].skew()))\nplt.legend(loc = 'best')\nplt.ylabel('Passenger Fare Distribution')",
            "class": "Visualization",
            "desc": "The code attempts to create a distribution plot to visualize the distribution of passenger classes ('Pclass') in the train DataFrame using Seaborn's distplot function and Matplotlib's pyplot, but it mislabels the y-axis with 'Passenger Fare Distribution'.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.99950254
            },
            "cluster": 1
        }, {
            "cell_id": 36,
            "code": "# Age distribution by survival\ng = sns.FacetGrid(train, col = 'Survived')\ng.map(sns.distplot, 'Pclass')",
            "class": "Visualization",
            "desc": "The code creates a facet grid to visualize the distribution of passenger classes ('Pclass') separated by survival status using Seaborn's FacetGrid and distplot functions.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9796997
            },
            "cluster": -1
        }, {
            "cell_id": 39,
            "code": "sns.barplot(x = 'Sex', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Gender')",
            "class": "Visualization",
            "desc": "The code creates a bar plot to visualize the survival probability by gender ('Sex') in the train DataFrame using Seaborn's barplot function, and labels the y-axis and title using Matplotlib's pyplot.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99602973
            },
            "cluster": 1
        }, {
            "cell_id": 42,
            "code": "sns.barplot(x = 'Embarked', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Point of Embarkation')",
            "class": "Visualization",
            "desc": "The code creates a bar plot to visualize the survival probability by the point of embarkation ('Embarked') in the train DataFrame using Seaborn's barplot function, and labels the y-axis and title using Matplotlib's pyplot.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9917911
            },
            "cluster": 1
        }, {
            "cell_id": 43,
            "code": "# Survival probability by all categorical variables\ngrid = sns.FacetGrid(train, row = 'Embarked', size = 2.2, aspect = 1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette = 'deep')\ngrid.add_legend()",
            "class": "Visualization",
            "desc": "The code creates a facet grid to visualize the survival probability by passenger class ('Pclass') and gender ('Sex'), separated by the point of embarkation ('Embarked'), using Seaborn's FacetGrid and pointplot functions, and adds a legend with add_legend.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.55584514
            },
            "cluster": 0
        }, {
            "cell_id": 54,
            "code": "sns.factorplot(y = 'Age', x = 'Sex', hue = 'Pclass', kind = 'box', data = combine)\nsns.factorplot(y = 'Age', x = 'Parch', kind = 'box', data = combine)\nsns.factorplot(y = 'Age', x = 'SibSp', kind = 'box', data = combine)",
            "class": "Visualization",
            "desc": "The code creates multiple box plots to visualize the distribution of 'Age' against 'Sex' and 'Pclass', 'Parch', and 'SibSp' respectively, using Seaborn's factorplot function.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.24961525
            },
            "cluster": 0
        }, {
            "cell_id": 55,
            "code": "sns.heatmap(train.drop(['Survived', 'Name', 'PassengerId', 'Fare'], axis = 1).corr(), annot = True, cmap = 'Reds')",
            "class": "Visualization",
            "desc": "The code creates a heatmap to visualize the correlation matrix of selected variables (excluding 'Survived', 'Name', 'PassengerId', and 'Fare') in the train DataFrame, with correlation coefficients displayed using Seaborn's heatmap function.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.8276658
            },
            "cluster": 0
        }, {
            "cell_id": 59,
            "code": "# Passenger fare distribution\nsns.distplot(combine['Fare'], label = 'Skewness: %.2f'%(combine['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Fare Distribution')",
            "class": "Visualization",
            "desc": "The code creates a distribution plot to visualize the fare distribution of passengers in the combined DataFrame using Seaborn's distplot function, adds a legend showing the skewness value, and sets the plot title using Matplotlib's pyplot.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9728184
            },
            "cluster": 1
        }, {
            "cell_id": 61,
            "code": "# Passenger fare distribution after log transformation\nsns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Fare Distribution After Log Transformation')",
            "class": "Visualization",
            "desc": "The code creates a distribution plot to visualize the fare distribution (after the log transformation) of passengers in the train DataFrame using Seaborn's distplot function, adds a legend showing the skewness value, and sets the plot title using Matplotlib's pyplot.",
            "testing": {
                "class": "Visualization",
                "subclass": "time_series",
                "subclass_id": 75,
                "predicted_subclass_probability": 0.9786926
            },
            "cluster": 1
        }, {
            "cell_id": 67,
            "code": "figure = plt.figure(figsize=(25, 7))\nsns.countplot(combine['Title'])",
            "class": "Visualization",
            "desc": "The code creates a count plot to visualize the frequency of each title in the combined DataFrame using Seaborn's countplot function, with a figure size of 25 by 7 inches set by Matplotlib's pyplot.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9991322
            },
            "cluster": 1
        }, {
            "cell_id": 69,
            "code": "sns.factorplot(x = 'Title', y = 'Survived', data = combine, kind = 'bar')\nplt.ylabel('Survival Probability')\nplt.title('Mean of survival by Title')",
            "class": "Visualization",
            "desc": "The code creates a bar plot to visualize the mean survival probability by title ('Title') in the combined DataFrame using Seaborn's factorplot function, and labels the y-axis and title using Matplotlib's pyplot.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997482
            },
            "cluster": 1
        }, {
            "cell_id": 109,
            "code": "sns.barplot('Cross Validation Mean', 'Algorithm', data = cv_res, order = cv_res.sort_values(by = 'Cross Validation Mean', ascending = False)['Algorithm'], palette = 'Set3', **{'xerr': cv_std})\nplt.ylabel('Algorithm')\nplt.title('Cross Validation Scores')",
            "class": "Visualization",
            "desc": "The code creates a bar plot to visualize the cross-validation mean scores of different algorithms, ordered by their mean scores, with error bars representing the standard deviation, using Seaborn's barplot function and specifying a color palette, and labels the y-axis and title using Matplotlib's pyplot.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.91900784
            },
            "cluster": 1
        }],
        "notebook_id": 18,
        "notebook_name": "titanic-com.ipynb",
        "user": "titanic-com.ipynb"
    }, {
        "cells": [{
            "cell_id": 10,
            "code": "sub = pd.read_csv(SAMPLE_SUBMISSION_PATH)\nsub[TARGET]=(pred_test > 0.5).astype(int)\nsub.to_csv(SUBMISSION_PATH,index=False)\nsub.head()",
            "class": "Data Export",
            "desc": "This code snippet reads the sample submission CSV file, updates it with the test set predictions, converts the predictions to binary format, writes the updated DataFrame to a new CSV file, and displays the first few rows of the result.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99933285
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "TRAIN_PATH = \"../input/titanic/train.csv\"\nTEST_PATH = \"../input/titanic/test.csv\"\nSAMPLE_SUBMISSION_PATH = \"../input/titanic/gender_submission.csv\"\nSUBMISSION_PATH = \"submission.csv\"\n\nID = \"PassengerId\"\nTARGET = \"Survived\"\nTEST_SIZE = 0.2\nRANDOM_SEED = 42\nMAX_TRIAL = 3 # for simple test \nEPOCHS = 5  # for simple test \nVALIDATION_SPLIT = 0.15",
            "class": "Data Extraction",
            "desc": "This code snippet defines file paths for the train, test, sample submission, and submission CSV files, along with various constants such as column identifiers and parameters for dataset splitting, random seed, and model training configurations.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.99899656
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "train = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)",
            "class": "Data Extraction",
            "desc": "This code snippet loads the train and test datasets from their respective CSV file paths into Pandas DataFrames.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99966717
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "#1. delete unnecessary columns\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp','Parch']\ntrain = train.drop(drop_elements, axis = 1)\ntest = test.drop(drop_elements, axis = 1)\n\n#2.find null data and fill new data \ndef checkNull_fillData(df):\n    for col in df.columns:\n        if len(df.loc[df[col].isnull() == True]) != 0:\n            if df[col].dtype == \"float64\" or df[col].dtype == \"int64\":\n                df.loc[df[col].isnull() == True,col] = df[col].mean()\n            else:\n                df.loc[df[col].isnull() == True,col] = df[col].mode()[0]\n                \ncheckNull_fillData(train)\ncheckNull_fillData(test)\n\n#3.one hot encoding \nstr_list = [] \nnum_list = []\nfor colname, colvalue in train.iteritems():\n    if type(colvalue[1]) == str:\n        str_list.append(colname)\n    else:\n        num_list.append(colname)\n        \ntrain = pd.get_dummies(train, columns=str_list)\ntest = pd.get_dummies(test, columns=str_list)",
            "class": "Data Transform",
            "desc": "This code snippet deletes unnecessary columns from the datasets, fills missing values with mean or mode using a custom function, and applies one-hot encoding to categorical columns with Pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.8790232
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "y = train[TARGET]\nX = train.drop([TARGET],axis=1)\nX_test = test\n\ngc.collect()",
            "class": "Data Transform",
            "desc": "This code snippet separates the target variable from the predictors in the training dataset and assigns the test dataset (excluding the target variable) to `X_test`, then it triggers garbage collection with `gc.collect()`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.99927086
            },
            "cluster": -1
        }, {
            "cell_id": 5,
            "code": "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=TEST_SIZE,random_state=RANDOM_SEED)",
            "class": "Data Transform",
            "desc": "This code snippet splits the dataset into training and validation sets using scikit-learn's `train_test_split` function, with parameters defined by `TEST_SIZE` and `RANDOM_SEED`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9979424
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "import numpy as np\nimport pandas as pd\n\nimport gc\n\nfrom IPython.display import clear_output\n!pip install -q -U keras-tuner\nclear_output()\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport kerastuner as kt\n\nfrom sklearn import ensemble\n\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier",
            "class": "Imports and Environment",
            "desc": "This code snippet imports various libraries and packages such as NumPy, Pandas, TensorFlow, Keras Tuner, and several machine learning models from scikit-learn, and it installs the Keras Tuner using pip.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.4513051
            },
            "cluster": -1
        }, {
            "cell_id": 8,
            "code": "pred_val = model.predict(X_val)\nprint(accuracy_score(y_val, pred_val))",
            "class": "Model Evaluation",
            "desc": "This code snippet makes predictions on the validation set using the trained model and prints the accuracy score using scikit-learn's `accuracy_score` function.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99506676
            },
            "cluster": -1
        }, {
            "cell_id": 9,
            "code": "pred_test = model.predict(X_test)",
            "class": "Model Evaluation",
            "desc": "This code snippet generates predictions on the test dataset using the trained RandomForestClassifier model.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99438924
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "def build_random_forest(hp):\n    model = ensemble.RandomForestClassifier(\n        n_estimators=hp.Int('n_estimators', 10, 50, step=10),\n        max_depth=hp.Int('max_depth', 3, 10))\n    return model\n\ntuner = kt.tuners.Sklearn(\n    oracle=kt.oracles.BayesianOptimization(\n        objective=kt.Objective('score', 'max'),\n        max_trials=10),\n    hypermodel= build_random_forest,\n    directory='.',\n    project_name='random_forest')\n\ntuner.search(X_train.values, y_train.values.ravel())\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]",
            "class": "Model Training",
            "desc": "This code defines a function to build a RandomForestClassifier with hyperparameters tuned by Keras Tuner's Bayesian Optimization, creates a Keras Tuner instance for hyperparameter tuning, conducts the search over the training data, and retrieves the best hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.7735085
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "model = tuner.hypermodel.build(best_hp)\nmodel.fit(X_train, y_train.values)",
            "class": "Model Training",
            "desc": "This code snippet builds a RandomForestClassifier model with the best hyperparameters obtained from the tuner and fits the model to the training data.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9974711
            },
            "cluster": -1
        }],
        "notebook_id": 19,
        "notebook_name": "keras-tuner-randomforest-titanic.ipynb",
        "user": "keras-tuner-randomforest-titanic.ipynb"
    }],
    "metadata": {
        "comp_name": "Titanic - Machine Learning from Disaster",
        "clusters": {
            "Data Transform": {
                "titles": {
                    "0": "Titanic Dataset Feature Engineering Using Pandas",
                    "1": "Titanic Data Analysis Using Pandas, NumPy",
                    "-1": "Data Preprocessing and Imputation in Python"
                },
                "accuracy": {
                    "silhouette_score": 0.0047632877482813325,
                    "ch_index": 3.7871982032990883,
                    "db_index": 2.6576242592944292
                }
            },
            "Data Extraction": {
                "titles": {
                    "0": "Reading CSV with Pandas DataFrames",
                    "1": "Titanic Data Loading Using Pandas",
                    "-1": "Titanic Dataset Handling with Pandas"
                },
                "accuracy": {
                    "silhouette_score": 0.14076548800429067,
                    "ch_index": 7.891525266555837,
                    "db_index": 1.9053961618394293
                }
            },
            "Visualization": {
                "titles": {
                    "0": "Titanic Data Visualization Using Seaborn",
                    "1": "Data Visualization in Seaborn and Matplotlib",
                    "-1": "Data Visualization with Seaborn, Matplotlib, Missingno"
                },
                "accuracy": {
                    "silhouette_score": 0.10987125995096908,
                    "ch_index": 10.019424924751009,
                    "db_index": 2.15374628546846
                }
            },
            "Model Training": {
                "titles": {
                    "0": "Hyperparameter Tuning in scikit-learn Models",
                    "1": "Machine Learning Models & Hyperparameter Tuning Techniques",
                    "2": "Scikit-Learn Classifier Training and Evaluation",
                    "-1": "scikit-learn and XGBoost Classifier Initialization and Training"
                },
                "accuracy": {
                    "silhouette_score": 0.1337605476889505,
                    "ch_index": 12.944384339035691,
                    "db_index": 2.337802164204171
                }
            },
            "Model Evaluation": {
                "titles": {
                    "0": "Scikit-learn Cross-Validation for Model Comparison",
                    "1": "Scikit-Learn Model Evaluation and Selection",
                    "-1": "Scikit-learn Model Evaluation and Prediction"
                },
                "accuracy": {
                    "silhouette_score": 0.06753874927165082,
                    "ch_index": 5.908861075351337,
                    "db_index": 2.922390351257967
                }
            },
            "Imports and Environment": {
                "titles": {
                    "-1": "Data Science & ML Libraries Setup"
                },
                "accuracy": {
                    "silhouette_score": 0,
                    "ch_index": 0,
                    "db_index": 0
                }
            },
            "Data Export": {
                "titles": {
                    "-1": "Pandas CSV Export for Titanic Predictions"
                },
                "accuracy": {
                    "silhouette_score": 0,
                    "ch_index": 0,
                    "db_index": 0
                }
            },
            "Exploratory Data Analysis": {
                "titles": {
                    "0": "Pandas DataFrame Summary Using `info()` Method",
                    "1": "Data Inspection and Summary with Pandas",
                    "2": "Pandas Display Rows Using `head()`",
                    "3": "Pandas Data Analysis: Missing Values Detection",
                    "4": "Titanic Data Analysis Using Pandas",
                    "5": "Titanic Dataset Analysis with Pandas",
                    "6": "Data Analysis with Pandas and Seaborn",
                    "7": "Pandas DataFrame Exploration and Summary",
                    "8": "Titanic Survival Analysis Using Pandas",
                    "-1": "Titanic Data Analysis Using Pandas"
                },
                "accuracy": {
                    "silhouette_score": 0.10595510858512205,
                    "ch_index": 11.586700501540054,
                    "db_index": 2.2644373883687483
                }
            }
        },
        "clustering_accuracy": 0.3593350383631714
    }
}