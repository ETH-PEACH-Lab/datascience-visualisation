{
    "notebooks": [{
        "cells": [{
            "cell_id": 22,
            "code": "sub = pd.read_csv(dir_path + \"sample_submission.csv\")\n\nprediction = (F.softmax(test_preds[0], dim=1)[:, 1]>min_threshold).int()\n\nsub = pd.read_csv(dir_path + \"sample_submission.csv\")\n\nsub[\"target\"] = prediction\n\nsub.to_csv(\"submission.csv\", index=False)",
            "class": "Data Export",
            "desc": "The code reads a sample submission file, computes binary predictions based on the previously determined optimal threshold, assigns these predictions to the \"target\" column, and writes the results to a new CSV file called \"submission.csv\".",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.974422
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "dir_path = \"/kaggle/input/nlp-getting-started/\"\n\ntrain_df = pd.read_csv(dir_path + \"train.csv\")\n\ntest_df = pd.read_csv(dir_path + \"test.csv\")",
            "class": "Data Extraction",
            "desc": "The code reads the training and testing data from CSV files located at the specified directory path using Pandas.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99974483
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "train_df = train_df.drop(columns=[\"id\", \"keyword\", \"location\"])",
            "class": "Data Transform",
            "desc": "The code drops the \"id\", \"keyword\", and \"location\" columns from the training dataframe `train_df`. ",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.99919885
            },
            "cluster": 2
        }, {
            "cell_id": 5,
            "code": "def remove_URL(text):\n\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n\n    return url.sub(r'',text)\n\n\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_URL)\n\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_URL)",
            "class": "Data Transform",
            "desc": "The code defines a function `remove_URL` to remove URLs from text using regular expressions (re) and applies this function to clean the \"text\" column in both the training and testing dataframes.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9983814
            },
            "cluster": -1
        }, {
            "cell_id": 6,
            "code": "def remove_html(text):\n\n    html=re.compile(r'<.*?>')\n\n    return html.sub(r'',text)\n\n\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_html)\n\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_html)",
            "class": "Data Transform",
            "desc": "The code defines a function `remove_html` to remove HTML tags from text using regular expressions (re) and applies this function to clean the \"text\" column in both the training and testing dataframes.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.995577
            },
            "cluster": -1
        }, {
            "cell_id": 7,
            "code": "def remove_emoji(text):\n\n    emoji_pattern = re.compile(\"[\"\n\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n\n                           u\"\\U00002702-\\U000027B0\"\n\n                           u\"\\U000024C2-\\U0001F251\"\n\n                           \"]+\", flags=re.UNICODE)\n\n    return emoji_pattern.sub(r'', text)\n\n\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_emoji)\n\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_emoji)",
            "class": "Data Transform",
            "desc": "The code defines a function `remove_emoji` to remove emojis from text using regular expressions (re) and applies this function to clean the \"text\" column in both the training and testing dataframes.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.62529504
            },
            "cluster": -1
        }, {
            "cell_id": 12,
            "code": "train_tensor = tokenizer(list(train_df[\"text\"]), padding=\"max_length\",\n\n                        truncation=True, max_length=30,\n\n                        return_tensors=\"pt\")[\"input_ids\"]",
            "class": "Data Transform",
            "desc": "The code tokenizes the \"text\" column of the training dataframe `train_df` into tensors, with padding and truncation to a maximum length of 30 tokens, using the \"roberta-large\" tokenizer from the Hugging Face Transformers library, and extracts the input IDs.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.96896535
            },
            "cluster": 1
        }, {
            "cell_id": 13,
            "code": "class TweetDataset:\n\n    def __init__(self, tensors, targ, ids):\n\n        self.text = tensors[ids, :]\n\n        self.targ = targ[ids].reset_index(drop=True)\n\n    \n\n    def __len__(self):\n\n        return len(self.text)\n\n    \n\n    def __getitem__(self, idx):\n\n        \n\n        t = self.text[idx]\n\n        y = self.targ[idx]\n\n        \n\n        return t, tensor(y)",
            "class": "Data Transform",
            "desc": "The code defines a custom dataset class `TweetDataset` that takes tokenized text tensors, target values, and indices, and provides methods to get the length and indexed items (tokenized text and corresponding target) for use in PyTorch.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.69012034
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "train_ids, valid_ids = RandomSplitter()(train_df)\n\n\n\n\n\ntarget = train_df[\"target\"]\n\n\n\ntrain_ds = TweetDataset(train_tensor, target, train_ids)\n\nvalid_ds = TweetDataset(train_tensor, target, valid_ids)\n\n\n\ntrain_dl = DataLoader(train_ds, bs=64)\n\nvalid_dl = DataLoader(valid_ds, bs=512)\n\ndls = DataLoaders(train_dl, valid_dl).to(\"cuda\")",
            "class": "Data Transform",
            "desc": "The code splits the training data into training and validation sets using `RandomSplitter`, creates custom `TweetDataset` instances for both sets, and then creates DataLoaders (`train_dl` and `valid_dl`) with batch sizes of 64 and 512 respectively, which are combined into a `DataLoaders` object and moved to the CUDA device.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.50315964
            },
            "cluster": 3
        }, {
            "cell_id": 19,
            "code": "test_tensor = tokenizer(list(test_df[\"text\"]),\n\n                        padding=\"max_length\",\n\n                        truncation=True,\n\n                        max_length=30,\n\n                        return_tensors=\"pt\")[\"input_ids\"]",
            "class": "Data Transform",
            "desc": "The code tokenizes the \"text\" column of the test dataframe `test_df` into tensors with padding and truncation to a maximum length of 30 tokens, using the \"roberta-large\" tokenizer from the Hugging Face Transformers library, and extracts the input IDs.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.9433598
            },
            "cluster": 1
        }, {
            "cell_id": 20,
            "code": "class TestDS:\n\n    def __init__(self, tensors):\n\n        self.tensors = tensors\n\n    \n\n    def __len__(self):\n\n        return len(self.tensors)\n\n    \n\n    def __getitem__(self, idx):\n\n        t = self.tensors[idx]\n\n        return t, tensor(0)\n\n\n\ntest_dl = DataLoader(TestDS(test_tensor), bs=128)",
            "class": "Data Transform",
            "desc": "The code defines a custom dataset class `TestDS` for the test data tensors, providing methods to get the length and indexed items (tokenized text), and then creates a DataLoader `test_dl` with a batch size of 128 for the test data.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9415805
            },
            "cluster": 1
        }, {
            "cell_id": 2,
            "code": "train_df",
            "class": "Exploratory Data Analysis",
            "desc": "The code outputs the training dataframe, `train_df`, which helps in understanding and inspecting the data. ",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99972683
            },
            "cluster": 3
        }, {
            "cell_id": 4,
            "code": "train_df[\"target\"].value_counts()",
            "class": "Exploratory Data Analysis",
            "desc": "The code counts the occurrences of each unique value in the \"target\" column of the training dataframe `train_df` using Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.999521
            },
            "cluster": 3
        }, {
            "cell_id": 8,
            "code": "train_df",
            "class": "Exploratory Data Analysis",
            "desc": "The code outputs the cleaned training dataframe, `train_df`, which helps in inspecting the data after the cleaning transformations have been applied.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99972683
            },
            "cluster": 3
        }, {
            "cell_id": 0,
            "code": "import numpy as np\n\nimport pandas as pd\n\nfrom fastai.text.all import *\n\nimport re",
            "class": "Imports and Environment",
            "desc": "The code imports necessary libraries including NumPy, Pandas, Fastai's text module, and re.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99934
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "from transformers import AutoTokenizer, AutoModelForSequenceClassification",
            "class": "Imports and Environment",
            "desc": "The code imports `AutoTokenizer` and `AutoModelForSequenceClassification` from the Hugging Face Transformers library.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99929786
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "from sklearn.metrics import f1_score\n\n\n\npreds, targs = learn.get_preds()\n\n\n\nmin_threshold = None\n\nmax_f1 = -float(\"inf\")\n\nthresholds = np.linspace(0.3, 0.7, 50)\n\nfor threshold in thresholds:\n\n    f1 = f1_score(targs, F.softmax(preds, dim=1)[:, 1]>threshold)\n\n    if f1 > max_f1:\n\n        min_threshold = threshold\n\n        min_f1 = f1\n\n    print(f\"threshold:{threshold:.4f} - f1:{f1:.4f}\")",
            "class": "Model Evaluation",
            "desc": "The code calculates predictions and targets from the trained learner, then iterates over a range of thresholds to find the one that maximizes the F1 score by evaluating the predictions as binary outcomes using the sklearn `f1_score` function.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.6100683
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "test_preds = learn.get_preds(dl=test_dl)",
            "class": "Model Evaluation",
            "desc": "The code obtains predictions for the test dataset by using the `get_preds` method on the learner with the test DataLoader `test_dl`.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9943605
            },
            "cluster": 2
        }, {
            "cell_id": 11,
            "code": "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")",
            "class": "Model Training",
            "desc": "The code initializes a tokenizer by loading the pre-trained \"roberta-large\" tokenizer from the Hugging Face Transformers library.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9938804
            },
            "cluster": -1
        }, {
            "cell_id": 15,
            "code": "bert = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2).train().to(\"cuda\")\n\n\n\nclass BertClassifier(Module):\n\n    def __init__(self, bert):\n\n        self.bert = bert\n\n    def forward(self, x):\n\n        return self.bert(x).logits\n\n\n\nmodel = BertClassifier(bert)",
            "class": "Model Training",
            "desc": "The code initializes the \"roberta-large\" pre-trained BERT model for sequence classification with 2 labels, moves it to training mode and CUDA, defines a custom `BertClassifier` module for the model, and assigns it to the variable `model`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.86304575
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "learn = Learner(dls, model, metrics=[accuracy, F1Score()]).to_fp16()\n\nlearn.lr_find()",
            "class": "Model Training",
            "desc": "The code creates a `Learner` object with the data loaders, model, and specified metrics (accuracy and F1Score), converts the model to 16-bit floating point precision (FP16) for faster training, and runs a learning rate finder to determine the optimal learning rate.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9948531
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "learn.fit_one_cycle(3, lr_max=1e-5)",
            "class": "Model Training",
            "desc": "The code trains the model using the one-cycle policy for 3 epochs with a maximum learning rate of 1e-5 using the Fastai library.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996973
            },
            "cluster": -1
        }, {
            "cell_id": 9,
            "code": "train_df[\"text\"].apply(lambda x:len(x.split())).plot(kind=\"hist\");",
            "class": "Visualization",
            "desc": "The code plots a histogram of the number of words per text entry in the \"text\" column of the training dataframe `train_df` using Pandas' plotting functionality.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.6361653
            },
            "cluster": 2
        }],
        "notebook_id": 0,
        "notebook_name": "roberta-with-pytorch-and-fastai.ipynb",
        "user": "roberta-with-pytorch-and-fastai.ipynb"
    }, {
        "cells": [{
            "cell_id": 22,
            "code": "my_submission.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "The code exports the `my_submission` DataFrame to a CSV file named `submission.csv` without including the DataFrame's index by using the `to_csv()` method in Pandas.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99912554
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nsample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")",
            "class": "Data Extraction",
            "desc": "The code reads the train, test, and sample submission CSV files into Pandas DataFrames using `pd.read_csv`.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99971575
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "from sklearn.model_selection import train_test_split\n\nX = train['text'] + ' ' +  train['keyword'].astype(str) + ' ' +  train['location'].astype(str) # the features we want to analyze\nylabels = train['target'] # the labels, or answers, we want to test against\n\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)",
            "class": "Data Transform",
            "desc": "The code concatenates the 'text', 'keyword', and 'location' fields from the `train` DataFrame to create the feature `X`, assigns the 'target' column to `ylabels`, and splits the data into training and testing sets using `train_test_split` from scikit-learn.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9936372
            },
            "cluster": 3
        }, {
            "cell_id": 9,
            "code": "\npunctuations = string.punctuation \nnlp = spacy.load('en_core_web_sm') #, exclude=[\"tok2vec\", \"parser\", \"ner\", \"attribute_ruler\"]\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\nparser = English() # Load English tokenizer, tagger, parser, NER and word vectors\n\ndef spacy_tokenizer(sentence):\n    mytokens = str(sentence)\n    mytokens = nlp(mytokens)\n    #mytokens = parser(sentence) \n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] \n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]    \n    return mytokens      # return preprocessed list of tokens\n\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\ndef clean_text(text):\n    text =  text.strip().lower()\n    #text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n    return text #.split()\n\nbow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1), stop_words = None)\ntfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer,  stop_words = None) #token_pattern='(?u)\\b\\w\\w+\\b', stop_words = 'english'\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = LogisticRegression()\n# classifier = RandomForestClassifier()\n\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', tfidf_vector),\n                 ('classifier', classifier)])\n\n#clean_text(X_train[1773])\n#spacy_tokenizer(X_train[1773])\n#mytokens = parser(X_train[1773])\n\n# mytokens = str(X_train[1773])\n# #mytokens = re.sub(r'[^A-Za-z0-9 ]+', '', mytokens)\n# #mytokens = parser(mytokens)\n# mytokens = nlp(mytokens)\n# mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n# print(mytokens)",
            "class": "Data Transform",
            "desc": "The code defines a custom tokenizer using spaCy, creates a custom transformer class for data cleaning, and initializes vectorizers and a logistic regression classifier to be used in a scikit-learn pipeline for text preprocessing and classification.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.5153541
            },
            "cluster": 1
        }, {
            "cell_id": 19,
            "code": "my_submission_preds = pipe.predict(test['text']+ ' ' +  test['keyword'].astype(str) + ' ' +  test['location'].astype(str))\n\nmy_submission = pd.DataFrame({\"id\":test['id'], 'target':my_submission_preds})",
            "class": "Data Transform",
            "desc": "The code generates predictions for the test dataset using the previously trained pipeline and then creates a new DataFrame containing the 'id' from the `test` DataFrame and the predicted 'target' values.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.7191785
            },
            "cluster": 1
        }, {
            "cell_id": 2,
            "code": "train.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first few rows of the `train` DataFrame using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997507
            },
            "cluster": 2
        }, {
            "cell_id": 3,
            "code": "test.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first few rows of the `test` DataFrame using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997483
            },
            "cluster": 2
        }, {
            "cell_id": 4,
            "code": "print(train.apply(lambda col: col.unique()))\nprint(train.apply(lambda col: col.nunique()))",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the unique values of each column and the count of unique values in each column of the `train` DataFrame using the `apply` method with `lambda` functions.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.95746493
            },
            "cluster": 1
        }, {
            "cell_id": 8,
            "code": "X_train[100:500]\n#type(X_train[1])\n#y_train[:100]",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays a subset of the `X_train` data from index 100 to 500 for inspection.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9915615
            },
            "cluster": 4
        }, {
            "cell_id": 14,
            "code": "predicted_df.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first few rows of the `predicted_df` DataFrame using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997552
            },
            "cluster": 2
        }, {
            "cell_id": 15,
            "code": "test.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first few rows of the `test` DataFrame using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997483
            },
            "cluster": 2
        }, {
            "cell_id": 16,
            "code": "sample_submission.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first few rows of the `sample_submission` DataFrame using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99975234
            },
            "cluster": 2
        }, {
            "cell_id": 18,
            "code": "test",
            "class": "Exploratory Data Analysis",
            "desc": "The code likely refers to displaying or inspecting the `test` DataFrame, which contains the test data imported earlier.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997199
            },
            "cluster": 4
        }, {
            "cell_id": 20,
            "code": "my_submission.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first few rows of the `my_submission` DataFrame using the `head()` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997489
            },
            "cluster": 2
        }, {
            "cell_id": 21,
            "code": "len(my_submission)",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the number of rows in the `my_submission` DataFrame using the `len()` function.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.99885213
            },
            "cluster": 2
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "The code imports necessary libraries such as NumPy and Pandas, and lists all files in the specified input directory using the os module.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.99921954
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "!pip install spacy -q\n!python -m spacy download en_core_web_sm -q",
            "class": "Imports and Environment",
            "desc": "The code installs the spaCy library and downloads the English language model `en_core_web_sm` using pip and spaCy's command line interface.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.993651
            },
            "cluster": 1
        }, {
            "cell_id": 6,
            "code": "import matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nimport spacy\nimport en_core_web_sm\nimport re",
            "class": "Imports and Environment",
            "desc": "The code imports necessary libraries and modules, including Matplotlib for plotting, various feature extraction tools from scikit-learn, spaCy and its language models, and regular expressions.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9992192
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "from sklearn import metrics\n# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))\nprint(\"Precision:\",metrics.precision_score(y_test, predicted))\nprint(\"Recall:\",metrics.recall_score(y_test, predicted))",
            "class": "Model Evaluation",
            "desc": "The code evaluates the trained model on the test data (`X_test`) by predicting labels and calculating accuracy, precision, and recall using scikit-learn's `metrics` module.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.758966
            },
            "cluster": 2
        }, {
            "cell_id": 12,
            "code": "predicted_df = pd.DataFrame(predicted)\npredicted_df.value_counts()",
            "class": "Model Evaluation",
            "desc": "The code creates a DataFrame from the predicted values and calculates the frequency count of each unique prediction using the `value_counts` method in Pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.99671495
            },
            "cluster": 2
        }, {
            "cell_id": 17,
            "code": "predicted",
            "class": "Model Evaluation",
            "desc": "The code outputs the predicted values generated by the model for the `X_test` data.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99976486
            },
            "cluster": 3
        }, {
            "cell_id": 10,
            "code": "pipe.fit(X_train, y_train)",
            "class": "Model Training",
            "desc": "The code trains the scikit-learn pipeline, which includes text preprocessing, vectorization, and classification steps, on the training data (`X_train`, `y_train`).",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99970585
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "predicted_df.plot.hist()",
            "class": "Visualization",
            "desc": "The code generates a histogram plot of the predicted values using the `plot.hist()` method in Pandas.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9975446
            },
            "cluster": 2
        }],
        "notebook_id": 1,
        "notebook_name": "nlp-starter-spacy-binary-text-classifier.ipynb",
        "user": "nlp-starter-spacy-binary-text-classifier.ipynb"
    }, {
        "cells": [{
            "cell_id": 13,
            "code": "submission = pd.DataFrame({\n    'id': test_raw.id,\n    'target':y_hat\n})",
            "class": "Data Export",
            "desc": "The code creates a pandas DataFrame named `submission` containing the IDs from the test data and the predicted target values.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.9941958
            },
            "cluster": -1
        }, {
            "cell_id": 14,
            "code": "submission.to_csv(\"my_submission_linear.csv\", index=False)",
            "class": "Data Export",
            "desc": "The code saves the `submission` DataFrame to a CSV file named \"my_submission_linear.csv\" without including the DataFrame index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99924576
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")",
            "class": "Data Extraction",
            "desc": "The code reads CSV files containing training, testing, and sample submission data into pandas DataFrames named `train_raw`, `test_raw`, and `submission_raw` respectively.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997112
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.3)",
            "class": "Data Extraction",
            "desc": "The code splits the transformed training data and labels into training and testing subsets, with 30% of the data reserved for testing, using the `train_test_split` method from sklearn.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.99791616
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "# remove stopwords,punct\n# remove duplicate tweet\ntexts = []\nlabels = []\ntexts_md5 = set()\nfor target, doc in zip(train_raw.target, nlp.pipe(train_raw.text)):\n    tokens = [token.lemma_ for token in doc if token.is_stop is False and token.is_punct is False and token.is_space is False]\n    temp_text = ' '.join(tokens)\n    # remove duplicate\n    md5 = hashlib.md5()\n    md5.update(temp_text.encode('utf-8'))\n    text_md5 = md5.hexdigest()\n    if text_md5 not in texts_md5:\n        texts.append(temp_text)\n        labels.append(target)\n        texts_md5.add(text_md5)",
            "class": "Data Transform",
            "desc": "The code processes the text data by removing stopwords, punctuation, and duplicates, then lemmatizes the words using spaCy, and stores the cleaned texts and their corresponding labels, ensuring no duplicates using MD5 hashing.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9552063
            },
            "cluster": -1
        }, {
            "cell_id": 5,
            "code": "tests = []\nfor doc in nlp.pipe(test_raw.text):\n    tokens = [token.lemma_ for token in doc if token.is_stop is False and token.is_punct is False and token.is_space is False]\n    tests.append(' '.join(tokens))",
            "class": "Data Transform",
            "desc": "The code processes the test data by removing stopwords, punctuation, and whitespace, and then lemmatizes the words using spaCy, storing the cleaned texts in the `tests` list.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.9769126
            },
            "cluster": -1
        }, {
            "cell_id": 6,
            "code": "tf_idf = TfidfVectorizer(max_features=10000).fit(texts)\ntrain = tf_idf.transform(texts)\ntest = tf_idf.transform(tests)",
            "class": "Data Transform",
            "desc": "The code creates a TF-IDF vectorizer limited to 10,000 features, fits it on the cleaned training texts, and transforms both the training and test texts into TF-IDF vectors.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9555307
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "The code imports essential libraries like NumPy and pandas for data processing and uses the os library to list all files in the Kaggle input directory.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.99921954
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "import hashlib\n\nimport spacy\nimport sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score",
            "class": "Imports and Environment",
            "desc": "The code imports various libraries and modules, including `hashlib`, `spacy`, and several components from `sklearn` such as SVM, text vectorizers, model selection tools, and evaluation metrics for use in machine learning tasks.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99932015
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "nlp = spacy.load('en')",
            "class": "Imports and Environment",
            "desc": "The code loads the English language model in spaCy and assigns it to the variable `nlp`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9950819
            },
            "cluster": -1
        }, {
            "cell_id": 10,
            "code": "best_params= {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\nsvc = SVC(**best_params)\nscores = cross_val_score(svc,X_train, y_train, cv=5, scoring='f1')\nprint(scores)\nprint(sum(scores)/len(scores))",
            "class": "Model Evaluation",
            "desc": "The code initializes an SVM model with the best parameters obtained from the grid search, evaluates it using 5-fold cross-validation on the training set, and prints the individual fold scores as well as their average f1-score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.9838514
            },
            "cluster": 0
        }, {
            "cell_id": 11,
            "code": "val_texts = [\"A happy day!\", 'An earthquake happened!']\nval_data = tf_idf.transform(val_texts)\nsvc.fit(X_train, y_train)\nprint(svc.predict(val_data))",
            "class": "Model Evaluation",
            "desc": "The code transforms specific validation texts into TF-IDF vectors, fits the SVM model with the training data, and prints the prediction results for these validation texts.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.6037878
            },
            "cluster": 3
        }, {
            "cell_id": 12,
            "code": "y_hat = svc.predict(test)",
            "class": "Model Evaluation",
            "desc": "The code uses the trained SVM model to make predictions on the transformed test data and stores the results in `y_hat`.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99424076
            },
            "cluster": 2
        }, {
            "cell_id": 8,
            "code": "param_grid = {\n    \"gamma\" : [0.001,0.01,1,10,100],\n    \"C\":[0.001,0.01,1,10,100],\n    'kernel' : ['poly', \"linear\", 'sigmoid', 'rbf']\n}",
            "class": "Model Training",
            "desc": "The code defines a parameter grid for hyperparameter tuning of an SVM model, specifying different values for `gamma`, `C`, and `kernel` types.",
            "testing": {
                "class": "Model_Train",
                "subclass": "define_search_space",
                "subclass_id": 5,
                "predicted_subclass_probability": 0.99403256
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "svc = SVC()\ngrid_searcher = GridSearchCV(svc, param_grid, cv=5, scoring='f1')\ngrid_searcher.fit(X_train, y_train)\ngrid_searcher.best_params_",
            "class": "Model Training",
            "desc": "The code initializes an SVM model (`SVC`), performs a grid search cross-validation (`GridSearchCV`) using the defined parameter grid, fits it to the training data, and identifies the best hyperparameters based on the `f1` scoring metric.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.99040365
            },
            "cluster": 0
        }],
        "notebook_id": 2,
        "notebook_name": "baseline-svc-79.ipynb",
        "user": "baseline-svc-79.ipynb"
    }, {
        "cells": [{
            "cell_id": 39,
            "code": "submission = pd.DataFrame({'id':test['id'].values.tolist(),'target':predictions})\nsubmission.to_csv('submission.csv',index=False)",
            "class": "Data Export",
            "desc": "This code creates a DataFrame for submission containing the test IDs and the corresponding predicted labels, and then exports it to a CSV file named 'submission.csv' without including the DataFrame index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992361
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')",
            "class": "Data Extraction",
            "desc": "This code reads the training and test datasets from CSV files using pandas' `read_csv` method.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997564
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "string.punctuation",
            "class": "Data Transform",
            "desc": "This code retrieves a string of all punctuation characters from the `string` module for potential use in text cleaning or processing tasks.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.73409814
            },
            "cluster": 1
        }, {
            "cell_id": 6,
            "code": "def remove_URL(text):\n    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n    return url.sub(r\"\", text)\n\ndef remove_punct(text):\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(translator)",
            "class": "Data Transform",
            "desc": "This code defines two functions, `remove_URL` and `remove_punct`, that remove URLs using regular expressions and punctuation using Python's `str.translate` method, respectively, to clean text data.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.7244959
            },
            "cluster": -1
        }, {
            "cell_id": 7,
            "code": "#regex pattern to remove links\npattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n#for train\nfor t in train.text:\n    matches = pattern.findall(t)\n    for match in matches:\n        print(t)\n        print('After Transformation:')\n        print(pattern.sub(r\"\", t))\n    if len(matches) > 0:\n        break",
            "class": "Data Transform",
            "desc": "This code defines a regex pattern to identify URLs in text and then iterates through the training dataset's text, printing each text and its transformation after removing the URLs using the regex pattern.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9533664
            },
            "cluster": 1
        }, {
            "cell_id": 8,
            "code": "#for test:\nfor t in test.text:\n    matches = pattern.findall(t)\n    for match in matches:\n        print(t)\n        print('After Transformation:')\n        print(pattern.sub(r\"\", t))\n    if len(matches) > 0:\n        break",
            "class": "Data Transform",
            "desc": "This code iterates through the test dataset's text, identifying and printing each text along with its transformation after removing URLs using the same previously defined regex pattern.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9523689
            },
            "cluster": 1
        }, {
            "cell_id": 9,
            "code": "#preprocess data frames:\n#train\ntrain[\"text\"] = train.text.map(remove_URL) \ntrain[\"text\"] = train.text.map(remove_punct)\n#test\ntest[\"text\"] = test.text.map(remove_URL) \ntest[\"text\"] = test.text.map(remove_punct)",
            "class": "Data Transform",
            "desc": "This code preprocesses the text data in both training and test datasets by applying the previously defined functions `remove_URL` and `remove_punct` to remove URLs and punctuation, respectively, using the `map` method of pandas DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.90332294
            },
            "cluster": -1
        }, {
            "cell_id": 10,
            "code": "# remove stopwords\nnltk.download('stopwords')\n\nstop = set(stopwords.words(\"english\"))\n\ndef remove_stopwords(text):\n    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n    return \" \".join(filtered_words)",
            "class": "Data Transform",
            "desc": "This code downloads the NLTK stopwords corpus and defines a function, `remove_stopwords`, to remove stopwords from text by filtering out any words present in the predefined stopwords set and then rejoining the filtered words into a string.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.9945669
            },
            "cluster": -1
        }, {
            "cell_id": 11,
            "code": "stop",
            "class": "Data Transform",
            "desc": "This code snippet retrieves the set of English stopwords from the NLTK stopwords corpus.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9975351
            },
            "cluster": 1
        }, {
            "cell_id": 12,
            "code": "#train\ntrain[\"text\"] = train.text.map(remove_stopwords)\n#test\ntest[\"text\"] = test.text.map(remove_stopwords)",
            "class": "Data Transform",
            "desc": "This code preprocesses the text data in both training and test datasets by applying the `remove_stopwords` function to remove stopwords using the `map` method of pandas DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.76800144
            },
            "cluster": -1
        }, {
            "cell_id": 20,
            "code": "#train/val\ntrain_sentences = train_sentences.to_numpy()\ntrain_labels = train_labels.to_numpy()\nval_sentences = val_sentences.to_numpy()\nval_labels = val_labels.to_numpy()",
            "class": "Data Transform",
            "desc": "This code converts the training and validation text sentences and labels into numpy arrays using the `to_numpy` method of pandas Series.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.9735998
            },
            "cluster": 4
        }, {
            "cell_id": 21,
            "code": "#test\ntest_sentences = test.text.to_numpy()",
            "class": "Data Transform",
            "desc": "This code converts the text data in the test dataset into a numpy array using the `to_numpy` method of a pandas Series.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.98378074
            },
            "cluster": 4
        }, {
            "cell_id": 23,
            "code": "# Tokenize\n# vectorize a text corpus by turning each text into a sequence of integers\n\ntokenizer = Tokenizer(num_words=num_unique_words)\ntokenizer.fit_on_texts(train_sentences) # fit only to training",
            "class": "Data Transform",
            "desc": "This code initializes a `Tokenizer` from `tensorflow.keras.preprocessing.text` with the number of unique words identified earlier, and fits it on the training sentences to map texts into sequences of integers.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.6493907
            },
            "cluster": 4
        }, {
            "cell_id": 24,
            "code": "# Now each word has unique index\nword_index = tokenizer.word_index\nword_index",
            "class": "Data Transform",
            "desc": "This code retrieves the dictionary `word_index` from the `Tokenizer`, which maps each word to a unique integer index, and then outputs this dictionary.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.66235614
            },
            "cluster": -1
        }, {
            "cell_id": 25,
            "code": "#apply on train, validation, and test sentences\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\nval_sequences = tokenizer.texts_to_sequences(val_sentences)\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)",
            "class": "Data Transform",
            "desc": "This code converts the training, validation, and test sentences into sequences of integers using the `texts_to_sequences` method of the previously fitted `Tokenizer`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.5915242
            },
            "cluster": -1
        }, {
            "cell_id": 26,
            "code": "#Check\nprint(train_sentences[10:15])\nprint(train_sequences[10:15])",
            "class": "Data Transform",
            "desc": "This code prints a sample of original training sentences alongside their corresponding sequences of integers, generated by the `Tokenizer`, to verify the transformation.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9992624
            },
            "cluster": 1
        }, {
            "cell_id": 27,
            "code": "# Pad the sequences to have the same length\nmax_length = 15 #arbitrary number\n\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\") #post-> 0\nval_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")",
            "class": "Data Transform",
            "desc": "This code pads the integer sequences of training, validation, and test datasets to a fixed length of 15 using the `pad_sequences` method from `tensorflow.keras.preprocessing.sequence` with post-padding and post-truncating.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.717097
            },
            "cluster": -1
        }, {
            "cell_id": 30,
            "code": "#Check\nprint(train_sentences[10])\nprint(train_sequences[10])\nprint(train_padded[10])",
            "class": "Data Transform",
            "desc": "This code snippet prints the original sentence, its corresponding sequence of integers, and the padded sequence for the 11th training sample to verify the transformations applied.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99758375
            },
            "cluster": 1
        }, {
            "cell_id": 31,
            "code": "# flip (key, value)\nreverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])",
            "class": "Data Transform",
            "desc": "This code creates a dictionary `reverse_word_index` by flipping the keys and values of the `word_index` dictionary, mapping integer indices back to their corresponding words.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9888599
            },
            "cluster": -1
        }, {
            "cell_id": 33,
            "code": "#decoding\ndef decode(sequence):\n    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])",
            "class": "Data Transform",
            "desc": "This code defines a function `decode` which takes an integer sequence and converts it back into a string of words using the `reverse_word_index` dictionary.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.95032895
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "train.head()",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first few rows of the training dataset using the `head` method of a pandas DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997507
            },
            "cluster": 2
        }, {
            "cell_id": 3,
            "code": "train.shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the dimensions of the training dataset using the `shape` attribute of a pandas DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.99950194
            },
            "cluster": 2
        }, {
            "cell_id": 4,
            "code": "print((train.target == 1).sum()) # Disaster\nprint((train.target == 0).sum()) # No Disaster",
            "class": "Exploratory Data Analysis",
            "desc": "This code prints the count of instances in the training dataset that are labeled as disasters (target=1) and non-disasters (target=0) using pandas Series comparison and the `sum` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99325174
            },
            "cluster": 1
        }, {
            "cell_id": 13,
            "code": "#Check\ntrain.text",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet outputs the transformed text data from the training dataset to inspect the results of the applied text preprocessing steps.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.53716487
            },
            "cluster": 4
        }, {
            "cell_id": 14,
            "code": "# Count unique words\ndef counter_word(text_col):\n    count = Counter()\n    for text in text_col.values:\n        for word in text.split():\n            count[word] += 1\n    return count\n\n\ncounter = counter_word(train.text)",
            "class": "Exploratory Data Analysis",
            "desc": "This code defines a function, `counter_word`, which counts the occurrences of unique words in a text column using the `Counter` class from the `collections` module, and then it applies this function to the preprocessed text data in the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.94201726
            },
            "cluster": 1
        }, {
            "cell_id": 15,
            "code": "len(counter)",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the number of unique words in the training dataset's text column by applying the `len` function to the `counter` object.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.98219407
            },
            "cluster": 1
        }, {
            "cell_id": 16,
            "code": "# counter",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet outputs the `counter` object containing the frequency of unique words in the training dataset's text column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.9903482
            },
            "cluster": 1
        }, {
            "cell_id": 17,
            "code": "counter.most_common(5)",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the five most common words in the training dataset's text column along with their frequencies by using the `most_common` method of the `Counter` object.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9768316
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "num_unique_words = len(counter)\nnum_unique_words",
            "class": "Exploratory Data Analysis",
            "desc": "This code assigns the number of unique words in the training dataset's text column to the variable `num_unique_words` and then outputs this value.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.9817012
            },
            "cluster": 4
        }, {
            "cell_id": 22,
            "code": "train_sentences.shape, val_sentences.shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the dimensions of the training and validation text sentences by using the `shape` attribute of the numpy arrays.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9996413
            },
            "cluster": 2
        }, {
            "cell_id": 28,
            "code": "#Check\ntrain_padded.shape, val_padded.shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the dimensions of the padded sequences for the training and validation datasets by using the `shape` attribute of the numpy arrays.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9994609
            },
            "cluster": 2
        }, {
            "cell_id": 29,
            "code": "#Check\ntrain_padded[10]",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the padded sequence corresponding to the 11th training sample to inspect the result of the padding process.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9994702
            },
            "cluster": 4
        }, {
            "cell_id": 32,
            "code": "#Check\nreverse_word_index",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the `reverse_word_index` dictionary to inspect the mapping of integer indices back to their corresponding words.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.98285496
            },
            "cluster": 3
        }, {
            "cell_id": 34,
            "code": "decoded_text = decode(train_sequences[10])\n#Check\nprint(train_sequences[10])\nprint(decoded_text)",
            "class": "Exploratory Data Analysis",
            "desc": "This code decodes the 11th training sequence back into text using the `decode` function and prints both the original sequence of integers and the decoded text for verification.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.42135346
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "import numpy as np \nimport pandas as pd \nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom tensorflow.keras import layers",
            "class": "Imports and Environment",
            "desc": "This code imports various libraries and modules such as numpy, pandas, nltk, collections, sklearn, and TensorFlow for data manipulation, text processing, machine learning, and deep learning tasks.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993357
            },
            "cluster": 0
        }, {
            "cell_id": 38,
            "code": "predictions = model.predict(test_padded)\npredictions = [1 if p > 0.5 else 0 for p in predictions]",
            "class": "Model Evaluation",
            "desc": "This code makes predictions on the padded test data using the trained LSTM model and converts the predicted probabilities to binary class labels (1 or 0) based on a threshold of 0.5.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99410737
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "# Split dataset into training and validation set\nX = train.text\ny = train.target\ntrain_sentences, val_sentences , train_labels, val_labels = train_test_split(X, y, test_size=0.2)",
            "class": "Model Training",
            "desc": "This code splits the dataset into training and validation sets by separating the text (`X`) and target labels (`y`), and then using the `train_test_split` method from `sklearn.model_selection` with 20% of the data assigned to the validation set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9967854
            },
            "cluster": 0
        }, {
            "cell_id": 35,
            "code": "# Create LSTM model\n\n# Embedding: Turns positive integers (indexes) into dense vectors of fixed size.\n\nmodel = keras.models.Sequential()\nmodel.add(layers.Embedding(num_unique_words, 100, input_length=max_length))\n\nmodel.add(layers.LSTM(32, dropout=0.25))\nmodel.add(layers.Dense(1, activation=\"sigmoid\"))\n\nmodel.summary()",
            "class": "Model Training",
            "desc": "This code creates a Sequential LSTM model using TensorFlow's Keras API, comprising an Embedding layer for mapping word indices to dense vectors, an LSTM layer with 32 units and 25% dropout for sequence processing, and a Dense layer with a sigmoid activation for binary classification, and then prints the model summary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.984528
            },
            "cluster": 2
        }, {
            "cell_id": 36,
            "code": "loss = keras.losses.BinaryCrossentropy(from_logits=False)\noptim = keras.optimizers.Adam(learning_rate=0.001)\nmetrics = [\"accuracy\"]\n\nmodel.compile(loss=loss, optimizer=optim, metrics=metrics)",
            "class": "Model Training",
            "desc": "This code compiles the LSTM model with binary crossentropy loss, Adam optimizer with a learning rate of 0.001, and accuracy as the evaluation metric using TensorFlow's Keras API.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9955751
            },
            "cluster": 2
        }, {
            "cell_id": 37,
            "code": "model.fit(train_padded, train_labels, epochs=25, validation_data=(val_padded, val_labels), verbose=2)",
            "class": "Model Training",
            "desc": "This code trains the compiled LSTM model for 25 epochs using the padded training data and labels, and evaluates it against the validation data and labels, while providing detailed output for each epoch with `verbose=2`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996841
            },
            "cluster": 1
        }],
        "notebook_id": 3,
        "notebook_name": "nlp-tokenization-embedding-lstm.ipynb",
        "user": "nlp-tokenization-embedding-lstm.ipynb"
    }, {
        "cells": [{
            "cell_id": 37,
            "code": "preds = np.squeeze(model.predict(test_multi_input_dataset.batch(32)))\npreds = (preds >= 0.5).astype(int)\npd.DataFrame({\"id\": test_df.id, \"target\": preds}).to_csv(\"submission.csv\", index=False)",
            "class": "Data Export",
            "desc": "The code generates predictions on the test dataset using the trained model, converts these predictions to binary labels, and saves the results as a CSV file named \"submission.csv\".",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992853
            },
            "cluster": -1
        }, {
            "cell_id": 4,
            "code": "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")",
            "class": "Data Extraction",
            "desc": "The code reads CSV files containing the training and test datasets into pandas DataFrames named 'train_df' and 'test_df', respectively.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99975425
            },
            "cluster": 1
        }, {
            "cell_id": 16,
            "code": "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n    train_df[[\"text\", \"keyword\"]], train_df[\"target\"], test_size=0.3, random_state=42, stratify=train_df[\"target\"]\n)",
            "class": "Data Extraction",
            "desc": "The code splits the 'train_df' DataFrame into training and validation sets for features ('text' and 'keyword') and the 'target' label using stratified sampling with a 30% validation size and a random state of 42.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9980861
            },
            "cluster": -1
        }, {
            "cell_id": 10,
            "code": "# We'll use these weights later on to make up for the slightly imbalanced dataset\nclasses = np.unique(train_df[\"target\"])\nclass_weights = sklearn.utils.class_weight.compute_class_weight(\n    \"balanced\", classes=classes, y=train_df[\"target\"]\n)\n\nclass_weights = {clazz : weight for clazz, weight in zip(classes, class_weights)}",
            "class": "Data Transform",
            "desc": "The code computes class weights to handle imbalanced data in the 'target' column of 'train_df' and stores these weights in a dictionary.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.3043228
            },
            "cluster": 4
        }, {
            "cell_id": 11,
            "code": "# Commented out the graceful handling of duplicated because the Kaggle kernel version of statistics.mode()\n# won't handle multimodal results\n\n# Duplicates aren't consistently labeled, so we keep one example of the most frequently occuring label\n# train_df[\"duplicated\"] = train_df.duplicated(subset=\"text\")\n# duplicated_tweets = train_df.loc[lambda df: df[\"duplicated\"] == True, :]\n# aggregated_duplicates = duplicated_tweets.groupby(\"text\", as_index=False).aggregate(\n#     statistics.mode\n# )\n\n# train_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\n# train_df = train_df.append(aggregated_duplicates, ignore_index=True)\n\ntrain_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\nprint(\"train rows:\", len(train_df.index))\nprint(\"test rows:\", len(test_df.index))",
            "class": "Data Transform",
            "desc": "The code removes duplicate rows in the 'train_df' based on the 'text' column and prints the updated number of rows in both the 'train_df' and 'test_df'.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.7884562
            },
            "cluster": 2
        }, {
            "cell_id": 12,
            "code": "class TweetPreProcessor:\n    \"\"\"\n    This class does some cleaning and normalization prior to BPE tokenization\n    \"\"\"\n\n    def __init__(self):\n\n        self.text_processor = TextPreProcessor(\n            # terms that will be normalized\n            normalize=[\n                \"url\",\n                \"email\",\n                \"phone\",\n                \"user\",\n                \"time\",\n                \"date\",\n            ],\n            # terms that will be annotated\n            annotate={\"repeated\", \"elongated\"},\n            # corpus from which the word statistics are going to be used\n            # for word segmentation\n            segmenter=\"twitter\",\n            # corpus from which the word statistics are going to be used\n            # for spell correction\n            spell_correction=True,\n            corrector=\"twitter\",\n            unpack_hashtags=False,  # perform word segmentation on hashtags\n            unpack_contractions=False,  # Unpack contractions (can't -> can not)\n            spell_correct_elong=True,  # spell correction for elongated words\n            fix_bad_unicode=True,\n            tokenizer=Tokenizer(lowercase=True).tokenize,\n            # list of dictionaries, for replacing tokens extracted from the text,\n            # with other expressions. You can pass more than one dictionaries.\n            dicts=[emoticons, slangdict],\n        )\n\n    def preprocess_tweet(self, tweet):\n        return \" \".join(self.text_processor.pre_process_doc(tweet))\n    \n    # this will return the tokenized text     \n    def __call__(self, tweet):\n        return self.text_processor.pre_process_doc(tweet)\n    \ntweet_preprocessor = TweetPreProcessor()",
            "class": "Data Transform",
            "desc": "The code defines a `TweetPreProcessor` class using the 'ekphrasis' library to perform cleaning and normalization of tweets, then instantiates an object of this class.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.95362186
            },
            "cluster": 0
        }, {
            "cell_id": 14,
            "code": "train_df[\"text\"] = train_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)\ntest_df[\"text\"] = test_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)",
            "class": "Data Transform",
            "desc": "The code applies the `TweetPreProcessor` to preprocess the 'text' column in both the 'train_df' and 'test_df' DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.72112745
            },
            "cluster": 2
        }, {
            "cell_id": 15,
            "code": "# Fill NA\ntrain_df[\"keyword\"].fillna(\"\", inplace=True)\ntest_df[\"keyword\"].fillna(\"\", inplace=True)\n\n# remove %20 from keywords\ntrain_df[\"keyword\"] = train_df[\"keyword\"].apply(urllib.parse.unquote)\ntest_df[\"keyword\"] = test_df[\"keyword\"].apply(urllib.parse.unquote)",
            "class": "Data Transform",
            "desc": "The code fills null values in the 'keyword' column with empty strings and removes '%20' from keywords by decoding URL-encoded strings in both the 'train_df' and 'test_df' DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.98655176
            },
            "cluster": 2
        }, {
            "cell_id": 17,
            "code": "def tokenize_encode(tweets, max_length=None):\n    return pretrained_bert_tokenizer(\n        tweets,\n        add_special_tokens=True,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        return_tensors=\"tf\",\n    )\n\n\n# need to be explicit about the lengths (instead of just specifying padding=True in the tokenizer)\n# otherwise train tweets end up being 71 and validation tweets end up as 70, which causes problems/warnings\nmax_length_tweet = 72\nmax_length_keyword = 8\n\ntrain_tweets_encoded = tokenize_encode(x_train[\"text\"].to_list(), max_length_tweet) \nvalidation_tweets_encoded = tokenize_encode(x_val[\"text\"].to_list(), max_length_tweet) \n\ntrain_keywords_encoded = tokenize_encode(x_train[\"keyword\"].to_list(), max_length_keyword) \nvalidation_keywords_encoded = tokenize_encode(x_val[\"keyword\"].to_list(), max_length_keyword) \n\ntrain_inputs_encoded = dict(train_tweets_encoded)\ntrain_inputs_encoded[\"keywords\"] = train_keywords_encoded[\"input_ids\"]\n\nvalidation_inputs_encoded = dict(validation_tweets_encoded)\nvalidation_inputs_encoded[\"keywords\"] = validation_keywords_encoded[\"input_ids\"]\n",
            "class": "Data Transform",
            "desc": "The code defines a function to tokenize and encode tweets using the 'DistilBERT' tokenizer, then applies this function to encode text and keyword features from the training and validation sets with specified maximum lengths, organizing the encoded inputs into dictionaries for training.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.8425683
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_tweets_encoded), y_train)\n)\n\nval_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(validation_tweets_encoded), y_val)\n)\n\ntrain_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\n    (train_inputs_encoded, y_train)\n)\n\nval_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\n    (validation_inputs_encoded, y_val)\n)\n",
            "class": "Data Transform",
            "desc": "The code creates TensorFlow datasets from the encoded training and validation data, including both single and multi-input variants, using the `tf.data.Dataset.from_tensor_slices` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.5969537
            },
            "cluster": -1
        }, {
            "cell_id": 19,
            "code": "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n    tokenizer=tweet_preprocessor, min_df=1, ngram_range=(1, 1), norm=\"l2\"\n)\n\ntrain_vectors = tfidf_vectorizer.fit_transform(raw_documents=x_train[\"text\"]).toarray()\nvalidation_vectors = tfidf_vectorizer.transform(x_val[\"text\"]).toarray()",
            "class": "Data Transform",
            "desc": "The code initializes a TF-IDF vectorizer with the predefined `tweet_preprocessor` as the tokenizer, then fits and transforms the text data from the training set and transforms the text data from the validation set into TF-IDF vectors using `sklearn.feature_extraction.text.TfidfVectorizer`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9926306
            },
            "cluster": -1
        }, {
            "cell_id": 35,
            "code": "test_tweets_encoded = tokenize_encode(test_df[\"text\"].to_list(), max_length_tweet)\ntest_inputs_encoded = dict(test_tweets_encoded)\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)\n\ntest_keywords_encoded = tokenize_encode(test_df[\"keyword\"].to_list(), max_length_keyword)\ntest_inputs_encoded[\"keywords\"] = test_keywords_encoded[\"input_ids\"]\ntest_multi_input_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)",
            "class": "Data Transform",
            "desc": "The code tokenizes and encodes the text and keyword features from the test dataset using the pretrained BERT tokenizer, organizes the encoded inputs into dictionaries, and creates TensorFlow datasets for both single and multi-input variants.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.78393567
            },
            "cluster": 1
        }, {
            "cell_id": 5,
            "code": "print(train_df.info())\n\nprint(\"\")\nprint(\"train rows:\", len(train_df.index))\nprint(\"test rows:\", len(test_df.index))",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the information about the 'train_df' DataFrame and the number of rows in both the 'train_df' and 'test_df'.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9827071
            },
            "cluster": 4
        }, {
            "cell_id": 6,
            "code": "print(\"label counts:\")\ntrain_df.target.value_counts()",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the counts of each label in the 'target' column of the 'train_df'.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.99948514
            },
            "cluster": 4
        }, {
            "cell_id": 7,
            "code": "print(\"train precentage of nulls:\")\nprint(round(train_df.isnull().sum() / train_df.count() * 100, 2))",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and prints the percentage of null values for each column in the 'train_df' DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9960549
            },
            "cluster": 4
        }, {
            "cell_id": 8,
            "code": "print(\"test precentage of nulls:\")\nprint(round(test_df.isnull().sum() / test_df.count() * 100, 2))",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and prints the percentage of null values for each column in the 'test_df' DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99783856
            },
            "cluster": 4
        }, {
            "cell_id": 9,
            "code": "# check that we don't have any keywords appearing in one set and not the other\ntrain_keywords = set(train_df[\"keyword\"].dropna())\ntest_keywords = set(test_df[\"keyword\"].dropna())\n\nall_keywords = train_keywords.union(test_keywords)\nunique_test_keywords = all_keywords - train_keywords\nunique_train_keywords = all_keywords - test_keywords\n\nprint(f\"unique_test_keywords: {unique_test_keywords}\")\nprint(f\"unique_train_keywords: {unique_train_keywords}\")",
            "class": "Exploratory Data Analysis",
            "desc": "The code checks for unique keywords that appear exclusively in the training set or the test set and prints these unique keywords.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_unique_values",
                "subclass_id": 57,
                "predicted_subclass_probability": 0.966651
            },
            "cluster": 4
        }, {
            "cell_id": 13,
            "code": "# Have a look at how the TweetProcessor is doing\nfor tweet in train_df[100:120][\"text\"]:\n    print(\"original:  \", tweet)\n    print(\"processed: \", tweet_preprocessor.preprocess_tweet(tweet))\n    print(\"\")",
            "class": "Exploratory Data Analysis",
            "desc": "The code tests the `TweetPreProcessor` class by printing both the original and processed versions of a sample of tweets from the 'train_df' dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.7224992
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "!pip install -q transformers ekphrasis keras-tuner",
            "class": "Imports and Environment",
            "desc": "The code installs the libraries 'transformers', 'ekphrasis', and 'keras-tuner' using pip.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.99613416
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "import numpy as np\nimport pandas as pd\nimport urllib\nimport statistics\nimport math\nimport pprint\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import (\n    Input,\n    Dense,\n    Embedding,\n    Flatten,\n    Dropout,\n    GlobalMaxPooling1D,\n    GRU,\n    concatenate,\n)\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom transformers import (\n    DistilBertTokenizerFast,\n    TFDistilBertModel,\n    DistilBertConfig,\n)\n\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import Tokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\nfrom ekphrasis.dicts.noslang.slangdict import slangdict\n\nimport kerastuner",
            "class": "Imports and Environment",
            "desc": "The code imports various libraries and modules including 'numpy', 'pandas', 'urllib', 'statistics', 'math', 'pprint', 'sklearn', 'tensorflow', 'transformers', 'ekphrasis', and 'kerastuner', setting up the environment for the machine learning task.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993593
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "def print_metrics(model, x_train, y_train, x_val, y_val):\n    train_acc = dict(model.evaluate(x_train, y_train, verbose=0, return_dict=True))[\n        \"accuracy\"\n    ]\n    val_acc = dict(model.evaluate(x_val, y_val, verbose=0, return_dict=True))[\n        \"accuracy\"\n    ]\n\n    val_preds = model.predict(x_val)\n    val_preds_bool = val_preds >= 0.5\n\n    print(\"\")\n    print(f\"Training Accuracy:   {train_acc:.2%}\")\n    print(f\"Validation Accuracy: {val_acc:.2%}\")\n    print(\"\")\n    print(f\"Validation f1 score: {sklearn.metrics.f1_score(val_preds_bool, y_val):.2%}\")",
            "class": "Model Evaluation",
            "desc": "The function evaluates a trained model on training and validation datasets, calculating the accuracy and F1 score, and then prints these metrics. ",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.5876385
            },
            "cluster": 2
        }, {
            "cell_id": 29,
            "code": "# tuner.results_summary()",
            "class": "Model Evaluation",
            "desc": "The code provides an option to print a summary of the results from the hyperparameter tuning process using the `kerastuner` library.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.9971831
            },
            "cluster": 3
        }, {
            "cell_id": 30,
            "code": "best_model = tuner.get_best_models()[0]\n# best_model.summary()\nprint(\"\")\nbest_arch_hp = tuner.get_best_hyperparameters()[0]\npprint.pprint(best_arch_hp.values, indent=4)\nprint(\"\")\n\nprint_metrics(best_model, train_inputs, y_train, validation_inputs, y_val)",
            "class": "Model Evaluation",
            "desc": "The code retrieves the best model and its hyperparameters identified by the tuner, prints the best hyperparameters, and evaluates the model's performance by printing the training and validation accuracy along with the F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.43187767
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "# Using DistilBERT:\nmodel_class, tokenizer_class, pretrained_weights = (TFDistilBertModel, DistilBertTokenizerFast, 'distilbert-base-uncased')\n\npretrained_bert_tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n\ndef get_pretrained_bert_model(config=pretrained_weights):\n    if not config:\n        config = DistilBertConfig(num_labels=2)\n\n    return model_class.from_pretrained(pretrained_weights, config=config)\n\n",
            "class": "Model Training",
            "desc": "The code specifies the use of the 'DistilBERT' model and tokenizer from the 'transformers' library, and defines a function to load a pretrained DistilBERT model with a specified configuration.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9911644
            },
            "cluster": -1
        }, {
            "cell_id": 20,
            "code": "# I obtained the value of C by experimenting with LogisticRegressionCV but I'm leaving it out for brevity\nlogisticRegressionClf = LogisticRegression(n_jobs=-1, C=2.78)\nlogisticRegressionClf.fit(train_vectors, y_train)\n\ndef print_metrics_sk(clf, x_train, y_train, x_val, y_val):\n    print(f\"Train Accuracy:         {clf.score(x_train, y_train):.2%}\")\n    print(f\"Validation Accuracy:    {clf.score(x_val, y_val):.2%}\")\n    print(\"\")\n    print(f\"f1 score:               {sklearn.metrics.f1_score(y_val, clf.predict(x_val)):.2%}\")\n\nprint_metrics_sk(logisticRegressionClf, train_vectors, y_train, validation_vectors, y_val)",
            "class": "Model Training",
            "desc": "The code trains a Logistic Regression classifier with a specified regularization parameter (`C=2.78`) on the TF-IDF vectors of the training set, and defines a function to print the training and validation accuracy along with the F1 score for the model.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9330293
            },
            "cluster": 1
        }, {
            "cell_id": 21,
            "code": "feature_extractor = get_pretrained_bert_model()\n\n# Run a forward pass on the tokenized inputs\n# model_outputs = feature_extractor(\n#     train_tweets_encoded[\"input_ids\"], train_tweets_encoded[\"attention_mask\"]\n# )\nmodel_outputs = feature_extractor.predict(\n    train_dataset.batch(32)\n)\n# BERT's sentence representation can be retrieved from a hidden vector at index 0 in the sequence, \n# (where the special token CLS was prepended by the tokenizer)\ntrain_sentence_vectors = model_outputs.last_hidden_state[:, 0, :]\n\n# The rest of the sequence contains the embeddings \n# (modified by successive layers of self-attention) for each token\ntrain_word_vectors = model_outputs.last_hidden_state[:, 1:, :]\n\n# And the same again for the validation set\n# model_outputs = feature_extractor(\n#     validation_tweets_encoded[\"input_ids\"], validation_tweets_encoded[\"attention_mask\"]\n# )\nmodel_outputs = feature_extractor.predict(\n    val_dataset.batch(32)\n)\nvalidation_sentence_vectors = model_outputs.last_hidden_state[:, 0, :]\nvalidation_word_vectors = model_outputs.last_hidden_state[:, 1:, :]",
            "class": "Model Training",
            "desc": "The code loads a pretrained DistilBERT model and runs a forward pass on tokenized inputs from the training and validation data, extracting sentence and word embeddings from the model's outputs.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_model_class",
                "subclass_id": 3,
                "predicted_subclass_probability": 0.2968278
            },
            "cluster": 1
        }, {
            "cell_id": 22,
            "code": "logisticRegressionClf = LogisticRegression(n_jobs=-1, class_weight=class_weights)\nlogisticRegressionClf.fit(train_sentence_vectors, y_train)\n\nprint_metrics_sk(\n    logisticRegressionClf,\n    train_sentence_vectors,\n    y_train,\n    validation_sentence_vectors,\n    y_val,\n)",
            "class": "Model Training",
            "desc": "The code trains a Logistic Regression classifier with class weights on the sentence vectors derived from the DistilBERT model, and evaluates the performance by printing training and validation accuracy along with the F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99934644
            },
            "cluster": 1
        }, {
            "cell_id": 23,
            "code": "def create_gru_model() -> keras.Model:\n\n    model = keras.Sequential()\n    model.add(keras.layers.InputLayer(input_shape=train_word_vectors.shape[1:]))\n    model.add(GRU(32, return_sequences=True))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\nmodel = create_gru_model()\n\nhistory = model.fit(\n    train_word_vectors,\n    y_train,\n    validation_data=(validation_word_vectors, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(model, train_word_vectors, y_train, validation_word_vectors, y_val)",
            "class": "Model Training",
            "desc": "The code defines a function to create and compile a GRU-based neural network model using keras, trains the model on the word vectors from the DistilBERT model for up to 20 epochs with early stopping, and then evaluates the model's performance by printing the training and validation accuracy along with the F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.509667
            },
            "cluster": 1
        }, {
            "cell_id": 24,
            "code": "def create_multi_input_model() -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    tweet_classification_vectors = keras.Input((train_sentence_vectors.shape[1],), name=\"tweets\")\n    tweet_features = Dense(1, activation='relu')(tweet_classification_vectors)    \n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_classification_vectors], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\n\nmodel = create_multi_input_model()\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_sentence_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_sentence_vectors}\n\nhistory = model.fit(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\n\nprint_metrics(model, train_inputs, y_train, validation_inputs, y_val)",
            "class": "Model Training",
            "desc": "The code defines a function to create a multi-input keras model that combines keyword embeddings and tweet sentence vectors, compiles the model, trains it on the training data with early stopping, and then evaluates the model's performance by printing the training and validation accuracy along with the F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9820961
            },
            "cluster": 1
        }, {
            "cell_id": 25,
            "code": "def create_multi_input_rnn_model() -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    tweet_token_embeddings = Input(train_word_vectors.shape[1:], name=\"tweets\")\n    tweet_features = GRU(32, return_sequences=True)(tweet_token_embeddings)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    tweet_features = Dense(1, activation='relu')(tweet_features)    \n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_token_embeddings], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\n\nmodel = create_multi_input_rnn_model()\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_word_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_word_vectors}\n\nhistory = model.fit(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(model, train_inputs, y_train, validation_inputs, y_val)",
            "class": "Model Training",
            "desc": "The code defines a function to create a multi-input keras model that combines keyword embeddings and tweet token embeddings processed through a GRU layer, compiles the model, trains it on the training data with early stopping, and then evaluates the model's performance by printing the training and validation accuracy along with the F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.8343507
            },
            "cluster": 1
        }, {
            "cell_id": 26,
            "code": "def create_candidate_model_with_fx(hp: kerastuner.HyperParameters) -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(hp.Choice(\"keyword_units\", values=[1, 8, 16, 32], default=1))(keyword_features)\n\n    tweet_token_embeddings = Input(train_word_vectors.shape[1:], name=\"tweets\")\n    \n    tweet_features = GRU(hp.Choice(\"GRU_units\", values=[8, 16, 32, 64, 128], default=32), return_sequences=True)(tweet_token_embeddings)\n    tweet_features = Dropout(hp.Float(\"GRU_dropout\", min_value=0.0, max_value=0.5, step=0.1))(tweet_features)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    \n    for i in range(hp.Int(\"num_layers\", min_value=0, max_value=3, step=1)):\n        tweet_features = Dense(hp.Choice(\"layer_\" + str(i) + \"_units\", values=[2, 8, 16, 32, 64, 128, 256]), activation=\"relu\")(tweet_features)\n        tweet_features = Dropout(hp.Float(\"layer_\" + str(i) + \"_dropout\", min_value=0.0, max_value=0.5, step=0.1))(tweet_features)\n    \n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_token_embeddings], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_word_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_word_vectors}\n",
            "class": "Model Training",
            "desc": "The code defines a function to create a candidate keras model with hyperparameters tunable via `kerastuner`, allowing optimization of various architecture parameters for combining keyword embeddings and tweet token embeddings processed through a GRU layer, and initializes the training and validation inputs.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.37123346
            },
            "cluster": 1
        }, {
            "cell_id": 27,
            "code": "# Hyperband Tuning\nMAX_EPOCHS = 10\nFACTOR = 3\nITERATIONS = 3\n\nprint(f\"Number of models in each bracket: {math.ceil(1 + math.log(MAX_EPOCHS, FACTOR))}\")\nprint(f\"Number of epochs over all trials: {round(ITERATIONS * (MAX_EPOCHS * (math.log(MAX_EPOCHS, FACTOR) ** 2)))}\")",
            "class": "Model Training",
            "desc": "The code sets parameters for Hyperband tuning including maximum epochs, factor, and iterations, and prints the number of models in each bracket and the total number of epochs over all trials.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.5920417
            },
            "cluster": 1
        }, {
            "cell_id": 28,
            "code": "tuner = kerastuner.Hyperband(\n    create_candidate_model_with_fx,\n    max_epochs=MAX_EPOCHS,\n    hyperband_iterations=ITERATIONS, \n    factor=FACTOR, \n    objective=\"val_accuracy\",\n    directory=\"hyperparam-search\",\n    project_name=\"architecture-hyperband\",\n)\n\ntuner.search(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=10,\n    verbose=1,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=3,\n            restore_best_weights=True,\n        )\n    ],\n)\n",
            "class": "Model Training",
            "desc": "The code initializes a `kerastuner.Hyperband` tuner to search for optimal hyperparameters of the model defined in `create_candidate_model_with_fx`, performing the search on the training data with early stopping and evaluating based on validation accuracy.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.7966217
            },
            "cluster": 0
        }, {
            "cell_id": 31,
            "code": "# To create a baseline for the simplest possible fine-tuned BERT\ndef create_bert_simple_for_ft():\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n\n    pretrained_bert_model = get_pretrained_bert_model()\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n\n    prediction = Dense(1, activation=\"sigmoid\")(bert_outputs.last_hidden_state[:, 0, :])\n    return keras.Model(inputs=[input_ids, attention_mask], outputs=prediction)\n\nmodel = create_bert_simple_for_ft()\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(\n    train_dataset.batch(32),\n    validation_data=val_dataset.batch(32),\n    class_weight=class_weights,\n    epochs=20,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(\n    model, dict(train_tweets_encoded), y_train, dict(validation_tweets_encoded), y_val\n)\n",
            "class": "Model Training",
            "desc": "The code defines a function to create a simple fine-tuned BERT model, compiles the model, trains it on the training data with early stopping, and then evaluates the model's performance by printing the training and validation accuracy along with the F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.98817086
            },
            "cluster": 1
        }, {
            "cell_id": 32,
            "code": "def create_bert_rnn_for_ft():\n    \n    pretrained_bert_model = get_pretrained_bert_model()\n    \n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=pretrained_bert_model.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n\n    bert_token_embeddings = bert_outputs.last_hidden_state[:, 1:, :]\n    tweet_features = GRU(32, return_sequences=True)(bert_token_embeddings)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, input_ids, attention_mask], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\nmodel = create_bert_rnn_for_ft()\n\nmodel.fit(\n    train_multi_input_dataset.batch(32),\n    validation_data=val_multi_input_dataset.batch(32),\n    epochs=20,\n    class_weight=class_weights,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=3,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(\n    model, train_inputs_encoded, y_train, validation_inputs_encoded, y_val\n)",
            "class": "Model Training",
            "desc": "The code defines a function to create a multi-input BERT-RNN model incorporating keyword embeddings and a GRU layer for fine-tuning, compiles the model, trains it on the multi-input dataset with early stopping, and evaluates the model's performance by printing the training and validation accuracy along with the F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.7985944
            },
            "cluster": 1
        }, {
            "cell_id": 33,
            "code": "def create_model_candidate() -> keras.Model:\n    pretrained_bert_model = get_pretrained_bert_model()\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=pretrained_bert_model.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(best_arch_hp.get(\"keyword_units\"))(keyword_features)\n\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n    bert_token_embeddings = bert_outputs.last_hidden_state[:, 1:, :]\n    tweet_features = GRU(best_arch_hp.get(\"GRU_units\"), return_sequences=True)(bert_token_embeddings)\n    tweet_features = Dropout(best_arch_hp.get(\"GRU_dropout\"))(tweet_features)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    \n    for i in range(best_arch_hp.get(\"num_layers\")):\n        tweet_features = Dense(best_arch_hp.get(\"layer_\" + str(i) + \"_units\"), activation=\"relu\")(tweet_features)\n        tweet_features = Dropout(best_arch_hp.get(\"layer_\" + str(i) + \"_dropout\"))(tweet_features)\n    \n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, input_ids, attention_mask], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n",
            "class": "Model Training",
            "desc": "The code defines a function to create a model candidate for fine-tuning by integrating the best hyperparameters obtained from the tuner, combining keyword embeddings and tweet token embeddings processed through a GRU layer and dense layers, and compiles the model using the Adam optimizer.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.90136576
            },
            "cluster": 1
        }, {
            "cell_id": 34,
            "code": "model = create_model_candidate()\n\nhistory = model.fit(\n    train_multi_input_dataset.batch(32),\n    validation_data=val_multi_input_dataset.batch(32),\n    epochs=6,\n    class_weight=class_weights,\n    callbacks=[\n        keras.callbacks.EarlyStopping(\n            monitor=\"val_accuracy\", restore_best_weights=True\n        )\n    ],\n)\n\nbest_epoch = len(history.history[\"val_accuracy\"]) - 1\n\nprint_metrics(\n    model, train_inputs_encoded, y_train, validation_inputs_encoded, y_val\n)",
            "class": "Model Training",
            "desc": "The code trains the model candidate created with the best hyperparameters on the multi-input dataset for up to 6 epochs with early stopping, then evaluates the model's performance by printing the training and validation accuracy along with the F1 score, and identifies the best epoch during validation.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99578863
            },
            "cluster": 1
        }, {
            "cell_id": 36,
            "code": "full_train_dataset = train_multi_input_dataset.concatenate(val_multi_input_dataset)\nmodel = create_model_candidate()\n\nmodel.fit(\n    full_train_dataset.batch(32),\n    epochs=best_epoch,\n    class_weight=class_weights,\n)",
            "class": "Model Training",
            "desc": "The code concatenates the training and validation datasets to create a full training dataset, reinitializes the model candidate using the best hyperparameters, and trains the model on the full dataset for the number of epochs corresponding to the best epoch identified during prior validation.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9959798
            },
            "cluster": 1
        }],
        "notebook_id": 4,
        "notebook_name": "bert-feature-extraction-and-fine-tuning.ipynb",
        "user": "bert-feature-extraction-and-fine-tuning.ipynb"
    }, {
        "cells": [{
            "cell_id": 7,
            "code": "y = model.predict(test_v)\noutput = pd.DataFrame({'id':test.id, 'target':y})\noutput.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "The code uses the trained model to predict the target labels for the test dataset, creates a DataFrame with the predictions, and exports it as a CSV file named 'submission.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99914074
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')",
            "class": "Data Extraction",
            "desc": "The code reads the training and testing datasets from CSV files using pandas' `read_csv` method.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99975055
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "y = train['target']",
            "class": "Data Extraction",
            "desc": "The code extracts the target labels from the training dataset into the variable `y`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.99906355
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "def filter_text(df):\n    df['text']=df['text'].str.replace('http\\S+', '', case=False, regex=True)\n    df['text']=df['text'].str.replace('@\\S+', '', regex=True)\n    df['text']=df['text'].str.replace('&\\S+', '', regex=True)\nfilter_text(train)\nfilter_text(test)",
            "class": "Data Transform",
            "desc": "The code defines and applies a function using pandas to clean the text data in the training and testing datasets by removing URLs, mentions, and special character sequences.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99559855
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "sw=['the', 'a', 'an', 'in', 'on', 'with', 'by', 'for', 'at',\n    'about', 'of', 'under', 'to', 'into', 'and', 'or', 'but',\n    'nor', 'be']",
            "class": "Data Transform",
            "desc": "The code creates a list of stopwords manually, which are common words that will likely be removed during the text preprocessing phase.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.99721485
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "v = CountVectorizer(stop_words=sw)\ntrain_v = v.fit_transform(train['text'])\ntest_v = v.transform(test['text'])",
            "class": "Data Transform",
            "desc": "The code initializes a `CountVectorizer` with the previously defined stopwords and applies it to transform the text data from the training and testing datasets into a matrix of token counts using the `fit_transform` and `transform` methods.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99726737
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import make_scorer, f1_score",
            "class": "Imports and Environment",
            "desc": "The code imports essential libraries and modules such as NumPy, pandas, CountVectorizer, MultinomialNB, GridSearchCV, cross_val_score, make_scorer, and f1_score from Scikit-Learn, necessary for data manipulation, feature extraction, model training, and evaluation.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99930215
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "clf = MultinomialNB()\nscorer = make_scorer(f1_score)\nparam_grid = {'alpha': [0.01, 0.1, 1]}\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scorer, cv=3)\nmodel.fit(train_v, y)\nprint(\"Best score: %f\" % model.best_score_)\nprint(cross_val_score(model, train_v, y, cv=3, scoring='f1'))",
            "class": "Model Training",
            "desc": "The code sets up and trains a `MultinomialNB` classifier using `GridSearchCV` to find the best hyperparameters based on F1 score, and then performs 3-fold cross-validation on the model, printing the best score and cross-validation scores.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.9837585
            },
            "cluster": 2
        }],
        "notebook_id": 5,
        "notebook_name": "disaster-tweets.ipynb",
        "user": "disaster-tweets.ipynb"
    }, {
        "cells": [{
            "cell_id": 11,
            "code": "def submission(model, test_df, fname = 'submission'):\n\n    y_hat = model.predict(test_df)\n\n    submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n\n    submission['target'] = y_hat\n\n    submission.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "This code snippet defines a function `submission` which generates predictions for the test dataset using a given model, updates the 'target' column in a sample submission file, and saves the results as 'submission.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9989374
            },
            "cluster": -1
        }, {
            "cell_id": 17,
            "code": "# submission\n\ny_hat = lr.predict_proba(test)\n\ny_hat = y_hat[:, 1]\n\n\n\npreds = to_class_label(y_hat, opt_thres)\n\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n\nsubmission['target'] = preds\n\n\n\nsubmission.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "This code snippet generates predicted probabilities for the test dataset using the trained logistic regression model, converts these probabilities to class labels using the optimal threshold, updates the 'target' column in the sample submission file, and saves it as 'submission.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9982597
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "# print files in input dir\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n\n    ",
            "class": "Data Extraction",
            "desc": "This code snippet iterates over the directories and files in the '/kaggle/input' directory and prints the paths of all files found.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.9993166
            },
            "cluster": 1
        }, {
            "cell_id": 2,
            "code": "train = pd.read_csv('../input/nlp-getting-started/train.csv')\n\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\n\n\n\ntrain.head()",
            "class": "Data Extraction",
            "desc": "This code snippet reads the 'train.csv' and 'test.csv' files from the specified directory using pandas and displays the first few rows of the training dataset.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9996737
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "# plot prop of missing for each feature\n\nsns.set_theme(style='white')\n\nsns.barplot(x=train.columns, y=train.isnull().mean())\n\nplt.show()\n\n\n\n# drop location and keyword\n\ntrain.drop(columns=['id', 'keyword', 'location'], inplace=True)\n\ntest.drop(columns=['id', 'keyword', 'location'], inplace=True)\n\ntrain.drop_duplicates(inplace=True, ignore_index=True)",
            "class": "Data Transform",
            "desc": "This code snippet visualizes the proportion of missing values for each feature using Seaborn's barplot and then drops the 'id', 'keyword', and 'location' columns from both the training and testing datasets, while also removing duplicate entries from the training dataset.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.8657076
            },
            "cluster": 5
        }, {
            "cell_id": 6,
            "code": "wordnet_lemmatizer = WordNetLemmatizer()\n\n\n\ndef quick_clean(text):\n\n    \"\"\"\n\n    adapted from: https://www.kaggle.com/sophiejermy/sj-eda1\n\n    \"\"\"\n\n#     text = text + ' '\n\n    #remove links\n\n    text = re.sub(r'(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', '', text)\n\n    #lower case\n\n    text = text.lower()    \n\n    #remove special characters\n\n    text = re.sub(r'[\\W]+', ' ', text)\n\n    #remove double spaces\n\n    text = re.sub(r'\\s+', ' ', text)\n\n    #tokenize\n\n    text = word_tokenize(text)\n\n    #remove stop words\n\n    text = [word for word in text if not word in stopwords.words('english')]    \n\n    #lemmatize\n\n    text= [wordnet_lemmatizer.lemmatize(word, pos='v') for word in text]\n\n    #rejoin text to string\n\n    text = ' '.join(text)\n\n    return text\n\n\n\ndef quick_clean_vectorized(col):\n\n    return pd.DataFrame(data=col.apply(lambda x: quick_clean(x)).tolist())\n\n\n\nquiklean_transformer = FunctionTransformer(quick_clean_vectorized) # to use in pipeline\n\n    ",
            "class": "Data Transform",
            "desc": "This code snippet defines a text cleaning function `quick_clean` utilizing regular expressions, tokenization, stop word removal, and lemmatization with nltk, and then wraps this function for vectorized application in a pandas DataFrame using `FunctionTransformer` for integration into a pipeline.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.75697106
            },
            "cluster": 1
        }, {
            "cell_id": 9,
            "code": "x_train, x_test, y_train, y_test = train_test_split(train.loc[:,train.columns != 'target'], train.target, test_size=0.2)\n\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)",
            "class": "Data Transform",
            "desc": "This code snippet splits the training dataset into training and testing subsets using Scikit-learn's `train_test_split` function, with 80% of the data used for training and 20% for testing, and prints the shapes of the resulting splits.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.99772364
            },
            "cluster": -1
        }, {
            "cell_id": 10,
            "code": "tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', max_features = 300)\n\n\n\npreprocess = Pipeline(steps=[\n\n                    ('clean',   ColumnTransformer([\n\n                                    ('cl', quiklean_transformer, 'text')\n\n                                    ],\n\n                                        remainder='drop')),\n\n                    ('TFIDF', ColumnTransformer([\n\n                        ('tfidf', tfidf_vectorizer, 0)\n\n                    ], \n\n                            remainder='passthrough')),\n\n                    ('dim_reduce', TruncatedSVD(n_components=250, random_state=42)),\n\n                    ('scale', MinMaxScaler())\n\n    \n\n        ])",
            "class": "Data Transform",
            "desc": "This code snippet creates a preprocessing pipeline using Scikit-learn that involves cleaning text data with a custom transformer, vectorizing text with TF-IDF, reducing dimensionality with TruncatedSVD, and scaling features with MinMaxScaler.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9810301
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "print(f'Train dims {train.shape}', f'Test dims {test.shape}', sep = '\\n')",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet prints the dimensions (number of rows and columns) of the training and testing datasets using the `shape` attribute of the pandas DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.99101853
            },
            "cluster": 2
        }, {
            "cell_id": 5,
            "code": "# plot target distribution\n\nsns.countplot(x='target', data=train)\n\nplt.title('Target distribution')\n\nplt.show()",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet uses Seaborn's countplot to visualize the distribution of the target variable in the training dataset and adds a title to the plot.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99668735
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "# setup\n\n\n\nfrom collections import Counter, defaultdict\n\n\n\nimport numpy as np \n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport os\n\nimport re\n\n\n\nfrom sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn import set_config\n\n\n\nimport optuna\n\nimport nltk\n\nfrom nltk.stem import WordNetLemmatizer\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.tokenize import word_tokenize\n\n\n\nplt.style.use('ggplot')\n\n%matplotlib inline\n\n\n\nset_config(display='diagram')",
            "class": "Imports and Environment",
            "desc": "This code snippet imports various libraries and modules such as NumPy, pandas, matplotlib, seaborn, Scikit-learn, optuna, and nltk, and sets up the matplotlib style and Scikit-learn configuration.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.99920374
            },
            "cluster": 0
        }, {
            "cell_id": 14,
            "code": "def to_class_label(probs, threshold):\n\n    \"\"\"convert predicted probabilities to class labels\"\"\"\n\n    return (probs >= threshold).astype('int')\n\n\n\ndef get_optimal_threshold(fitted_model, x_test, y_test):\n\n    \"\"\"Threshold tuning\"\"\"\n\n    thresholds = np.arange(0, 1, 0.0005)\n\n    y_hat = fitted_model.predict_proba(x_test)\n\n    pos_clas_probs = y_hat[:, 1]\n\n    acc_scores = [accuracy_score(y_test, to_class_label(pos_clas_probs, thres)) for thres in thresholds]\n\n    idx = np.argmax(acc_scores)\n\n    \n\n    return thresholds[idx]\n\n    ",
            "class": "Model Evaluation",
            "desc": "This code snippet defines two functions: `to_class_label` to convert predicted probabilities to class labels based on a given threshold, and `get_optimal_threshold` to find the optimal threshold that maximizes accuracy by evaluating a range of thresholds on the test dataset.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.54912084
            },
            "cluster": 2
        }, {
            "cell_id": 16,
            "code": "# get optimal threshold\n\nopt_thres = get_optimal_threshold(lr, x_test, y_test)\n\nprint(f'Optimal threshold for trained LR {get_optimal_threshold(lr, x_test, y_test):.4f}')",
            "class": "Model Evaluation",
            "desc": "This code snippet calculates and prints the optimal threshold for converting predicted probabilities to class labels for the trained Logistic Regression model using the test dataset.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.6867679
            },
            "cluster": 3
        }, {
            "cell_id": 12,
            "code": "# Tune logistic regression\n\ndef objective(trial):\n\n    x, y = x_train, y_train\n\n    C = trial.suggest_float('C', 1e-6, 1e6, log=True)\n\n    penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])\n\n    l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n\n    if penalty != 'elasticnet':\n\n        l1_ratio = None\n\n    \n\n    clf = make_pipeline(preprocess, LogisticRegression(C=C,\n\n                                                      penalty=penalty,\n\n                                                      l1_ratio=l1_ratio,\n\n                                                      solver='saga',\n\n                                                      max_iter=800))\n\n    clf.fit(x,y)\n\n    \n\n    acc = accuracy_score(y_test, clf.predict(x_test))\n\n    \n\n    return acc\n\n\n\nclass EarlyStopping:\n\n    \"\"\"stop tuning after value remains unchanged after 10 successive trials\"\"\"\n\n    def __init__(self, max_rounds = 10):\n\n        self.max_rounds = max_rounds\n\n        self.current_rounds = 0\n\n        \n\n    def __call__(self, study, trial, tol = 1e-6):\n\n        if abs(trial.value - study.best_value) <= tol:\n\n            self.current_rounds += 1\n\n        elif trial.value == study.best_value:\n\n            self.current_rounds = 0\n\n        if self.current_rounds >= self.max_rounds:\n\n            study.stop()",
            "class": "Model Training",
            "desc": "This code snippet defines an objective function to tune a logistic regression model using Optuna by suggesting hyperparameters such as 'C', 'penalty', and 'l1_ratio', fitting a pipeline, and then evaluating accuracy, and it also includes an `EarlyStopping` class to halt tuning if there are no improvements after 10 successive trials.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_model_class",
                "subclass_id": 3,
                "predicted_subclass_probability": 0.3142369
            },
            "cluster": 2
        }, {
            "cell_id": 13,
            "code": "# # create study and run trials\n\nes = EarlyStopping()\n\n\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())  # using Tree-structured Parzen Estimator to sample\n\nstudy.optimize(objective, n_trials=250, callbacks=[es])",
            "class": "Model Training",
            "desc": "This code snippet creates an Optuna study using the Tree-structured Parzen Estimator (TPE) sampler to maximize the objective function and performs hyperparameter optimization with 250 trials and early stopping based on the provided callback.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.80090225
            },
            "cluster": 2
        }, {
            "cell_id": 15,
            "code": "# train LR on best parameters\n\nlr = LogisticRegression(**study.best_params, solver='saga', max_iter=800)\n\nlr = make_pipeline(preprocess, lr)\n\nlr.fit(x_train, y_train)",
            "class": "Model Training",
            "desc": "This code snippet trains a Logistic Regression model using the best hyperparameters found by the Optuna study and fits it to the training data within a Scikit-learn pipeline.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99761343
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "def plot_top_n_words(target = 1, n=50):\n\n    \n\n    count_dict = defaultdict(int)\n\n\n\n    for tweet in train.query(f'target=={target}')['text']:\n\n        for word in word_tokenize(tweet):\n\n            count_dict[word] += 1\n\n\n\n    wc_df = pd.DataFrame(data=count_dict.items(), columns = ['word', 'count'])\n\n    sns.barplot(x = 'count', y='word', data=wc_df.sort_values(by=['count'], ascending=False)[:n])",
            "class": "Visualization",
            "desc": "This code snippet defines a function `plot_top_n_words` which creates a barplot using Seaborn to visualize the top N words and their counts from the tweets grouped by the specified target variable in the training dataset.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.8610096
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "plot_top_n_words()",
            "class": "Visualization",
            "desc": "This code snippet calls the `plot_top_n_words` function to visualize the top 50 most frequent words in tweets with the target variable set to 1 using a barplot.",
            "testing": {
                "class": "Visualization",
                "subclass": "relationship",
                "subclass_id": 81,
                "predicted_subclass_probability": 0.58346504
            },
            "cluster": 0
        }],
        "notebook_id": 6,
        "notebook_name": "logistic-regression-with-threshold-tuning.ipynb",
        "user": "logistic-regression-with-threshold-tuning.ipynb"
    }, {
        "cells": [{
            "cell_id": 28,
            "code": "submission['target'] = test_pred_BERT_int\nsubmission.to_csv(\"submission_BERT.csv\", index=False, header=True)",
            "class": "Data Export",
            "desc": "The code adds the predicted labels to the submission DataFrame and saves it to a CSV file named \"submission_BERT.csv\" with headers and without the index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9993593
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n\nprint(\"Training Shape rows = {}, columns = {}\".format(train_df.shape[0],train_df.shape[1]))\nprint(\"Testing Shape rows = {}, columns = {}\".format(test_df.shape[0],test_df.shape[1]))",
            "class": "Data Extraction",
            "desc": "The code reads the training, testing, and submission datasets from CSV files into pandas DataFrames and prints the shape (number of rows and columns) of the training and testing datasets.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99957687
            },
            "cluster": 1
        }, {
            "cell_id": 23,
            "code": "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)",
            "class": "Data Extraction",
            "desc": "The code extracts the vocabulary file and case sensitivity configuration from the loaded BERT layer and initializes a BERT tokenizer using the `FullTokenizer` class from the `tokenization` module.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.56229645
            },
            "cluster": 1
        }, {
            "cell_id": 11,
            "code": "keyword_dist = train_df.groupby(\"keyword\")['target'].value_counts().unstack(fill_value=0)\nkeyword_dist = keyword_dist.add_prefix(keyword_dist.columns.name).rename_axis(columns=None).reset_index()",
            "class": "Data Transform",
            "desc": "The code creates a new DataFrame that shows the distribution of the 'target' variable for each unique 'keyword' by grouping the training data by 'keyword' and performing a value count for 'target', then reshaping and resetting the index of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.8236565
            },
            "cluster": 5
        }, {
            "cell_id": 14,
            "code": "#word count\ntrain_df['word_count'] = train_df['text'].apply(lambda x : len(str(x).split()))\ntest_df['word_count'] = test_df['text'].apply(lambda x : len(str(x).split()))\n#Unique word count\ntrain_df['unique_word_count'] = train_df['text'].apply(lambda x : len(set(str(x).split())))\ntest_df['unique_word_count'] = test_df['text'].apply(lambda x : len(set(str(x).split())))\n#Count of letters\ntrain_df['count_letters'] = train_df['text'].apply(lambda x : len(str(x)))\ntest_df['count_letters'] = test_df['text'].apply(lambda x : len(str(x)))\n#Count of punctuations\ntrain_df['count_punctuations'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest_df['count_punctuations'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#count of stopwords\ntrain_df['stop_word_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest_df['stop_word_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Count of hashtag\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda x : len([c for c in str(x) if c == '#']))\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x : len([c for c in str(x) if c == '#']))\n#Count of mentions\ntrain_df['mention_count'] = train_df['text'].apply(lambda x : len([c for c in str(x) if c=='@']))\ntest_df['mention_count'] = test_df['text'].apply(lambda x : len([c for c in str(x) if c=='@']))",
            "class": "Data Transform",
            "desc": "The code creates several new features in the training and testing datasets by counting the number of words, unique words, letters, punctuations, stopwords, hashtags, and mentions in the 'text' column by applying various lambda functions.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9991715
            },
            "cluster": 5
        }, {
            "cell_id": 19,
            "code": "# Refrenced from Gunes Evitan and Vitalii Mokin Notebook\ndef clean(tweet): \n            \n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n           \n    # Urls\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n        \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    # ... and ..\n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ')      \n        \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m\u00cc\u00bcsica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    #Remove Emoji\n    tweet = re.sub(u\"\\U0001F600-\\U0001F64F\",\"\", tweet)  # emoticons\n    tweet = re.sub(u\"\\U0001F300-\\U0001F5FF\",\"\", tweet)  # symbols & pictographs\n    tweet = re.sub(u\"\\U0001F680-\\U0001F6FF\",\"\", tweet)  # transport & map symbols\n    tweet = re.sub(u\"\\U0001F1E0-\\U0001F1FF\",\"\", tweet)  # flags (iOS)\n    tweet = re.sub(u\"\\U00002702-\\U000027B0\",\"\", tweet)\n    tweet = re.sub(u\"\\U000024C2-\\U0001F251\",\"\", tweet)\n    \n    return tweet\n\ntrain_df['text_cleaned'] = train_df['text'].apply(lambda s : clean(s))\ntest_df['text_cleaned'] = test_df['text'].apply(lambda s : clean(s))",
            "class": "Data Transform",
            "desc": "The code defines a function to clean and preprocess text data by removing special characters, expanding contractions, correcting typos and slang, and then applies this function to create a new 'text_cleaned' column in both the training and testing datasets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9979898
            },
            "cluster": 5
        }, {
            "cell_id": 20,
            "code": "def encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        # Tokenise text\n        text = tokenizer.tokenize(text)\n        #Reduce 2 slots for start and end tag\n        text = text[:max_len-2]\n        #Add start and end tag\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        #Padding to be added\n        pad_len = max_len - len(input_sequence)\n        #Get token ids\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        #add padding\n        tokens += [0] * pad_len\n        #Create padding mask with 1's of length of input and 0's with padding length\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        #Create segment ids with all 0's \n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)",
            "class": "Data Transform",
            "desc": "The code defines a function to encode texts using a tokenizer by converting them to token IDs, adding special tokens, padding, and creating masks and segment IDs for input to a BERT model, returning the encoded texts as numpy arrays.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9931865
            },
            "cluster": 1
        }, {
            "cell_id": 24,
            "code": "train_input = encode(train_df.text_cleaned.values, tokenizer, max_len=160)\ntest_input = encode(test_df.text_cleaned.values, tokenizer, max_len=160)\ntrain_labels = train_df.target.values",
            "class": "Data Transform",
            "desc": "The code encodes the cleaned text data from the training and testing datasets into token IDs, masks, and segment IDs using the previously defined `encode` function and the BERT tokenizer, and extracts the target labels from the training DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.997544
            },
            "cluster": 1
        }, {
            "cell_id": 2,
            "code": "print(\"Train columns = {}\".format(train_df.columns))\nprint(\"Test columns = {}\".format(test_df.columns))",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the column names of the training and testing datasets.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_columns",
                "subclass_id": 71,
                "predicted_subclass_probability": 0.99450326
            },
            "cluster": 4
        }, {
            "cell_id": 4,
            "code": "print(\"So there are {} occourance of disastrous twitts and {} occourances of non disastrous\".format(x[1],x[0]))",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the number of occurrences of tweets labeled as disastrous and non-disastrous in the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.91736937
            },
            "cluster": 4
        }, {
            "cell_id": 5,
            "code": "train_df.head(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first 10 rows of the training DataFrame to give an initial look at the data.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997634
            },
            "cluster": 4
        }, {
            "cell_id": 6,
            "code": "train_df.isnull().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the number of missing values in each column of the training DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.99896073
            },
            "cluster": 4
        }, {
            "cell_id": 7,
            "code": "test_df.isnull().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and displays the number of missing values in each column of the testing DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9990055
            },
            "cluster": 4
        }, {
            "cell_id": 8,
            "code": "train_df[train_df.keyword.notnull()].head(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first 10 rows of the training DataFrame where the 'keyword' column is not null to inspect how 'keyword' values are populated in the dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997584
            },
            "cluster": 4
        }, {
            "cell_id": 9,
            "code": "train_df[train_df.keyword.notnull()].tail(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the last 10 rows of the training DataFrame where the 'keyword' column is not null to further inspect the populated 'keyword' values in the dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997335
            },
            "cluster": 4
        }, {
            "cell_id": 10,
            "code": "train_df[train_df.keyword.isnull()].head(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first 10 rows of the training DataFrame where the 'keyword' column is null to examine instances without 'keyword' values.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9996885
            },
            "cluster": 4
        }, {
            "cell_id": 12,
            "code": "keyword_dist.sort_values('target1',ascending = False).head(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The code sorts the `keyword_dist` DataFrame by the count of 'disastrous' tweets in descending order and displays the top 10 rows to identify the keywords most frequently associated with disaster tweets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.8409343
            },
            "cluster": 4
        }, {
            "cell_id": 13,
            "code": "keyword_dist.sort_values('target0',ascending = False).head(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The code sorts the `keyword_dist` DataFrame by the count of 'non-disastrous' tweets in descending order and displays the top 10 rows to identify the keywords most frequently associated with non-disaster tweets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.7933884
            },
            "cluster": 4
        }, {
            "cell_id": 0,
            "code": "# Import packeges\nimport os\nimport gc\nimport re\nimport time\nimport warnings\nimport string\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams # function for making ngrams\nfrom collections import defaultdict\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization\n\nwarnings.filterwarnings(\"ignore\")\neng_stopwords = set(stopwords.words(\"english\"))",
            "class": "Imports and Environment",
            "desc": "The code imports various libraries and sets some display options, including `numpy`, `pandas`, `matplotlib.pyplot`, `seaborn`, `nltk`, `tensorflow`, and `tensorflow_hub`, and it constructs a set of English stopwords from `nltk`.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9975701
            },
            "cluster": 0
        }, {
            "cell_id": 22,
            "code": "%%time\n\nbert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)",
            "class": "Imports and Environment",
            "desc": "The code imports a pre-trained BERT model from TensorFlow Hub and makes it trainable using the KerasLayer API.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.8483843
            },
            "cluster": 1
        }, {
            "cell_id": 27,
            "code": "# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n# Prediction by BERT model\nmodel.load_weights('model.h5')\ntest_pred_BERT = model.predict(test_input)\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')",
            "class": "Model Evaluation",
            "desc": "The code loads the best model weights saved during training and predicts the labels for the testing data using the BERT model, then rounds the predictions to the nearest integer.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9870588
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model",
            "class": "Model Training",
            "desc": "The code defines a function to build a binary classification model using a BERT layer by creating input layers for token IDs, masks, and segment IDs, feeding them to the BERT layer, extracting the sequence output, and adding a dense layer with a sigmoid activation, compiling the model with Adam optimizer and binary crossentropy loss.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9912102
            },
            "cluster": 1
        }, {
            "cell_id": 25,
            "code": "model = build_model(bert_layer, max_len=160)\nmodel.summary()",
            "class": "Model Training",
            "desc": "The code builds a binary classification model using the previously defined `build_model` function with the BERT layer and a maximum sequence length of 160, and then prints the model's summary.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9372223
            },
            "cluster": -1
        }, {
            "cell_id": 26,
            "code": "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=32\n)",
            "class": "Model Training",
            "desc": "The code trains the BERT-based model on the encoded training data for 3 epochs, using a validation split of 20%, and saves the best model based on validation loss using the ModelCheckpoint callback.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9943159
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "x=train_df.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('# of occurrence')",
            "class": "Visualization",
            "desc": "The code generates a bar plot using seaborn to visualize the distribution of the 'target' variable in the training dataset, with the y-axis representing the number of occurrences.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9978562
            },
            "cluster": 3
        }, {
            "cell_id": 15,
            "code": "plt.figure(figsize=(12,6))\n## sentenses\nplt.subplot(121)\nplt.suptitle(\"Are longer comments more Disastrous\",fontsize=20)\nsns.violinplot(y='word_count',x='target', data=train_df,split=True)\nplt.xlabel('Target?', fontsize=12)\nplt.ylabel('# of words', fontsize=12)\nplt.title(\"Number of words in each comment\", fontsize=15)\n\n# words\nplt.subplot(122)\nsns.violinplot(y='count_letters',x='target', data=train_df,split=True,inner=\"quart\")\nplt.xlabel('Target?', fontsize=12)\nplt.ylabel('# of letters', fontsize=12)\nplt.title(\"Number of letters in each comment\", fontsize=15)\n\nplt.show()",
            "class": "Visualization",
            "desc": "The code creates a figure with two violin plots using seaborn to visualize the distribution of word count and letter count in tweets, split by the 'target' variable to see if there is a correlation between tweet length and whether it is labeled as disastrous.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9839205
            },
            "cluster": 3
        }, {
            "cell_id": 16,
            "code": "train_df['word_unique_percent']=train_df['unique_word_count']*100/train_df['word_count']\ntest_df['word_unique_percent']=test_df['unique_word_count']*100/test_df['word_count']\nplt.figure(figsize=(12,6))\nplt.subplot(121)\nplt.title(\"Percentage of unique words of total words in comment\")\n#sns.boxplot(x='clean', y='word_unique_percent', data=train_feats)\nax=sns.kdeplot(train_df[train_df.target == 0].word_unique_percent, label=\"Disastrous\",shade=True,color='r')\nax=sns.kdeplot(train_df[train_df.target == 1].word_unique_percent, label=\" Non Disastrous\")\nplt.legend()\nplt.ylabel('Number of occurances', fontsize=12)\nplt.xlabel('Percent unique words', fontsize=12)",
            "class": "Visualization",
            "desc": "The code calculates the percentage of unique words for each tweet in the training and testing datasets and then creates a kernel density plot using seaborn to compare the distribution of this percentage between disastrous and non-disastrous tweets.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.97993535
            },
            "cluster": 3
        }, {
            "cell_id": 17,
            "code": "def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in eng_stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\n# Bigrams\ndisaster_bigrams = defaultdict(int)\nnondisaster_bigrams = defaultdict(int)\n\nfor tweet in train_df[train_df['target']==1]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        disaster_bigrams[word] += 1\n        \nfor tweet in train_df[train_df['target']==0]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        nondisaster_bigrams[word] += 1\n        \ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 10))\nplt.tight_layout()\nsns.barplot(y=df_disaster_bigrams[0].values[:10], x=df_disaster_bigrams[1].values[:10], ax=axes[0], color='cyan')\nsns.barplot(y=df_nondisaster_bigrams[0].values[:10], x=df_nondisaster_bigrams[1].values[:10], ax=axes[1], color='pink')\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=10)\n    axes[i].tick_params(axis='y', labelsize=10)\naxes[0].set_title('most common bigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title('most common bigrams in Non-disaster Tweets', fontsize=15)\nplt.show()",
            "class": "Visualization",
            "desc": "The code defines a function to generate n-grams, counts the occurrences of bigrams in disaster and non-disaster tweets, and visualizes the most common bigrams for each category using bar plots created with seaborn.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.95510435
            },
            "cluster": -1
        }, {
            "cell_id": 18,
            "code": "# Trigrams\ndisaster_trigrams = defaultdict(int)\nnondisaster_trigrams = defaultdict(int)\n\nfor tweet in train_df[train_df['target']==1]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        disaster_trigrams[word] += 1\n        \nfor tweet in train_df[train_df['target']==0]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        nondisaster_trigrams[word] += 1\n        \ndf_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 10))\nplt.tight_layout()\nsns.barplot(y=df_disaster_trigrams[0].values[:10], x=df_disaster_trigrams[1].values[:10], ax=axes[0], color='cyan')\nsns.barplot(y=df_nondisaster_trigrams[0].values[:10], x=df_nondisaster_trigrams[1].values[:10], ax=axes[1], color='pink')\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=10)\n    axes[i].tick_params(axis='y', labelsize=10)\naxes[0].set_title('most common trigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title('most common trigrams in Non-disaster Tweets', fontsize=15)\nplt.show()",
            "class": "Visualization",
            "desc": "The code counts the occurrences of trigrams in disaster and non-disaster tweets, and visualizes the most common trigrams for each category using bar plots created with seaborn.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.94842196
            },
            "cluster": -1
        }],
        "notebook_id": 7,
        "notebook_name": "nlp-eda-cleaning-bert.ipynb",
        "user": "nlp-eda-cleaning-bert.ipynb"
    }, {
        "cells": [{
            "cell_id": 89,
            "code": "output = pd.DataFrame({'id': full_test_df.id, 'target': predictions_rf})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")",
            "class": "Data Export",
            "desc": "The code creates a DataFrame with `id` from `full_test_df` and `target` as the predicted values, then exports this DataFrame to a CSV file named 'my_submission.csv' without including the index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9990031
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "# loading the dataset\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")",
            "class": "Data Extraction",
            "desc": "The code loads training and testing datasets from specified CSV files into pandas DataFrames named `train_df` and `test_df`.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99973744
            },
            "cluster": 1
        }, {
            "cell_id": 10,
            "code": "ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_df.at[train_df['id'].isin(ids_with_target_error),'target'] = 0\ntrain_df[train_df['id'].isin(ids_with_target_error)]",
            "class": "Data Transform",
            "desc": "The code corrects specific entries in the `train_df` DataFrame by setting the `target` column to 0 for rows where the `id` is in the `ids_with_target_error` list and then displays these corrected rows.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9975581
            },
            "cluster": 4
        }, {
            "cell_id": 13,
            "code": "set1 = set(train_df[train_df.duplicated(subset=['text'])]['id'].values)",
            "class": "Data Transform",
            "desc": "The code creates a set named `set1` that contains the `id` values of duplicated rows in the `train_df` DataFrame based on the `text` column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.8107515
            },
            "cluster": 4
        }, {
            "cell_id": 14,
            "code": "set2 = (train_df[train_df.duplicated(subset=['text','target'])]['id'].values)",
            "class": "Data Transform",
            "desc": "The code creates an array named `set2` containing the `id` values of rows in the `train_df` DataFrame that are duplicated based on both the `text` and `target` columns.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.8105881
            },
            "cluster": 4
        }, {
            "cell_id": 15,
            "code": "tweet_ids_possible_wrong = set1.difference(set2)\n# those are the tweets which have duplicated text but different label\nprint(train_df[train_df['id'].isin(tweet_ids_possible_wrong)].text.values)\nprint(train_df[train_df['id'].isin(tweet_ids_possible_wrong)].target.values)",
            "class": "Data Transform",
            "desc": "The code determines `tweet_ids_possible_wrong`, a set of `id` values that have duplicated text but different labels, and then prints the text and target values for these rows in the `train_df` DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.4053326
            },
            "cluster": 1
        }, {
            "cell_id": 16,
            "code": "# setting the argument keep=False, drops all the duplicates\ntrain_df.drop_duplicates(subset=['text'], keep=False, inplace=True)",
            "class": "Data Transform",
            "desc": "The code removes all duplicates in the `train_df` DataFrame based on the `text` column, ensuring that none of the duplicates are kept.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "remove_duplicates",
                "subclass_id": 19,
                "predicted_subclass_probability": 0.89899284
            },
            "cluster": 4
        }, {
            "cell_id": 27,
            "code": "train_df['location'] = train_df['location'].str.lower()\ntrain_df['location'] = train_df['location'].str.strip()\ntest_df['location'] = train_df['location'].str.lower()\ntest_df['location'] = train_df['location'].str.strip()",
            "class": "Data Transform",
            "desc": "The code standardizes the `location` column in both `train_df` and `test_df` DataFrames by converting text to lowercase and stripping whitespace.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.7562164
            },
            "cluster": 2
        }, {
            "cell_id": 28,
            "code": "loc_dict = {'united states':'usa',\n                            'us':'usa',\n                            'united kingdom':'uk',\n                              'nyc':'new york',\n                             'london, uk': 'london',\n                              'london, england':'london',\n                            'new york, ny':'new york',\n                            'everywhere':'worldwide'}",
            "class": "Data Transform",
            "desc": "The code defines a dictionary named `loc_dict` to map specific location strings to standardized location names.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.99767166
            },
            "cluster": -1
        }, {
            "cell_id": 29,
            "code": "train_df['location'].replace(loc_dict, inplace=True)\ntest_df['location'].replace(loc_dict, inplace=True)",
            "class": "Data Transform",
            "desc": "The code replaces location strings in both `train_df` and `test_df` DataFrames using the mappings defined in the `loc_dict` dictionary.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.92937183
            },
            "cluster": 2
        }, {
            "cell_id": 33,
            "code": "abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}",
            "class": "Data Transform",
            "desc": "The code defines a dictionary named `abbreviations` that maps common abbreviations and slang to their expanded forms for text normalization purposes.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9984091
            },
            "cluster": -1
        }, {
            "cell_id": 34,
            "code": "def convert_abb(x):\n    word_list = x.split()\n    r_string = []\n    for word in word_list:\n        r_string.append(abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word)\n    return ' '.join(r_string)\n\ntest = 'afk hello world!'\nconvert_abb(test)",
            "class": "Data Transform",
            "desc": "The code defines a function `convert_abb` that splits a string into words, replaces any abbreviations found in the `abbreviations` dictionary with their expanded forms, and then joins the words back into a single string, demonstrated with an example string `test`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.8039966
            },
            "cluster": -1
        }, {
            "cell_id": 35,
            "code": "train_df['text'] = train_df.text.apply(convert_abb)\ntest_df['text'] = test_df.text.apply(convert_abb)",
            "class": "Data Transform",
            "desc": "The code applies the `convert_abb` function to the `text` column in both `train_df` and `test_df` DataFrames to replace abbreviations with their expanded forms.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9974021
            },
            "cluster": 2
        }, {
            "cell_id": 36,
            "code": "train_df['clean_text'] = train_df.text.apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', '', x))\ntest_df['clean_text'] = test_df.text.apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', '', x))",
            "class": "Data Transform",
            "desc": "The code creates a new column `clean_text` in both `train_df` and `test_df` DataFrames by applying a regular expression to remove URLs from the text.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9974329
            },
            "cluster": 2
        }, {
            "cell_id": 37,
            "code": "train_df['clean_text'] = train_df.clean_text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))",
            "class": "Data Transform",
            "desc": "The code updates the `clean_text` column in both `train_df` and `test_df` DataFrames by removing non-ASCII characters through encoding and decoding steps.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.9229938
            },
            "cluster": 2
        }, {
            "cell_id": 38,
            "code": "non_alpha = string.punctuation + '0123456789'",
            "class": "Data Transform",
            "desc": "The code creates a string named `non_alpha` containing all punctuation characters and digits to be used for filtering non-alphabetic characters.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9975495
            },
            "cluster": -1
        }, {
            "cell_id": 39,
            "code": "train_df['clean_text'] = train_df.clean_text.apply(lambda x: x.translate(str.maketrans('','',non_alpha)))\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: x.translate(str.maketrans('','',non_alpha)))",
            "class": "Data Transform",
            "desc": "The code updates the `clean_text` column in both `train_df` and `test_df` DataFrames by removing all punctuation and digits using the `translate` method with a translation table created from the `non_alpha` string.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.5500296
            },
            "cluster": 2
        }, {
            "cell_id": 40,
            "code": "train_df['token_text'] = train_df.clean_text.str.lower()\ntrain_df['token_text'] = train_df.token_text.apply(lambda x: nltk.word_tokenize(x))\ntest_df['token_text'] = test_df.clean_text.str.lower()\ntest_df['token_text'] = test_df.token_text.apply(lambda x: nltk.word_tokenize(x))",
            "class": "Data Transform",
            "desc": "The code creates a new column `token_text` in both `train_df` and `test_df` DataFrames by converting the `clean_text` to lowercase and then tokenizing the text into words using NLTK's `word_tokenize` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.3968303
            },
            "cluster": 2
        }, {
            "cell_id": 41,
            "code": "stopwords = nltk.corpus.stopwords.words(\"english\")\naddingStopWords = ['im','get','dont','got','amp']\nstopwords.extend(addingStopWords)\ntrain_df['token_text'] = train_df['token_text'].apply(lambda x: [word for word in x if word not in stopwords])\ntest_df['token_text'] = test_df['token_text'].apply(lambda x: [word for word in x if word not in stopwords])",
            "class": "Data Transform",
            "desc": "The code defines a list of stopwords by extending NLTK's English stopwords with additional custom stopwords, and then filters out these stopwords from the `token_text` column in both `train_df` and `test_df` DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9972505
            },
            "cluster": 2
        }, {
            "cell_id": 42,
            "code": "# since we have a dataset containing text from social media, there might be many spelling mistakes and words which cannot be found in the word lemmatizer corpus, so they would be remained untouched.\n# To this end we would use the Porter Stemming which just removes affixes of the words\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ntrain_df['token_text'] = train_df['token_text'].apply(lambda x: [ps.stem(word) for word in x ])\ntest_df['token_text'] = test_df['token_text'].apply(lambda x: [ps.stem(word) for word in x ])",
            "class": "Data Transform",
            "desc": "The code initializes a Porter Stemmer from NLTK and applies it to stem each word in the `token_text` column for both `train_df` and `test_df` DataFrames, which involves removing word affixes.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.988744
            },
            "cluster": 2
        }, {
            "cell_id": 43,
            "code": "train_df.reset_index(inplace=True, drop=True)",
            "class": "Data Transform",
            "desc": "The code resets the index of the `train_df` DataFrame and drops the old index, ensuring a continuous numerical index.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9991253
            },
            "cluster": 4
        }, {
            "cell_id": 45,
            "code": "def dummy(doc):\n    return doc\ntfidf_vect = TfidfVectorizer(analyzer=dummy)",
            "class": "Data Transform",
            "desc": "The code defines a dummy analyzer function and initializes a `TfidfVectorizer` with this dummy function as the analyzer, indicating that the input documents are pre-tokenized lists.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9941847
            },
            "cluster": 0
        }, {
            "cell_id": 46,
            "code": "tfidf_fit = tfidf_vect.fit(train_df['token_text'])",
            "class": "Data Transform",
            "desc": "The code fits the `TfidfVectorizer` to the `token_text` column of the `train_df` DataFrame, learning the vocabulary and IDF values from the training data.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996877
            },
            "cluster": 4
        }, {
            "cell_id": 47,
            "code": "matrix_train = tfidf_fit.transform(train_df['token_text'])\nmatrix_test = tfidf_fit.transform(test_df['token_text'])",
            "class": "Data Transform",
            "desc": "The code transforms the `token_text` column in both `train_df` and `test_df` DataFrames into TF-IDF feature matrices using the fitted `TfidfVectorizer`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9759149
            },
            "cluster": 2
        }, {
            "cell_id": 49,
            "code": "counts_df_train = pd.DataFrame(matrix_train.toarray())\ncounts_df_test = pd.DataFrame(matrix_test.toarray())",
            "class": "Data Transform",
            "desc": "The code converts the TF-IDF feature matrices for both training and testing datasets into pandas DataFrames named `counts_df_train` and `counts_df_test`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.9986713
            },
            "cluster": -1
        }, {
            "cell_id": 50,
            "code": "train_df['length'] = train_df.text.apply(lambda x: len(x) - x.count(' '))\ntest_df['length'] = test_df.text.apply(lambda x: len(x) - x.count(' '))",
            "class": "Data Transform",
            "desc": "The code adds a new column `length` to both `train_df` and `test_df` DataFrames, calculating the length of each tweet excluding spaces.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9990262
            },
            "cluster": 2
        }, {
            "cell_id": 51,
            "code": "train_df['punct_perc'] = train_df.text.apply(lambda x: sum([1 for char in x if char in non_alpha])/(len(x) - x.count(' '))*100)\ntest_df['punct_perc'] = test_df.text.apply(lambda x: sum([1 for char in x if char in non_alpha])/(len(x) - x.count(' '))*100)",
            "class": "Data Transform",
            "desc": "The code adds a new column `punct_perc` to both `train_df` and `test_df` DataFrames, calculating the percentage of characters in each tweet that are punctuation or digits.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9990037
            },
            "cluster": 2
        }, {
            "cell_id": 52,
            "code": "train_df['word_count'] = train_df.token_text.apply(len)\ntest_df['word_count'] = train_df.token_text.apply(len)",
            "class": "Data Transform",
            "desc": "The code adds a new column `word_count` to both `train_df` and `test_df` DataFrames, calculating the number of words in each tweet based on the `token_text` column.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9987459
            },
            "cluster": 2
        }, {
            "cell_id": 54,
            "code": "train_df['length_int'] =pd.cut(train_df.length, 14, include_lowest=True)\n                               #bins=[0, 15, 30, 40, 50,60, 80, 100, 120, 140, 180]\ntest_df['length_int'] =pd.cut(test_df.length, 14, include_lowest=True)\n                              #bins=[0, 15, 30, 40, 50,60, 80, 100, 120, 140, 180]",
            "class": "Data Transform",
            "desc": "The code creates a new column `length_int` in both `train_df` and `test_df` DataFrames by binning the `length` values into 14 intervals.",
            "testing": {
                "class": "Visualization",
                "subclass": "relationship",
                "subclass_id": 81,
                "predicted_subclass_probability": 0.30801016
            },
            "cluster": 2
        }, {
            "cell_id": 58,
            "code": "## data transformation\nplt.hist(train_df[train_df.target==1]['length']**2.3, bins = 40, color = 'blue', alpha=0.5)\nplt.hist(train_df[train_df.target==0]['length']**2.3, bins = 40, color = 'red', alpha=0.5);\n## by transforming the distribution into a bimodal we can notice that the data are more separated",
            "class": "Data Transform",
            "desc": "The code applies a power transformation (raising to the power of 2.3) to the `length` column in the `train_df` DataFrame and then creates histograms to visualize the distribution for disaster and non-disaster tweets, improving the separation between the classes.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9983096
            },
            "cluster": 4
        }, {
            "cell_id": 59,
            "code": "train_df['length'] = train_df['length']**2.3\ntest_df['length'] = train_df['length']**2.3",
            "class": "Data Transform",
            "desc": "The code applies the same power transformation (raising to the power of 2.3) to the `length` column in both `train_df` and `test_df` DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9994217
            },
            "cluster": 2
        }, {
            "cell_id": 60,
            "code": "plt.hist(train_df['punct_perc']**(1/3), bins = 40);",
            "class": "Data Transform",
            "desc": "The code applies a cube root transformation (raising to the power of 1/3) to the `punct_perc` column in the `train_df` DataFrame and creates a histogram to visualize the transformed distribution.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9975884
            },
            "cluster": 4
        }, {
            "cell_id": 61,
            "code": "train_df['punct_perc'] = train_df['punct_perc']**(1/3)\ntest_df['punct_perc'] = train_df['punct_perc']**(1/3)",
            "class": "Data Transform",
            "desc": "The code applies a cube root transformation (raising to the power of 1/3) to the `punct_perc` column in both `train_df` and `test_df` DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99934
            },
            "cluster": 2
        }, {
            "cell_id": 64,
            "code": "# assign an emotion to each tweet\ntrain_df['emotion'] = train_df.text.apply(lambda x: te.get_emotion(x))\ntest_df['emotion'] = test_df.text.apply(lambda x: te.get_emotion(x))",
            "class": "Data Transform",
            "desc": "The code creates a new column `emotion` in both `train_df` and `test_df` DataFrames by applying the `text2emotion` library to extract emotions from each tweet.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.8783199
            },
            "cluster": 2
        }, {
            "cell_id": 65,
            "code": "# exploding the dictionary into 4 different columns, based on the dictionary keys\ntrain_df = pd.concat([train_df, pd.DataFrame(train_df['emotion'].tolist())], axis =1)\ntest_df = pd.concat([test_df, pd.DataFrame(test_df['emotion'].tolist())], axis =1)",
            "class": "Data Transform",
            "desc": "The code expands the `emotion` column, which contains dictionaries of emotions, into separate columns for each emotion and concatenates these new columns with `train_df` and `test_df` DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.99380034
            },
            "cluster": 3
        }, {
            "cell_id": 66,
            "code": "total_emotions = train_df[['Happy', 'Angry', 'Surprise', 'Sad', 'Fear','target']].groupby('target').sum()",
            "class": "Data Transform",
            "desc": "The code calculates the sum of each emotion category (Happy, Angry, Surprise, Sad, Fear) for disaster and non-disaster tweets, grouped by the `target` column, and stores the result in `total_emotions`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.99574137
            },
            "cluster": -1
        }, {
            "cell_id": 67,
            "code": "mean_emotions = train_df[['Happy', 'Angry', 'Surprise', 'Sad', 'Fear','target']].groupby('target').mean()",
            "class": "Data Transform",
            "desc": "The code calculates the mean of each emotion category (Happy, Angry, Surprise, Sad, Fear) for disaster and non-disaster tweets, grouped by the `target` column, and stores the result in `mean_emotions`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.99561524
            },
            "cluster": -1
        }, {
            "cell_id": 72,
            "code": "train_df['sentiment'] = train_df.text.astype(str).apply(lambda x: sia.polarity_scores(x))\ntest_df['sentiment'] = test_df.text.astype(str).apply(lambda x: sia.polarity_scores(x))",
            "class": "Data Transform",
            "desc": "The code creates a new column `sentiment` in both `train_df` and `test_df` DataFrames by applying NLTK's `SentimentIntensityAnalyzer` to each tweet to extract sentiment scores.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.9847083
            },
            "cluster": 2
        }, {
            "cell_id": 73,
            "code": "train_df = pd.concat([train_df, pd.DataFrame(train_df['sentiment'].tolist())], axis =1)\ntest_df = pd.concat([test_df, pd.DataFrame(test_df['sentiment'].tolist())], axis =1)",
            "class": "Data Transform",
            "desc": "The code expands the `sentiment` column, which contains dictionaries of sentiment scores, into separate columns and concatenates these new columns with `train_df` and `test_df` DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.98916924
            },
            "cluster": 3
        }, {
            "cell_id": 74,
            "code": "mean_sentiment = train_df[['neg', 'neu', 'pos', 'compound','target']].groupby('target').mean()\ntotal_sentiment = train_df[['neg', 'neu', 'pos', 'compound','target']].groupby('target').sum()",
            "class": "Data Transform",
            "desc": "The code calculates the mean and total sentiment scores (`neg`, `neu`, `pos`, `compound`) for disaster and non-disaster tweets, grouped by the `target` column, and stores the results in `mean_sentiment` and `total_sentiment` DataFrames respectively.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.9969091
            },
            "cluster": -1
        }, {
            "cell_id": 78,
            "code": "#full_train_df = pd.concat([train_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int'], axis=1), counts_df_train], axis=1)\nfull_train_df = pd.concat([train_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int','Happy', 'Angry', 'Surprise', 'Sad', 'Fear'], axis=1), counts_df_train], axis=1)",
            "class": "Data Transform",
            "desc": "The code creates a `full_train_df` DataFrame by dropping certain columns related to location, keyword, original text, tokenized text, sentiment scores, and emotions from `train_df`, and concatenating the remaining columns with `counts_df_train`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.64140093
            },
            "cluster": -1
        }, {
            "cell_id": 79,
            "code": "#full_test_df = pd.concat([test_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int'], axis=1), counts_df_test], axis=1)\nfull_test_df = pd.concat([test_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int','Happy', 'Angry', 'Surprise', 'Sad', 'Fear'], axis=1), counts_df_test], axis=1)",
            "class": "Data Transform",
            "desc": "The code creates a `full_test_df` DataFrame by dropping certain columns related to location, keyword, original text, tokenized text, sentiment scores, and emotions from `test_df`, and concatenating the remaining columns with `counts_df_test`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.699594
            },
            "cluster": -1
        }, {
            "cell_id": 80,
            "code": "# deleting unnecessary dataframes to save memory\ndel train_df\ndel test_df\ndel counts_df_train\ndel counts_df_test",
            "class": "Data Transform",
            "desc": "The code deletes the `train_df`, `test_df`, `counts_df_train`, and `counts_df_test` DataFrames to free up memory.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9989153
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "train_df.head(5)",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first five rows of the `train_df` DataFrame to provide an initial look at the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997615
            },
            "cluster": 4
        }, {
            "cell_id": 4,
            "code": "train_df.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code outputs a concise summary of the `train_df` DataFrame, including the data types and non-null counts for each column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993624
            },
            "cluster": 3
        }, {
            "cell_id": 5,
            "code": "test_df.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code outputs a concise summary of the `test_df` DataFrame, including the data types and non-null counts for each column.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993699
            },
            "cluster": 3
        }, {
            "cell_id": 6,
            "code": "print(f\"Rows | Columns\\n{train_df.shape}\")\nprint(test_df.shape)",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the number of rows and columns for both `train_df` and `test_df` DataFrames.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.99951255
            },
            "cluster": 4
        }, {
            "cell_id": 9,
            "code": "# we notice many missing values for the location column",
            "class": "Exploratory Data Analysis",
            "desc": "The code comments on the observation of numerous missing values in the `location` column of the dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.2938043
            },
            "cluster": 4
        }, {
            "cell_id": 11,
            "code": "# checking for duplicates\nprint(train_df.duplicated(subset=['text']).sum())\nprint(test_df.duplicated(subset=['text']).sum())",
            "class": "Exploratory Data Analysis",
            "desc": "The code checks and prints the number of duplicated rows based on the `text` column for both `train_df` and `test_df` DataFrames.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.87618726
            },
            "cluster": 4
        }, {
            "cell_id": 12,
            "code": "print(train_df.duplicated(subset=['text','target']).sum())",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the number of duplicated rows in the `train_df` DataFrame based on both the `text` and `target` columns.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.7811897
            },
            "cluster": 4
        }, {
            "cell_id": 17,
            "code": "train_df.shape",
            "class": "Exploratory Data Analysis",
            "desc": "The code outputs the dimensions of the `train_df` DataFrame after removing duplicates.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9995821
            },
            "cluster": 3
        }, {
            "cell_id": 22,
            "code": "# Somes actual tweets (non-disaster)\nprint(train_df.text[train_df['target'] == 0][:10].values)",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the text of the first 10 tweets in the `train_df` DataFrame that are labeled as non-disaster (target=0).",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.8921785
            },
            "cluster": 1
        }, {
            "cell_id": 23,
            "code": "print(train_df.text[train_df['target'] == 1][:10].values)",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the text of the first 10 tweets in the `train_df` DataFrame that are labeled as disaster (target=1).",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.5046063
            },
            "cluster": 1
        }, {
            "cell_id": 48,
            "code": "print(matrix_train.shape)\nprint(matrix_test.shape)",
            "class": "Exploratory Data Analysis",
            "desc": "The code prints the dimensions of the TF-IDF feature matrices for both the training and testing datasets (`matrix_train` and `matrix_test`).",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9989506
            },
            "cluster": 4
        }, {
            "cell_id": 70,
            "code": "# From the graphs and tables above we notice that, in cases of actual disasters the tweets have greater fear score\n# while for the non-disaster the mean score of 'Happy' is higher.\n# however those variables appear not to have a great importance. They might be dropped from the model.",
            "class": "Exploratory Data Analysis",
            "desc": "The code comments on the observation from the visualizations and tables, noting that disaster tweets tend to have higher fear scores, while non-disaster tweets have higher mean happiness scores, and suggests that these variables might not be crucial for the model.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.57685375
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "# importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\nfrom collections import Counter, defaultdict\n%matplotlib inline",
            "class": "Imports and Environment",
            "desc": "The code imports necessary libraries such as pandas, numpy, matplotlib, seaborn, and collections, and configures inline plotting for Jupyter Notebook.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9993229
            },
            "cluster": 0
        }, {
            "cell_id": 1,
            "code": "# display 100 columns\npd.set_option('display.max_columns', 100)",
            "class": "Imports and Environment",
            "desc": "The code sets the pandas display option to show a maximum of 100 columns when displaying DataFrame contents.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9973706
            },
            "cluster": 1
        }, {
            "cell_id": 24,
            "code": "from wordcloud import WordCloud",
            "class": "Imports and Environment",
            "desc": "The code imports the `WordCloud` class from the `wordcloud` library for generating word cloud visualizations.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9992773
            },
            "cluster": 1
        }, {
            "cell_id": 32,
            "code": "import string\nimport emoji\nimport nltk\nimport re",
            "class": "Imports and Environment",
            "desc": "The code imports the `string`, `emoji`, `nltk`, and `re` libraries for text processing and manipulation tasks.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99923146
            },
            "cluster": -1
        }, {
            "cell_id": 44,
            "code": "from sklearn.feature_extraction.text import TfidfVectorizer",
            "class": "Imports and Environment",
            "desc": "The code imports the `TfidfVectorizer` class from the `sklearn.feature_extraction.text` module for converting text data into TF-IDF feature vectors.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99927324
            },
            "cluster": 1
        }, {
            "cell_id": 62,
            "code": "!pip install text2emotion",
            "class": "Imports and Environment",
            "desc": "The code installs the `text2emotion` library, which is used for extracting emotions from text.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.99531555
            },
            "cluster": 1
        }, {
            "cell_id": 63,
            "code": "import text2emotion as te",
            "class": "Imports and Environment",
            "desc": "The code imports the `text2emotion` library as `te` for analyzing emotions in text data.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993248
            },
            "cluster": 1
        }, {
            "cell_id": 71,
            "code": "!pip install twython\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()",
            "class": "Imports and Environment",
            "desc": "The code installs the `twython` library and initializes the `SentimentIntensityAnalyzer` from NLTK for analyzing the sentiment of text data.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.8762451
            },
            "cluster": 1
        }, {
            "cell_id": 81,
            "code": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV",
            "class": "Imports and Environment",
            "desc": "The code imports the `RandomForestClassifier` class from `sklearn.ensemble` and the `GridSearchCV` class from `sklearn.model_selection` for building and tuning a random forest model.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99927956
            },
            "cluster": 1
        }, {
            "cell_id": 85,
            "code": "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)",
            "class": "Model Evaluation",
            "desc": "The code creates a DataFrame from the cross-validation results of the `GridSearchCV` object, sorts the results by `mean_test_score` in descending order, and displays it for evaluation.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.9971615
            },
            "cluster": 1
        }, {
            "cell_id": 88,
            "code": "predictions_rf = rf.predict(full_test_df.drop(['id'], axis=1))",
            "class": "Model Evaluation",
            "desc": "The code generates predictions using the fitted `RandomForestClassifier` on the features of the `full_test_df` DataFrame, excluding the `id` column.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9943375
            },
            "cluster": 1
        }, {
            "cell_id": 82,
            "code": "forest =  RandomForestClassifier()\nparam = {'n_estimators':[200, 500],\n        'max_depth':[200, 300]}",
            "class": "Model Training",
            "desc": "The code initializes a `RandomForestClassifier` and defines a parameter grid for `GridSearchCV` with different values for `n_estimators` and `max_depth`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.99327236
            },
            "cluster": 0
        }, {
            "cell_id": 83,
            "code": "gs = GridSearchCV(forest, param, cv=3)",
            "class": "Model Training",
            "desc": "The code initializes a `GridSearchCV` object with the `RandomForestClassifier` and the provided parameter grid, setting cross-validation to 3 folds.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.40436822
            },
            "cluster": 0
        }, {
            "cell_id": 84,
            "code": "gs_fit = gs.fit(full_train_df.drop(['target','id'], axis=1), full_train_df['target'])",
            "class": "Model Training",
            "desc": "The code fits the `GridSearchCV` object using the features (excluding `target` and `id` columns) and the target variable from the `full_train_df` DataFrame to find the best hyperparameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996861
            },
            "cluster": -1
        }, {
            "cell_id": 86,
            "code": "rf =  RandomForestClassifier(n_estimators=200, max_depth=200)",
            "class": "Model Training",
            "desc": "The code initializes a `RandomForestClassifier` with the hyperparameters `n_estimators` set to 200 and `max_depth` set to 200.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9980399
            },
            "cluster": 0
        }, {
            "cell_id": 87,
            "code": "rf.fit(full_train_df.drop(['target','id'], axis=1), full_train_df['target'])",
            "class": "Model Training",
            "desc": "The code fits the `RandomForestClassifier` using the features (excluding `target` and `id` columns) and the target variable from the `full_train_df` DataFrame.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99969363
            },
            "cluster": -1
        }, {
            "cell_id": 7,
            "code": "table = plt.table(cellText=[[train_df.location.isnull().sum()], [test_df.location.isnull().sum()]],\n         rowLabels=['Train Data', 'Test Data'],\n         colLabels=['Number of Missing values'],\n         loc='top')\nplt.box(on=None)\nplt.axis('off')\n# plt.subplots_adjust(top = 1, bottom = 0.1, right = 1, left = 0, \n#             hspace = 0, wspace = 0)\n#plt.margins(0,0)\ntable.set_fontsize(14);",
            "class": "Visualization",
            "desc": "The code creates and customizes a table using Matplotlib to display the number of missing values in the `location` column for both `train_df` and `test_df`.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.86134624
            },
            "cluster": 2
        }, {
            "cell_id": 8,
            "code": "# checking for missing values\nfig, (ax1, ax2) = plt.subplots(2,1, figsize=(8,5))\n\nsns.heatmap(train_df.isnull(),yticklabels=False,cbar=False,cmap='flare', ax = ax1)\nax1.tick_params(axis='x', labelsize=13, rotation = 45)\nax1.set_title('train data')\n\nsns.heatmap(test_df.isnull(),yticklabels=False,cbar=False,cmap='flare', ax = ax2)\nax2.tick_params(axis='x', labelsize=13, rotation = 45)\nax2.set_title('test data')\nfig.tight_layout();",
            "class": "Visualization",
            "desc": "The code creates heatmaps using Seaborn to visualize missing values in the `train_df` and `test_df` DataFrames, displaying the plots vertically stacked in a single figure.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.99526525
            },
            "cluster": 2
        }, {
            "cell_id": 18,
            "code": "fig = plt.figure(figsize=(20,5))\ntrain_df.keyword.value_counts().head(25).plot(kind = 'bar')\nplt.tick_params(axis='x', labelsize=11, rotation = 45)\nplt.title('Top keywords');",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Matplotlib to visualize the top 25 most frequent keywords in the `train_df` DataFrame.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99566513
            },
            "cluster": 2
        }, {
            "cell_id": 19,
            "code": "fig = plt.figure(figsize=(20,5))\ntrain_df.location.value_counts().head(25).plot(kind = 'bar')\nplt.tick_params(axis='x', labelsize=9, rotation = 45)\nplt.title('Top Locations');",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Matplotlib to visualize the top 25 most frequent locations in the `train_df` DataFrame.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99453616
            },
            "cluster": 2
        }, {
            "cell_id": 20,
            "code": "fig = plt.figure(figsize=(7,5))\nsns.countplot(data = train_df, x = \"target\")\nplt.title('Non-actual vs actual distater tweets counts');",
            "class": "Visualization",
            "desc": "The code creates a count plot using Seaborn to compare the number of non-actual disaster tweets versus actual disaster tweets in the `train_df` DataFrame.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9882289
            },
            "cluster": 2
        }, {
            "cell_id": 21,
            "code": "fig = plt.figure(figsize=(7,5))\n(train_df.groupby(['target']).count() / len(train_df['target']))['id'].plot(kind='bar', width = 0.85, color = ['tomato', 'steelblue'])\nplt.title('Non-actual vs actual distater tweets percentage');",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Matplotlib to display the percentage distribution of non-actual versus actual disaster tweets in the `train_df` DataFrame.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9961461
            },
            "cluster": 2
        }, {
            "cell_id": 25,
            "code": "wordcloud_non_dis = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(train_df.text[train_df['target'] == 0]))\nwordcloud_dis = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(train_df.text[train_df['target'] == 1]))",
            "class": "Visualization",
            "desc": "The code generates word clouds for non-disaster and disaster tweets in the `train_df` DataFrame using the `WordCloud` class, with specified dimensions and background color.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.68369985
            },
            "cluster": 2
        }, {
            "cell_id": 26,
            "code": "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,14))\n\nax1.imshow(wordcloud_non_dis, interpolation='bilinear')\nax1.set_title('Non disaster')\nax1.axis(\"off\")\nax2.imshow(wordcloud_dis, interpolation='bilinear')\nax2.set_title('Disaster')\nax2.axis(\"off\");",
            "class": "Visualization",
            "desc": "The code visualizes the previously generated word clouds for non-disaster and disaster tweets side-by-side using Matplotlib, with appropriate titles and axis settings.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99756926
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "fig, axes = plt.subplots(1,2,figsize=(25,6))\nax1 = train_df[train_df['target']==0]['location'].value_counts().head(25).plot(kind = 'bar', color = 'b', alpha = 0.5, ax=axes[0])\nax1.tick_params(axis='x', labelsize=9, rotation = 45)\nax1.set_title('Non-Disaster')\nax2 = train_df[train_df['target']==1]['location'].value_counts().head(25).plot(kind = 'bar', color = 'r', alpha = 0.5, ax=axes[1])\nax2.tick_params(axis='x', labelsize=9, rotation = 45)\nax2.set_title('Disaster');",
            "class": "Visualization",
            "desc": "The code creates two bar plots using Matplotlib to visualize the top 25 most frequent locations for non-disaster and disaster tweets in the `train_df` DataFrame, displaying them side-by-side.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99818254
            },
            "cluster": 0
        }, {
            "cell_id": 31,
            "code": "fig, axes = plt.subplots(1,2,figsize=(25,6))\nax1 = train_df[train_df['target']==0]['keyword'].value_counts().head(25).plot(kind = 'bar', color = 'b', alpha = 0.5, ax=axes[0])\nax1.tick_params(axis='x', labelsize=9, rotation = 45)\nax1.set_title('Top Keywords (non disaster)')\nax2 = train_df[train_df['target']==1]['keyword'].value_counts().head(25).plot(kind = 'bar', color = 'r', alpha = 0.5, ax=axes[1])\nax2.tick_params(axis='x', labelsize=9, rotation = 45)\nax2.set_title('Top Keywords (disaster)');",
            "class": "Visualization",
            "desc": "The code creates two bar plots using Matplotlib to visualize the top 25 most frequent keywords for non-disaster and disaster tweets in the `train_df` DataFrame, displaying them side-by-side.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.981807
            },
            "cluster": 0
        }, {
            "cell_id": 53,
            "code": "fig, axes = plt.subplots(1,2,figsize=(17,5))\nsns.histplot(data = train_df, \n             x= 'length',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0])\nsns.boxplot(data = train_df, x = 'target', y = 'length',ax=axes[1]);",
            "class": "Visualization",
            "desc": "The code creates a histogram and a box plot using Seaborn to visualize the distribution and summary statistics of tweet lengths in the `train_df` DataFrame, separated by the `target` label.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9957883
            },
            "cluster": 2
        }, {
            "cell_id": 55,
            "code": "fig = plt.figure(figsize=(7,5))\ntrain_df[train_df['target']==0]['length_int'].value_counts(sort=False).plot(kind='bar', alpha = 0.5, color='blue', label = 'No')\ntrain_df[train_df['target']==1]['length_int'].value_counts(sort=False).plot(kind='bar', alpha = 0.5, color='orange', label = 'Yes')\nplt.legend(title='Actual Disaster', fontsize=11, title_fontsize=12);\n                             ",
            "class": "Visualization",
            "desc": "The code creates a bar plot using Matplotlib to compare the distribution of tweet lengths, binned into intervals, for non-disaster and disaster tweets in the `train_df` DataFrame.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9890703
            },
            "cluster": 2
        }, {
            "cell_id": 56,
            "code": "fig, axes = plt.subplots(1,2,figsize=(17,5))\nsns.histplot(data = train_df, \n             x= 'punct_perc',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0])\nsns.boxplot(data = train_df, x = 'target', y = 'punct_perc',ax=axes[1]);",
            "class": "Visualization",
            "desc": "The code creates a histogram and a box plot using Seaborn to visualize the distribution and summary statistics of the punctuation percentage in tweets in the `train_df` DataFrame, separated by the `target` label.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99661547
            },
            "cluster": 2
        }, {
            "cell_id": 57,
            "code": "fig, axes = plt.subplots(1,2,figsize=(17,5))\nsns.histplot(data = train_df, \n             x= 'word_count',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0])\nsns.boxplot(data = train_df, x = 'target', y = 'word_count',ax=axes[1]);",
            "class": "Visualization",
            "desc": "The code creates a histogram and a box plot using Seaborn to visualize the distribution and summary statistics of word counts in tweets in the `train_df` DataFrame, separated by the `target` label.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9946801
            },
            "cluster": 2
        }, {
            "cell_id": 68,
            "code": "fig, axes = plt.subplots(1,2,figsize=(14,5))\nax1 = total_emotions.plot(kind='bar', ax = axes[0])\nax1.set_title('Total values of emotion scores per target class')\nax2 = mean_emotions.plot(kind='bar', ax = axes[1])\nax2.set_title('Mean Scores of emotions per target class');",
            "class": "Visualization",
            "desc": "The code creates two bar plots using Matplotlib to visualize the total and mean emotion scores for disaster and non-disaster tweets, displaying them side-by-side.",
            "testing": {
                "class": "Visualization",
                "subclass": "time_series",
                "subclass_id": 75,
                "predicted_subclass_probability": 0.8920648
            },
            "cluster": 0
        }, {
            "cell_id": 69,
            "code": "fig, axes = plt.subplots(2,2,figsize=(20,10))\nsns.histplot(data = train_df, \n             x= 'Happy',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0,0])\nsns.boxplot(data = train_df, x = 'target', y = 'Happy',ax=axes[0,1])\nsns.histplot(data = train_df, \n             x= 'Fear',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[1,0])\nsns.boxplot(data = train_df, x = 'target', y = 'Fear',ax=axes[1,1])",
            "class": "Visualization",
            "desc": "The code creates a series of histograms and box plots using Seaborn to visualize the distribution and summary statistics of `Happy` and `Fear` emotion scores in the `train_df` DataFrame, separated by the `target` label, and displays them in a 2x2 grid.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99813986
            },
            "cluster": 2
        }, {
            "cell_id": 75,
            "code": "fig, axes = plt.subplots(1,2,figsize=(14,5))\nax1 = total_sentiment.plot(kind='bar', ax = axes[0])\nax1.set_title('Total values of emotion scores per target class')\nax2 = mean_sentiment.plot(kind='bar', ax = axes[1])\nax2.set_title('Mean Scores of emotions per target class');",
            "class": "Visualization",
            "desc": "The code creates two bar plots using Matplotlib to visualize the total and mean sentiment scores for disaster and non-disaster tweets, displaying them side-by-side.",
            "testing": {
                "class": "Visualization",
                "subclass": "time_series",
                "subclass_id": 75,
                "predicted_subclass_probability": 0.86384803
            },
            "cluster": 0
        }, {
            "cell_id": 76,
            "code": "fig, axes = plt.subplots(1,2,figsize=(17,5))\nsns.histplot(data = train_df, \n             x= 'compound',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0])\nsns.boxplot(data = train_df, x = 'target', y = 'compound',ax=axes[1]);",
            "class": "Visualization",
            "desc": "The code creates a histogram and a box plot using Seaborn to visualize the distribution and summary statistics of the `compound` sentiment scores in the `train_df` DataFrame, separated by the `target` label.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9953399
            },
            "cluster": 2
        }, {
            "cell_id": 77,
            "code": "fig, axes = plt.subplots(2,2,figsize=(20,10))\nsns.histplot(data = train_df, \n             x= 'neg',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0,0])\nsns.boxplot(data = train_df, x = 'target', y = 'neg',ax=axes[0,1])\nsns.histplot(data = train_df, \n             x= 'pos',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[1,0])\nsns.boxplot(data = train_df, x = 'target', y = 'pos',ax=axes[1,1])",
            "class": "Visualization",
            "desc": "The code creates a series of histograms and box plots using Seaborn to visualize the distribution and summary statistics of `neg` and `pos` sentiment scores in the `train_df` DataFrame, separated by the `target` label, and displays them in a 2x2 grid.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99770266
            },
            "cluster": 2
        }],
        "notebook_id": 8,
        "notebook_name": "nlp-disaster-tweets-text2emotion-vader.ipynb",
        "user": "nlp-disaster-tweets-text2emotion-vader.ipynb"
    }, {
        "cells": [{
            "cell_id": 7,
            "code": "clf.fit(train_vectors, train_df[\"target\"])\n\nsample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsample_submission[\"target\"] = clf.predict(test_vectors)\n\nsample_submission.head()\n\nsample_submission.to_csv(\"submission.csv\", index=False)",
            "class": "Data Export",
            "desc": "The code retrains the LinearSVC model, generates predictions on the test data, assigns these predictions to a sample submission DataFrame, and exports the DataFrame to a CSV file.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9986327
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")",
            "class": "Data Extraction",
            "desc": "The code loads training and testing datasets from CSV files into pandas DataFrames.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99975425
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "count_vectorizer = feature_extraction.text.CountVectorizer()",
            "class": "Data Transform",
            "desc": "The code initializes a CountVectorizer from scikit-learn to convert text data into a matrix of token counts.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.59570295
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n\n\n\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])",
            "class": "Data Transform",
            "desc": "The code transforms the text data from the training and testing datasets into count vector representations using the previously initialized CountVectorizer.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.965019
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "import numpy as np\n\nimport pandas as pd\n\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.pipeline import make_pipeline",
            "class": "Imports and Environment",
            "desc": "The code imports essential libraries for data manipulation (numpy and pandas), machine learning (scikit-learn modules), and pipeline construction.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.999337
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n\nscores",
            "class": "Model Evaluation",
            "desc": "The code performs cross-validation on the LinearSVC model using 3 folds and evaluates its performance based on the F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.967577
            },
            "cluster": 3
        }, {
            "cell_id": 4,
            "code": "clf = LinearSVC()",
            "class": "Model Training",
            "desc": "The code initializes a Linear Support Vector Classification (LinearSVC) model using scikit-learn.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.2675037
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "clf.fit(train_vectors, train_df[\"target\"])",
            "class": "Model Training",
            "desc": "The code trains the LinearSVC model using the count vector representations of the training data and their corresponding target labels.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.999706
            },
            "cluster": 1
        }],
        "notebook_id": 9,
        "notebook_name": "nlp-assignment-1.ipynb",
        "user": "nlp-assignment-1.ipynb"
    }, {
        "cells": [{
            "cell_id": 9,
            "code": "test_pred = model.predict(test_input)\n\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "This code generates predictions on the test data using the trained model, assigns the rounded predictions to the 'target' column of the submission DataFrame, and saves the results to a CSV file named 'submission.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992293
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "#reading input data with pandas\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\n#visualizing some of the tweets\nfor i, val in enumerate(train.iloc[:2][\"text\"].to_list()):\n    print(\"Tweet {}: {}\".format(i+1, val))",
            "class": "Data Extraction",
            "desc": "This code reads training, test, and sample submission data from CSV files into pandas DataFrames and prints the first two tweets from the training set.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99962354
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)",
            "class": "Data Transform",
            "desc": "This code defines a function `bert_encode` that tokenizes and encodes text inputs into BERT-compatible format by creating tokens, attention masks, and segment IDs arrays using a given tokenizer.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.80152774
            },
            "cluster": 1
        }, {
            "cell_id": 6,
            "code": "#vocab file from pre-trained BERT for tokenization\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n\n#returns true/false depending on if we selected cased/uncased bert layer\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\n#Create the tokenizer\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\n#tokenizing the training and testing data\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values",
            "class": "Data Transform",
            "desc": "This code retrieves the vocabulary file and case sensitivity setting from the loaded BERT model to create a tokenizer, then tokenizes the training and testing data into BERT-compatible format, and extracts the training labels from the training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.95965594
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\nimport tokenization",
            "class": "Imports and Environment",
            "desc": "This code imports necessary libraries and dependencies such as NumPy, pandas, TensorFlow, and tokenization.py script for a machine learning task involving NLP and BERT.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_url",
                "subclass_id": 42,
                "predicted_subclass_probability": 0.39663005
            },
            "cluster": 0
        }, {
            "cell_id": 1,
            "code": "#setting a seed for reproducability\nSEED = 2718\ndef seed_everything(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed) \n    \nseed_everything(SEED) ",
            "class": "Imports and Environment",
            "desc": "This code sets a seed for reproducibility by defining a function `seed_everything` that sets seeds for NumPy and TensorFlow, and then calls it with a specific seed value.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.98266476
            },
            "cluster": 1
        }, {
            "cell_id": 5,
            "code": "#load uncased bert model\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)",
            "class": "Imports and Environment",
            "desc": "This code loads an uncased BERT model from TensorFlow Hub and assigns it to `bert_layer`, making it trainable for subsequent model training.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.8462454
            },
            "cluster": -1
        }, {
            "cell_id": 4,
            "code": "def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    #could be pooled_output, sequence_output yet sequence output provides for each input token (in context)\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    \n    #specifying optimizer\n    model.compile(Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model",
            "class": "Model Training",
            "desc": "This code defines a function `build_model` that constructs a BERT-based neural network model using TensorFlow and Keras, which includes input layers for word IDs, attention masks, and segment IDs, followed by BERT layer processing, a dense output layer, and compiling with the Adam optimizer and binary cross-entropy loss function.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.96481687
            },
            "cluster": 2
        }, {
            "cell_id": 7,
            "code": "model = build_model(bert_layer, max_len=160)\nmodel.summary()",
            "class": "Model Training",
            "desc": "This code builds a BERT-based model using the previously defined `build_model` function and the loaded BERT layer, with a maximum sequence length of 160, and then prints the model summary.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9372223
            },
            "cluster": -1
        }, {
            "cell_id": 8,
            "code": "checkpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.1,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)",
            "class": "Model Training",
            "desc": "This code trains the BERT-based model using the training data and labels with a 10% validation split over 3 epochs, a batch size of 16, and saves the best model based on validation accuracy using the `ModelCheckpoint` callback.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9952106
            },
            "cluster": -1
        }],
        "notebook_id": 10,
        "notebook_name": "roberta-w-tensorflow-explained-0-844.ipynb",
        "user": "roberta-w-tensorflow-explained-0-844.ipynb"
    }, {
        "cells": [{
            "cell_id": 10,
            "code": "pd.DataFrame(np.where(pred>0.5,1,0)).value_counts()",
            "class": "Data Export",
            "desc": "The code converts the prediction results into binary outcomes (0 or 1), creates a DataFrame from these outcomes using pandas, and then generates a count of each unique value in the DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9979062
            },
            "cluster": -1
        }, {
            "cell_id": 11,
            "code": "pd.DataFrame({\n    'id':test.id,\n    'target':np.where(pred>0.5,1,0)[:,0]\n}).to_csv('submission.csv',index=False)",
            "class": "Data Export",
            "desc": "The code creates a DataFrame with 'id' and 'target' columns from the 'test' dataframe and the predicted binary outcomes, then exports it to a CSV file named 'submission.csv' without including the index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992298
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train=pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')",
            "class": "Data Extraction",
            "desc": "The code reads the 'train.csv' and 'test.csv' files from the specified input directory using pandas and loads them into the 'train' and 'test' dataframes, respectively.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99974996
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "stop_words=nltk.corpus.stopwords.words('english')\ni=0\n#sc=SpellChecker()\n#data=pd.concat([train,test])\nwnl=WordNetLemmatizer()\nstemmer=SnowballStemmer('english')\nfor doc in train.text:\n    doc=re.sub(r'https?://\\S+|www\\.\\S+','',doc)\n    doc=re.sub(r'<.*?>','',doc)\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\n    doc=contractions.fix(doc)\n    tokens=nltk.word_tokenize(doc)\n    filtered=[token for token in tokens if token not in stop_words]\n    doc=' '.join(filtered)\n    train.text[i]=doc\n    i+=1\ni=0\nfor doc in test.text:\n    doc=re.sub(r'https?://\\S+|www\\.\\S+','',doc)\n    doc=re.sub(r'<.*?>','',doc)\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\n    doc=contractions.fix(doc)\n    tokens=nltk.word_tokenize(doc)\n    filtered=[token for token in tokens if token not in stop_words]\n    doc=' '.join(filtered)\n    test.text[i]=doc\n    i+=1",
            "class": "Data Transform",
            "desc": "The code cleans and preprocesses text data in the 'train' and 'test' dataframes by removing URLs, HTML tags, and non-alphabetic characters, expanding contractions, tokenizing, lemmatizing, and filtering out stop words using nltk, WordNetLemmatizer, SnowballStemmer, and contractions libraries.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.54864067
            },
            "cluster": -1
        }, {
            "cell_id": 6,
            "code": "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\ntext_test = ['this is such an amazing movie!']\ntext_preprocessed = bert_preprocess_model(text_test)\n\nprint(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')",
            "class": "Data Transform",
            "desc": "The code uses the BERT preprocessing model from TensorFlow Hub to preprocess a sample text, then prints the keys, shapes, and first 12 elements of the word IDs, input mask, and type IDs from the preprocessed output.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.3991709
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
            "class": "Imports and Environment",
            "desc": "The code imports the NumPy library for numerical operations and the pandas library for data manipulation and analysis.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993907
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nimport re\n!pip install contractions\nimport contractions\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker",
            "class": "Imports and Environment",
            "desc": "The code imports the nltk library for natural language processing, downloads essential nltk datasets, installs and imports the contractions package, imports stemming and lemmatization tools from nltk, and installs and imports the pyspellchecker library.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.799391
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "import tensorflow_hub as hub\n!pip install tensorflow_text\nimport tensorflow as tf\nimport tensorflow_text as text",
            "class": "Imports and Environment",
            "desc": "The code imports TensorFlow Hub for accessing pre-trained models, installs the tensorflow_text package, and then imports TensorFlow and tensorflow_text for text processing tasks.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.93685156
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "classifier_model.load_weights('./model.h5')\npred=classifier_model.predict(test.text)",
            "class": "Model Evaluation",
            "desc": "The code loads the best model weights from a file called 'model.h5' and uses the trained classifier model to predict outcomes for the text data in the 'test' dataframe.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.54062176
            },
            "cluster": 3
        }, {
            "cell_id": 5,
            "code": " # choose any one of the below models and try them out\n\nbert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')",
            "class": "Model Training",
            "desc": "The code selects a BERT model and its corresponding preprocessing model based on predefined mappings and prints the selected model URLs, which are sourced from TensorFlow Hub. ",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9735415
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "bert_model = hub.KerasLayer(tfhub_handle_encoder)\nbert_results = bert_model(text_preprocessed)\n\nprint(f'Loaded BERT: {tfhub_handle_encoder}')\nprint(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\nprint(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\nprint(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\nprint(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')",
            "class": "Model Training",
            "desc": "The code initializes a BERT model using TensorFlow Hub, processes preprocessed text, and prints the model's pooled and sequence output shapes and the first 12 values of each output type.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.5216088
            },
            "cluster": 1
        }, {
            "cell_id": 8,
            "code": "def build_classifier_model():\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n    return tf.keras.Model(text_input, net)\n\nclassifier_model = build_classifier_model()\n\nclassifier_model.compile(optimizer=tf.keras.optimizers.Adam(3e-5), # can also try 1e-5,5e-5,2e-5 whichever performs best\n                         loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                         metrics=tf.metrics.BinaryAccuracy())\ncheckpoint=tf.keras.callbacks.ModelCheckpoint('model.h5',monitor='val_loss',save_best_only=True)\nes=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\nhistory = classifier_model.fit(train.text,train.target,validation_split=0.2,epochs=10,callbacks=[checkpoint,es],batch_size=8)",
            "class": "Model Training",
            "desc": "The code defines a function to build a BERT-based text classification model using TensorFlow and TensorFlow Hub, compiles the model with Adam optimizer and binary cross-entropy loss, and trains it with early stopping and model checkpoint callbacks.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.5413505
            },
            "cluster": 1
        }],
        "notebook_id": 11,
        "notebook_name": "nlp-disaster-tweets-bert-using-tf-hub.ipynb",
        "user": "nlp-disaster-tweets-bert-using-tf-hub.ipynb"
    }, {
        "cells": [{
            "cell_id": 26,
            "code": "# Copy the results to a pandas dataframe with an \"id\" column and a \"target\" column\nfinal_submission = pd.DataFrame( data={\"id\":test_data[\"id\"], \"target\":y_test_predictions})\n# Save the submission file\nfinal_submission.to_csv(\"submissionTweets.csv\", index=False)",
            "class": "Data Export",
            "desc": "This code snippet creates a Pandas DataFrame containing the test data IDs and their corresponding predicted target labels, and then saves this DataFrame to a CSV file named \"submissionTweets.csv\".",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99925834
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "train_data = pd.read_csv('../input/nlp-getting-started/train.csv')\nprint(train_data.shape)\ntrain_data.head(3)",
            "class": "Data Extraction",
            "desc": "This code reads the training data from a CSV file into a Pandas DataFrame and prints its shape and the first three rows.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9993906
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "# load test dataset\ntest_data = pd.read_csv('../input/nlp-getting-started/test.csv')\nprint(test_data.shape)\ntest_data.head(3)",
            "class": "Data Extraction",
            "desc": "This code snippet loads test data from a CSV file into a Pandas DataFrame, and displays its shape and the first three rows.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99944836
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "def prepare_sequence(text):\n    \"\"\"\n    Tokenize and prepare a sequence for the model. It tokenizes the text sequence\n    adding special tokens ([CLS], [SEP]), padding  to the max length and truncate \n    reviews longer than the max length.\n    Return the token IDs, the segment IDs and the mask IDs.\n    \"\"\"\n\n    prepared_sequence = tokenizer.encode_plus(\n                            text, \n                            add_special_tokens = True, \n                            max_length = MAX_LENGHT, \n                            padding = 'max_length',\n                            return_attention_mask = True\n                            )\n    return prepared_sequence",
            "class": "Data Transform",
            "desc": "This code defines a function `prepare_sequence` that tokenizes and prepares text sequences for the BERT model by adding special tokens, padding to the maximum length, and returning the token IDs, segment IDs, and mask IDs.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9985875
            },
            "cluster": 1
        }, {
            "cell_id": 10,
            "code": "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n    \"\"\"\n    Map to the expected input to TFBertForSequenceClassification.\n    \"\"\"\n    mapped_example = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"attention_mask\": attention_masks,\n    }\n    return mapped_example, label \n\ndef encode_examples(texts_and_labels):\n    \"\"\"\n    Prepare all sequences of text and build TF dataset.\n    \"\"\"\n\n    input_ids_list = []\n    token_type_ids_list = []\n    attention_mask_list = []\n    label_list = []\n        \n    for text, label in texts_and_labels:\n\n        bert_input = prepare_sequence(text)\n\n        input_ids_list.append(bert_input['input_ids'])\n        token_type_ids_list.append(bert_input['token_type_ids'])\n        attention_mask_list.append(bert_input['attention_mask'])\n        label_list.append([label])\n\n    # Create TF dataset\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (input_ids_list, attention_mask_list, token_type_ids_list,\n         label_list)\n    )\n    # Map to the expected input to TFBertForSequenceClassification\n    dataset_mapped = dataset.map(map_example_to_dict)\n    return dataset_mapped",
            "class": "Data Transform",
            "desc": "This code defines two functions: `map_example_to_dict`, which maps BERT input features to the expected input format for `TFBertForSequenceClassification`, and `encode_examples`, which prepares text sequences, tokenizes them using the `prepare_sequence` function, and builds a TensorFlow dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9974004
            },
            "cluster": 3
        }, {
            "cell_id": 11,
            "code": "X = train_data[\"text\"]\ny = train_data[\"target\"]",
            "class": "Data Transform",
            "desc": "This code snippet extracts the text and target columns from the training DataFrame into separate variables X and y, respectively.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.99927586
            },
            "cluster": 1
        }, {
            "cell_id": 12,
            "code": "# Split the training dataset for training and test\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, \n                                                    random_state=1)",
            "class": "Data Transform",
            "desc": "This code uses the `train_test_split` function from Scikit-learn to split the dataset into training and validation sets, with 10% of the data allocated for validation.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.99780315
            },
            "cluster": -1
        }, {
            "cell_id": 14,
            "code": "train_dataset = list(zip(X_train, y_train))\nval_dataset = list(zip(X_val, y_val))",
            "class": "Data Transform",
            "desc": "This code snippet pairs the training and validation text data with their corresponding labels into tuples and stores them in lists `train_dataset` and `val_dataset`, respectively.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.7672821
            },
            "cluster": 3
        }, {
            "cell_id": 15,
            "code": "# Prepare sequences of text and build TF train dataset\nds_train_encoded = encode_examples(train_dataset).shuffle(10000).batch(BATCH_SIZE)\n\n# Prepare sequences of text and build TF validation dataset\nds_val_encoded = encode_examples(val_dataset).batch(BATCH_SIZE)",
            "class": "Data Transform",
            "desc": "This code encodes the training and validation datasets into TensorFlow datasets using the `encode_examples` function, shuffles and batches the training dataset, and batches the validation dataset to prepare them for model training.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.8710919
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "def encode_test_examples(texts):\n    \"\"\"\n    Prepare all sequences of text and build TF dataset.\n    \"\"\"\n\n    input_ids_list = []\n    token_type_ids_list = []\n    attention_mask_list = []\n        \n    for text in texts:\n\n        bert_input = prepare_sequence(text)\n\n        input_ids_list.append(bert_input['input_ids'])\n        token_type_ids_list.append(bert_input['token_type_ids'])\n        attention_mask_list.append(bert_input['attention_mask'])\n\n    # Create TF dataset\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (input_ids_list, attention_mask_list, token_type_ids_list)\n    )\n    # Map to the expected input to TFBertForSequenceClassification\n    dataset_mapped = dataset.map(map_test_example_to_dict)\n    return dataset_mapped\n\ndef map_test_example_to_dict(input_ids, attention_masks, token_type_ids):\n    \"\"\"\n    Map to the expected input to TFBertForSequenceClassification.\n    \"\"\"\n    mapped_example = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"attention_mask\": attention_masks,\n    }\n    return mapped_example",
            "class": "Data Transform",
            "desc": "This code snippet defines two functions: `encode_test_examples`, which prepares and tokenizes text sequences from the test dataset and builds them into a TensorFlow dataset, and `map_test_example_to_dict`, which formats these test examples to match the expected input structure for `TFBertForSequenceClassification`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9800521
            },
            "cluster": 3
        }, {
            "cell_id": 24,
            "code": "X_test = test_data[\"text\"]\ntest_dataset = list(X_test)\nds_test_encoded = encode_test_examples(test_dataset).batch(BATCH_SIZE)",
            "class": "Data Transform",
            "desc": "This code extracts the text data from the test dataset into a list `test_dataset`, encodes it into a TensorFlow dataset using the `encode_test_examples` function, and batches it for testing.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9627081
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "for tweet_index in range(1,30,5):\n    print(f'Text of the tweet: {train_data[\"text\"][tweet_index]}')\n    print(f'Target: {\"Real disaster\" if train_data[\"target\"][tweet_index]==1 else \"Not real disaster\"}\\n')",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet iterates through specific rows in the training dataset to print the text of the tweet along with its corresponding target label, indicating whether it is a real disaster or not.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.29876474
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "# Print some words of the vocabulary\nvocabulary = tokenizer.get_vocab()\nprint(f'Size of the vocabulary: {len(vocabulary)}')\nprint(f'Some tokens of the vocabulary: {list(vocabulary.keys())[5000:5010]}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet retrieves and prints the size of the vocabulary and a sample of tokens from the BERT tokenizer's vocabulary to understand the tokenization process.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.9397918
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "# Prepare a test sentence\ntest_sentence = 'Is this jacksonville?'\ntest_sentence_encoded = prepare_sequence(test_sentence)\ntoken_ids = test_sentence_encoded[\"input_ids\"]\nprint(f'Test sentence:   {test_sentence}')\nprint(f'Keys:            {test_sentence_encoded.keys()}')\nprint(f'Tokens:          {tokenizer.convert_ids_to_tokens(token_ids)[:12]}')\nprint(f'Token IDs:       {token_ids[:12]}')\nprint(f'Segment IDs:     {test_sentence_encoded[\"token_type_ids\"][:12]}')\nprint(f'Mask IDs         {test_sentence_encoded[\"attention_mask\"][:12]}')\nprint(f'Input dimension: {len(token_ids)}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet prepares the test sentence for the BERT model, encoding it to obtain token IDs, segment IDs, and mask IDs, and then prints these along with the tokens and input dimension to illustrate the encoding process.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.42718402
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "n_training_examples = X_train.shape[0]\nn_positive_training_examples = y_train.value_counts()[1]\nn_negative_training_examples = y_train.value_counts()[0]\nprint(f'Number examples in training dataset: {n_training_examples}')\nprint(f'Number of positive examples in training dataset: {n_positive_training_examples}')\nprint(f'Number of negative examples in training dataset: {n_negative_training_examples}')",
            "class": "Exploratory Data Analysis",
            "desc": "This code snippet calculates and prints the number of training examples, as well as the number of positive and negative examples in the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.9978163
            },
            "cluster": 4
        }, {
            "cell_id": 0,
            "code": "import random\n\nimport pandas as pd\nimport numpy as np \nfrom scipy.special import softmax\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (roc_auc_score, classification_report, \n                             confusion_matrix)\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nfrom transformers import TFBertForSequenceClassification\nfrom transformers import AutoConfig\n",
            "class": "Imports and Environment",
            "desc": "This code imports various libraries and modules, including Pandas, NumPy, SciPy, Seaborn, Matplotlib, Scikit-learn, Tensorflow, and Transformers, which are essential for data manipulation, visualization, model training, and evaluation in the machine learning task.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99931514
            },
            "cluster": 0
        }, {
            "cell_id": 1,
            "code": "# The name of the BERT model used\nPRETRAINED_MODEL_NAME = 'bert-base-uncased'\n# The number of labels of the target variable\nLABELS_NUMBER = 2\n\n# The max lenght of text can be up to 512 for BERT\nMAX_LENGHT = 512\n\nBATCH_SIZE = 6\nLEARNING_RATE = 2e-5\nEPOCHS_NUMBER = 1\n\nN_PREDICTIONS_TO_SHOW = 10",
            "class": "Imports and Environment",
            "desc": "This code snippet sets up fundamental hyperparameters and configurations for the BERT model, including the model name, number of labels, maximum text length, batch size, learning rate, number of epochs, and the number of predictions to display.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.99904436
            },
            "cluster": 1
        }, {
            "cell_id": 6,
            "code": "# Get the Bert tokenizer\ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, \n                                          do_lower_case=True)",
            "class": "Imports and Environment",
            "desc": "This code snippet initializes the BERT tokenizer from the pre-trained 'bert-base-uncased' model with the option to convert all text to lower case.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9954691
            },
            "cluster": 1
        }, {
            "cell_id": 20,
            "code": "# Get predictions in the validation dataset\nval_predictions = model.predict(ds_val_encoded)\nval_probabilities = softmax(val_predictions[0], axis=1)\ny_val_predictions = np.argmax(val_probabilities, axis=1).flatten()",
            "class": "Model Evaluation",
            "desc": "This code snippet generates predictions for the validation dataset using the trained BERT model, applies softmax to obtain class probabilities, and then determines the predicted class labels by selecting the highest probability class.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9947659
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "# Compute metrics to evaluate the model\nclassification_metrics = classification_report(y_val, y_val_predictions)\n# Compute the area under the ROC curve\narea_under_the_curve = roc_auc_score(y_val, val_probabilities[:,1:2], multi_class=\"ovr\")\n# Compute the confusion matrix\nerror_matrix = confusion_matrix(y_val, y_val_predictions)\nprint(f'Area under the ROC curve: {area_under_the_curve}')\nprint(f'Classification metrics:\\n{classification_metrics}')\n# Plot the confusion matrix\nax = plt.axes()\nsns.heatmap(error_matrix, annot=True, fmt=\"d\")\nax.set_title('Confusion matrix Validation set')",
            "class": "Model Evaluation",
            "desc": "This code calculates and prints the classification metrics, area under the ROC curve, and confusion matrix for the validation dataset, and plots the confusion matrix using Seaborn.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.68382823
            },
            "cluster": 2
        }, {
            "cell_id": 22,
            "code": "# Show some predictions in the validation dataset\nfor i in random.sample(range(len(val_dataset)), k=N_PREDICTIONS_TO_SHOW):\n    print(f'\\nText:       {X_test.values[i]}')\n    print(f'Ground truth: {\"Real disaster\" if y_val.values[i]==1 else \"Not real disaster\"}')\n    print(f'Predicted:    {\"Real disaster\" if y_val_predictions[i]==1 else \"Not real disaster\"}')",
            "class": "Model Evaluation",
            "desc": "This code snippet randomly selects and displays a specified number of predictions from the validation dataset, showing the text, ground truth, and predicted labels for each selected example.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.49180317
            },
            "cluster": 0
        }, {
            "cell_id": 25,
            "code": "test_predictions = model.predict(ds_test_encoded)\ntest_probabilities = softmax(test_predictions[0], axis=1)\ny_test_predictions = np.argmax(test_probabilities, axis=1).flatten()",
            "class": "Model Evaluation",
            "desc": "This code snippet generates predictions for the test dataset using the trained BERT model, applies softmax to obtain class probabilities, and then determines the predicted class labels by selecting the highest probability class.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99444926
            },
            "cluster": 0
        }, {
            "cell_id": 16,
            "code": "def get_model():\n    # Define the configuration of the model\n    config = AutoConfig.from_pretrained(PRETRAINED_MODEL_NAME,\n                                        hidden_dropout_prob=0.2,\n                                        num_labels=LABELS_NUMBER)\n    # Model initialization\n    model = TFBertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, \n                                                            config=config)\n    return model",
            "class": "Model Training",
            "desc": "This code defines a function `get_model` that initializes and returns a BERT model for sequence classification using the pre-trained 'bert-base-uncased' model and a specified configuration with a dropout probability and a defined number of labels.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.98904186
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "# Model initialization\nmodel = get_model()\n\n# Define the optimizer, the loss function and metrics\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n# Compile the model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])",
            "class": "Model Training",
            "desc": "This code initializes the BERT model, defines the optimizer (Adam), loss function (Sparse Categorical Crossentropy), and evaluation metric (Sparse Categorical Accuracy), and then compiles the model with these parameters.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9943469
            },
            "cluster": 2
        }, {
            "cell_id": 18,
            "code": "# Scaling by total/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nweight_for_0 = (1 / n_negative_training_examples)*(n_training_examples)/2.0 \nweight_for_1 = (1 / n_positive_training_examples)*(n_training_examples)/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))",
            "class": "Model Training",
            "desc": "This code snippet calculates the class weights for the training dataset to balance the class distribution, which helps in handling imbalanced datasets by giving more importance to the minority class during model training.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.44395435
            },
            "cluster": -1
        }, {
            "cell_id": 19,
            "code": "# Train the model\nmodel.fit(ds_train_encoded, epochs=EPOCHS_NUMBER, validation_data=ds_val_encoded,\n          class_weight = class_weight)",
            "class": "Model Training",
            "desc": "This code snippet trains the BERT model using the encoded training dataset and validation dataset for the specified number of epochs, incorporating class weights to handle class imbalance.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996909
            },
            "cluster": 1
        }, {
            "cell_id": 5,
            "code": "sns.countplot(train_data[\"target\"])",
            "class": "Visualization",
            "desc": "This code snippet uses Seaborn to create a count plot of the target variable in the training data, showing the distribution of real and not real disaster tweets.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9974095
            },
            "cluster": -1
        }],
        "notebook_id": 12,
        "notebook_name": "detecting-disaster-tweets-fine-tuning-bert.ipynb",
        "user": "detecting-disaster-tweets-fine-tuning-bert.ipynb"
    }, {
        "cells": [{
            "cell_id": 34,
            "code": "def submission(model, test):\n    sample_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n    predictions =  model.predict(test)\n    y_preds = [ int(i) for i in np.rint(predictions)]\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "This code defines a function `submission` that generates predictions using the trained model on the test dataset, formats them according to the sample submission file, and exports the results to a CSV file named 'submission.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9988925
            },
            "cluster": -1
        }, {
            "cell_id": 35,
            "code": "submission(bert_classifier, test_ds)",
            "class": "Data Export",
            "desc": "This code calls the `submission` function, using the trained BERT classifier model to generate predictions on the test dataset, and exports the final results to a 'submission.csv' file.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.61737144
            },
            "cluster": -1
        }, {
            "cell_id": 4,
            "code": "train_full = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_full = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()/2**20))",
            "class": "Data Extraction",
            "desc": "This code reads the training and test datasets from CSV files using pandas and prints their shapes and memory usage.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9972459
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "train_full.text",
            "class": "Data Extraction",
            "desc": "This code accesses and outputs the 'text' column from the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9863797
            },
            "cluster": -1
        }, {
            "cell_id": 11,
            "code": "# Read commited-dataset\ndf_train = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/train_prepared.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/test_prepared.csv\")",
            "class": "Data Extraction",
            "desc": "This code reads preprocessed training and test datasets from CSV files into pandas DataFrames.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99974114
            },
            "cluster": 0
        }, {
            "cell_id": 17,
            "code": "df_train",
            "class": "Data Extraction",
            "desc": "This code outputs the content of the `df_train` DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9994585
            },
            "cluster": -1
        }, {
            "cell_id": 12,
            "code": "# Only apply 'keyword' columns in full data, because other features cleaned in df_train/test\ntrain_full = clean_text(train_full,'keyword')\ntest_full = clean_text(test_full, 'keyword')",
            "class": "Data Transform",
            "desc": "This code applies text cleaning to the 'keyword' column in both the full training and test datasets using the `clean_text` function from the Dataprep library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9898755
            },
            "cluster": 4
        }, {
            "cell_id": 13,
            "code": "# Adding cleaned data into df_train/test\ndf_train['keyword'] = train_full['keyword']\ndf_test['keyword'] = test_full['keyword']",
            "class": "Data Transform",
            "desc": "This code adds the cleaned 'keyword' column from the full datasets into the preprocessed training and test DataFrames.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99662316
            },
            "cluster": 4
        }, {
            "cell_id": 15,
            "code": "def extract_keywords(text):\n    potential_keywords = []\n    TOP_KEYWORD = -1\n    # Create a list for keyword parts of speech\n    pos_tag = ['ADJ', 'NOUN', 'PROPN']\n    doc = nlp_spacy(text)\n    \n    for i in doc:\n        if i.pos_ in pos_tag:\n            potential_keywords.append(i.text)\n\n    document_embed = sentence_enc([text])\n    potential_embed = sentence_enc(potential_keywords)    \n    \n    vector_distances = cosine_similarity(document_embed, potential_embed)\n    keyword = [potential_keywords[i] for i in vector_distances.argsort()[0][TOP_KEYWORD:]]\n\n    return keyword\n\ndef keyword_filler(keyword, text):\n    if pd.isnull(keyword):\n        try:\n            keyword = extract_keywords(text)[0]\n        except:\n            keyword = '' \n        \n    return keyword",
            "class": "Data Transform",
            "desc": "This code defines two functions: `extract_keywords`, which extracts potential keywords based on part-of-speech tags and their similarity to the document context using the Universal Sentence Encoder and cosine similarity; and `keyword_filler`, which fills missing keywords in text by calling `extract_keywords`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.41046342
            },
            "cluster": -1
        }, {
            "cell_id": 16,
            "code": "df_train.keyword = pd.DataFrame(list(map(keyword_filler, df_train.keyword, df_train.text))).astype(str)\ndf_test.keyword = pd.DataFrame(list(map(keyword_filler, df_test.keyword, df_test.text))).astype(str)\n\nprint('Null Training Keywords => ', df_train['keyword'].isnull().any())\nprint('Null Test Keywords => ', df_test['keyword'].isnull().any())",
            "class": "Data Transform",
            "desc": "This code fills missing keywords in the training and test datasets using the `keyword_filler` function and then verifies if there are any remaining null values in the 'keyword' column by printing the result.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.95129913
            },
            "cluster": 4
        }, {
            "cell_id": 20,
            "code": "# Spilt data\nX_train, X_val, y_train, y_val = train_test_split(df_train[['text','keyword']],\n                                                    df_train.target, \n                                                    test_size=0.2, \n                                                    random_state=42)\nX_train.shape, X_val.shape",
            "class": "Data Transform",
            "desc": "This code splits the training dataset into training and validation sets using an 80-20 split ratio with a fixed random state of 42 for reproducibility, employing the `train_test_split` function from scikit-learn.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.998221
            },
            "cluster": -1
        }, {
            "cell_id": 21,
            "code": "train_ds = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(df_test[['text','keyword']]))",
            "class": "Data Transform",
            "desc": "This code converts the training, validation, and test datasets into TensorFlow datasets using `tf.data.Dataset.from_tensor_slices` for efficient data loading and processing operations.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.8992567
            },
            "cluster": -1
        }, {
            "cell_id": 22,
            "code": "AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nBUFFER_SIZE = 1000\nBATCH_SIZE = 32\nRANDOM_SEED = 319\n\ndef configure_dataset(dataset, shuffle=False, test=False):\n    if shuffle:\n        dataset = dataset.cache()\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\n                        .prefetch(AUTOTUNE)\n    elif test:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=False)\\\n                        .prefetch(AUTOTUNE)\n    else:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\n                        .prefetch(AUTOTUNE)\n    return dataset",
            "class": "Data Transform",
            "desc": "This code defines a function `configure_dataset` that configures TensorFlow datasets for training, validation, or testing by applying caching, shuffling, batching, and prefetching optimizations.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.49705026
            },
            "cluster": -1
        }, {
            "cell_id": 23,
            "code": "a3 = configure_dataset(train_ds, shuffle=True)\ndict3 = []\nfor elem in a3:\n    dict3.append(elem[0]['text'][0])\ndict3[:10]",
            "class": "Data Transform",
            "desc": "This code configures the `train_ds` dataset for training with optimizations, and then iterates over it to extract and display the first element from the 'text' field of the first 10 batches.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.61363506
            },
            "cluster": 1
        }, {
            "cell_id": 24,
            "code": "# Configure the datasets\ntrain_ds = configure_dataset(train_ds, shuffle=True)\nval_ds = configure_dataset(val_ds)\ntest_ds = configure_dataset(test_ds, test=True)",
            "class": "Data Transform",
            "desc": "This code configures the training, validation, and test datasets using the `configure_dataset` function, applying appropriate shuffling, batching, and prefetching for each dataset type.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.36260855
            },
            "cluster": -1
        }, {
            "cell_id": 5,
            "code": "plot(train_full)",
            "class": "Exploratory Data Analysis",
            "desc": "This code generates an exploratory data analysis plot for the training dataset using the `plot` function from the Dataprep library.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99482274
            },
            "cluster": 1
        }, {
            "cell_id": 6,
            "code": "create_report(train_full)",
            "class": "Exploratory Data Analysis",
            "desc": "This code generates an extensive exploratory data analysis report for the training dataset using the `create_report` function from the Dataprep library.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.5819901
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "plot(train_full, 'text')",
            "class": "Exploratory Data Analysis",
            "desc": "This code generates and displays visualizations specifically for the 'text' column in the training dataset using the `plot` function from the Dataprep library.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99791676
            },
            "cluster": 1
        }, {
            "cell_id": 9,
            "code": "plot(train_full, \"text\", \"target\")",
            "class": "Exploratory Data Analysis",
            "desc": "This code generates visualizations to explore the relationship between the 'text' and 'target' columns in the training dataset using the `plot` function from the Dataprep library.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.988972
            },
            "cluster": 1
        }, {
            "cell_id": 10,
            "code": "df1 = train_full.text[train_full.target == 0]\ndf2 = train_full.text[train_full.target == 1]\nplot_diff([df1, df2])",
            "class": "Exploratory Data Analysis",
            "desc": "This code separates the 'text' column into two groups based on the 'target' column values (0 and 1) and then visualizes their differences using the `plot_diff` function from the Dataprep library.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9446237
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "! pip install tf-models-official==2.4.0 -q\n! pip install tensorflow-gpu==2.4.1 -q\n! pip install tensorflow-text==2.4.1 -q\n! python -m spacy download en_core_web_sm -q\n! pip install dataprep | grep -v 'already satisfied'",
            "class": "Imports and Environment",
            "desc": "This code installs necessary packages including specific versions of TensorFlow, TensorFlow Text, and tf-models-official, alongside downloading a Spacy model and installing Dataprep.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.9604686
            },
            "cluster": 0
        }, {
            "cell_id": 1,
            "code": "import pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom dataprep.eda import plot, plot_diff, plot_correlation, create_report\nfrom dataprep.clean import clean_text\n\n# Preprocessing and Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport spacy\nimport tensorflow_text as text\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, concatenate \nfrom tensorflow.keras import Model, regularizers \nfrom tensorflow.keras.metrics import BinaryAccuracy\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom official.nlp.optimization import create_optimizer # AdamW optimizer\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')",
            "class": "Imports and Environment",
            "desc": "This code imports various libraries and modules such as pandas, numpy, TensorFlow, scikit-learn, Spacy, dataprep, and visualization libraries including seaborn and matplotlib necessary for data preprocessing, analysis, and machine learning model development.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.98988277
            },
            "cluster": 0
        }, {
            "cell_id": 2,
            "code": "tf.__version__",
            "class": "Imports and Environment",
            "desc": "This code outputs the version of the TensorFlow library that is currently installed in the environment.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9983907
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "# Random seeds\nimport random\nimport numpy as np\nimport tensorflow as tf\nrandom.seed(319)\nnp.random.seed(319)\ntf.random.set_seed(319)",
            "class": "Imports and Environment",
            "desc": "This code sets the random seeds for the random, numpy, and TensorFlow libraries to ensure reproducibility of results.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9086065
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "# Load Spacy Library\nnlp_spacy = spacy.load('en_core_web_sm')\n# Load the sentence encoder\nsentence_enc = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')",
            "class": "Imports and Environment",
            "desc": "This code loads the Spacy language model 'en_core_web_sm' and the Universal Sentence Encoder model from TensorFlow Hub.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.99203765
            },
            "cluster": 1
        }, {
            "cell_id": 25,
            "code": "# Free memory\ndel X_train, X_val, y_train, y_val, df_train, df_test, train_full, test_full",
            "class": "Imports and Environment",
            "desc": "This code frees up memory by deleting intermediate data structures including training, validation and test sets, and the original full datasets using the `del` statement.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.4480442
            },
            "cluster": 1
        }, {
            "cell_id": 26,
            "code": "# Bidirectional Encoder Representations from Transformers (BERT).\nbert_encoder_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n# Text preprocessing for BERT.\nbert_preprocessor_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n# Token based text embedding trained on English Google News 200B corpus.\nkeyword_embedding_path = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"",
            "class": "Imports and Environment",
            "desc": "This code specifies the URLs for loading a BERT encoder model, its preprocessor, and a keyword embedding model, all from TensorFlow Hub.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9857056
            },
            "cluster": 1
        }, {
            "cell_id": 27,
            "code": "bert_encoder = hub.KerasLayer(bert_encoder_path, trainable=True, name=\"BERT_Encoder\")\nbert_preprocessor = hub.KerasLayer(bert_preprocessor_path, name=\"BERT_Preprocessor\")\nnnlm_embed = hub.KerasLayer(keyword_embedding_path, name=\"NNLM_Embedding\")",
            "class": "Imports and Environment",
            "desc": "This code initializes BERT encoder, BERT preprocessor, and NNLM embedding layers from TensorFlow Hub as Keras layers, allowing them to be incorporated into a TensorFlow model.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.506471
            },
            "cluster": 1
        }, {
            "cell_id": 28,
            "code": "kernel_initializer = tf.keras.initializers.GlorotNormal(seed=319)\n# Model function\ndef create_model():\n    # Keyword Branch\n    text_input = Input(shape=(), dtype=tf.string, name=\"text\")\n    encoder_inputs = bert_preprocessor(text_input)\n    encoder_outputs = bert_encoder(encoder_inputs)\n    # Pooled output\n    pooled_output = encoder_outputs[\"pooled_output\"]\n    bert_branch = Dropout(0.1,\n                          seed=319,\n                          name=\"BERT_Dropout\")(pooled_output)\n    # Construct keyword layers\n    keyword_input = Input(shape=(), dtype=tf.string, name='keyword')\n    keyword_embed = nnlm_embed(keyword_input)\n    keyword_flat = Flatten(name=\"Keyword_Flatten\")(keyword_embed)\n    keyword_dense1 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense1\"\n                         )(keyword_flat)\n    keyword_branch1 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout1'\n                            )(keyword_dense1)\n    keyword_dense2 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense2\"\n                         )(keyword_branch1)\n    keyword_branch2 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout2'\n                            )(keyword_dense2)\n    keyword_dense3 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense3\"\n                         )(keyword_branch2)\n    keyword_branch3 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout3'\n                            )(keyword_dense3)\n    \n    # Merge the layers and classify\n    merge = concatenate([bert_branch, keyword_branch3], name=\"Concatenate\")\n    dense = Dense(128, \n                  activation='relu',\n                  kernel_initializer=kernel_initializer,\n                  kernel_regularizer=regularizers.l2(1e-4), \n                  name=\"Merged_Dense\")(merge)\n    dropout = Dropout(0.5,\n                      seed=319,\n                      name=\"Merged_Dropout\"\n                     )(dense)\n    clf = Dense(1,\n                activation=\"sigmoid\", \n                kernel_initializer=kernel_initializer,\n                name=\"Classifier\"\n               )(dropout)\n    return Model([text_input, keyword_input], \n                 clf, \n                 name=\"BERT_Classifier\")",
            "class": "Model Training",
            "desc": "This code defines a function `create_model` that creates a neural network model using TensorFlow and Keras, incorporating BERT for text input and NNLM for keyword input, and combining their outputs through a series of dense and dropout layers for binary classification.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99885345
            },
            "cluster": 2
        }, {
            "cell_id": 29,
            "code": "bert_classifier = create_model()\nbert_classifier.summary()",
            "class": "Model Training",
            "desc": "This code creates an instance of the BERT classifier model using the `create_model` function and prints a summary of the model architecture.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.99306774
            },
            "cluster": -1
        }, {
            "cell_id": 31,
            "code": "EPOCHS = 3\nLEARNING_RATE = 5e-5\n\nSTEPS_PER_EPOCH = int(train_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\nVAL_STEPS = int(val_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\n# Calculate the train and warmup steps for the optimizer\nTRAIN_STEPS = STEPS_PER_EPOCH * EPOCHS\nWARMUP_STEPS = int(TRAIN_STEPS * 0.1)\n\nadamw_optimizer = create_optimizer(\n    init_lr=LEARNING_RATE,\n    num_train_steps=TRAIN_STEPS,\n    num_warmup_steps=WARMUP_STEPS,\n    optimizer_type='adamw'\n)",
            "class": "Model Training",
            "desc": "This code sets the training parameters including the number of epochs, learning rate, and steps per epoch for both training and validation datasets, and then creates an AdamW optimizer using these parameters through the `create_optimizer` function from the `official.nlp.optimization` module.",
            "testing": {
                "class": "Model_Train",
                "subclass": "init_hyperparams",
                "subclass_id": 59,
                "predicted_subclass_probability": 0.6065405
            },
            "cluster": 0
        }, {
            "cell_id": 32,
            "code": "STEPS_PER_EPOCH, VAL_STEPS, TRAIN_STEPS, WARMUP_STEPS",
            "class": "Model Training",
            "desc": "This code outputs the calculated values for steps per epoch, validation steps, total training steps, and warmup steps which were established for training the model.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.94944763
            },
            "cluster": 1
        }, {
            "cell_id": 33,
            "code": "bert_classifier.compile(loss=BinaryCrossentropy(from_logits=True),\n                   optimizer=adamw_optimizer, \n                   metrics=[BinaryAccuracy(name=\"accuracy\")]\n                  )\nhistory = bert_classifier.fit(train_ds, \n                         epochs=EPOCHS,\n                         steps_per_epoch=STEPS_PER_EPOCH,\n                         validation_data=val_ds,\n                         validation_steps=VAL_STEPS\n                        )",
            "class": "Model Training",
            "desc": "This code compiles the BERT classifier model with the AdamW optimizer and binary cross-entropy loss, and then trains the model for a specified number of epochs while evaluating its performance on the validation data, capturing the training history.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.97614974
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "keyword_non_disaster = df_train.keyword[df_train.target==0].value_counts().reset_index()\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()",
            "class": "Visualization",
            "desc": "This code creates a bar plot using Seaborn to visualize the top 10 most frequent keywords in non-disaster tweets from the training dataset.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.88048565
            },
            "cluster": -1
        }, {
            "cell_id": 19,
            "code": "keyword_disaster = df_train.keyword[df_train.target==1].value_counts().reset_index()\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()",
            "class": "Visualization",
            "desc": "This code creates a bar plot using Seaborn to visualize the top 10 most frequent keywords in disaster tweets from the training dataset.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.8303329
            },
            "cluster": -1
        }, {
            "cell_id": 30,
            "code": "keras.utils.plot_model(bert_classifier, \n                      show_shapes=False)",
            "class": "Visualization",
            "desc": "This code generates a visual representation of the BERT classifier model architecture using `keras.utils.plot_model`.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9785347
            },
            "cluster": 1
        }],
        "notebook_id": 13,
        "notebook_name": "using-keywords-embedding-to-improve-bert-model.ipynb",
        "user": "using-keywords-embedding-to-improve-bert-model.ipynb"
    }, {
        "cells": [{
            "cell_id": 26,
            "code": "# SAVE SUBMISSION FILE\n\n\n\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n\nsubmission.target = flat_predictions\n\nsubmission.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "The snippet reads a sample submission file into a Pandas DataFrame, updates the target column with the predicted labels, and saves the modified DataFrame to a CSV file named 'submission.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99789846
            },
            "cluster": -1
        }, {
            "cell_id": 0,
            "code": "# LOADING THE TRAIN DATA\n\n\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))       \n\ndata = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n\ndata.sample(10)",
            "class": "Data Extraction",
            "desc": "The snippet imports necessary libraries (NumPy, Pandas, OS), lists the files in the input directory, and loads the training data from a CSV file into a Pandas DataFrame, then prints 10 random samples.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9061924
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "# GET THE LISTS OF TWEETS AND THEIR LABELS\n\n\n\nsentences = data.text.values\n\nlabels =data.target.values",
            "class": "Data Extraction",
            "desc": "This snippet extracts the text of the tweets and their corresponding target labels from the DataFrame into separate arrays.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9971167
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "# PREPARE TEST DATA\n\n\n\n# Load the dataset into a pandas dataframe.\n\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n\n\n# Report the number of sentences.\n\nprint('Number of test sentences: {:,}\\n'.format(test_data.shape[0]))\n\n\n\n# Create sentence and label lists\n\nsentences = test_data.text.values\n\n#labels = test_data.target.values\n\n\n\n# Tokenize all of the sentences and map the tokens to thier word IDs.\n\ninput_ids = []\n\nattention_masks = []\n\n\n\n# For every sentence...\n\nfor sent in sentences:\n\n    # `encode_plus` will:\n\n    #   (1) Tokenize the sentence.\n\n    #   (2) Prepend the `[CLS]` token to the start.\n\n    #   (3) Append the `[SEP]` token to the end.\n\n    #   (4) Map tokens to their IDs.\n\n    #   (5) Pad or truncate the sentence to `max_length`\n\n    #   (6) Create attention masks for [PAD] tokens.\n\n    encoded_dict = tokenizer.encode_plus(\n\n                        sent,                      # Sentence to encode.\n\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n\n                        max_length = 64,           # Pad & truncate all sentences.\n\n                        pad_to_max_length = True,\n\n                        return_attention_mask = True,   # Construct attn. masks.\n\n                        return_tensors = 'pt',     # Return pytorch tensors.\n\n                   )\n\n    \n\n    # Add the encoded sentence to the list.    \n\n    input_ids.append(encoded_dict['input_ids'])\n\n    \n\n    # And its attention mask (simply differentiates padding from non-padding).\n\n    attention_masks.append(encoded_dict['attention_mask'])\n\n\n\n# Convert the lists into tensors.\n\ninput_ids = torch.cat(input_ids, dim=0)\n\nattention_masks = torch.cat(attention_masks, dim=0)\n\n#labels = torch.tensor(labels)\n\n\n\n# Set the batch size.  \n\nbatch_size = 32  \n\n\n\n# Create the DataLoader.\n\nprediction_data = TensorDataset(input_ids, attention_masks, ) #labels\n\nprediction_sampler = SequentialSampler(prediction_data)\n\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)",
            "class": "Data Extraction",
            "desc": "The snippet loads the test data into a Pandas DataFrame, tokenizes the sentences, maps them to their word IDs, creates attention masks, converts them to PyTorch tensors, and sets up a DataLoader for predictions with a batch size of 32.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99884653
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "# DROP DUPLICATE SAMPLES WITH CONFLICTING LABELS\n\n\n\nconflicting = conflicting_check.loc[(conflicting_check.target != 1) & (conflicting_check.target != 0)].index\n\ndata = data.drop(data[text.isin(conflicting)].index)\n\nprint ('Conflicting samples count:', conflicting.shape[0])",
            "class": "Data Transform",
            "desc": "The snippet drops duplicate samples with conflicting labels from the dataset and prints the count of such conflicting samples.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "filter",
                "subclass_id": 14,
                "predicted_subclass_probability": 0.43520904
            },
            "cluster": -1
        }, {
            "cell_id": 11,
            "code": "# TOKENIZE ALL THE SENTENCES AND MAP THE TOKENS TO THEIR WORD IDs\n\n\n\ninput_ids = []\n\nattention_masks = []\n\n\n\n# For every sentence...\n\nfor sent in sentences:\n\n    # `encode_plus` will:\n\n    #   (1) Tokenize the sentence.\n\n    #   (2) Prepend the `[CLS]` token to the start.\n\n    #   (3) Append the `[SEP]` token to the end.\n\n    #   (4) Map tokens to their IDs.\n\n    #   (5) Pad or truncate the sentence to `max_length`\n\n    #   (6) Create attention masks for [PAD] tokens.\n\n    encoded_dict = tokenizer.encode_plus(\n\n                        sent,                      # Sentence to encode.\n\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n\n                        max_length = 64,           # Pad & truncate all sentences.\n\n                        pad_to_max_length = True,\n\n                        return_attention_mask = True,   # Construct attn. masks.\n\n                        return_tensors = 'pt',     # Return pytorch tensors.\n\n                   )\n\n    \n\n    # Add the encoded sentence to the list.    \n\n    input_ids.append(encoded_dict['input_ids'])\n\n    \n\n    # And its attention mask (simply differentiates padding from non-padding).\n\n    attention_masks.append(encoded_dict['attention_mask'])\n\n\n\n# Convert the lists into tensors.\n\ninput_ids = torch.cat(input_ids, dim=0)\n\nattention_masks = torch.cat(attention_masks, dim=0)\n\nlabels = torch.tensor(labels)\n\n\n\n# Print sentence 0, now as a list of IDs.\n\nprint('Original: ', sentences[0])\n\nprint('Token IDs:', input_ids[0])",
            "class": "Data Transform",
            "desc": "The snippet tokenizes all sentences, maps the tokens to their word IDs, creates attention masks, pads or truncates them to a maximum length of 64, and converts the results to PyTorch tensors.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.99864405
            },
            "cluster": -1
        }, {
            "cell_id": 12,
            "code": "# SPLIT TRAIN DATA INTO TRAIN AND TEST SET\n\n# I used small test set (SPLIT=0,999) in order to train the model on the majority of the data, after all parameters were tuned\n\n# Use 0,9 or lower to train the model and look at the perfomance/ tune parameters\n\n\n\nSPLIT = 0.999\n\n\n\nfrom torch.utils.data import TensorDataset, random_split\n\n\n\n# Combine the training inputs into a TensorDataset.\n\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n\n\n# Create a 90-10 train-validation split.\n\n\n\n# Calculate the number of samples to include in each set.\n\ntrain_size = int(SPLIT * len(dataset))\n\nval_size = len(dataset) - train_size\n\n\n\n# Divide the dataset by randomly selecting samples.\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n\n\nprint('{:>5,} training samples'.format(train_size))\n\nprint('{:>5,} validation samples'.format(val_size))",
            "class": "Data Transform",
            "desc": "The snippet splits the dataset into training and validation sets using a specified split ratio, creates a TensorDataset from input IDs, attention masks, and labels, and prints the number of samples in each set.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9402513
            },
            "cluster": 1
        }, {
            "cell_id": 13,
            "code": "# CREATE DATA ITERATOR TO SAVE MEMORY\n\n\n\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n\n\n# The DataLoader needs to know our batch size for training, so we specify it \n\n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n\n# size of 16 or 32.\n\nbatch_size = 32\n\n\n\n# Create the DataLoaders for our training and validation sets.\n\n# We'll take training samples in random order. \n\ntrain_dataloader = DataLoader(\n\n            train_dataset,  # The training samples.\n\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n\n            batch_size = batch_size # Trains with this batch size.\n\n        )\n\n\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\n\nvalidation_dataloader = DataLoader(\n\n            val_dataset, # The validation samples.\n\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n\n            batch_size = batch_size # Evaluate with this batch size.\n\n        )",
            "class": "Data Transform",
            "desc": "The snippet creates data loaders for the training and validation sets with a batch size of 32, using random sampling for the training set and sequential sampling for the validation set to save memory.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.98473305
            },
            "cluster": -1
        }, {
            "cell_id": 25,
            "code": "# PREPARE PREDICTIONS FOR SUBMISSION\n\n\n\n# Combine the results across all batches. \n\nflat_predictions = np.concatenate(predictions, axis=0)\n\n\n\n# For each sample, pick the label (0 or 1) with the higher score.\n\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()",
            "class": "Data Transform",
            "desc": "The snippet combines the model predictions across all batches and selects the label with the higher score for each sample, creating a flattened array of predicted labels.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.5844843
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "print ('Train data shape:', data.shape)",
            "class": "Exploratory Data Analysis",
            "desc": "This snippet prints the shape of the training data DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9942768
            },
            "cluster": 4
        }, {
            "cell_id": 2,
            "code": "# CHECK FOR DUPLICATE SAMPLES WITH CONFLICTING LABELS\n\n\n\ntext = data.text\n\nduplicates = data[text.isin(text[text.duplicated()])].sort_values(by='text')\n\n\n\n# If the mean target value is different from 0 or 1 - we have duplicate samples with conflicting value\n\nconflicting_check = pd.DataFrame(duplicates.groupby(['text']).target.mean())\n\nconflicting_check.sample(10)",
            "class": "Exploratory Data Analysis",
            "desc": "The snippet identifies text duplicates in the dataset, groups them by their text content, and checks for conflicting labels by examining the mean target value for these duplicates.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_duplicates",
                "subclass_id": 38,
                "predicted_subclass_probability": 0.8007328
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "# LOOK HOW THE TOKENIZER WORK\n\n\n\n# Print the original sentence.\n\nprint(' Original: ', sentences[0])\n\n\n\n# Print the sentence split into tokens.\n\nprint('Tokenized: ', tokenizer.tokenize(sentences[0]))\n\n\n\n# Print the sentence mapped to token ids.\n\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))",
            "class": "Exploratory Data Analysis",
            "desc": "The snippet demonstrates how the BERT tokenizer processes a sample sentence by printing the original sentence, its tokenized form, and the corresponding token IDs.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.74620485
            },
            "cluster": 0
        }, {
            "cell_id": 10,
            "code": "# GET MAX LENGTH OF THE TWEETS\n\n\n\nmax_len = 0\n\n# For every sentence...\n\nfor sent in sentences:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n\n    max_len = max(max_len, len(input_ids))\n\n\n\nprint('Max tweet length: ', max_len)",
            "class": "Exploratory Data Analysis",
            "desc": "The snippet calculates the maximum length of the tokenized tweets in the dataset, including special tokens, and prints this maximum length.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.858368
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "# PRINT NAMES AND DIMENSIONS FOR THE MODEL LAYERS\n\n\n\n# Get all of the model's parameters as a list of tuples.\n\nparams = list(model.named_parameters())\n\n\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\n\n\nprint('==== Embedding Layer ====\\n')\n\n\n\nfor p in params[0:5]:\n\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\n\n\nprint('\\n==== First Transformer ====\\n')\n\n\n\nfor p in params[5:21]:\n\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\n\n\nprint('\\n==== Output Layer ====\\n')\n\n\n\nfor p in params[-4:]:\n\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))",
            "class": "Exploratory Data Analysis",
            "desc": "The snippet prints the names and dimensions of the parameters for the embedding layer, the first transformer layer, and the output layer of the BERT model.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.24847664
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "# CONNECT KAGGLE GPU FOR SPEED UP\n\n\n\nimport tensorflow as tf\n\n# Get the GPU device name.\n\ndevice_name = tf.test.gpu_device_name()\n\nif device_name == '/device:GPU:0':\n\n    print('Found GPU at: {}'.format(device_name))\n\nelse:\n\n    raise SystemError('GPU device not found')",
            "class": "Imports and Environment",
            "desc": "This snippet imports TensorFlow, checks for the availability of a GPU, and prints the name of the GPU device if found, otherwise raises an error.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9054986
            },
            "cluster": 1
        }, {
            "cell_id": 5,
            "code": "# SPECIFY THE GPU AS THE TORCH DEVICE\n\n\n\nimport torch\n\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n\n    print('No GPU available, using the CPU instead.')\n\n    device = torch.device(\"cpu\")",
            "class": "Imports and Environment",
            "desc": "This snippet imports PyTorch, checks for GPU availability, and specifies whether to use the GPU or CPU as the device for PyTorch computations, printing relevant information about the detected GPU.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9812268
            },
            "cluster": 1
        }, {
            "cell_id": 6,
            "code": "# INSTALL THE TRANSFORMERS PACKAGE TO GET A PYTORCH INTERFACE FOR BERT\n\n!pip install transformers",
            "class": "Imports and Environment",
            "desc": "The snippet installs the \"transformers\" package, which provides a PyTorch interface for BERT and other transformer models, using the pip package manager.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.989985
            },
            "cluster": 1
        }, {
            "cell_id": 8,
            "code": "# LOAD THE BERT TOKENIZER\n\n\n\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)",
            "class": "Imports and Environment",
            "desc": "This snippet imports the BERT tokenizer from the \"transformers\" library and initializes it using the pre-trained 'bert-base-uncased' model, with lowercase transformation enabled.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9928005
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "# HELPER FUNCTION TO CALCULATE ACCURACY\n\n\n\nimport numpy as np\n\n\n\n# Function to calculate the accuracy of our predictions vs labels\n\ndef flat_accuracy(preds, labels):\n\n    pred_flat = np.argmax(preds, axis=1).flatten()\n\n    labels_flat = labels.flatten()\n\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)",
            "class": "Model Evaluation",
            "desc": "The snippet defines a helper function to calculate the accuracy of predictions by comparing the flattened predicted labels with the actual labels.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.984026
            },
            "cluster": 0
        }, {
            "cell_id": 19,
            "code": "# HELPER FUNCTION FOR TIME FORMAT\n\n\n\nimport time\n\nimport datetime\n\n\n\ndef format_time(elapsed):\n\n    '''\n\n    Takes a time in seconds and returns a string hh:mm:ss\n\n    '''\n\n    # Round to the nearest second.\n\n    elapsed_rounded = int(round((elapsed)))\n\n    \n\n    # Format as hh:mm:ss\n\n    return str(datetime.timedelta(seconds=elapsed_rounded))",
            "class": "Model Evaluation",
            "desc": "The snippet defines a helper function to convert elapsed time in seconds to a formatted string in \"hh:mm:ss.\" ",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.5951723
            },
            "cluster": 2
        }, {
            "cell_id": 21,
            "code": "# THE SUMMARY OF THE TRAIN PROCESS\n\n\n\n# Display floats with two decimal places\n\npd.set_option('precision', 2)\n\n\n\n# Create a DataFrame from our training statistics\n\ndf_stats = pd.DataFrame(data=training_stats)\n\n\n\n# Use the 'epoch' as the row index\n\ndf_stats = df_stats.set_index('epoch')\n\n\n\n# A hack to force the column headers to wrap\n\n#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n\n\n\n# Display the table\n\ndf_stats",
            "class": "Model Evaluation",
            "desc": "The snippet summarizes the training process by creating a DataFrame from the collected training statistics, setting the epoch as the row index, and configuring Pandas to display floats with two decimal places.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.46057627
            },
            "cluster": 2
        }, {
            "cell_id": 24,
            "code": "# GET PREDICTIONS\n\n\n\nprint('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n\n\n\n# Put model in evaluation mode\n\nmodel.eval()\n\n\n\n# Tracking variables \n\npredictions = []\n\n#true_labels = []\n\n\n\n# Predict \n\nfor batch in prediction_dataloader:\n\n  # Add batch to GPU\n\n  batch = tuple(t.to(device) for t in batch)\n\n  \n\n  # Unpack the inputs from our dataloader\n\n  b_input_ids, b_input_mask = batch #b_labels\n\n  \n\n  # Telling the model not to compute or store gradients, saving memory and \n\n  # speeding up prediction\n\n  with torch.no_grad():\n\n      # Forward pass, calculate logit predictions\n\n      outputs = model(b_input_ids, token_type_ids=None, \n\n                      attention_mask=b_input_mask)\n\n\n\n  logits = outputs[0]\n\n\n\n  # Move logits and labels to CPU\n\n  logits = logits.detach().cpu().numpy()\n\n  label_ids = b_labels.to('cpu').numpy()\n\n  \n\n  # Store predictions and true labels\n\n  predictions.append(logits)\n\n  #true_labels.append(label_ids)\n\n\n\nprint('    DONE.')",
            "class": "Model Evaluation",
            "desc": "The snippet puts the model in evaluation mode, predicts labels for the test sentences by performing a forward pass without gradient computations, and stores the predictions in a list.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.75300366
            },
            "cluster": 2
        }, {
            "cell_id": 14,
            "code": "# GET BERT MODEL FOR CLASSIFICATION\n\n\n\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\n\n\n\n# Load BertForSequenceClassification, the pretrained BERT model with a single \n\n# linear classification layer on top. \n\nmodel = BertForSequenceClassification.from_pretrained(\n\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n\n    num_labels = 2, # The number of output labels--2 for binary classification.\n\n                    # You can increase this for multi-class tasks.   \n\n    output_attentions = False, # Whether the model returns attentions weights.\n\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n\n)\n\n\n\n# Tell pytorch to run this model on the GPU.\n\nmodel.cuda()",
            "class": "Model Training",
            "desc": "The snippet loads the pre-trained BERT model for sequence classification with a binary classification layer on top, and configures PyTorch to run the model on the GPU.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.567976
            },
            "cluster": -1
        }, {
            "cell_id": 16,
            "code": "# SET UP THE OPTIMIZER\n\n\n\n# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n\n# I believe the 'W' stands for 'Weight Decay fix\"\n\noptimizer = AdamW(model.parameters(),\n\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n\n                )",
            "class": "Model Training",
            "desc": "The snippet sets up the AdamW optimizer from the Hugging Face library for the BERT model, with specified learning rate and epsilon values.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9948212
            },
            "cluster": 1
        }, {
            "cell_id": 17,
            "code": "# SET UP MODEL HYPERPARAMETERS\n\n\n\nfrom transformers import get_linear_schedule_with_warmup\n\n\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \n\n# We chose to run for 4, but we'll see later that this may be over-fitting the\n\n# training data.\n\nepochs = 2\n\n\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n\n# (Note that this is not the same as the number of training samples).\n\ntotal_steps = len(train_dataloader) * epochs\n\n\n\n# Create the learning rate scheduler.\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n\n                                            num_warmup_steps = 0, # Default value in run_glue.py\n\n                                            num_training_steps = total_steps)",
            "class": "Model Training",
            "desc": "The snippet sets model hyperparameters by specifying the number of training epochs, calculates the total number of training steps, and creates a learning rate scheduler using the linear warm-up schedule provided by the Hugging Face library.",
            "testing": {
                "class": "Model_Train",
                "subclass": "init_hyperparams",
                "subclass_id": 59,
                "predicted_subclass_probability": 0.6898819
            },
            "cluster": 1
        }, {
            "cell_id": 20,
            "code": "# TRAINING SCRIPT\n\n\n\nimport random\n\nimport numpy as np\n\n\n\n# This training code is based on the `run_glue.py` script here:\n\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n\n\n# Set the seed value all over the place to make this reproducible.\n\nseed_val = 42\n\n\n\nrandom.seed(seed_val)\n\nnp.random.seed(seed_val)\n\ntorch.manual_seed(seed_val)\n\ntorch.cuda.manual_seed_all(seed_val)\n\n\n\n# We'll store a number of quantities such as training and validation loss, \n\n# validation accuracy, and timings.\n\ntraining_stats = []\n\n\n\n# Measure the total training time for the whole run.\n\ntotal_t0 = time.time()\n\n\n\n# For each epoch...\n\nfor epoch_i in range(0, epochs):\n\n    \n\n    # ========================================\n\n    #               Training\n\n    # ========================================\n\n    \n\n    # Perform one full pass over the training set.\n\n\n\n    print(\"\")\n\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n\n    print('Training...')\n\n\n\n    # Measure how long the training epoch takes.\n\n    t0 = time.time()\n\n\n\n    # Reset the total loss for this epoch.\n\n    total_train_loss = 0\n\n\n\n    # Put the model into training mode. Don't be mislead--the call to \n\n    # `train` just changes the *mode*, it doesn't *perform* the training.\n\n    # `dropout` and `batchnorm` layers behave differently during training\n\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n\n    model.train()\n\n\n\n    # For each batch of training data...\n\n    for step, batch in enumerate(train_dataloader):\n\n\n\n        # Progress update every 40 batches.\n\n        if step % 40 == 0 and not step == 0:\n\n            # Calculate elapsed time in minutes.\n\n            elapsed = format_time(time.time() - t0)\n\n            \n\n            # Report progress.\n\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n\n\n        # Unpack this training batch from our dataloader. \n\n        #\n\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n\n        # `to` method.\n\n        #\n\n        # `batch` contains three pytorch tensors:\n\n        #   [0]: input ids \n\n        #   [1]: attention masks\n\n        #   [2]: labels \n\n        b_input_ids = batch[0].to(device)\n\n        b_input_mask = batch[1].to(device)\n\n        b_labels = batch[2].to(device)\n\n\n\n        # Always clear any previously calculated gradients before performing a\n\n        # backward pass. PyTorch doesn't do this automatically because \n\n        # accumulating the gradients is \"convenient while training RNNs\". \n\n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n\n        model.zero_grad()        \n\n\n\n        # Perform a forward pass (evaluate the model on this training batch).\n\n        # The documentation for this `model` function is here: \n\n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n\n        # It returns different numbers of parameters depending on what arguments\n\n        # arge given and what flags are set. For our useage here, it returns\n\n        # the loss (because we provided labels) and the \"logits\"--the model\n\n        # outputs prior to activation.\n\n        outputs = model(b_input_ids, \n\n                             token_type_ids=None, \n\n                             attention_mask=b_input_mask, \n\n                             labels=b_labels)\n\n        \n\n        loss = outputs[0]\n\n        logits = outputs[1]\n\n\n\n        # Accumulate the training loss over all of the batches so that we can\n\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n\n        # single value; the `.item()` function just returns the Python value \n\n        # from the tensor.\n\n        total_train_loss += loss.item()\n\n\n\n        # Perform a backward pass to calculate the gradients.\n\n        loss.backward()\n\n\n\n        # Clip the norm of the gradients to 1.0.\n\n        # This is to help prevent the \"exploding gradients\" problem.\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n\n\n        # Update parameters and take a step using the computed gradient.\n\n        # The optimizer dictates the \"update rule\"--how the parameters are\n\n        # modified based on their gradients, the learning rate, etc.\n\n        optimizer.step()\n\n\n\n        # Update the learning rate.\n\n        scheduler.step()\n\n\n\n    # Calculate the average loss over all of the batches.\n\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n\n    \n\n    # Measure how long this epoch took.\n\n    training_time = format_time(time.time() - t0)\n\n\n\n    print(\"\")\n\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n\n    print(\"  Training epcoh took: {:}\".format(training_time))\n\n        \n\n    # ========================================\n\n    #               Validation\n\n    # ========================================\n\n    # After the completion of each training epoch, measure our performance on\n\n    # our validation set.\n\n\n\n    print(\"\")\n\n    print(\"Running Validation...\")\n\n\n\n    t0 = time.time()\n\n\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n\n    # during evaluation.\n\n    model.eval()\n\n\n\n    # Tracking variables \n\n    total_eval_accuracy = 0\n\n    total_eval_loss = 0\n\n    nb_eval_steps = 0\n\n\n\n    # Evaluate data for one epoch\n\n    for batch in validation_dataloader:\n\n        \n\n        # Unpack this training batch from our dataloader. \n\n        #\n\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n\n        # the `to` method.\n\n        #\n\n        # `batch` contains three pytorch tensors:\n\n        #   [0]: input ids \n\n        #   [1]: attention masks\n\n        #   [2]: labels \n\n        b_input_ids = batch[0].to(device)\n\n        b_input_mask = batch[1].to(device)\n\n        b_labels = batch[2].to(device)\n\n        \n\n        # Tell pytorch not to bother with constructing the compute graph during\n\n        # the forward pass, since this is only needed for backprop (training).\n\n        with torch.no_grad():        \n\n\n\n            # Forward pass, calculate logit predictions.\n\n            # token_type_ids is the same as the \"segment ids\", which \n\n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n\n            # The documentation for this `model` function is here: \n\n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n\n            # values prior to applying an activation function like the softmax.\n\n            output = model(b_input_ids, \n\n                                   token_type_ids=None, \n\n                                   attention_mask=b_input_mask,\n\n                                   labels=b_labels)\n\n            \n\n            loss = output[0]\n\n            logits = output[1]\n\n            \n\n        # Accumulate the validation loss.\n\n        total_eval_loss += loss.item()\n\n\n\n        # Move logits and labels to CPU\n\n        logits = logits.detach().cpu().numpy()\n\n        label_ids = b_labels.to('cpu').numpy()\n\n\n\n        # Calculate the accuracy for this batch of test sentences, and\n\n        # accumulate it over all batches.\n\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n\n        \n\n\n\n    # Report the final accuracy for this validation run.\n\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n\n\n    # Calculate the average loss over all of the batches.\n\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n\n    \n\n    # Measure how long the validation run took.\n\n    validation_time = format_time(time.time() - t0)\n\n    \n\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n\n    print(\"  Validation took: {:}\".format(validation_time))\n\n\n\n    # Record all statistics from this epoch.\n\n    training_stats.append(\n\n        {\n\n            'epoch': epoch_i + 1,\n\n            'Training Loss': avg_train_loss,\n\n            'Valid. Loss': avg_val_loss,\n\n            'Valid. Accur.': avg_val_accuracy,\n\n            'Training Time': training_time,\n\n            'Validation Time': validation_time\n\n        }\n\n    )\n\n\n\nprint(\"\")\n\nprint(\"Training complete!\")\n\n\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))",
            "class": "Model Training",
            "desc": "The snippet provides a complete training script, including setting random seeds, training the model for a specified number of epochs, evaluating it on a validation set each epoch, and recording training statistics like loss and accuracy, while also managing model parameters, gradients, and learning rate scheduling.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.7243838
            },
            "cluster": 2
        }, {
            "cell_id": 22,
            "code": "# PLOT THE VALIDATION LOSS\n\n\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\n\nimport seaborn as sns\n\n\n\n# Use plot styling from seaborn.\n\nsns.set(style='darkgrid')\n\n\n\n# Increase the plot size and font size.\n\nsns.set(font_scale=1.5)\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n\n\n# Plot the learning curve.\n\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n\n\n# Label the plot.\n\nplt.title(\"Training & Validation Loss\")\n\nplt.xlabel(\"Epoch\")\n\nplt.ylabel(\"Loss\")\n\nplt.legend()\n\nplt.xticks([1, 2, 3, 4])\n\n\n\nplt.show()",
            "class": "Visualization",
            "desc": "The snippet uses Matplotlib and Seaborn to plot the training and validation loss over epochs, enhancing the plot aesthetics and size.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9963574
            },
            "cluster": 1
        }],
        "notebook_id": 14,
        "notebook_name": "bert-with-disaster-tweets.ipynb",
        "user": "bert-with-disaster-tweets.ipynb"
    }, {
        "cells": [{
            "cell_id": 43,
            "code": "test_bow = vectorizer.transform(test.tokens)\n\ntest_bow = selector.transform(test_bow)\n\nclassifier = LogisticRegression(C=0.1)\n\n\n\n# use the whole training dataset now\n\nclassifier.fit(x, y)\n\npredicted = classifier.predict(test_bow)\n\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted})\n\nsubmission.to_csv('bow-linear.csv', index=False)",
            "class": "Data Export",
            "desc": "This code transforms the test data using the trained `CountVectorizer` and feature selector, fits a Logistic Regression classifier on the entire training dataset, makes predictions on the test dataset, and saves the results to a CSV file named 'bow-linear.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99929416
            },
            "cluster": -1
        }, {
            "cell_id": 62,
            "code": "predicted = logits > 0\n\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted.astype(np.int)})\n\nsubmission.to_csv('embeddings.csv', index=False)",
            "class": "Data Export",
            "desc": "This code converts the logits into binary class predictions, creates a DataFrame with the test IDs and predicted targets, and saves the results to a CSV file named 'embeddings.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992009
            },
            "cluster": -1
        }, {
            "cell_id": 73,
            "code": "# trainer.predict returns a list with batch results\n\nlogits = np.concatenate(trainer.predict(model, test_loader), axis=0)\n\npredicted = logits.argmax(1)\n\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted})\n\nsubmission.to_csv('roberta.csv', index=False)",
            "class": "Data Export",
            "desc": "This code uses the `trainer.predict` method to generate predictions from the `TransformerWrapper` model on the test data, concatenates the resulting logits from each batch, converts them into binary class predictions, creates a DataFrame with the test IDs and predicted targets, and saves the results to a CSV file named 'roberta.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9993868
            },
            "cluster": -1
        }, {
            "cell_id": 74,
            "code": "!head *.csv",
            "class": "Data Export",
            "desc": "This code executes a shell command to display the first 10 lines of all CSV files in the current directory, providing a preview of their contents.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99973243
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')",
            "class": "Data Extraction",
            "desc": "This code reads the train and test CSV files from the specified paths using Pandas, loading the datasets into DataFrame objects named `train` and `test`.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99975055
            },
            "cluster": 1
        }, {
            "cell_id": 44,
            "code": "filename = '/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt'\n\nword_dict = {}\n\nembeddings = []\n\nwith open(filename, 'r') as f:\n\n    for line in tqdm(f, total=400000):\n\n        word, vector_string = line.split(' ', 1)\n\n        vector = [float(value) for value in vector_string.split()]\n\n        embeddings.append(vector)\n\n        word_dict[word] = len(word_dict)\n\n\n\nembeddings = torch.tensor(embeddings)",
            "class": "Data Extraction",
            "desc": "This code reads pre-trained GloVe word embeddings from a specified text file, creates a dictionary mapping words to their embedding indices, and stores the embeddings in a PyTorch tensor for further usage.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9478619
            },
            "cluster": 1
        }, {
            "cell_id": 63,
            "code": "pretrained_name = 'distilroberta-base'\n\ntokenizer = RobertaTokenizerFast.from_pretrained(pretrained_name)\n\nroberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)",
            "class": "Data Extraction",
            "desc": "This code initializes a tokenizer and a pre-trained DistilRoberta model for sequence classification with 2 labels using the Hugging Face Transformers library, preparing them for further tasks such as fine-tuning or inference.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.9948885
            },
            "cluster": 1
        }, {
            "cell_id": 8,
            "code": "min_freq = 5\n\nabove_threshold = train.location.value_counts() > min_freq\n\nfrequent_places = above_threshold.index[above_threshold]\n\ndata = train[train.location.isin(frequent_places)].location\n\nprint(f'{data.nunique()} unique locations with more than {min_freq} occurrences')",
            "class": "Data Transform",
            "desc": "This code filters the locations in the training DataFrame to only those that appear more than five times and prints the number of unique locations that meet this frequency threshold.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.9539694
            },
            "cluster": 1
        }, {
            "cell_id": 9,
            "code": "train.drop(['location', 'keyword'], axis=1, inplace=True)\n\ntest.drop(['location', 'keyword'], axis=1, inplace=True)",
            "class": "Data Transform",
            "desc": "This code removes the 'location' and 'keyword' columns from both the train and test DataFrames using the Pandas `drop` method, modifying the DataFrames in place.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9991115
            },
            "cluster": 2
        }, {
            "cell_id": 12,
            "code": "text = \"Don't split #hashtags!\"\n\nprint('Before:', [t for t in tokenizer(text)])\n\n\n\nprefixes = list(nlp.Defaults.prefixes)\n\nprefixes.remove('#')\n\nprefix_regex = spacy.util.compile_prefix_regex(prefixes)\n\ntokenizer.prefix_search = prefix_regex.search\n\n\n\nprint('After:', [t for t in tokenizer(text)])",
            "class": "Data Transform",
            "desc": "This code customizes the spacy tokenizer to avoid splitting hashtags by removing the '#' character from the list of tokenization prefixes and recompiling the prefix regular expression, demonstrating the token transformation before and after this modification.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.5677337
            },
            "cluster": -1
        }, {
            "cell_id": 13,
            "code": "text = 'This is  a test\\n , ok?'\n\nprint('All tokens:', [t.text for t in tokenizer(text)])\n\n\n\nprint('Check for is_space():', [t.text for t in tokenizer(text) if not t.is_space])",
            "class": "Data Transform",
            "desc": "This code tokenizes a sample text and filters out tokens that represent whitespace using the spacy tokenizer, demonstrating the removal of space tokens for cleaner processing.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.9729691
            },
            "cluster": -1
        }, {
            "cell_id": 14,
            "code": "train['tokens'] = train['text'].apply(lambda row: [t.text.lower() for t in tokenizer(row) if not t.is_space])\n\ntest['tokens'] = test['text'].apply(lambda row: [t.text.lower() for t in tokenizer(row) if not t.is_space])",
            "class": "Data Transform",
            "desc": "This code creates a new 'tokens' column in both the train and test DataFrames by tokenizing the 'text' column, converting tokens to lowercase, and filtering out space tokens using a lambda function and spacy tokenizer.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.5443664
            },
            "cluster": 5
        }, {
            "cell_id": 18,
            "code": "from sklearn.feature_extraction.text import CountVectorizer\n\n\n\n# min and max document frequency (ratio of documents containing that token)\n\nmin_df = 5\n\nmax_df = 0.6\n\n\n\n# limit vocabulary size as a function of the training data\n\nmax_features = len(train) * 2\n\n\n\nvectorizer = CountVectorizer(lowercase=False, tokenizer=lambda x: x, min_df=min_df, max_df=max_df, max_features=max_features, binary=True)\n\ntrain_bow = vectorizer.fit_transform(train.tokens)\n\ntrain_bow",
            "class": "Data Transform",
            "desc": "This code initializes a `CountVectorizer` from scikit-learn with specific parameters to transform the 'tokens' column of the train DataFrame into a sparse Bag-of-Words (BoW) matrix representation.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.97597075
            },
            "cluster": -1
        }, {
            "cell_id": 33,
            "code": "# min and max document frequency (ratio of documents containing that token)\n\nmin_df = 10\n\nmax_df = 0.6\n\n\n\n# limit vocabulary size as a function of the training data\n\nmax_features = len(train) * 2\n\n\n\n# single words to 3-grams\n\nngram_range = (1, 3)\n\n\n\nvectorizer = CountVectorizer(lowercase=False, tokenizer=lambda x: x, min_df=min_df, max_df=max_df, max_features=max_features, binary=True, ngram_range=ngram_range)\n\nx = train_bow = vectorizer.fit_transform(train.tokens)\n\n\n\nvocab = vectorizer.get_feature_names()\n\nword_count = train_bow.toarray().sum(0)\n\n\n\nplot_top_values(word_count, k, vocab, 'Count', 'Type')",
            "class": "Data Transform",
            "desc": "This code initializes a `CountVectorizer` to create a Bag-of-Words representation that includes single words to 3-grams, transforms the 'tokens' column of the train DataFrame, and uses the `plot_top_values` function to visualize the top 50 n-grams based on their counts.",
            "testing": {
                "class": "Visualization",
                "subclass": "relationship",
                "subclass_id": 81,
                "predicted_subclass_probability": 0.74612176
            },
            "cluster": -1
        }, {
            "cell_id": 46,
            "code": "oov_count = Counter()\n\nall_tokens = []\n\n\n\nfor row in train.tokens:\n\n    tokens = [t[1:] if t.startswith('#') else t for t in row]\n\n    all_tokens.append(tokens)\n\n    oov_count.update(set(t for t in tokens if t not in word_dict))",
            "class": "Data Transform",
            "desc": "This code processes the tokens in the train DataFrame by handling hashtags and counting out-of-vocabulary (OOV) tokens that are not in the GloVe word dictionary, while also collecting all tokens into a list.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.68699807
            },
            "cluster": 1
        }, {
            "cell_id": 47,
            "code": "test_tokens = []\n\nfor row in test.tokens:\n\n    tokens = [t[1:] if t.startswith('#') else t for t in row]\n\n    test_tokens.append(tokens)",
            "class": "Data Transform",
            "desc": "This code processes the tokens in the test DataFrame by handling hashtags, ensuring consistency in token formatting and creating a list of all processed tokens for the test dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9887138
            },
            "cluster": 1
        }, {
            "cell_id": 49,
            "code": "words_to_add = [w for w in oov_count if oov_count[w] > 2]\n\nfor word in words_to_add:\n\n    word_dict[word] = len(word_dict)\n\n\n\nnew_vectors = torch.zeros((len(words_to_add), embeddings.shape[1]))\n\nembeddings = torch.cat([embeddings, new_vectors], dim=0)\n\nprint(len(word_dict), embeddings.shape)",
            "class": "Data Transform",
            "desc": "This code adds new out-of-vocabulary (OOV) words that appear more than twice into the word dictionary and extends the embeddings tensor with zero vectors corresponding to these new words, updating the dictionary and embedding matrix dimensions.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.89962006
            },
            "cluster": -1
        }, {
            "cell_id": 51,
            "code": "def convert_to_indices(all_tokens):\n\n    word_indices = []\n\n\n\n    for tokens in all_tokens:\n\n        tweet_inds = torch.tensor([word_dict[t] for t in tokens if t in word_dict], dtype=torch.long)\n\n        word_indices.append(tweet_inds)\n\n    \n\n    return word_indices\n\n\n\nword_indices = convert_to_indices(all_tokens)\n\ntest_word_indices = convert_to_indices(test_tokens)",
            "class": "Data Transform",
            "desc": "This code defines a function to convert tokens into their respective indices based on the word dictionary, and applies this function to both the training and test datasets, resulting in lists of tensor indices for each tweet.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.92438155
            },
            "cluster": 1
        }, {
            "cell_id": 55,
            "code": "validation_size = int(0.1 * len(train))\n\nvalidation_inds = np.random.choice(np.arange(len(train)), size=validation_size, replace=False)\n\nis_train = np.ones(len(train), dtype=np.bool)\n\nis_train[validation_inds] = False\n\n\n\n# use an object array since we have varied size tensors\n\ntweets = np.array(word_indices, dtype=object)\n\ntarget = train.target.to_numpy()\n\n# train_tweets, valid_tweets, train_target, valid_target = train_test_split(tweets, target, test_size=0.1, stratify=target)\n\ntrain_tweets = tweets[is_train].tolist()\n\ntrain_target = target[is_train]\n\nvalid_tweets = tweets[~is_train].tolist()\n\nvalid_target = target[~is_train]\n\n\n\ntrain_data = WordIndexDataset(train_tweets, train_target)\n\nvalid_data = WordIndexDataset(valid_tweets, valid_target)\n\ntest_data = WordIndexDataset(test_word_indices)\n\ntrain_loader = DataLoader(train_data, batch_size=32, collate_fn=collate_as_list)\n\nvalid_loader = DataLoader(valid_data, batch_size=256, collate_fn=collate_as_list)\n\ntest_loader = DataLoader(test_data, batch_size=256, collate_fn=collate_as_list)",
            "class": "Data Transform",
            "desc": "This code splits the training data into training and validation subsets based on specified indices, creates `WordIndexDataset` objects for training, validation, and test datasets, and initializes DataLoaders with specified batch sizes and a custom collate function for batching.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.46862042
            },
            "cluster": 1
        }, {
            "cell_id": 64,
            "code": "# create tensors of variable sizes\n\n# note that the tokenizer returns a tensor with shape [1, num_tokens]\n\ntrain_tokens = train.text[is_train].apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\n\nvalid_tokens = train.text[~is_train].apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\n\ntest_tokens = test.text.apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\n\n\n\n# add padding to have a fixed size matrix. With bigger datasets we should be careful about memory usage, but this is small enough to skip this kind of optimization\n\npadding = tokenizer.pad_token_id\n\nx_train = pad_sequence(train_tokens, batch_first=True, padding_value=padding)\n\nx_valid = pad_sequence(valid_tokens, batch_first=True, padding_value=padding)\n\nx_test = pad_sequence(test_tokens, batch_first=True, padding_value=padding)\n\n\n\nx_train_mask = x_train != padding\n\nx_valid_mask = x_valid != padding\n\nx_test_mask = x_test != padding\n\nprint(f'x_train shape: {x_train.shape}, x_valid shape: {x_valid.shape}, x_test shape: {x_test.shape}')",
            "class": "Data Transform",
            "desc": "This code tokenizes the text data in the training, validation, and test sets using the DistilRoberta tokenizer, pads the token sequences to create fixed-size matrices of token IDs, and generates corresponding attention masks to indicate non-padding tokens, ensuring consistency for model input.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9920512
            },
            "cluster": 1
        }, {
            "cell_id": 65,
            "code": "train_data = TensorDataset(x_train, x_train_mask, torch.tensor(train_target))\n\nvalid_data = TensorDataset(x_valid, x_valid_mask, torch.tensor(valid_target))\n\ntest_data = TensorDataset(x_test, x_test_mask)\n\n\n\ntrain_loader = DataLoader(train_data, batch_size=32)\n\nvalid_loader = DataLoader(valid_data, batch_size=256)\n\ntest_loader = DataLoader(test_data, batch_size=256)",
            "class": "Data Transform",
            "desc": "This code creates `TensorDataset` objects for the training, validation, and test datasets, including input IDs, attention masks, and targets where applicable, and initializes DataLoaders with specified batch sizes for efficient batching and data loading.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.97746176
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "train.head()",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first few rows of the `train` DataFrame to provide an initial overview of the data's structure and contents.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997507
            },
            "cluster": 4
        }, {
            "cell_id": 10,
            "code": "train.text.isna().sum(), test.text.isna().sum()",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and returns the number of missing (NaN) values in the 'text' column for both the train and test DataFrames, providing insight into the completeness of the textual data.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9989147
            },
            "cluster": 1
        }, {
            "cell_id": 11,
            "code": "nlp = English()\n\ntokenizer = nlp.tokenizer\n\ntokens = tokenizer('This is a test!')\n\nprint(tokens)\n\nprint(type(tokens))\n\nprint([t.text for t in tokens])",
            "class": "Exploratory Data Analysis",
            "desc": "This code initializes a tokenizer using the spacy English model, tokenizes a sample sentence, and prints the resulting tokens along with their types and text representations, demonstrating the tokenization process.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.46164313
            },
            "cluster": 0
        }, {
            "cell_id": 15,
            "code": "train.sample(10)",
            "class": "Exploratory Data Analysis",
            "desc": "This code randomly selects and displays 10 rows from the train DataFrame to provide a sample view of the dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.99975437
            },
            "cluster": 4
        }, {
            "cell_id": 25,
            "code": "def get_rows_containing(data, term):\n\n    \"\"\"Return rows containing a term\"\"\"\n\n    has_term = data.tokens.apply(lambda row: term in row)\n\n    return data[has_term]\n\n\n\nterms = ['bags', 'australia']\n\nfor term in terms:\n\n    rows = get_rows_containing(train, term)\n\n    print(f'Distribution containing {term}:')\n\n    print(rows.target.value_counts())\n\n    for i, row in rows.sample(5).iterrows():\n\n        print(row.target, row.text)\n\n    print()",
            "class": "Exploratory Data Analysis",
            "desc": "This code defines a function to return rows containing a specified term and then iterates over a list of terms to display the distribution of target values and sample text for rows containing each term in the train DataFrame.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_values",
                "subclass_id": 72,
                "predicted_subclass_probability": 0.57849276
            },
            "cluster": 0
        }, {
            "cell_id": 34,
            "code": "x.shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the shape of the feature matrix `x` to provide information about the dimensionality of the Bag-of-Words representation.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9995432
            },
            "cluster": 2
        }, {
            "cell_id": 45,
            "code": "print(embeddings.shape)\n\nprint(len(word_dict))",
            "class": "Exploratory Data Analysis",
            "desc": "This code prints the shape of the embeddings tensor and the length of the word dictionary, providing information on the dimensionality of the embeddings and the size of the vocabulary.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9995413
            },
            "cluster": 0
        }, {
            "cell_id": 48,
            "code": "oov_count.most_common(10)",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the 10 most common out-of-vocabulary (OOV) tokens from the training data, providing insight into words that were not found in the GloVe word dictionary.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.94204676
            },
            "cluster": 4
        }, {
            "cell_id": 50,
            "code": "len(oov_count)",
            "class": "Exploratory Data Analysis",
            "desc": "This code calculates and returns the number of unique out-of-vocabulary (OOV) tokens identified in the training dataset.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9988147
            },
            "cluster": 4
        }, {
            "cell_id": 0,
            "code": "from collections import Counter\n\n\n\nimport seaborn as sns\n\nimport numpy as np \n\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\n\nimport spacy\n\nfrom tqdm import tqdm\n\nfrom spacy.lang.en import English\n\nimport torch\n\nfrom torch import nn\n\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom torch.nn import functional as F\n\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader\n\nimport pytorch_lightning as pl\n\n\n\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\n\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizerFast",
            "class": "Imports and Environment",
            "desc": "This code imports a variety of libraries and modules necessary for tasks such as data manipulation (Pandas, Numpy), visualization (Seaborn, Matplotlib), natural language processing (spacy, transformers), machine learning model training/evaluation (scikit-learn, xgboost), and deep learning (PyTorch, PyTorch Lightning).",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993081
            },
            "cluster": 0
        }, {
            "cell_id": 1,
            "code": "np.random.seed(42)\n\n\n\n# prettier graphs!\n\nplt.style.use('ggplot')",
            "class": "Imports and Environment",
            "desc": "This code sets a random seed for NumPy to ensure reproducibility and configures Matplotlib to use the 'ggplot' style for prettier graphs, ensuring consistency in visuals.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9983991
            },
            "cluster": 1
        }, {
            "cell_id": 22,
            "code": "majority = y.mode()[0] == y\n\nprint(f'Majority class baseline: {majority.mean()}')",
            "class": "Model Evaluation",
            "desc": "This code calculates the baseline accuracy for the majority class by determining how often the most frequent class in the target column matches the actual target values and prints the result.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.91397643
            },
            "cluster": 0
        }, {
            "cell_id": 23,
            "code": "classifier = LogisticRegression()\n\ncv_scores = cross_val_score(classifier, x, y, scoring='f1', cv=10, n_jobs=-1)\n\nprint(f'Mean F1: {cv_scores.mean()}')",
            "class": "Model Evaluation",
            "desc": "This code trains a Logistic Regression classifier using 10-fold cross-validation, evaluates the model based on the F1 score using scikit-learn's `cross_val_score`, and prints the mean F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.97591215
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "k = 50\n\nclassifier = LogisticRegression(max_iter=500)\n\nclassifier.fit(x, y)\n\nplot_top_values(classifier.coef_[0], k, vocab, 'Type', 'Weight', use_abs=True)",
            "class": "Model Evaluation",
            "desc": "This code trains a Logistic Regression classifier on the training data with increased iterations, and then uses the previously defined `plot_top_values` function to visualize the top 50 feature weights in absolute value from the classifier's coefficients.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.7323191
            },
            "cluster": 0
        }, {
            "cell_id": 26,
            "code": "from sklearn.feature_selection import chi2, SelectKBest\n\n\n\nnum_features = [1000, 500, 250, 100, 50]\n\nf1 = []\n\nfor k in num_features:\n\n    selector = SelectKBest(chi2, k=k)\n\n    x_selected = selector.fit_transform(x, y)\n\n    scores = cross_val_score(classifier, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\n\n    f1.append(scores.mean())\n",
            "class": "Model Evaluation",
            "desc": "This code uses Chi-Square feature selection with different numbers of features to transform the training data, then performs 10-fold cross-validation with a Logistic Regression classifier for each feature set size, and stores the mean F1 scores.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_params",
                "subclass_id": 2,
                "predicted_subclass_probability": 0.48363486
            },
            "cluster": 0
        }, {
            "cell_id": 28,
            "code": "selector = SelectKBest(chi2, k=250)\n\nx_selected = selector.fit_transform(x, y)\n\nvocab = [vocab[i] for i, selected in enumerate(selector.get_support()) if selected]\n\nclassifier.fit(x_selected, y)\n\nplot_top_values(classifier.coef_[0], k, vocab, 'Type', 'Weight', use_abs=True)",
            "class": "Model Evaluation",
            "desc": "This code applies Chi-Square feature selection to reduce the feature set to 250 features, retrains the Logistic Regression classifier on the selected features, and uses the previously defined `plot_top_values` function to visualize the top 50 absolute feature weights of the trained classifier.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.58552986
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "regularization = [1, 0.1, 0.01, 0.001, 0.0001]\n\nl1_scores = []\n\nl2_scores = []\n\nl1_std = []\n\nl2_std = []\n\n\n\nfor value in regularization:\n\n    log_reg = LogisticRegression(C=value)\n\n    results = cross_val_score(log_reg, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\n\n    l2_scores.append(results.mean())\n\n    l2_std.append(results.std())\n\n    \n\n    alpha = 1 / (2 * value)  # as defined in sklearn\n\n    ridge = RidgeClassifier(alpha=alpha)\n\n    results = cross_val_score(ridge, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\n\n    l1_scores.append(results.mean())\n\n    l1_std.append(results.std())",
            "class": "Model Evaluation",
            "desc": "This code compares Logistic Regression and Ridge Classifier models using various regularization strengths by performing 10-fold cross-validation on the selected feature set, storing the mean and standard deviation of the F1 scores for each regularization value.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.39513838
            },
            "cluster": 0
        }, {
            "cell_id": 32,
            "code": "print(f'Best baseline F1: {l2_scores[1]}')",
            "class": "Model Evaluation",
            "desc": "This code prints the mean F1 score of the Logistic Regression model with the second regularization value, which is referred to as the best baseline F1 score.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.5057336
            },
            "cluster": 3
        }, {
            "cell_id": 35,
            "code": "classifier = LogisticRegression(C=0.1)\n\nselector = SelectKBest(chi2, k=500)\n\nx = selector.fit_transform(x, y)\n\ncv_scores = cross_validate(classifier, x, y, scoring='f1', cv=10, n_jobs=-1, return_train_score=True)\n\nmean_f1 = cv_scores['test_score'].mean()\n\nprint(f'Mean F1: {mean_f1}')",
            "class": "Model Evaluation",
            "desc": "This code applies Chi-Square feature selection to reduce the feature set to 500 features, trains a Logistic Regression classifier with a regularization parameter `C=0.1`, performs 10-fold cross-validation to evaluate the F1 score, and prints the mean F1 score.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.98264277
            },
            "cluster": 0
        }, {
            "cell_id": 38,
            "code": "c = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\n\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])",
            "class": "Model Evaluation",
            "desc": "This code initializes a Random Forest Classifier with 100 estimators, performs 8-fold cross-validation to evaluate the F1 score on both training and validation sets, and uses the `plot_model_score` function to visualize and compare the model's performance.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.7403155
            },
            "cluster": 0
        }, {
            "cell_id": 39,
            "code": "c = RandomForestClassifier(n_estimators=100, min_samples_leaf=3)\n\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\n\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])",
            "class": "Model Evaluation",
            "desc": "This code initializes a Random Forest Classifier with 100 estimators and a minimum of 3 samples per leaf, performs 8-fold cross-validation to evaluate the F1 score on both training and validation sets, and uses the `plot_model_score` function to visualize and compare the model's performance.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.8847018
            },
            "cluster": 0
        }, {
            "cell_id": 40,
            "code": "c = RandomForestClassifier(n_estimators=500, min_samples_split=10)\n\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\n\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])",
            "class": "Model Evaluation",
            "desc": "This code initializes a Random Forest Classifier with 500 estimators and a minimum of 10 samples required to split an internal node, performs 8-fold cross-validation to evaluate the F1 score on both training and validation sets, and uses the `plot_model_score` function to visualize and compare the model's performance.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.88807696
            },
            "cluster": 0
        }, {
            "cell_id": 41,
            "code": "c = RandomForestClassifier(n_estimators=200, min_samples_split=5, max_depth=50)\n\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\n\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])",
            "class": "Model Evaluation",
            "desc": "This code initializes a Random Forest Classifier with 200 estimators, a minimum of 5 samples required to split an internal node, and a maximum depth of 50, performs 8-fold cross-validation to evaluate the F1 score on both training and validation sets, and uses the `plot_model_score` function to visualize and compare the model's performance.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.41321388
            },
            "cluster": 0
        }, {
            "cell_id": 42,
            "code": "c = XGBClassifier()\n\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\n\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])",
            "class": "Model Evaluation",
            "desc": "This code initializes an XGBoost Classifier, performs 8-fold cross-validation to evaluate the F1 score on both training and validation sets, and uses the `plot_model_score` function to visualize and compare the model's performance.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.8236538
            },
            "cluster": 0
        }, {
            "cell_id": 61,
            "code": "# trainer.predict returns a list with batch results\n\nlogits = np.concatenate(trainer.predict(model, test_loader))",
            "class": "Model Evaluation",
            "desc": "This code uses the `trainer.predict` method to generate predictions from the `BagOfEmbeddingsClassifier` model on the test data and concatenates the resulting logits from each batch into a single array.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99380153
            },
            "cluster": 1
        }, {
            "cell_id": 21,
            "code": "x = train_bow\n\ny = train['target']",
            "class": "Model Training",
            "desc": "This code assigns the Bag-of-Words matrix (`train_bow`) to the variable `x` and the 'target' column of the train DataFrame to the variable `y`, preparing the feature matrix and labels for model training.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.99928766
            },
            "cluster": 0
        }, {
            "cell_id": 52,
            "code": "class BagOfEmbeddingsClassifier(pl.LightningModule):\n\n    def __init__(self, embeddings, learning_rate=0.001, l2=0.001):\n\n        super().__init__()\n\n        self.learning_rate = learning_rate\n\n        self.l2 = l2\n\n        \n\n        vocab_size, embedding_dim = embeddings.shape\n\n        self.embedding_bag = nn.EmbeddingBag.from_pretrained(embeddings, freeze=False)\n\n        \n\n        # a single output value determines the probability of class 1 with a sigmoid function\n\n        self.linear = nn.Linear(embedding_dim, 1, bias=True)\n\n    \n\n    def forward(self, x):\n\n        \"\"\"x is a list of tensors with any shape\"\"\"\n\n        # embedding bag operates with a single tensor of concatenated inputs and another of offsets\n\n        lengths = torch.tensor([0] + [len(sample) for sample in x[:-1]])\n\n        offsets = lengths.cumsum(0).to(x[0].device)\n\n        x = torch.cat(x)\n\n        embedded = self.embedding_bag(x, offsets)\n\n        logits = self.linear(embedded).squeeze(-1)\n\n        return logits\n\n    \n\n    def _get_loss_and_acc(self, logits, y):\n\n        \"\"\"Internal function\"\"\"\n\n        predicted = logits > 0\n\n        acc = (predicted == y).float().mean()\n\n        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n\n        \n\n        return loss, acc\n\n    \n\n    def on_fit_start(self):        \n\n        self.train_losses = []\n\n        self.train_accs = []\n\n        self.valid_losses = []\n\n        self.valid_accs = []\n\n        \n\n        self.reset_metrics()\n\n    \n\n    def reset_metrics(self):\n\n        self.partial_train_losses = []\n\n        self.partial_train_accs = []\n\n        self.partial_valid_losses = []\n\n        self.partial_valid_accs = []\n\n    \n\n    def on_validation_end(self):\n\n        self.train_losses.append(np.array(self.partial_train_losses).mean())\n\n        self.train_accs.append(np.array(self.partial_train_accs).mean())\n\n        self.valid_losses.append(np.array(self.partial_valid_losses).mean())\n\n        self.valid_accs.append(np.array(self.partial_valid_accs).mean())\n\n        self.reset_metrics()\n\n    \n\n    def training_step(self, batch, batch_idx):\n\n        \"\"\"\n\n        batch is a tuple (x, y)\n\n        x is a list of tensors as in forward\n\n        y is a tensor with the classes\n\n        \"\"\"\n\n        x, y = batch\n\n        logits = self(x)\n\n        loss, acc = self._get_loss_and_acc(logits, y)\n\n        \n\n        # ideally we'd use tensorboard to see the graphs, but currently it is disabled in Kaggle\n\n        # so we resort to manually plotting\n\n#         self.log('train_loss', loss)\n\n#         self.log('train_acc', acc)\n\n        self.partial_train_losses.append(loss.detach().cpu().numpy())\n\n        self.partial_train_accs.append(acc.detach().cpu().numpy())\n\n        \n\n        return loss\n\n    \n\n    def validation_step(self, batch, batch_idx):\n\n        x, y = batch\n\n        logits = self(x)\n\n        loss, acc = self._get_loss_and_acc(logits, y)\n\n        \n\n#         self.log('valid_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n#         self.log('valid_acc', acc)\n\n        self.partial_valid_losses.append(loss.detach().cpu().numpy())\n\n        self.partial_valid_accs.append(acc.detach().cpu().numpy())\n\n        \n\n        return loss\n\n    \n\n    def configure_optimizers(self):\n\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.l2)\n\n        return optimizer",
            "class": "Model Training",
            "desc": "This code defines a `BagOfEmbeddingsClassifier` class using PyTorch Lightning, which employs an EmbeddingBag layer initialized with pre-trained embeddings and a linear layer to predict class probabilities, and includes methods for forward propagation, loss calculation, and training/validation steps.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.94578296
            },
            "cluster": 2
        }, {
            "cell_id": 54,
            "code": "def collate_as_list(samples):\n\n    \"\"\"Function for the DataLoader to combine samples in a batch. Each sample is a (x, y) pair.\"\"\"\n\n    x, y = list(zip(*samples))\n\n    if y[0] is None:\n\n        return x\n\n    return x, torch.tensor(y).float()\n\n\n\n\n\nclass WordIndexDataset(Dataset):\n\n    def __init__(self, x, y=None):\n\n        self.x = x\n\n        self.y = y\n\n    \n\n    def __getitem__(self, i):\n\n        if self.y is not None:\n\n            return self.x[i], self.y[i]\n\n        else:\n\n            return self.x[i], None\n\n    \n\n    def __len__(self):\n\n        return len(self.x)\n",
            "class": "Model Training",
            "desc": "This code defines a collate function for the DataLoader to combine samples in a batch and implements a custom `WordIndexDataset` class that extends `torch.utils.data.Dataset`, handling the input feature indices (`x`) and optionally the labels (`y`) for data loading.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.7811093
            },
            "cluster": 0
        }, {
            "cell_id": 56,
            "code": "model = BagOfEmbeddingsClassifier(embeddings, 0.001, l2=0)\n\nbatch = next(iter(train_loader))\n\n\n\n# batch is x, y\n\nlogits = model(batch[0])\n\nprint(logits)",
            "class": "Model Training",
            "desc": "This code initializes the `BagOfEmbeddingsClassifier` model with pre-trained embeddings and a learning rate of 0.001, retrieves a batch from the training DataLoader, and computes the logits for the batch using the forward method of the model, printing the resulting logits.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9270925
            },
            "cluster": 0
        }, {
            "cell_id": 57,
            "code": "trainer = pl.Trainer(gpus=1, max_epochs=5, val_check_interval=0.5)\n\ntrainer.fit(model, train_loader, valid_loader)",
            "class": "Model Training",
            "desc": "This code initializes a PyTorch Lightning `Trainer` to train the `BagOfEmbeddingsClassifier` model on a GPU for a maximum of 5 epochs, validating the model's performance at regular intervals, and then starts the training and validation process using the specified DataLoaders.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.999678
            },
            "cluster": 0
        }, {
            "cell_id": 59,
            "code": "model = BagOfEmbeddingsClassifier(embeddings, 0.001, l2=0.0001)\n\ntrainer = pl.Trainer(gpus=1, max_epochs=6, val_check_interval=0.5)\n\ntrainer.fit(model, train_loader, valid_loader)",
            "class": "Model Training",
            "desc": "This code re-initializes the `BagOfEmbeddingsClassifier` model with a learning rate of 0.001 and L2 regularization of 0.0001, sets up a PyTorch Lightning `Trainer` for training on a GPU for up to 6 epochs with regular validation checks, and starts the training and validation process.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9926635
            },
            "cluster": 0
        }, {
            "cell_id": 66,
            "code": "class TransformerWrapper(pl.LightningModule):\n\n    def __init__(self, transformer, learning_rate=0.001, l2=0.0001):\n\n        super().__init__()\n\n        self.model = transformer\n\n        self.learning_rate = learning_rate\n\n        self.l2 = l2\n\n    \n\n    def forward(self, batch):\n\n        x, mask = batch\n\n        output = self.model(x, mask)\n\n        return output.logits\n\n    \n\n    def training_step(self, batch, batch_idx):\n\n        loss, acc = self._get_loss_and_acc(batch)\n\n        self.partial_train_losses.append(loss.detach().cpu().numpy())\n\n        self.partial_train_accs.append(acc.detach().cpu().numpy())\n\n        \n\n        return loss\n\n    \n\n    def _get_loss_and_acc(self, batch):\n\n        x, mask, y = batch\n\n        output = self.model(x, mask, labels=y)\n\n        loss = output.loss\n\n        logits = output.logits\n\n        \n\n        predicted = logits.argmax(1)\n\n        acc = (predicted == y).float().mean()\n\n        \n\n        return loss, acc\n\n    \n\n    # these functions are copied from the BagOfWords class to allow ploting without tensorboard\n\n    # ideally, we'd inherit from a common base class. well, ideally we'd have access to tensorboard and none of this would exist :)\n\n    def on_fit_start(self):        \n\n        self.train_losses = []\n\n        self.train_accs = []\n\n        self.valid_losses = []\n\n        self.valid_accs = []\n\n        \n\n        self.reset_metrics()\n\n    \n\n    def reset_metrics(self):\n\n        self.partial_train_losses = []\n\n        self.partial_train_accs = []\n\n        self.partial_valid_losses = []\n\n        self.partial_valid_accs = []\n\n    \n\n    def on_validation_end(self):\n\n        self.train_losses.append(np.array(self.partial_train_losses).mean())\n\n        self.train_accs.append(np.array(self.partial_train_accs).mean())\n\n        self.valid_losses.append(np.array(self.partial_valid_losses).mean())\n\n        self.valid_accs.append(np.array(self.partial_valid_accs).mean())\n\n        self.reset_metrics()\n\n        \n\n    def validation_step(self, batch, batch_idx):\n\n        loss, acc = self._get_loss_and_acc(batch)\n\n        \n\n#         self.log('valid_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n#         self.log('valid_acc', acc)\n\n        self.partial_valid_losses.append(loss.cpu().numpy())\n\n        self.partial_valid_accs.append(acc.cpu().numpy())\n\n        \n\n        return loss\n\n    \n\n    def configure_optimizers(self):\n\n        # to make it lighter, fine tune only the classifier on top of the language model\n\n        parameters = [p[1] for p in self.model.named_parameters() if p[0].startswith('classifier')]\n\n        optimizer = torch.optim.AdamW(parameters, lr=self.learning_rate, weight_decay=self.l2)\n\n        return optimizer",
            "class": "Model Training",
            "desc": "This code defines a `TransformerWrapper` class using PyTorch Lightning to wrap a pre-trained transformer model such as DistilRoberta, including methods for forward propagation, loss and accuracy calculation, and training and validation steps, while also configuring the optimizer to only fine-tune the classifier layer.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.48894703
            },
            "cluster": 2
        }, {
            "cell_id": 67,
            "code": "model = TransformerWrapper(roberta, 0.001, l2=0)\n\ntrainer = pl.Trainer(gpus=1, max_epochs=6, val_check_interval=0.5)\n\ntrainer.fit(model, train_loader, valid_loader)",
            "class": "Model Training",
            "desc": "This code initializes the `TransformerWrapper` model with the DistilRoberta transformer, a learning rate of 0.001, and no L2 regularization, sets up a PyTorch Lightning `Trainer` for training on a GPU for up to 6 epochs with regular validation checks, and starts the training and validation process.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9910779
            },
            "cluster": 0
        }, {
            "cell_id": 69,
            "code": "roberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)\n\nmodel = TransformerWrapper(roberta, 0.01, l2=0)\n\ntrainer = pl.Trainer(gpus=1, max_epochs=4, val_check_interval=0.5)\n\ntrainer.fit(model, train_loader, valid_loader)",
            "class": "Model Training",
            "desc": "This code reinitializes the DistilRoberta transformer for sequence classification, wraps it with the `TransformerWrapper` class with a higher learning rate of 0.01 and no L2 regularization, sets up a PyTorch Lightning `Trainer` for training on a GPU for up to 4 epochs with regular validation checks, and starts the training and validation process.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.98994815
            },
            "cluster": 0
        }, {
            "cell_id": 71,
            "code": "roberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)\n\n\n\ntrain_loader = DataLoader(train_data, batch_size=128)\n\n\n\nmodel = TransformerWrapper(roberta, 0.005, l2=0)\n\ntrainer = pl.Trainer(gpus=1, max_epochs=4, val_check_interval=0.5)\n\ntrainer.fit(model, train_loader, valid_loader)",
            "class": "Model Training",
            "desc": "This code reinitializes the DistilRoberta transformer for sequence classification, wraps it with the `TransformerWrapper` class with a learning rate of 0.005 and no L2 regularization, reconfigures the DataLoader for the training data with a batch size of 128, sets up a PyTorch Lightning `Trainer` for training on a GPU for up to 4 epochs with regular validation checks, and starts the training and validation process.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.898724
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "target_counts = train.target.value_counts()\n\nsns.barplot(y=target_counts, x=target_counts.index)\n\nplt.ylabel('Samples')\n\nplt.title('Target')\n\nplt.show()",
            "class": "Visualization",
            "desc": "This code counts the occurrences of each value in the target column of the train DataFrame and uses Seaborn and Matplotlib to create and display a bar plot visualizing the distribution of the target variable.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99522096
            },
            "cluster": 3
        }, {
            "cell_id": 5,
            "code": "has_kw = ~train.keyword.isna()\n\nfig, ax = plt.subplots(1, 2, sharey=True)\n\ntrain[has_kw]\n\nsns.countplot(data=train[has_kw], x='target', ax=ax[0])\n\nax[0].set_title('With keyword')\n\nsns.countplot(data=train[~has_kw], x='target', ax=ax[1])\n\nax[1].set_title('Without keyword')\n\nplt.show()",
            "class": "Visualization",
            "desc": "This code creates and displays count plots using Seaborn and Matplotlib to compare the distribution of the target variable in the training DataFrame based on the presence or absence of a keyword, effectively visualizing the influence of the keyword attribute on the target labels.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.98656905
            },
            "cluster": 3
        }, {
            "cell_id": 6,
            "code": "has_loc = ~train.location.isna()\n\nsns.countplot(x=has_loc)\n\nplt.xlabel('Has location')",
            "class": "Visualization",
            "desc": "This code produces a count plot using Seaborn to visualize the number of entries in the training DataFrame that have or do not have a non-null value in the location column.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99126256
            },
            "cluster": 3
        }, {
            "cell_id": 7,
            "code": "loc_count = train.location.value_counts()\n\ntop_loc = loc_count.iloc[:50]\n\nplt.subplots(figsize=(20, 8))\n\nplt.xticks(rotation=80)\n\nsns.barplot(x=top_loc.index, y=top_loc)",
            "class": "Visualization",
            "desc": "This code identifies the top 50 most frequent locations in the training DataFrame and displays a bar plot of these locations using Seaborn, with Matplotlib used to set the figure size and rotate the x-axis labels for better readability.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99783665
            },
            "cluster": 3
        }, {
            "cell_id": 16,
            "code": "train['num_tokens'] = train.tokens.apply(len)\n\nplt.hist(train.num_tokens, bins=20)\n\nplt.show()",
            "class": "Visualization",
            "desc": "This code calculates the number of tokens for each row in the train DataFrame and creates a histogram using Matplotlib to visualize the distribution of token counts.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99763453
            },
            "cluster": 3
        }, {
            "cell_id": 17,
            "code": "inds40 = train.num_tokens <= 40\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nplt.hist(train[inds40 & train.target].num_tokens, bins=20, alpha=0.5, label='Positive', density=True)\n\nplt.hist(train[inds40 & ~train.target].num_tokens, bins=20, alpha=0.5, label='Negative', density=True)\n\nplt.legend()\n\nplt.title('Tweet length distribution')\n\nplt.show()",
            "class": "Visualization",
            "desc": "This code filters the train DataFrame to include only rows with 40 or fewer tokens and creates a normalized histogram to visualize the tweet length distribution for positive and negative target classes using Matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9983352
            },
            "cluster": 3
        }, {
            "cell_id": 19,
            "code": "def plot_top_values(data, k, names, xlabel=None, ylabel=None, use_abs=False):\n\n    \"\"\"\n\n    Function to plot a barplot with counts of the top k items in data and their corresponding names.\n\n    \n\n    Args:\n\n        data: a numpy array\n\n        k: int\n\n        names: list of strings corresponding to the positions in data\n\n        use_abs: if True, take the highest absolute values\n\n    \"\"\"\n\n    if use_abs:\n\n        inds = np.abs(data).argsort()\n\n    else:\n\n        inds = data.argsort()\n\n            \n\n    # inverted argsort and top k\n\n    top_inds = inds[::-1][:k]\n\n    top_values = data[top_inds]\n\n    top_names = [names[i] for i in top_inds]\n\n    \n\n    fig, ax = plt.subplots(figsize=(16, 8))\n\n    plt.bar(np.arange(k), top_values)\n\n    if ylabel:\n\n        ax.set_ylabel(ylabel)\n\n    if xlabel:\n\n        ax.set_xlabel(xlabel)\n\n    ax.set_xticks(np.arange(k))\n\n    ax.set_xticklabels(top_names, rotation=80)\n\n    fig.tight_layout()",
            "class": "Visualization",
            "desc": "This code defines a function to plot a bar chart using Matplotlib, showcasing the top k items and their corresponding names from the provided data, with options for axis labeling and absolute value sorting.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9957224
            },
            "cluster": 3
        }, {
            "cell_id": 20,
            "code": "k = 50\n\n\n\nvocab = vectorizer.get_feature_names()\n\nword_count = train_bow.toarray().sum(0)\n\n\n\nplot_top_values(word_count, k, vocab, 'Count', 'Type')",
            "class": "Visualization",
            "desc": "This code retrieves the vocabulary terms from the `CountVectorizer`, calculates the word count of each term across the training data, and uses the previously defined `plot_top_values` function to create a bar chart of the top 50 terms and their counts.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.97462577
            },
            "cluster": 0
        }, {
            "cell_id": 27,
            "code": "ticks = np.arange(len(f1))\n\nplt.plot(ticks, f1)\n\nplt.xticks(ticks, [str(k) for k in num_features])\n\nplt.title('F1 per number of features (chi2 selector)')\n\nplt.show()",
            "class": "Visualization",
            "desc": "This code plots the mean F1 scores for different numbers of selected features using Chi-Square selection, creating a line plot with labeled x-axis ticks to visualize the performance impact of feature selection size.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.91531163
            },
            "cluster": 3
        }, {
            "cell_id": 29,
            "code": "rows = get_rows_containing(train, 'ebay')\n\nsns.countplot(x='target', data=rows)\n\nplt.title('Target distribution containing \"ebay\"')\n\nplt.show()",
            "class": "Visualization",
            "desc": "This code filters the train DataFrame for rows containing the term \"ebay\" and uses Seaborn to create and display a count plot showing the target distribution for these rows, with a title indicating the term being analyzed.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9911773
            },
            "cluster": 3
        }, {
            "cell_id": 31,
            "code": "n = np.arange(len(regularization)) + 1\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\nwidth = 0.4\n\n\n\nax.bar(n, l1_scores, width, label='L1 reg', yerr=l1_std)\n\nax.bar(n + width, l2_scores, width, label='L2 reg', yerr=l2_std)\n\nax.set_xlabel('Regularization (lower is stronger)')\n\nax.set_ylabel('Mean F1')\n\nax.set_xticks(n + width / 2)\n\nax.set_xticklabels([str(val) for val in regularization])\n\nax.legend(loc='best')\n",
            "class": "Visualization",
            "desc": "This code creates a grouped bar plot using Matplotlib to compare the mean F1 scores with error bars (standard deviations) for Logistic Regression using L1 regularization and Ridge Classifier using L2 regularization across different regularization strengths.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.97958606
            },
            "cluster": 3
        }, {
            "cell_id": 36,
            "code": "def plot_model_score(train_scores, valid_scores):\n\n    \"\"\"Plot train and validation score for comparison and checking overfitting\"\"\"\n\n    mean_train = train_scores.mean()\n\n    mean_valid = valid_scores.mean()\n\n    fig, ax = plt.subplots()\n\n    plt.bar(0, mean_train, yerr=train_scores.std())\n\n    plt.bar(1, mean_valid, yerr=valid_scores.std())\n\n    ax.text(0, mean_train + 0.01, f'{mean_train:.4f}')\n\n    ax.text(1, mean_valid + 0.01, f'{mean_valid:.4f}')\n\n    plt.title('Model F1 and standard deviation')\n\n    plt.xticks([0, 1], ['Train', 'Validation'])\n\n    ymin = np.min([mean_train, mean_valid]) * 0.8\n\n    plt.ylim(bottom=ymin)\n\n    plt.show()",
            "class": "Visualization",
            "desc": "This code defines a function to create a bar plot using Matplotlib that compares the mean F1 scores and standard deviations of training and validation sets, allowing for an assessment of overfitting by visualizing the model's performance.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.91516846
            },
            "cluster": 3
        }, {
            "cell_id": 37,
            "code": "plot_model_score(cv_scores['train_score'], cv_scores['test_score'])",
            "class": "Visualization",
            "desc": "This code uses the `plot_model_score` function to create a bar plot comparing the mean F1 scores and standard deviations of the Logistic Regression model's training and validation performance from the cross-validation results.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.977454
            },
            "cluster": 3
        }, {
            "cell_id": 53,
            "code": "def plot_model_performance(model):\n\n    fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n\n    ax[0].set_title('Loss')\n\n    ax[1].set_title('Accuracy')\n\n\n\n    n = np.arange(len(model.train_losses))\n\n    ax[0].plot(n, model.train_losses, 'bo', label='Train', linestyle='--')\n\n    ax[1].plot(n, model.train_accs, 'bo', linestyle='--')\n\n    ax[0].plot(n, model.valid_losses, 'ro', label='Validation', linestyle='--')\n\n    ax[1].plot(n, model.valid_accs, 'ro', linestyle='--')\n\n    ax[0].legend()\n\n    plt.show()",
            "class": "Visualization",
            "desc": "This code defines a function to plot the training and validation loss and accuracy for the `BagOfEmbeddingsClassifier` model using Matplotlib, providing a visual representation of the model's performance over epochs.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9959293
            },
            "cluster": 1
        }, {
            "cell_id": 58,
            "code": "plot_model_performance(model)",
            "class": "Visualization",
            "desc": "This code uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy for the `BagOfEmbeddingsClassifier` model, providing a visual representation of the model's performance over epochs.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.8608386
            },
            "cluster": 1
        }, {
            "cell_id": 60,
            "code": "plot_model_performance(model)",
            "class": "Visualization",
            "desc": "This code uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy for the re-initialized `BagOfEmbeddingsClassifier` model, providing a visual representation of its performance over epochs.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.8608386
            },
            "cluster": 1
        }, {
            "cell_id": 68,
            "code": "plot_model_performance(model)",
            "class": "Visualization",
            "desc": "This code uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy for the `TransformerWrapper` model, providing a visual representation of its performance over epochs.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.8608386
            },
            "cluster": 1
        }, {
            "cell_id": 70,
            "code": "plot_model_performance(model)",
            "class": "Visualization",
            "desc": "This code uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy for the re-initialized `TransformerWrapper` model with DistilRoberta, providing a visual representation of its performance over epochs.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.8608386
            },
            "cluster": 1
        }, {
            "cell_id": 72,
            "code": "plot_model_performance(model)",
            "class": "Visualization",
            "desc": "This code uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy for the reconfigured `TransformerWrapper` model with the updated DataLoader, providing a visual representation of its performance over epochs.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.8608386
            },
            "cluster": 1
        }],
        "notebook_id": 15,
        "notebook_name": "nlp-for-tweets-from-bag-of-words-to-transformers.ipynb",
        "user": "nlp-for-tweets-from-bag-of-words-to-transformers.ipynb"
    }, {
        "cells": [{
            "cell_id": 13,
            "code": "def submission(submission_file_path,model,test_vectors):\n\n    sample_submission = pd.read_csv(submission_file_path)\n\n    sample_submission[\"target\"] = model.predict(test_vectors)\n\n    sample_submission.to_csv(\"submission.csv\", index=False)",
            "class": "Data Export",
            "desc": "This code snippet defines a function to load a sample submission file, use a given model to predict target labels on test vectors, and save the results as a 'submission.csv' file using pandas.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9992994
            },
            "cluster": -1
        }, {
            "cell_id": 14,
            "code": "submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\n\ntest_vectors=test_tfidf\n\nclf = clf_xgb_TFIDF\n\nsubmission(submission_file_path,clf,test_vectors)",
            "class": "Data Export",
            "desc": "This code snippet calls the previously defined `submission` function to generate and save the prediction results using the `clf_xgb_TFIDF` model and the `test_tfidf` test vectors, with submission data read from a CSV file.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.7684079
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\nX_train = train.iloc[:, :4]\n\ny_train = train.iloc[:, 4]\n\nX_test = test\n\nprint(X_train.shape, y_train.shape, X_test.shape)",
            "class": "Data Extraction",
            "desc": "This code snippet reads the training and test datasets from CSV files using pandas, separates the features and labels for the training data, and prints the shapes of these datasets.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99967587
            },
            "cluster": 1
        }, {
            "cell_id": 2,
            "code": "def lowercase_text(text):\n\n    return text.lower()\n\n\n\nX_train.text=X_train.text.apply(lambda x: lowercase_text(x))\n\nX_test.text=X_test.text.apply(lambda x: lowercase_text(x))\n\nX_train.head()",
            "class": "Data Transform",
            "desc": "This code snippet defines a function to convert text to lowercase and then applies this function to the text columns of the training and test datasets using the `apply` method in pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9221044
            },
            "cluster": 0
        }, {
            "cell_id": 3,
            "code": "import re\n\nimport string\n\ndef remove_noise(text):\n\n    text = re.sub('\\[.*?\\]', '', text)\n\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n\n    text = re.sub('<.*?>+', '', text)\n\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n\n    text = re.sub('\\n', '', text)\n\n    text = re.sub('\\w*\\d\\w*', '', text)\n\n    text = re.sub('\u0089\u00fb\u00f2', '', text)\n\n    return text\n\nX_train.text=X_train.text.apply(lambda x: remove_noise(x))\n\nX_test.text=X_test.text.apply(lambda x: remove_noise(x))\n\nX_train.head()",
            "class": "Data Transform",
            "desc": "This code snippet defines a function to remove various types of noise from text data using regular expressions and then applies this function to the text columns of the training and test datasets using the `apply` method in pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.7514645
            },
            "cluster": 0
        }, {
            "cell_id": 4,
            "code": "# Tokenizing the training and the test set\n\nimport nltk\n\nfrom nltk.corpus import stopwords\n\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nX_train['text'] = X_train['text'].apply(lambda x: tokenizer.tokenize(x))\n\nX_test['text'] = X_test['text'].apply(lambda x: tokenizer.tokenize(x))\n\nX_train['text'].head()",
            "class": "Data Transform",
            "desc": "This code snippet imports NLTK, sets up a regular expression tokenizer, and applies it to tokenize the text columns of the training and test datasets.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.92383534
            },
            "cluster": 1
        }, {
            "cell_id": 5,
            "code": "# Removing stopwords belonging to english language\n\ndef remove_stopwords(text):\n\n    words = [w for w in text if w not in stopwords.words('english')]\n\n    return words\n\n\n\nX_train['text'] = X_train['text'].apply(lambda x : remove_stopwords(x))\n\nX_test['text'] = X_test['text'].apply(lambda x : remove_stopwords(x))\n\nX_train.head()",
            "class": "Data Transform",
            "desc": "This code snippet defines a function to remove English stopwords using NLTK and then applies this function to the tokenized text columns of the training and test datasets using the `apply` method in pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.98372287
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "# After preprocessing, the text format\n\ndef combine_text(list_of_text):\n\n    '''Takes a list of text and combines them into one large chunk of text.'''\n\n    combined_text = ' '.join(list_of_text)\n\n    return combined_text\n\n\n\nX_train['text'] = X_train['text'].apply(lambda x : combine_text(x))\n\nX_test['text'] = X_test['text'].apply(lambda x : combine_text(x))\n\n# X_train['text']\n\nX_train.head()",
            "class": "Data Transform",
            "desc": "This code snippet defines a function to combine a list of tokenized words back into a single text string and then applies this function to the processed text columns of the training and test datasets using the `apply` method in pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.7641233
            },
            "cluster": 0
        }, {
            "cell_id": 7,
            "code": "# Stemming\n\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer(\"english\")\n\n\n\ndef stemming(text):\n\n    text = [stemmer.stem(word) for word in text.split()]\n\n    return ' '.join(text)\n\n\n\n#X_train['text'] = X_train['text'].apply(lambda x : stemming(x))\n\n#X_test['text'] = X_test['text'].apply(lambda x : stemming(x))\n\n#X_train",
            "class": "Data Transform",
            "desc": "This code snippet imports the SnowballStemmer from NLTK, sets up a stemming function to reduce words to their root form, and defines code to apply this function to the text columns of the training and test datasets, although the applying lines are commented out.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.6512052
            },
            "cluster": 1
        }, {
            "cell_id": 10,
            "code": "from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer=CountVectorizer() # analyzer='word', stop_words = \"english\"\n\ntrain_vec = count_vectorizer.fit_transform(X_train.text)\n\ntest_vec = count_vectorizer.transform(X_test.text)\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nTfidf_vectorizer = TfidfVectorizer() # min_df=2, max_df=0.5, ngram_range=(1, 2)analyzer='word', stop_words = \"english\"analyzer='word', stop_words='english'# , ngram_range=(1, 2), lowercase=True, max_features=150000\n\ntrain_tfidf = Tfidf_vectorizer.fit_transform(X_train.text)\n\ntest_tfidf = Tfidf_vectorizer.transform(X_test.text)\n\n\n\nprint(\"train_vec\" ,train_vec[7].todense())\n\nprint(\"test_vec\", test_vec[7].todense())\n\n\n\nprint(\"train_tfidf\" ,train_tfidf[7].todense())\n\nprint(\"test_tfidf\", test_vec[7].todense())",
            "class": "Data Transform",
            "desc": "This code snippet uses the `CountVectorizer` and `TfidfVectorizer` from Scikit-learn to convert the text data in the training and test datasets into their respective vectorized forms and prints the dense representation of specific entries.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.8695518
            },
            "cluster": 0
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# For example, here's several helpful packages to load\n\n\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "This code snippet imports essential libraries like NumPy and pandas, and uses the `os` module to list all files in the '/kaggle/input' directory.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.99921954
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "# MultinomialNB\n\nclf.fit(train_vec,y_train)\n\ny_pred = clf.predict(test_vec)\n\nscores = model_selection.cross_val_score(clf,test_vec,y_pred,cv=kF,scoring='f1')\n\nprint(\"MultinomialNB prediction score: \" ,scores.mean())\n\n\n\n# LogisticRegression\n\nclf_tfidf.fit(train_tfidf, y_train)\n\ny_pred_tfidf = clf_tfidf.predict(test_tfidf)\n\nscores_tfidf = model_selection.cross_val_score(clf_tfidf,test_tfidf,y_pred_tfidf,cv=kF,\n\n                                         scoring='f1')\n\nprint(\"LogisticRegretion prediction score: \" ,scores_tfidf.mean())\n\n\n\n# SVC\n\nclf_svc.fit(train_tfidf, y_train)\n\ny_pred_svc = clf_svc.predict(test_tfidf)\n\nscores_svc = model_selection.cross_val_score(clf_svc,test_tfidf,y_pred_svc, cv=kF,\n\n                                         scoring='f1')  \n\nprint(\"SVC prediction score: \" ,scores_svc.mean())\n\n\n\n# XGBoost\n\nclf_xgb_TFIDF.fit(train_tfidf, y_train)\n\ny_pred_xgb = clf_xgb_TFIDF.predict(test_tfidf)\n\nscores_xgb = model_selection.cross_val_score(clf_xgb_TFIDF,test_tfidf,y_pred_xgb, cv=kF,\n\n                                         scoring='f1')  \n\nprint(\"XGBoosting prediction score: \" ,scores_xgb.mean())",
            "class": "Model Evaluation",
            "desc": "This code snippet fits the previously trained models (Multinomial Naive Bayes, Logistic Regression, SVC, and XGBoost) to the training data, makes predictions on the test data, and calculates the F1 scores using cross-validation for each model.",
            "testing": {
                "class": "Model_Train",
                "subclass": "find_best_model_class",
                "subclass_id": 3,
                "predicted_subclass_probability": 0.7733168
            },
            "cluster": 2
        }, {
            "cell_id": 11,
            "code": "from sklearn.model_selection import KFold\n\nkF = KFold(shuffle=True, random_state=241)      # \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 5 \u0432\u044b\u0431\u043e\u0440\u043e\u043a\n\n# MultinomialNB\n\nfrom sklearn import model_selection\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB() \n\nscores = model_selection.cross_val_score(clf,train_vec,y_train,cv=kF,scoring='f1')\n\nprint(\"MultinomialNB score: \" ,scores.mean())\n\n\n\n# LogisticRegression\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf_tfidf = LogisticRegression()\n\nscores_tfidf = model_selection.cross_val_score(clf_tfidf,train_tfidf,y_train,\n\n                                               cv=kF,scoring='f1')\n\nprint(\"LogisticRegretion score: \" ,scores_tfidf.mean())\n\n\n\n# SVC\n\nfrom sklearn.svm import SVC   # \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043c\u0435\u0442\u043e\u0434\u0430 \u043e\u043f\u043e\u0440\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432\n\nclf_svc = SVC()#kernel='linear', random_state=241\n\nscores_svc = model_selection.cross_val_score(clf_svc,train_tfidf,y_train,\n\n                                               cv=kF,scoring='f1')\n\nprint(\"SVC score: \" ,scores_svc.mean())\n\n\n\n# XGBoost\n\nimport xgboost as xgb\n\nclf_xgb_TFIDF = xgb.XGBClassifier()#max_depth=7, n_estimators=150, colsample_bytree=0.8, \n\n                        #subsample=0.8, nthread=10, learning_rate=0.1\n\nscores_xgb = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, y_train, cv=kF, scoring=\"f1\")\n\nprint(\"XGBost score: \" ,scores_xgb.mean())\n\n\n",
            "class": "Model Training",
            "desc": "This code snippet uses Scikit-learn's `KFold` for cross-validation and trains multiple models\u2014Multinomial Naive Bayes, Logistic Regression, Support Vector Classifier (SVC), and XGBoost classifier\u2014on the vectorized training data, and evaluates their F1 scores through cross-validation.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.6108105
            },
            "cluster": 2
        }, {
            "cell_id": 8,
            "code": "from wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n%matplotlib inline\n\ndef wordsCloud (dF):\n\n    fig , ax1 = plt.subplots(1,figsize=(12,12))\n\n    stopword_list = stopwords.words(\"english\")\n\n    wordcloud=WordCloud(stopwords = stopword_list, background_color='white',collocations = False , width=600,height=600).generate(\" \".join(dF))\n\n    ax1.imshow(wordcloud)\n\n    ax1.axis('off')\n\n    ax1.set_title(\"Frequent Words\",fontsize=24)    \n\n    # print(stopword_list)\n\n    return\n\nwordsCloud(X_train.text)",
            "class": "Visualization",
            "desc": "This code snippet defines a function to create and display a word cloud of the most frequent words in the given text data using the WordCloud library and Matplotlib for visualization.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.55752915
            },
            "cluster": -1
        }, {
            "cell_id": 9,
            "code": "wordsCloud(X_test.text)",
            "class": "Visualization",
            "desc": "This code snippet calls the previously defined `wordsCloud` function to generate and display a word cloud for the text data in the test dataset using the WordCloud library and Matplotlib for visualization.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.8625756
            },
            "cluster": -1
        }],
        "notebook_id": 16,
        "notebook_name": "distweetrhinosceros.ipynb",
        "user": "distweetrhinosceros.ipynb"
    }, {
        "cells": [{
            "cell_id": 44,
            "code": "submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntest_pred = model_glove.predict(X_test_seq)\ntest_pred_int = test_pred.round().astype('int')\nsubmission['target'] = test_pred_int\nsubmission.to_csv('submission.csv', index=False)",
            "class": "Data Export",
            "desc": "The code reads the sample submission file into a DataFrame, predicts the target values for the test data using the `model_glove`, rounds the predictions to integers, updates the 'target' column in the submission DataFrame with these predictions, and saves the DataFrame to a new CSV file named 'submission.csv'.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99928826
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain_data.head(5)",
            "class": "Data Extraction",
            "desc": "The code reads the 'train.csv' file from the input directory into a pandas DataFrame named 'train_data' and displays the first five rows using the pandas 'head' method.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9996433
            },
            "cluster": 1
        }, {
            "cell_id": 2,
            "code": "test_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntest_data.head(5)",
            "class": "Data Extraction",
            "desc": "The code reads the 'test.csv' file from the input directory into a pandas DataFrame named 'test_data' and displays the first five rows using the pandas 'head' method.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9996711
            },
            "cluster": 1
        }, {
            "cell_id": 34,
            "code": "# Loading the embedding dictionary from file\n\nembedding_dict={}\nwith open('../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()",
            "class": "Data Extraction",
            "desc": "The code reads the GloVe Twitter word embeddings from the specified file, processes each line to extract words and their corresponding vectors, and stores them in a dictionary called `embedding_dict`.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.23091643
            },
            "cluster": 1
        }, {
            "cell_id": 7,
            "code": "from bs4 import BeautifulSoup # Text Cleaning\nimport re, string # Regular Expressions, String\nfrom nltk.corpus import stopwords # stopwords\nfrom nltk.stem.porter import PorterStemmer # for word stemming\nfrom nltk.stem import WordNetLemmatizer # for word lemmatization\nimport unicodedata\nimport html\n\n# set of stopwords to be removed from text\nstop = set(stopwords.words('english'))\n\n# update stopwords to have punctuation too\nstop.update(list(string.punctuation))\n\ndef clean_tweets(text):\n    \n    # Remove unwanted html characters\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n    '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    text = re1.sub(' ', html.unescape(x1))\n    \n    # remove non-ascii characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # strip html\n    soup = BeautifulSoup(text, 'html.parser')\n    text = soup.get_text()\n    \n    # remove between square brackets\n    text = re.sub('\\[[^]]*\\]', '', text)\n    \n    # remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    \n    # remove twitter tags\n    text = text.replace(\"@\", \"\")\n    \n    # remove hashtags\n    text = text.replace(\"#\", \"\")\n    \n    # remove all non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    \n    # remove stopwords from text\n    final_text = []\n    for word in text.split():\n        if word.strip().lower() not in stop:\n            final_text.append(word.strip().lower())\n    \n    text = \" \".join(final_text)\n    \n    # lemmatize words\n    lemmatizer = WordNetLemmatizer()    \n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n    \n    # replace all numbers with \"num\"\n    text = re.sub(\"\\d\", \"num\", text)\n    \n    return text.lower()\n\ntrain_data['prep_text'] = train_data['text'].apply(clean_tweets)\ntrain_data['prep_text'].head(5)",
            "class": "Data Transform",
            "desc": "The code defines a function 'clean_tweets' to clean and preprocess tweet text data by removing unwanted characters, URLs, stopwords, and performing lemmatization, and then applies this cleaning function to the 'text' column in 'train_data', storing the results in a new column 'prep_text'.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.27900088
            },
            "cluster": -1
        }, {
            "cell_id": 8,
            "code": "test_data['text'] = test_data['text'].apply(clean_tweets)\ntest_data['text'].head(5)",
            "class": "Data Transform",
            "desc": "The code applies the previously defined 'clean_tweets' cleaning function to the 'text' column in 'test_data' and overwrites the cleaned text back into the 'text' column, displaying the first five rows of the resulting DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9986481
            },
            "cluster": 4
        }, {
            "cell_id": 9,
            "code": "from keras.preprocessing.text import Tokenizer # Text tokenization\n\n# Setting up the tokenizer\nvocab_size = 1000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))",
            "class": "Data Transform",
            "desc": "The code initializes a Keras `Tokenizer` with a vocabulary size of 1000 words and an out-of-vocabulary token 'UNK', and fits this tokenizer on the text data from both the 'prep_text' column in 'train_data' and the 'text' column in 'test_data' to build the word index.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9994017
            },
            "cluster": 2
        }, {
            "cell_id": 10,
            "code": "# Representing texts as one hot encoded sequence\n\nX_train_ohe = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'binary')\nX_test_ohe = tokenizer.texts_to_matrix(test_data['text'], mode = 'binary')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_ohe.shape}\")\nprint(f\"X_test shape: {X_test_ohe.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")",
            "class": "Data Transform",
            "desc": "The code represents the preprocessed text data in 'train_data' and 'test_data' as one-hot encoded binary matrices using the Keras `texts_to_matrix` method, and converts the target variable 'target' in 'train_data' to a NumPy array of integers, then prints the shapes of the resulting matrices and array.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.5156552
            },
            "cluster": 3
        }, {
            "cell_id": 11,
            "code": "from sklearn.model_selection import train_test_split\nX_train_ohe, X_val_ohe, y_train, y_val = train_test_split(X_train_ohe, y_train, random_state = 42, test_size = 0.2)\n\nprint(f\"X_train shape: {X_train_ohe.shape}\")\nprint(f\"X_val shape: {X_val_ohe.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")",
            "class": "Data Transform",
            "desc": "The code splits the one-hot encoded training data `X_train_ohe` and the target variable `y_train` into training and validation sets using the `train_test_split` function from scikit-learn, with 20% of the data allocated for validation, and prints the shapes of the resulting splits.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.961794
            },
            "cluster": -1
        }, {
            "cell_id": 16,
            "code": "X_train_wc = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'count')\nX_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_test shape: {X_test_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n",
            "class": "Data Transform",
            "desc": "The code converts the preprocessed text data in 'train_data' and 'test_data' into count matrices using the Keras `texts_to_matrix` method set to 'count' mode, converts the target variable 'target' in 'train_data' to a NumPy array of integers, and prints the shapes of the resulting count matrices and array.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.61748403
            },
            "cluster": 3
        }, {
            "cell_id": 17,
            "code": "X_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, random_state = 42, test_size = 0.2)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_val shape: {X_val_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")",
            "class": "Data Transform",
            "desc": "The code splits the count matrices `X_train_wc` and the target variable `y_train` into training and validation sets using the `train_test_split` function from scikit-learn, with 20% of the data allocated for validation, and prints the shapes of the resulting splits.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.80229384
            },
            "cluster": -1
        }, {
            "cell_id": 22,
            "code": "X_train_freq = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'freq')\nX_test_freq = tokenizer.texts_to_matrix(test_data['text'], mode = 'freq')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_freq.shape}\")\nprint(f\"X_test shape: {X_test_freq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")",
            "class": "Data Transform",
            "desc": "The code converts the preprocessed text data in 'train_data' and 'test_data' into frequency matrices using the Keras `texts_to_matrix` method set to 'freq' mode, converts the target variable 'target' in 'train_data' to a NumPy array of integers, and prints the shapes of the resulting frequency matrices and array.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "data_type_conversions",
                "subclass_id": 16,
                "predicted_subclass_probability": 0.74439836
            },
            "cluster": 3
        }, {
            "cell_id": 23,
            "code": "X_train_freq, X_val_freq, y_train, y_val = train_test_split(X_train_freq, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_freq.shape}\")\nprint(f\"X_val shape: {X_val_freq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")",
            "class": "Data Transform",
            "desc": "The code splits the frequency matrices `X_train_freq` and the target variable `y_train` into training and validation sets using the `train_test_split` function from scikit-learn, with 20% of the data allocated for validation, and prints the shapes of the resulting splits.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.84670115
            },
            "cluster": -1
        }, {
            "cell_id": 28,
            "code": "from sklearn.feature_extraction.text import TfidfVectorizer # Term Frequency - Inverse Document Frequency\n\nvectorizer = TfidfVectorizer(max_features = vocab_size)\nvectorizer.fit(list(train_data['prep_text']) + list(test_data['text']))\n\n# Fitting on training and testing data\nX_train_tfidf = vectorizer.transform(list(train_data['prep_text'])).toarray() \nX_test_tfidf = vectorizer.transform(list(test_data['text'])).toarray()\n\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape {X_train_tfidf.shape}\")\nprint(f\"X_test shape {X_test_tfidf.shape}\")\nprint(f\"y_train shape {y_train.shape}\")",
            "class": "Data Transform",
            "desc": "The code initializes a `TfidfVectorizer` with a maximum of `vocab_size` features, fits it on the combined text data from 'prep_text' in `train_data` and 'text' in `test_data`, transforms these texts into TF-IDF matrices for both datasets, and prints the shapes of the resulting matrices and the target variable array `y_train`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.57227826
            },
            "cluster": 3
        }, {
            "cell_id": 29,
            "code": "X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_tfidf.shape}\")\nprint(f\"X_val shape: {X_val_tfidf.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")",
            "class": "Data Transform",
            "desc": "The code splits the TF-IDF matrices `X_train_tfidf` and the target variable `y_train` into training and validation sets using the `train_test_split` function from scikit-learn, with 20% of the data allocated for validation, and prints the shapes of the resulting splits.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.70971876
            },
            "cluster": -1
        }, {
            "cell_id": 35,
            "code": "# Sequences creation, truncation and padding\n\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Setting up the tokenizer\nvocab_size = 10000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))\n\nmax_len = 15\nX_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\nX_test_seq = tokenizer.texts_to_sequences(test_data['text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")",
            "class": "Data Transform",
            "desc": "The code initializes a Keras `Tokenizer` with a vocabulary size of 10,000 and fits it on the combined preprocessed text data from `train_data` and `test_data`, converts the texts to sequences, truncates or pads these sequences to a maximum length of 15 tokens using `pad_sequences`, and prints the shapes of the resulting sequences and the target variable array `y_train`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.7396719
            },
            "cluster": 3
        }, {
            "cell_id": 36,
            "code": "X_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_val shape: {X_val_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")",
            "class": "Data Transform",
            "desc": "The code splits the padded sequences `X_train_seq` and the target variable `y_train` into training and validation sets using the `train_test_split` function from scikit-learn, with 20% of the data allocated for validation, and prints the shapes of the resulting splits.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9086002
            },
            "cluster": -1
        }, {
            "cell_id": 38,
            "code": "# Applying GloVE representations on our corpus\n\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tokenizer.word_index.items():\n    if i < num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec    ",
            "class": "Data Transform",
            "desc": "The code creates an embedding matrix where each word in the tokenizer's word index is mapped to its corresponding GloVe vector from `embedding_dict`, with words not found in `embedding_dict` assigned a vector of zeros.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.91961074
            },
            "cluster": -1
        }, {
            "cell_id": 3,
            "code": "train_data.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code provides a concise summary of the DataFrame 'train_data', including the column names, non-null counts, and data types using the pandas 'info' method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99936634
            },
            "cluster": 3
        }, {
            "cell_id": 4,
            "code": "test_data.info()",
            "class": "Exploratory Data Analysis",
            "desc": "The code provides a concise summary of the DataFrame 'test_data', including the column names, non-null counts, and data types using the pandas 'info' method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.9993579
            },
            "cluster": 3
        }, {
            "cell_id": 27,
            "code": "train_data.head()",
            "class": "Exploratory Data Analysis",
            "desc": "The code displays the first five rows of the 'train_data' DataFrame using the pandas `head` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997532
            },
            "cluster": 2
        }, {
            "cell_id": 37,
            "code": "num_words = len(tokenizer.word_index)\nprint(f\"Number of unique words: {num_words}\")",
            "class": "Exploratory Data Analysis",
            "desc": "The code calculates and prints the number of unique words in the tokenizer's word index.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_unique_values",
                "subclass_id": 54,
                "predicted_subclass_probability": 0.9865096
            },
            "cluster": 4
        }, {
            "cell_id": 0,
            "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "class": "Imports and Environment",
            "desc": "The snippet imports essential libraries such as NumPy, pandas, and Seaborn, and lists the available files in the Kaggle input directory using the os library.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.99922085
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "!pip install BeautifulSoup4",
            "class": "Imports and Environment",
            "desc": "The code installs the BeautifulSoup4 library, which is used for parsing HTML and XML documents.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.9954203
            },
            "cluster": 1
        }, {
            "cell_id": 14,
            "code": "_, accuracy = model.evaluate(X_val_ohe, y_val)",
            "class": "Model Evaluation",
            "desc": "The code evaluates the trained Keras model on the validation data `X_val_ohe` and `y_val`, and stores the resulting accuracy in the variable `accuracy`.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.99220455
            },
            "cluster": 2
        }, {
            "cell_id": 20,
            "code": "_, accuracy = model.evaluate(X_val_wc, y_val)",
            "class": "Model Evaluation",
            "desc": "The code evaluates the newly trained Keras model on the count-encoded validation data `X_val_wc` and `y_val`, and stores the resulting accuracy in the variable `accuracy`.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9896338
            },
            "cluster": 2
        }, {
            "cell_id": 12,
            "code": "from keras.models import Sequential\nfrom keras import layers, metrics, optimizers, losses\n\ndef setup_model():\n    \n    model = Sequential()\n#     model.add(layers.Dense(16, activation='relu', input_shape=(vocab_size,)))\n#     model.add(layers.Dense(16, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid', input_shape=(vocab_size,)))\n    \n    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n    \n    return model\n\nmodel = setup_model()\nmodel.summary()",
            "class": "Model Training",
            "desc": "The code defines a function `setup_model` to create and compile a Keras `Sequential` model with a single dense layer using sigmoid activation for binary classification, and then initializes this model and prints its summary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.990493
            },
            "cluster": 0
        }, {
            "cell_id": 13,
            "code": "history = model.fit(X_train_ohe, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_ohe, y_val))",
            "class": "Model Training",
            "desc": "The code trains the compiled Keras model on the one-hot encoded training data `X_train_ohe` and target variable `y_train` for 20 epochs with a batch size of 512, and validates it on the validation data `X_val_ohe` and `y_val`, storing the training history in the variable `history`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996803
            },
            "cluster": 0
        }, {
            "cell_id": 18,
            "code": "model = setup_model()\nmodel.summary()",
            "class": "Model Training",
            "desc": "The code initializes a new Keras model using the previously defined `setup_model` function and prints its summary.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9821353
            },
            "cluster": -1
        }, {
            "cell_id": 19,
            "code": "history = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))",
            "class": "Model Training",
            "desc": "The code trains the newly initialized Keras model on the count-encoded training data `X_train_wc` and target variable `y_train` for 20 epochs with a batch size of 512, and validates it on the validation data `X_val_wc` and `y_val`, storing the training history in the variable `history`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99967945
            },
            "cluster": 0
        }, {
            "cell_id": 24,
            "code": "model = setup_model()\nmodel.summary()",
            "class": "Model Training",
            "desc": "The code initializes a new Keras model using the previously defined `setup_model` function and prints its summary.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9821353
            },
            "cluster": -1
        }, {
            "cell_id": 25,
            "code": "history = model.fit(X_train_freq, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_freq, y_val))",
            "class": "Model Training",
            "desc": "The code trains the newly initialized Keras model on the frequency-encoded training data `X_train_freq` and target variable `y_train` for 20 epochs with a batch size of 512, and validates it on the validation data `X_val_freq` and `y_val`, storing the training history in the variable `history`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99967873
            },
            "cluster": 0
        }, {
            "cell_id": 30,
            "code": "model = setup_model()\nmodel.summary()",
            "class": "Model Training",
            "desc": "The code initializes a new Keras model using the previously defined `setup_model` function and prints its summary.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9821353
            },
            "cluster": -1
        }, {
            "cell_id": 31,
            "code": "history = model.fit(X_train_tfidf, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_tfidf, y_val))",
            "class": "Model Training",
            "desc": "The code trains the newly initialized Keras model on the TF-IDF encoded training data `X_train_tfidf` and target variable `y_train` for 20 epochs with a batch size of 512, and validates it on the validation data `X_val_tfidf` and `y_val`, storing the training history in the variable `history`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9996803
            },
            "cluster": 0
        }, {
            "cell_id": 39,
            "code": "# Setting up the model\n\nn_latent_factors = 100\nmodel_glove = Sequential()\nmodel_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                           input_length = max_len, trainable=True))\nmodel_glove.add(layers.Flatten())\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dropout(0.5))\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dense(1, activation='sigmoid'))\nmodel_glove.summary()",
            "class": "Model Training",
            "desc": "The code sets up a Keras `Sequential` model with an `Embedding` layer initialized with GloVe embedding vectors, followed by a `Flatten` layer, a `Dropout` layer, and a dense layer with sigmoid activation for binary classification, and prints the model summary.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9871293
            },
            "cluster": 2
        }, {
            "cell_id": 40,
            "code": "model_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\n              loss = losses.binary_crossentropy,\n              metrics = [metrics.binary_accuracy])\n\nhistory = model_glove.fit(X_train_seq,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(X_val_seq, y_val))",
            "class": "Model Training",
            "desc": "The code compiles the `model_glove` using the RMSprop optimizer, binary cross-entropy loss, and binary accuracy as a metric, and then trains the model on the padded sequence training data `X_train_seq` and target variable `y_train` for 20 epochs with a batch size of 512, validating on `X_val_seq` and `y_val`, and storing the training history in the variable `history`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99457717
            },
            "cluster": 0
        }, {
            "cell_id": 42,
            "code": "max_len = 15\nX_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\nX_test_seq = tokenizer.texts_to_sequences(test_data['text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\\n\")\n\n# Setting up the model\n\nn_latent_factors = 100\nmodel_glove = Sequential()\nmodel_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                           input_length = max_len, trainable=True))\nmodel_glove.add(layers.Flatten())\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dropout(0.5))\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dense(1, activation='sigmoid'))\nprint(f\"{model_glove.summary()}\\n\")\n\n\nmodel_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\n              loss = losses.binary_crossentropy,\n              metrics = [metrics.binary_accuracy])\n\nhistory = model_glove.fit(X_train_seq,\n                    y_train,\n                    epochs=20,\n                    batch_size=512)",
            "class": "Model Training",
            "desc": "The code re-prepares the padded sequence data `X_train_seq` and `X_test_seq` for texts, reinitializes and compiles a Keras model `model_glove` with GloVe embeddings, and trains it on the padded sequence training data `X_train_seq` and target variable `y_train` for 20 epochs with a batch size of 512, storing the training history in the variable `history`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.80148226
            },
            "cluster": 0
        }, {
            "cell_id": 43,
            "code": "# Setting up the tokenizer\nvocab_size = 1000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['text']) + list(test_data['text']))\n\n# Word count representation\nX_train_wc = tokenizer.texts_to_matrix(train_data['text'], mode = 'count')\nX_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_test shape: {X_test_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\n# Train Validation Split\nX_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, test_size = 0.2, random_state = 42)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_val shape: {X_val_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\\n\")\n\n# Setting up the model\nmodel = setup_model()\n\n# Fitting the model on un-preprocessed text\nhistory = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))",
            "class": "Model Training",
            "desc": "The code reinitializes a Keras `Tokenizer` for a vocabulary size of 1000, transforms the original (un-preprocessed) text data into word count matrices, splits these matrices into training and validation sets, reinitializes and trains the Keras `model` on this data for 20 epochs with a batch size of 512, storing the training history in the variable `history`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.59386915
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "sns.countplot(train_data['target'])",
            "class": "Visualization",
            "desc": "The code generates a count plot of the 'target' column in the 'train_data' DataFrame using the Seaborn 'countplot' function.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.99602413
            },
            "cluster": 2
        }, {
            "cell_id": 15,
            "code": "import matplotlib.pyplot as plt\n\ndef plot_history(history): \n\n    history_dict = history.history\n    history_dict.keys()\n\n\n    acc = history.history['binary_accuracy']\n    val_acc = history.history['val_binary_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    # \"bo\" is for \"blue dot\"\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    # b is for \"solid blue line\"\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()\n    \nplot_history(history)",
            "class": "Visualization",
            "desc": "The code defines a function `plot_history` that plots the training and validation loss over epochs using `matplotlib.pyplot`, and then calls this function to visualize the history of the model training stored in `history`.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9967168
            },
            "cluster": 1
        }, {
            "cell_id": 21,
            "code": "plot_history(history)",
            "class": "Visualization",
            "desc": "The code calls the previously defined `plot_history` function to visualize the training and validation loss over epochs for the model training history stored in `history`.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9654706
            },
            "cluster": 1
        }, {
            "cell_id": 26,
            "code": "plot_history(history)",
            "class": "Visualization",
            "desc": "The code calls the previously defined `plot_history` function to visualize the training and validation loss over epochs for the model training history stored in `history`.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9654706
            },
            "cluster": 1
        }, {
            "cell_id": 32,
            "code": "plot_history(history)",
            "class": "Visualization",
            "desc": "The code calls the previously defined `plot_history` function to visualize the training and validation loss over epochs for the model training history stored in `history`.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9654706
            },
            "cluster": 1
        }, {
            "cell_id": 33,
            "code": "plt.hist(list(train_data['prep_text'].str.split().map(lambda x: len(x))))",
            "class": "Visualization",
            "desc": "The code generates a histogram to visualize the distribution of the number of words per tweet in the 'prep_text' column of the 'train_data' DataFrame using `matplotlib.pyplot`.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9977956
            },
            "cluster": 2
        }, {
            "cell_id": 41,
            "code": "plot_history(history)",
            "class": "Visualization",
            "desc": "The code calls the previously defined `plot_history` function to visualize the training and validation loss over epochs for the GloVe-based model training history stored in `history`.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9654706
            },
            "cluster": 1
        }],
        "notebook_id": 17,
        "notebook_name": "baseline-nlp.ipynb",
        "user": "baseline-nlp.ipynb"
    }, {
        "cells": [{
            "cell_id": 31,
            "code": "def Kaggle_submission(file_name,model,test_data,ids_list):\n\n    #if TARGET in test_data.columns:\n\n    #    test_data.drop([TARGET],axis=1,inplace=True)\n\n    #test_pred=model.predict(test_data)[:,1]\n\n    test_pred=model.predict(test_data)\n\n    predictions = []\n\n    predictions = oc.adjusted_classes(test_pred, 0.5)\n\n\n\n    submit=pd.DataFrame()\n\n    submit['id'] = ids_list\n\n    submit['target'] = predictions\n\n    submit.to_csv(file_name,index=False)\n\n    return submit",
            "class": "Data Export",
            "desc": "This code defines a function `Kaggle_submission` that generates predictions using a given model on test data, adjusts the predictions based on a threshold, and saves the results to a CSV file for submission to Kaggle.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9981369
            },
            "cluster": -1
        }, {
            "cell_id": 45,
            "code": "submit=pd.DataFrame()\n\nsubmit['id'] = test_df['id'].tolist()\n\nsubmit['target'] = test_pred_BERT_int",
            "class": "Data Export",
            "desc": "This code creates a DataFrame for submission with columns 'id' and 'target', where 'id' is taken from the original test dataset and 'target' is filled with the integer-rounded predictions from the BERT model.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.90852714
            },
            "cluster": -1
        }, {
            "cell_id": 46,
            "code": "submit.to_csv('BERT_model_v3.csv',index=False)",
            "class": "Data Export",
            "desc": "This code saves the submission DataFrame to a CSV file named `BERT_model_v3.csv` without including the DataFrame index.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.9991627
            },
            "cluster": -1
        }, {
            "cell_id": 47,
            "code": "submit.head(3)",
            "class": "Data Export",
            "desc": "This code displays the first three rows of the submission DataFrame using the `head()` method from the `pandas` library.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997651
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")",
            "class": "Data Extraction",
            "desc": "This code reads the training and test datasets from CSV files located in the specified paths using the `pandas` library.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99975425
            },
            "cluster": 0
        }, {
            "cell_id": 6,
            "code": "def wordcount(x):\n\n    length = len(str(x).split())\n\n    return length\n\ndef charcount(x):\n\n    s = x.split()\n\n    x = ''.join(s)\n\n    return len(x)\n\n\n\ndef hashtag_count(x):\n\n    l = len([t for t in x.split() if t.startswith('#')])\n\n    return l\n\n\n\ndef mentions_count(x):\n\n    l = len([t for t in x.split() if t.startswith('@')])\n\n    return l\n\n\n\n\n\ntrain_df['char_count'] = train_df['text'].apply(lambda x: charcount(x))\n\ntrain_df['word_count'] = train_df['text'].apply(lambda x: wordcount(x))\n\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda x: hashtag_count(x))\n\ntrain_df['mention_count'] = train_df['text'].apply(lambda x: mentions_count(x))\n\ntrain_df['length']=train_df['text'].apply(len)\n\n\n\ntest_df['char_count'] = test_df['text'].apply(lambda x: charcount(x))\n\ntest_df['word_count'] = test_df['text'].apply(lambda x: wordcount(x))\n\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x: hashtag_count(x))\n\ntest_df['mention_count'] = test_df['text'].apply(lambda x: mentions_count(x))\n\ntest_df['length']=test_df['text'].apply(len)\n\n\n\ntrain_df.head(2)",
            "class": "Data Transform",
            "desc": "This code defines functions to count words, characters, hashtags, and mentions in text data, then applies these functions to create new features in both the training and test datasets using the `pandas` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9946966
            },
            "cluster": 5
        }, {
            "cell_id": 11,
            "code": "# Taken from - Craig Thomas https://www.kaggle.com/craigmthomas/logistic-regression-lightgbm-fe\n\ntrain_df.drop(\n\n    [\n\n        6449, 7034, 3589, 3591, 3597, 3600, 3603, 3604, 3610, 3613, 3614, 119, 106, 115,\n\n        2666, 2679, 1356, 7609, 3382, 1335, 2655, 2674, 1343, 4291, 4303, 1345, 48, 3374,\n\n        7600, 164, 5292, 2352, 4308, 4306, 4310, 1332, 1156, 7610, 2441, 2449, 2454, 2477,\n\n        2452, 2456, 3390, 7611, 6656, 1360, 5771, 4351, 5073, 4601, 5665, 7135, 5720, 5723,\n\n        5734, 1623, 7533, 7537, 7026, 4834, 4631, 3461, 6366, 6373, 6377, 6378, 6392, 2828,\n\n        2841, 1725, 3795, 1251, 7607\n\n    ], inplace=True\n\n)\n\n\n\ntrain_df.drop(\n\n    [\n\n        4290, 4299, 4312, 4221, 4239, 4244, 2830, 2831, 2832, 2833, 4597, 4605, 4618, 4232, 4235, 3240,\n\n        3243, 3248, 3251, 3261, 3266, 4285, 4305, 4313, 1214, 1365, 6614, 6616, 1197, 1331, 4379, 4381,\n\n        4284, 4286, 4292, 4304, 4309, 4318, 610, 624, 630, 634, 3985, 4013, 4019, 1221, 1349, 6091, 6094, \n\n        6103, 6123, 5620, 5641\n\n    ], inplace=True\n\n)",
            "class": "Data Transform",
            "desc": "This code removes specific rows from the training dataset by their index using the `drop` method from the `pandas` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "drop_column",
                "subclass_id": 10,
                "predicted_subclass_probability": 0.9402679
            },
            "cluster": 4
        }, {
            "cell_id": 13,
            "code": "def preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n\n    \n\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n\n    lst_text = text.split()\n\n    if lst_stopwords is not None:\n\n        lst_text = [word for word in lst_text if word not in \n\n                    lst_stopwords]\n\n                \n\n    ## Stemming (remove -ing, -ly, ...)\n\n    if flg_stemm == True:\n\n        ps = nltk.stem.porter.PorterStemmer()\n\n        lst_text = [ps.stem(word) for word in lst_text]\n\n\n\n    if flg_lemm == True:\n\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n\n            \n\n                            \n\n    ## back to string from list\n\n    text = \" \".join(lst_text)\n\n    return text",
            "class": "Data Transform",
            "desc": "This code defines a `preprocess_text` function that cleans and preprocesses text by removing punctuation, converting to lowercase, optionally removing stopwords, and performing stemming or lemmatization using the `nltk` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.9333879
            },
            "cluster": -1
        }, {
            "cell_id": 14,
            "code": "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n\n#lst_stopwords\n",
            "class": "Data Transform",
            "desc": "This code loads a list of English stopwords using the `nltk` library and assigns it to the variable `lst_stopwords`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.9620199
            },
            "cluster": 2
        }, {
            "cell_id": 15,
            "code": "contractions = { \n\n\"ain't\": \"am not\",\n\n\"aren't\": \"are not\",\n\n\"can't\": \"cannot\",\n\n\"can't've\": \"cannot have\",\n\n\"'cause\": \"because\",\n\n\"could've\": \"could have\",\n\n\"couldn't\": \"could not\",\n\n\"couldn't've\": \"could not have\",\n\n\"didn't\": \"did not\",\n\n\"doesn't\": \"does not\",\n\n\"don't\": \"do not\",\n\n\"hadn't\": \"had not\",\n\n\"hadn't've\": \"had not have\",\n\n\"hasn't\": \"has not\",\n\n\"haven't\": \"have not\",\n\n\"he'd\": \"he would\",\n\n\"he'd've\": \"he would have\",\n\n\"he'll\": \"he will\",\n\n\"he'll've\": \"he will have\",\n\n\"he's\": \"he is\",\n\n\"how'd\": \"how did\",\n\n\"how'd'y\": \"how do you\",\n\n\"how'll\": \"how will\",\n\n\"how's\": \"how does\",\n\n\"i'd\": \"i would\",\n\n\"i'd've\": \"i would have\",\n\n\"i'll\": \"i will\",\n\n\"i'll've\": \"i will have\",\n\n\"i'm\": \"i am\",\n\n\"i've\": \"i have\",\n\n\"isn't\": \"is not\",\n\n\"it'd\": \"it would\",\n\n\"it'd've\": \"it would have\",\n\n\"it'll\": \"it will\",\n\n\"it'll've\": \"it will have\",\n\n\"it's\": \"it is\",\n\n\"let's\": \"let us\",\n\n\"ma'am\": \"madam\",\n\n\"mayn't\": \"may not\",\n\n\"might've\": \"might have\",\n\n\"mightn't\": \"might not\",\n\n\"mightn't've\": \"might not have\",\n\n\"must've\": \"must have\",\n\n\"mustn't\": \"must not\",\n\n\"mustn't've\": \"must not have\",\n\n\"needn't\": \"need not\",\n\n\"needn't've\": \"need not have\",\n\n\"o'clock\": \"of the clock\",\n\n\"oughtn't\": \"ought not\",\n\n\"oughtn't've\": \"ought not have\",\n\n\"shan't\": \"shall not\",\n\n\"sha'n't\": \"shall not\",\n\n\"shan't've\": \"shall not have\",\n\n\"she'd\": \"she would\",\n\n\"she'd've\": \"she would have\",\n\n\"she'll\": \"she will\",\n\n\"she'll've\": \"she will have\",\n\n\"she's\": \"she is\",\n\n\"should've\": \"should have\",\n\n\"shouldn't\": \"should not\",\n\n\"shouldn't've\": \"should not have\",\n\n\"so've\": \"so have\",\n\n\"so's\": \"so is\",\n\n\"that'd\": \"that would\",\n\n\"that'd've\": \"that would have\",\n\n\"that's\": \"that is\",\n\n\"there'd\": \"there would\",\n\n\"there'd've\": \"there would have\",\n\n\"there's\": \"there is\",\n\n\"they'd\": \"they would\",\n\n\"they'd've\": \"they would have\",\n\n\"they'll\": \"they will\",\n\n\"they'll've\": \"they will have\",\n\n\"they're\": \"they are\",\n\n\"they've\": \"they have\",\n\n\"to've\": \"to have\",\n\n\"wasn't\": \"was not\",\n\n\" u \": \" you \",\n\n\" ur \": \" your \",\n\n\" n \": \" and \",\n\n\"won't\": \"would not\",\n\n'dis': 'this',\n\n'bak': 'back',\n\n'brng': 'bring'}\n\n\n\ndef cont_to_exp(x):\n\n    if type(x) is str:\n\n        for key in contractions:\n\n            value = contractions[key]\n\n            x = x.replace(key, value)\n\n        return x\n\n    else:\n\n        return x\n\n    \n\ntrain_df['text_clean'] = train_df['text'].apply(lambda x: cont_to_exp(x))\n\ntest_df['text_clean'] = test_df['text'].apply(lambda x: cont_to_exp(x))\n\n\n\n\n\ndef remove_emails(x):\n\n     return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n\n\n\n\n\ndef remove_urls(x):\n\n    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n\n\n\ndef remove_rt(x):\n\n    return re.sub(r'\\brt\\b', '', x).strip()\n\n\n\ndef remove_special_chars(x):\n\n    x = re.sub(r'[^\\w ]+', \"\", x)\n\n    x = ' '.join(x.split())\n\n    return x\n\n\n\n\n\ndef remove_accented_chars(x):\n\n    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n    return x\n\n\n\n\n\n\n\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_emails(x))\n\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_urls(x))\n\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_rt(x))\n\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_special_chars(x))\n\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_accented_chars(x))",
            "class": "Data Transform",
            "desc": "This code defines functions to handle contractions, and remove emails, URLs, retweets, special characters, and accented characters from text, and applies these transformations to clean the text data in both the training and test datasets using the `pandas` and `re` libraries.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.99555004
            },
            "cluster": 5
        }, {
            "cell_id": 16,
            "code": "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: preprocess_text(x, flg_stemm=True, flg_lemm=False, lst_stopwords=lst_stopwords))\n\ntrain_df.head()",
            "class": "Data Transform",
            "desc": "This code further preprocesses the cleaned text in the training dataset by applying the `preprocess_text` function to each text entry with stemming enabled and lemmatization disabled, along with stopwords removal, using the `pandas` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9974553
            },
            "cluster": -1
        }, {
            "cell_id": 17,
            "code": "vec=TfidfVectorizer(max_features = 10000,ngram_range=(1,4))\n\nvec.fit(train_df['text_clean'])",
            "class": "Data Transform",
            "desc": "This code initializes a `TfidfVectorizer` from the `sklearn.feature_extraction.text` module with specific parameters, and fits it to the cleaned text data in the training dataset to generate TF-IDF features.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.5606868
            },
            "cluster": 4
        }, {
            "cell_id": 18,
            "code": "matrix = vec.transform(train_df['text_clean']).toarray()\n\nfeatures = vec.get_feature_names()\n\nmatrix_df = pd.DataFrame(data=matrix, columns=features)\n",
            "class": "Data Transform",
            "desc": "This code transforms the cleaned text data in the training dataset into a TF-IDF matrix, converts it to an array, retrieves the feature names, and stores the resulting matrix as a DataFrame using the `pandas` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.99546874
            },
            "cluster": 1
        }, {
            "cell_id": 21,
            "code": "matrix_df['length']=train_df['length']\n\nmatrix_df['char_count']=train_df['char_count']\n\nmatrix_df['word_count']=train_df['word_count']\n\nmatrix_df['hashtag_count']=train_df['hashtag_count']\n\nmatrix_df['mention_count']=train_df['mention_count']\n\ny=train_df['target']",
            "class": "Data Transform",
            "desc": "This code adds additional features such as text length, character count, word count, hashtag count, and mention count to the TF-IDF matrix DataFrame and assigns the target variable to `y` using the `pandas` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.8321087
            },
            "cluster": 5
        }, {
            "cell_id": 32,
            "code": "test_df[\"text_clean\"]=test_df['text']\n\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_emails(x))\n\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_urls(x))\n\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_rt(x))\n\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_special_chars(x))\n\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_accented_chars(x))\n\n\n\ntest_df[\"text_clean\"] = test_df[\"text\"].apply(lambda x: preprocess_text(x, flg_stemm=True, flg_lemm=False, lst_stopwords=lst_stopwords))\n\ntest_df['length']=test_df['text'].apply(len)\n\n\n\ntest_df.head()\n\n\n\n#vec=TfidfVectorizer(max_features = 20000,ngram_range=(1,4))\n\n#vec.fit(test_df['text_clean'])\n\n\n\n\n\n\n\nmatrix = vec.transform(test_df['text_clean']).toarray()\n\nfeatures = vec.get_feature_names()\n\nmatrix_df = pd.DataFrame(data=matrix, columns=features)\n\n\n\nmatrix_df['length']=test_df['length']\n\nmatrix_df['char_count']=test_df['char_count']\n\nmatrix_df['word_count']=test_df['word_count']\n\nmatrix_df['hashtag_count']=test_df['hashtag_count']\n\nmatrix_df['mention_count']=test_df['mention_count']",
            "class": "Data Transform",
            "desc": "This code preprocesses the text data in the test dataset by cleaning the text, creating new features such as length and counts, transforming the text data into a TF-IDF matrix, and then adding the additional features to this matrix using the `pandas` and `scikit-learn` libraries.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.99814296
            },
            "cluster": 5
        }, {
            "cell_id": 36,
            "code": "#Credit: https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n\ndef bert_encode(texts, tokenizer, max_len=512):\n\n    all_tokens = []\n\n    all_masks = []\n\n    all_segments = []\n\n    \n\n    for text in texts:\n\n        text = tokenizer.tokenize(text)\n\n            \n\n        text = text[:max_len-2]\n\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n\n        pad_len = max_len - len(input_sequence)\n\n        \n\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n\n        tokens += [0] * pad_len\n\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n\n        segment_ids = [0] * max_len\n\n        \n\n        all_tokens.append(tokens)\n\n        all_masks.append(pad_masks)\n\n        all_segments.append(segment_ids)\n\n    \n\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)",
            "class": "Data Transform",
            "desc": "This code defines a `bert_encode` function that tokenizes input texts using a BERT tokenizer, truncates or pads the sequences to a specified maximum length, and returns arrays of tokens, masks, and segment IDs for input to a BERT model using the `numpy` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9645982
            },
            "cluster": 1
        }, {
            "cell_id": 39,
            "code": "# Load tokenizer from the bert layer\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)",
            "class": "Data Transform",
            "desc": "This code loads the BERT tokenizer configuration, including the vocabulary file and the setting for case sensitivity, and initializes the `FullTokenizer` from the `tokenization` module using these configurations.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.4494561
            },
            "cluster": 4
        }, {
            "cell_id": 40,
            "code": "# Encode the text into tokens, masks, and segment flags\n\ntrain_input = bert_encode(train_df.text_clean.values, tokenizer, max_len=160)\n\ntest_input = bert_encode(test_df.text_clean.values, tokenizer, max_len=160)\n\ntrain_labels = train_df.target.values",
            "class": "Data Transform",
            "desc": "This code encodes the cleaned text data from both the training and test datasets into tokens, masks, and segment flags using the `bert_encode` function and the initialized BERT tokenizer, and also extracts the target values from the training dataset.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9929838
            },
            "cluster": 1
        }, {
            "cell_id": 3,
            "code": "train_df.head(5)",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first five rows of the training dataset using the `head()` method from the `pandas` library.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997615
            },
            "cluster": 2
        }, {
            "cell_id": 4,
            "code": "# DataFrane Summary by pandas summary package (extension of pandas.describe method) \n\ndfs = DataFrameSummary(train_df)\n\ndfs.summary()",
            "class": "Exploratory Data Analysis",
            "desc": "This code generates a summary of the training dataset using the `DataFrameSummary` class from the `pandas-summary` package, which provides an extended version of the `pandas.describe` method.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "create_dataframe",
                "subclass_id": 12,
                "predicted_subclass_probability": 0.988304
            },
            "cluster": 4
        }, {
            "cell_id": 10,
            "code": "duplicates = pd.concat(x for _, x in train_df.groupby([\"text\"]) if len(x) > 1)\n\n\n\n#with pd.option_context(\"display.max_rows\", None, \"max_colwidth\", 80):\n\n#    display(duplicates[[\"id\", \"target\", \"text\"]])",
            "class": "Exploratory Data Analysis",
            "desc": "This code identifies duplicate entries in the training dataset based on the 'text' column and stores these duplicates in a new DataFrame using the `pandas` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.87380403
            },
            "cluster": 1
        }, {
            "cell_id": 19,
            "code": "matrix_df.head(2)",
            "class": "Exploratory Data Analysis",
            "desc": "This code displays the first two rows of the TF-IDF matrix DataFrame using the `head()` method from the `pandas` library.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9997633
            },
            "cluster": 2
        }, {
            "cell_id": 20,
            "code": "matrix_df.shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the dimensions of the TF-IDF matrix DataFrame using the `shape` attribute from the `pandas` library.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9996574
            },
            "cluster": 2
        }, {
            "cell_id": 29,
            "code": "top_features=feature_imp_list.sort_values(by='Value', ascending=False).head(20)\n\ntop_features",
            "class": "Exploratory Data Analysis",
            "desc": "This code sorts the feature importance list in descending order based on their importance values and displays the top 20 features using the `pandas` library.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "sort_values",
                "subclass_id": 9,
                "predicted_subclass_probability": 0.992605
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "# Octopus ML pakage - github.com/gershonc/octopus-ml\n\n!pip install octopus-ml",
            "class": "Imports and Environment",
            "desc": "This code installs the `octopus-ml` package from GitHub using the `pip` package manager.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.99379325
            },
            "cluster": 1
        }, {
            "cell_id": 1,
            "code": "import warnings\n\nwarnings.simplefilter(\"ignore\")\n\nimport seaborn as sns \n\nimport matplotlib.pyplot as plt\n\nimport time\n\nimport pandas as pd\n\nimport numpy as np\n\nimport lightgbm as lgb\n\nimport tracemalloc\n\nfrom pandas_summary import DataFrameSummary\n\nfrom sklearn.metrics import classification_report\n\n\n\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\n\n\n%matplotlib inline\n\nsns.set_style(\"whitegrid\")\n\n\n\npd.set_option('display.max_columns', None)  # or 1000\n\npd.set_option('display.max_rows', None)  # or 1000\n\npd.set_option('display.max_colwidth', -1)  # or 199\n\n\n\n#check out https://github.com/gershonc/octopus-ml\n\nimport octopus_ml as oc\n\n\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split",
            "class": "Imports and Environment",
            "desc": "This code imports a variety of Python libraries and sets up the environment, including standard libraries like `warnings`, `pandas`, and `numpy`, visualizations with `seaborn` and `matplotlib`, machine learning with `lightgbm`, `sklearn`, and `octopus_ml`, and configurations for display settings with `pandas`.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9992041
            },
            "cluster": 0
        }, {
            "cell_id": 12,
            "code": "## for data\n\nimport json\n\nimport pandas as pd\n\nimport numpy as np\n\n## for plotting\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n## for processing\n\nimport re\n\nimport nltk\n\n## for bag-of-words\n\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n\n## for explainer\n\nfrom lime import lime_text\n\n## for word embedding\n\nimport gensim\n\nimport gensim.downloader as gensim_api\n\n## for deep learning\n\nfrom tensorflow.keras import models, layers, preprocessing as kprocessing\n\nfrom tensorflow.keras import backend as K\n\n## for bert language model\n\nimport transformers\n\nimport unicodedata",
            "class": "Imports and Environment",
            "desc": "This code imports additional libraries and modules essential for data processing, plotting, machine learning, deep learning, and natural language processing tasks using libraries including `nltk`, `gensim`, `tensorflow`, and `transformers`.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993574
            },
            "cluster": 0
        }, {
            "cell_id": 34,
            "code": "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py",
            "class": "Imports and Environment",
            "desc": "This code downloads the `tokenization.py` script from the TensorFlow models repository on GitHub using the `wget` command.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_url",
                "subclass_id": 42,
                "predicted_subclass_probability": 0.8866123
            },
            "cluster": 1
        }, {
            "cell_id": 35,
            "code": "import tensorflow as tf\n\nfrom tensorflow.keras.layers import Dense, Input\n\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport tensorflow_hub as hub\n\n\n\nimport tokenization",
            "class": "Imports and Environment",
            "desc": "This code imports essential modules and functions from the `tensorflow` library, including layers, optimizers, models, and callbacks, as well as the `tensorflow_hub` library and the `tokenization.py` script.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99932015
            },
            "cluster": -1
        }, {
            "cell_id": 38,
            "code": "# Load BERT from the Tensorflow Hub\n\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n\nbert_layer = hub.KerasLayer(module_url, trainable=True)",
            "class": "Imports and Environment",
            "desc": "This code loads a BERT model from TensorFlow Hub using the `KerasLayer` class, making the BERT layers trainable during model training.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.87859446
            },
            "cluster": 1
        }, {
            "cell_id": 23,
            "code": "oc.cv_plot(metrics['f1_weighted'],metrics['f1_macro'],metrics['f1_positive'],'Titanic Kaggle competition')",
            "class": "Model Evaluation",
            "desc": "This code plots the F1 scores (weighted, macro, and positive) obtained from cross-validation using the `octopus_ml` library's `cv_plot` function, with the title \"Titanic Kaggle competition\".",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.64448214
            },
            "cluster": 2
        }, {
            "cell_id": 24,
            "code": "print(classification_report(metrics['y'], metrics['predictions_folds']))",
            "class": "Model Evaluation",
            "desc": "This code prints the classification report, including precision, recall, and F1-score, for the model's predictions using the `classification_report` function from the `sklearn.metrics` module.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9977271
            },
            "cluster": 1
        }, {
            "cell_id": 25,
            "code": "oc.roc_curve_plot(metrics['y'], metrics['predictions_proba'])",
            "class": "Model Evaluation",
            "desc": "This code plots the ROC curve and calculates the Area Under the Curve (AUC) for the model's predictions using the `roc_curve_plot` function from the `octopus_ml` library.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.67355084
            },
            "cluster": 1
        }, {
            "cell_id": 26,
            "code": "oc.confusion_matrix_plot(metrics['y'], metrics['predictions_folds'])",
            "class": "Model Evaluation",
            "desc": "This code generates and plots the confusion matrix for the model's predictions using the `confusion_matrix_plot` function from the `octopus_ml` library.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.65878206
            },
            "cluster": 1
        }, {
            "cell_id": 27,
            "code": "feature_imp_list=oc.plot_imp(metrics['final_clf'],matrix_df,'LightGBM Mortality Kaggle',num=40)",
            "class": "Model Evaluation",
            "desc": "This code generates and plots the importance of the top 40 features in the LightGBM model using the `plot_imp` function from the `octopus_ml` library.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9963425
            },
            "cluster": 1
        }, {
            "cell_id": 28,
            "code": "oc.preds_distribution(metrics['y'], metrics['predictions_proba'], bins=40)",
            "class": "Model Evaluation",
            "desc": "This code plots the distribution of the model's prediction probabilities using the `preds_distribution` function from the `octopus_ml` library.",
            "testing": {
                "class": "Model_Train",
                "subclass": "compute_train_metric",
                "subclass_id": 28,
                "predicted_subclass_probability": 0.558016
            },
            "cluster": 1
        }, {
            "cell_id": 30,
            "code": "list_for_correlations=top_features['Feature'].to_list()\n\nlist_for_correlations.append('target')\n\noc.correlations(matrix_df,list_for_correlations)",
            "class": "Model Evaluation",
            "desc": "This code appends the target variable to the list of top features and then computes and plots the correlations among these features using the `correlations` function from the `octopus_ml` library.",
            "testing": {
                "class": "Visualization",
                "subclass": "model_coefficients",
                "subclass_id": 79,
                "predicted_subclass_probability": 0.9603935
            },
            "cluster": 1
        }, {
            "cell_id": 33,
            "code": "test_pred=metrics['final_clf'].predict(matrix_df)\n\npredictions = []\n\n#predictions = oc.adjusted_classes(test_pred, 0.5)",
            "class": "Model Evaluation",
            "desc": "This code uses the final trained classifier from the model evaluation metrics to predict outcomes on the test dataset's feature matrix.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.9896577
            },
            "cluster": 3
        }, {
            "cell_id": 43,
            "code": "model_BERT.load_weights('model_BERT.h5')\n\ntest_pred_BERT = model_BERT.predict(test_input)\n\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')",
            "class": "Model Evaluation",
            "desc": "This code loads the best weights into the BERT model using the `load_weights` method and then generates predictions on the encoded test data, rounding the predictions to integer values for binary classification.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.991269
            },
            "cluster": 3
        }, {
            "cell_id": 44,
            "code": "train_pred_BERT = model_BERT.predict(train_input)\n\ntrain_pred_BERT_int = train_pred_BERT.round().astype('int')",
            "class": "Model Evaluation",
            "desc": "This code generates predictions on the encoded training data using the BERT model and rounds the predictions to integer values for binary classification.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.990164
            },
            "cluster": 3
        }, {
            "cell_id": 22,
            "code": "params = {\n\n        'boosting_type': 'gbdt',\n\n        'objective': 'binary',\n\n        'metric': 'auc',\n\n        'learning_rate': 0.01,\n\n        'num_leaves':32,\n\n        'subsample': 1,\n\n        #'colsample_bytree': 0.25,\n\n        #'reg_alpha': 0,\n\n        #'reg_lambda': 1,\n\n        #'scale_pos_weight': 5,\n\n        'n_estimators': 10000,\n\n        'verbose': -1,\n\n        'max_depth': -1,\n\n        'seed':100, \n\n        'colsample_bytree':0.4,\n\n        'force_col_wise': True\n\n\n\n\n\n}\n\n\"\"\"\n\n    boosting_type='gbdt', class_weight=None, colsample_bytree=0.4,\n\n               importance_type='split', learning_rate=0.04, max_depth=-1,\n\n               metric='auc', min_child_samples=20, min_child_weight=0.001,\n\n               min_split_gain=0.0, n_estimators=1500, n_jobs=-1, num_leaves=31,\n\n               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n\n               silent=True, subsample=1.0, subsample_for_bin=200000,\n\n               subsample_freq=0 \n\n\"\"\"\n\nmetrics = oc.cv_adv(matrix_df,y,0.5,2000,shuffle=True,params=params)",
            "class": "Model Training",
            "desc": "This code defines a set of hyperparameters for a LightGBM model and performs cross-validation using the `cv_adv` function from the `octopus_ml` library on the TF-IDF matrix DataFrame and target variable `y`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "init_hyperparams",
                "subclass_id": 59,
                "predicted_subclass_probability": 0.99348336
            },
            "cluster": 0
        }, {
            "cell_id": 37,
            "code": "def build_model(bert_layer, max_len=512):\n\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n\n    clf_output = sequence_output[:, 0, :]\n\n    \n\n    if Dropout_num == 0:\n\n        # Without Dropout\n\n        out = Dense(1, activation='sigmoid')(clf_output)\n\n    else:\n\n        # With Dropout(Dropout_num), Dropout_num > 0\n\n        x = Dropout(Dropout_num)(clf_output)\n\n        out = Dense(1, activation='sigmoid')(x)\n\n\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n\n    \n\n    return model",
            "class": "Model Training",
            "desc": "This code defines a function `build_model` that constructs a BERT-based classification model using TensorFlow's Keras API, including input layers for word IDs, masks, and segment IDs, a BERT layer, an optional dropout layer, and a final dense layer for binary classification with a sigmoid activation function.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9445593
            },
            "cluster": 2
        }, {
            "cell_id": 41,
            "code": "random_state_split = 2\n\nDropout_num = 0\n\nlearning_rate = 6e-6\n\nvalid = 0.2\n\nepochs_num = 3\n\nbatch_size_num = 16\n\ntarget_corrected = False\n\ntarget_big_corrected = False\n\n\n\n# Build BERT model with my tuning\n\nmodel_BERT = build_model(bert_layer, max_len=160)\n\nmodel_BERT.summary()",
            "class": "Model Training",
            "desc": "This code sets various hyperparameters and configurations, then builds a BERT-based classification model with a maximum sequence length of 160 tokens using the `build_model` function and prints a summary of the model architecture with the `summary()` method from TensorFlow's Keras API.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.66016126
            },
            "cluster": 0
        }, {
            "cell_id": 42,
            "code": "checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n\n\n\ntrain_history = model_BERT.fit(\n\n    train_input, train_labels,\n\n    validation_split = valid,\n\n    epochs = epochs_num, # recomended 3-5 epochs\n\n    callbacks=[checkpoint],\n\n    batch_size = batch_size_num\n\n)",
            "class": "Model Training",
            "desc": "This code trains the BERT-based classification model using the training data, with validation split, specified number of epochs, and batch size while saving the best model based on validation loss using a `ModelCheckpoint` callback.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.99751294
            },
            "cluster": -1
        }, {
            "cell_id": 5,
            "code": "# Target distribution analysis\n\nfig, ax =plt.subplots(1,2)\n\n\n\n\n\nplt.style.use('fivethirtyeight')\n\nplt.figure(figsize=(3,4))\n\nsns.set_context(\"paper\", font_scale=1.2)                                                  \n\nsns.countplot('target',data=train_df, ax=ax[0])\n\ntrain_df['target'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.2f%%',ax=ax[1])\n\nfig.show()",
            "class": "Visualization",
            "desc": "This code creates a target distribution analysis by generating a count plot and a pie chart of the target variable from the training dataset using `seaborn` and `matplotlib` libraries.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9649334
            },
            "cluster": 2
        }, {
            "cell_id": 7,
            "code": "sns.displot(data = train_df, kind = 'hist', x = 'length', hue = 'target', multiple = 'stack',bins=50,height = 5, aspect = 1.9)\n\n\n\n# The distibution of tweet text length vs target - there is a correlation between tweet length and target ",
            "class": "Visualization",
            "desc": "This code creates a stacked histogram to visualize the distribution of tweet text lengths versus the target variable using the `displot` function from the `seaborn` library.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9802619
            },
            "cluster": 2
        }, {
            "cell_id": 8,
            "code": "sns.displot(data = train_df, kind = 'hist', x = 'hashtag_count', hue = 'target', multiple = 'stack',bins=50,height = 5, aspect = 1.9)",
            "class": "Visualization",
            "desc": "This code creates a stacked histogram to visualize the distribution of the hashtag count versus the target variable in the training dataset using the `displot` function from the `seaborn` library.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9985002
            },
            "cluster": 2
        }, {
            "cell_id": 9,
            "code": "sns.displot(data = train_df, kind = 'hist', x = 'word_count', hue = 'target', multiple = 'stack',bins=50,height = 5, aspect = 1.9)\n",
            "class": "Visualization",
            "desc": "This code creates a stacked histogram to visualize the distribution of word count versus the target variable in the training dataset using the `displot` function from the `seaborn` library.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9985385
            },
            "cluster": 2
        }],
        "notebook_id": 18,
        "notebook_name": "nlp-twitter-tuned-lgbm-model-tfidf-bert.ipynb",
        "user": "nlp-twitter-tuned-lgbm-model-tfidf-bert.ipynb"
    }, {
        "cells": [{
            "cell_id": 65,
            "code": "# submit\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = np.round(test_pred).astype('int')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.groupby('target').count()",
            "class": "Data Export",
            "desc": "This code creates a submission DataFrame by assigning the rounded predicted values as target labels, saves it as a CSV file named `submission.csv`, and displays the count of entries by target labels to verify the distribution.",
            "testing": {
                "class": "Data_Export",
                "subclass": "save_to_csv",
                "subclass_id": 25,
                "predicted_subclass_probability": 0.99921453
            },
            "cluster": -1
        }, {
            "cell_id": 1,
            "code": "train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\nsubmit_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')",
            "class": "Data Extraction",
            "desc": "This code loads the training and test datasets from CSV files into pandas DataFrames named `train_data` and `submit_data`, respectively.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.9997477
            },
            "cluster": 1
        }, {
            "cell_id": 26,
            "code": "import requests\nurl = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\nfilename = url.split('/')[-1]\nr = requests.get(url)\nwith open(filename, \"wb\") as file:\n        file.write(r.content)\n        \n!ls",
            "class": "Data Extraction",
            "desc": "This code downloads the pre-trained GoogleNews word vectors from a specified URL and saves the file locally, then lists the files in the current directory using the `ls` command.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "list_files",
                "subclass_id": 88,
                "predicted_subclass_probability": 0.48958522
            },
            "cluster": 0
        }, {
            "cell_id": 31,
            "code": "import gensim\nword2vec_path='./GoogleNews-vectors-negative300.bin.gz'\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)",
            "class": "Data Extraction",
            "desc": "This code loads the pre-trained GoogleNews word vectors into a Gensim KeyedVectors model from a binary file.",
            "testing": {
                "class": "Model_Train",
                "subclass": "load_pretrained",
                "subclass_id": 30,
                "predicted_subclass_probability": 0.99531686
            },
            "cluster": 1
        }, {
            "cell_id": 58,
            "code": "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py",
            "class": "Data Extraction",
            "desc": "This code downloads the `tokenization.py` script from the specified GitHub repository to the local directory using `wget`.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_url",
                "subclass_id": 42,
                "predicted_subclass_probability": 0.8866123
            },
            "cluster": 0
        }, {
            "cell_id": 61,
            "code": "%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)",
            "class": "Data Extraction",
            "desc": "This code downloads the BERT base uncased model from TensorFlow Hub and initializes it as a KerasLayer that is trainable, while also measuring the time taken for this operation.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.856542
            },
            "cluster": 1
        }, {
            "cell_id": 62,
            "code": "# read and encode train data\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\ntrain_input = bert_encode(train.text.values, bert_layer, max_len=128)\ntrain_labels = np.array(train.target)",
            "class": "Data Extraction",
            "desc": "This code reads the training data from a CSV file into a pandas DataFrame, then encodes the text for BERT input and converts the target labels to a numpy array.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.99927765
            },
            "cluster": 0
        }, {
            "cell_id": 9,
            "code": "data = pd.concat([train_data, submit_data])\ndata.shape",
            "class": "Data Transform",
            "desc": "This code concatenates the `train_data` and `submit_data` DataFrames along the rows and then outputs the shape of the combined DataFrame using pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "concatenate",
                "subclass_id": 11,
                "predicted_subclass_probability": 0.9939737
            },
            "cluster": 3
        }, {
            "cell_id": 10,
            "code": "data['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'https?\\S+'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'[\\//:,.!?@&\\-\\'\\`\\\"\\_\\n\\#]'), ' ', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'<.*?>'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  \n                           u\"\\U0001F300-\\U0001F5FF\"  \n                           u\"\\U0001F680-\\U0001F6FF\"  \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'\\d'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'[^\\w]'), ' ', x))\ndata['text'] = data['text'].str.lower()",
            "class": "Data Transform",
            "desc": "This code applies a series of regular expression substitutions to the 'text' column of the combined `data` DataFrame, including removing URLs, special characters, HTML tags, emojis, digits, and converting text to lowercase using pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9965412
            },
            "cluster": 5
        }, {
            "cell_id": 11,
            "code": "'''\ntext_series = data.loc[:,'text']\nfor i in range(len(text_series)):\n    content = text_series.iloc[i]\n    textblob = TextBlob(content)\n    text_series.iloc[i] = textblob.correct()\n'''",
            "class": "Data Transform",
            "desc": "This commented-out code is intended to correct spelling errors in the 'text' column of the combined `data` DataFrame by iterating through each entry and applying TextBlob's `correct` method.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.9023682
            },
            "cluster": 4
        }, {
            "cell_id": 12,
            "code": "clean_train = data[0:train_data.shape[0]]\nclean_submit = data[train_data.shape[0]:-1]\n\nX_train, X_test, y_train, y_test = train_test_split(clean_train['text'], clean_train['target'],\n                                                   test_size = 0.2, random_state = 4)",
            "class": "Data Transform",
            "desc": "This code splits the combined `data` DataFrame back into `clean_train` and `clean_submit` DataFrames, and then further splits `clean_train` into training and testing sets for features ('text') and target ('target') using scikit-learn's `train_test_split` function.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9978915
            },
            "cluster": 3
        }, {
            "cell_id": 13,
            "code": "def tfidf(words):\n    tfidf_vectorizer = TfidfVectorizer()\n    data_feature = tfidf_vectorizer.fit_transform(words)\n    return data_feature, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train.tolist())\nX_test_tfidf = tfidf_vectorizer.transform(X_test.tolist())",
            "class": "Data Transform",
            "desc": "This code defines a function `tfidf` to vectorize text data using TF-IDF, applies it to the training data to obtain `X_train_tfidf` and the fitted vectorizer, and then transforms the test data to obtain `X_test_tfidf` using scikit-learn's `TfidfVectorizer`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.9976406
            },
            "cluster": 3
        }, {
            "cell_id": 20,
            "code": "# fail to sort and plot the top 10 most important features in disaster and non-disaster text\n'''\nindex_to_word = [(v,k) for k,v in tfidf_vectorizer.vocabulary_.items()]\nsorted(index_to_word, key=lambda x: x[0], reverse=True)\n'''",
            "class": "Data Transform",
            "desc": "This commented-out code attempts to create a list of (index, word) tuples from the TF-IDF vectorizer's vocabulary, and then sort it in descending order by index.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.8911329
            },
            "cluster": -1
        }, {
            "cell_id": 27,
            "code": "stop_words = stopwords.words('english')\nfor word in ['us','no','yet']:\n    stop_words.append(word)\n\ndata_list = []\ntext_series = data['text']\nfor i in range(len(text_series)):\n    content = text_series.iloc[i]\n    cutwords = [word for word in content.split(' ') if word not in  stop_words if len(word) != 0]\n    data_list.append(cutwords)",
            "class": "Data Transform",
            "desc": "This code generates a list of tokenized and stopword-filtered text entries by removing English stopwords (plus 'us', 'no', and 'yet') from the 'text' column of the `data` DataFrame.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "string_transform",
                "subclass_id": 78,
                "predicted_subclass_probability": 0.9944119
            },
            "cluster": -1
        }, {
            "cell_id": 28,
            "code": "for i in range(len(data_list)):\n    content = data_list[i]\n    if len(content) <1:\n        print(i)",
            "class": "Data Transform",
            "desc": "This code iterates through the `data_list` and prints the indices of any entries that have an empty list after stopword filtering, to identify documents that may have no remaining words.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.8840107
            },
            "cluster": 1
        }, {
            "cell_id": 33,
            "code": "def get_textVector(data_list, word2vec, textsVectors_list):\n    for i in range(len(data_list)):\n        words_perText = data_list[i]\n        if len(words_perText) < 1:\n            words_vector = [np.zeros(300)]\n        else:\n            words_vector = [word2vec.wv[k]  if k in word2vec_model else  np.zeros(300) for k in words_perText]\n        text_vector = np.array(words_vector).mean(axis=0)\n        textsVectors_list.append(text_vector)\n    return textsVectors_list",
            "class": "Data Transform",
            "desc": "This code defines a function `get_textVector` that computes average word vectors for each text entry in `data_list` by retrieving vectors from the given `word2vec` model, and returns a list of these text vectors, filling in with zero vectors when words are not found.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "feature_engineering",
                "subclass_id": 8,
                "predicted_subclass_probability": 0.966133
            },
            "cluster": -1
        }, {
            "cell_id": 34,
            "code": "textsVectors_list = []\nget_textVector(data_list, word2vec_model, textsVectors_list)\nX = np.array(textsVectors_list)",
            "class": "Data Transform",
            "desc": "This code initializes an empty list `textsVectors_list`, computes the average word vectors for each text entry using the `get_textVector` function, and converts the resulting list into a numpy array `X`.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.91989833
            },
            "cluster": -1
        }, {
            "cell_id": 36,
            "code": "word2vec_X = X[0:train_data.shape[0]]\ny = data['target'][0:train_data.shape[0]]\nword2vec_submit = X[train_data.shape[0]:-1]\n\nX_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(word2vec_X, y,\n                                                   test_size = 0.2, random_state = 4)",
            "class": "Data Transform",
            "desc": "This code splits the numpy array `X` into training feature vectors (`word2vec_X`) and submission feature vectors (`word2vec_submit`), then further splits `word2vec_X` and the target values `y` into training and testing sets using scikit-learn's `train_test_split` function.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.9981791
            },
            "cluster": 3
        }, {
            "cell_id": 44,
            "code": "tokenizer = Tokenizer()\ntokenizer.fit_on_texts(data_list)\nsequences = tokenizer.texts_to_sequences(data_list)\nword_index = tokenizer.word_index\ncnn_data = pad_sequences(sequences, maxlen = max_sequence_length)\ncnn_label = to_categorical(np.asarray(train_data['target']))\nprint('len of word_index:', len(word_index))\nprint('shape of data tensor:', cnn_data.shape)\nprint('shape of label tensoe:', cnn_label.shape)",
            "class": "Data Transform",
            "desc": "This code initializes a `Tokenizer`, fits it on the `data_list`, converts the texts to sequences of integers, pads these sequences to a maximum length of 26, and transforms target labels into one-hot encoded vectors, then prints the length of the word index, and shapes of the data and label tensors using Keras.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.6572918
            },
            "cluster": -1
        }, {
            "cell_id": 45,
            "code": "trainCNN_data = cnn_data[0:train_data.shape[0]]\nX_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(trainCNN_data, cnn_label,\n                                                   test_size = 0.2, random_state = 4)\nX_cnn, X_val_cnn, y_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train_cnn,\n                                                   test_size = 0.2, random_state = 4)",
            "class": "Data Transform",
            "desc": "This code extracts training data from `cnn_data`, splits it into training and testing sets (`X_train_cnn`, `X_test_cnn`, `y_train_cnn`, `y_test_cnn`), and further splits the training set into train and validation sets (`X_cnn`, `X_val_cnn`, `y_cnn`, `y_val_cnn`) using scikit-learn's `train_test_split` function.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "split",
                "subclass_id": 13,
                "predicted_subclass_probability": 0.99836665
            },
            "cluster": -1
        }, {
            "cell_id": 50,
            "code": "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\nfor word, i in word_index.items(): \n    if word in word2vec_model:\n        embedding_matrix[i] = np.asarray(word2vec_model.wv[word])",
            "class": "Data Transform",
            "desc": "This code initializes an embedding matrix of zeros and populates it with word vectors from the `word2vec_model` for words present in the `word_index`, ensuring each index corresponds to the correct word vector as a numpy array.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "prepare_x_and_y",
                "subclass_id": 21,
                "predicted_subclass_probability": 0.739272
            },
            "cluster": -1
        }, {
            "cell_id": 2,
            "code": "train_data[train_data['text'].isna()]",
            "class": "Exploratory Data Analysis",
            "desc": "This code filters and displays rows from `train_data` where the 'text' column contains missing values using pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.88515306
            },
            "cluster": 3
        }, {
            "cell_id": 3,
            "code": "train_data.info()",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs a summary of the `train_data` DataFrame, including the number of non-null entries, data types, and memory usage using pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table_attributes",
                "subclass_id": 40,
                "predicted_subclass_probability": 0.99936634
            },
            "cluster": 1
        }, {
            "cell_id": 4,
            "code": "train_data.groupby('target').count()",
            "class": "Exploratory Data Analysis",
            "desc": "This code groups the `train_data` DataFrame by the 'target' column and counts the number of entries in each group using pandas.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "groupby",
                "subclass_id": 60,
                "predicted_subclass_probability": 0.9970409
            },
            "cluster": 3
        }, {
            "cell_id": 14,
            "code": "X_train_tfidf.shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the shape of the `X_train_tfidf` matrix to understand the dimensions of the TF-IDF transformed training dataset using pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9994598
            },
            "cluster": 3
        }, {
            "cell_id": 29,
            "code": "data_list[7626]",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the tokenized and stopword-filtered text entry at index 7626 in the `data_list` to examine its content.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_table",
                "subclass_id": 41,
                "predicted_subclass_probability": 0.9993057
            },
            "cluster": 3
        }, {
            "cell_id": 32,
            "code": "word2vec_model.wv['earthquake'].shape",
            "class": "Exploratory Data Analysis",
            "desc": "This code outputs the shape of the word vector for the word 'earthquake' from the `word2vec_model` to check its dimensionality using Gensim.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9989441
            },
            "cluster": 3
        }, {
            "cell_id": 35,
            "code": "pd.isnull(X).any()",
            "class": "Exploratory Data Analysis",
            "desc": "This code checks for the presence of any null values in the numpy array `X` and returns a boolean array indicating where nulls are present, using pandas.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "count_missing_values",
                "subclass_id": 39,
                "predicted_subclass_probability": 0.9978543
            },
            "cluster": 1
        }, {
            "cell_id": 37,
            "code": "print(X_train_word2vec.shape, y_train_word2vec.shape)",
            "class": "Exploratory Data Analysis",
            "desc": "This code prints the shapes of the training feature vectors array (`X_train_word2vec`) and the training labels array (`y_train_word2vec`) to verify their dimensions.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "show_shape",
                "subclass_id": 58,
                "predicted_subclass_probability": 0.9980696
            },
            "cluster": 4
        }, {
            "cell_id": 42,
            "code": "lenlen = []\nfor i in range(len(data_list)):\n    content = data_list[i]\n    perlen = len(content)\n    lenlen.append(perlen)\nprint(max(lenlen))",
            "class": "Exploratory Data Analysis",
            "desc": "This code computes the number of words in each entry of the tokenized `data_list`, stores these lengths in `lenlen`, and prints the maximum number of words in a single entry.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.7533818
            },
            "cluster": 1
        }, {
            "cell_id": 0,
            "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom textblob import TextBlob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom gensim.models import Word2Vec\nfrom gensim.models.keyedvectors import KeyedVectors\nimport time\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Sequential\nfrom keras import losses\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model",
            "class": "Imports and Environment",
            "desc": "This code imports libraries and modules required for data manipulation (pandas, numpy), visualization (matplotlib, seaborn), natural language processing (nltk, textblob), machine learning (scikit-learn), deep learning (tensorflow, keras), and word embeddings (gensim).",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9993228
            },
            "cluster": 0
        }, {
            "cell_id": 5,
            "code": "%matplotlib inline",
            "class": "Imports and Environment",
            "desc": "This line configures the Jupyter Notebook to display matplotlib plots inline within the notebook itself.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "set_options",
                "subclass_id": 23,
                "predicted_subclass_probability": 0.9991172
            },
            "cluster": -1
        }, {
            "cell_id": 25,
            "code": "!pip install gensim -i http://pypi.douban.com/simple --trusted-host pypi.douban.com",
            "class": "Imports and Environment",
            "desc": "This code installs the `gensim` library from a specific PyPI mirror using pip.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "install_modules",
                "subclass_id": 87,
                "predicted_subclass_probability": 0.9962657
            },
            "cluster": 1
        }, {
            "cell_id": 43,
            "code": "max_sequence_length = 26\nembedding_dim = 300",
            "class": "Imports and Environment",
            "desc": "This code sets the `max_sequence_length` to 26 and the `embedding_dim` to 300, likely for use in preparing sequences and embeddings for a deep learning model.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.9975586
            },
            "cluster": -1
        }, {
            "cell_id": 56,
            "code": "tf.__version__",
            "class": "Imports and Environment",
            "desc": "This code outputs the version of the TensorFlow library currently being used, which is obtained by accessing `tf.__version__`.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.9983907
            },
            "cluster": -1
        }, {
            "cell_id": 57,
            "code": "import tensorflow_hub as hub\nhub.__version__",
            "class": "Imports and Environment",
            "desc": "This code imports the TensorFlow Hub library as `hub` and outputs its version by accessing `hub.__version__`.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99918145
            },
            "cluster": -1
        }, {
            "cell_id": 59,
            "code": "import tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization",
            "class": "Imports and Environment",
            "desc": "This code imports TensorFlow, TensorFlow Hub, the ModelCheckpoint callback from tensorflow.keras, and the `tokenization` module downloaded earlier.",
            "testing": {
                "class": "Imports_and_Environment",
                "subclass": "import_modules",
                "subclass_id": 22,
                "predicted_subclass_probability": 0.99932754
            },
            "cluster": 1
        }, {
            "cell_id": 16,
            "code": "def score_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted)\n    recall = recall_score(y_test, y_predicted)\n    print(\"accuracy = %0.3f, precision = %0.3f, recall = %0.3f\" % (accuracy, precision, recall))",
            "class": "Model Evaluation",
            "desc": "This code defines a function `score_metrics` that calculates and prints accuracy, precision, and recall metrics for the given true labels and predicted labels using scikit-learn's scoring methods.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9981325
            },
            "cluster": 2
        }, {
            "cell_id": 17,
            "code": "score_metrics(y_test, y_predicted_lr)",
            "class": "Model Evaluation",
            "desc": "This code calls the `score_metrics` function to compute and display the accuracy, precision, and recall of the Logistic Regression model's predictions on the test data.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9981065
            },
            "cluster": 1
        }, {
            "cell_id": 18,
            "code": "def plot_confusion_matrix(y_test, y_predicted, title='Confusion Matrix'):\n    cm = confusion_matrix(y_test, y_predicted)\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm,annot=True, fmt='.20g')\n    plt.title(title)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')",
            "class": "Model Evaluation",
            "desc": "This code defines a function `plot_confusion_matrix` that computes the confusion matrix for the true and predicted labels, and visualizes it as a heatmap using seaborn and matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "heatmap",
                "subclass_id": 80,
                "predicted_subclass_probability": 0.7525936
            },
            "cluster": 2
        }, {
            "cell_id": 19,
            "code": "plot_confusion_matrix(y_test, y_predicted_lr)",
            "class": "Model Evaluation",
            "desc": "This code calls the `plot_confusion_matrix` function to compute and visualize the confusion matrix for the Logistic Regression model's predictions on the test data.",
            "testing": {
                "class": "Visualization",
                "subclass": "plot_predictions",
                "subclass_id": 56,
                "predicted_subclass_probability": 0.76843596
            },
            "cluster": 1
        }, {
            "cell_id": 23,
            "code": "score_metrics(y_test, y_predicted_dt)",
            "class": "Model Evaluation",
            "desc": "This code calls the `score_metrics` function to compute and print the accuracy, precision, and recall of the Decision Tree model's predictions on the test data.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9980464
            },
            "cluster": 1
        }, {
            "cell_id": 24,
            "code": "plot_confusion_matrix(y_test, y_predicted_dt)",
            "class": "Model Evaluation",
            "desc": "This code calls the `plot_confusion_matrix` function to compute and visualize the confusion matrix for the Decision Tree model's predictions on the test data.",
            "testing": {
                "class": "Visualization",
                "subclass": "plot_predictions",
                "subclass_id": 56,
                "predicted_subclass_probability": 0.7584198
            },
            "cluster": 1
        }, {
            "cell_id": 39,
            "code": "score_metrics(y_test_word2vec, y_predicted_word2vec_lr)",
            "class": "Model Evaluation",
            "desc": "This code calls the `score_metrics` function to compute and print the accuracy, precision, and recall of the Logistic Regression model's predictions on the word2vec-transformed test data.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9982146
            },
            "cluster": 1
        }, {
            "cell_id": 40,
            "code": "plot_confusion_matrix(y_test_word2vec, y_predicted_word2vec_lr)",
            "class": "Model Evaluation",
            "desc": "This code calls the `plot_confusion_matrix` function to compute and visualize the confusion matrix for the Logistic Regression model's predictions on the word2vec-transformed test data.",
            "testing": {
                "class": "Visualization",
                "subclass": "plot_predictions",
                "subclass_id": 56,
                "predicted_subclass_probability": 0.84631115
            },
            "cluster": 1
        }, {
            "cell_id": 41,
            "code": "compare_list = []\nfor (i,j) in zip(y_test_word2vec, y_predicted_word2vec_lr):\n    k = i - j\n    compare_list.append(k)\n\nwrong_num = [i for i,j in enumerate(compare_list) if j != 0]\ntext_series[0:train_data.shape[0]][wrong_num]",
            "class": "Model Evaluation",
            "desc": "This code computes the difference between the actual and predicted labels for the word2vec-transformed test data, identifies the indices of misclassified samples, and outputs the text entries corresponding to these misclassifications from the training data.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "define_variables",
                "subclass_id": 77,
                "predicted_subclass_probability": 0.96445125
            },
            "cluster": 3
        }, {
            "cell_id": 49,
            "code": "test_loss, test_acc = CNNmodel.evaluate(X_test_cnn, y_test_cnn, verbose=2)\nprint('test loss:',test_loss)\nprint('test acc:',test_acc)",
            "class": "Model Evaluation",
            "desc": "This code evaluates the trained CNN model on the test set, printing the test loss and test accuracy using Keras' `evaluate` method.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9945109
            },
            "cluster": 3
        }, {
            "cell_id": 55,
            "code": "test_loss, test_acc = model.evaluate(X_test_cnn, y_test_cnn, verbose=2)\nprint('test loss:',test_loss)\nprint('test acc:',test_acc)",
            "class": "Model Evaluation",
            "desc": "This code evaluates the new CNN model on the test set, printing the test loss and test accuracy using Keras' `evaluate` method.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "compute_test_metric",
                "subclass_id": 49,
                "predicted_subclass_probability": 0.9897344
            },
            "cluster": 3
        }, {
            "cell_id": 64,
            "code": "# predict\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ntest_input = bert_encode(test.text.values, bert_layer, max_len=128)\nmodel.load_weights('model.h5')\ntest_pred = model.predict(test_input)",
            "class": "Model Evaluation",
            "desc": "This code reads the test data from a CSV file, encodes the text for BERT input, loads the best model weights saved during training, and makes predictions on the test data using the trained BERT-based model.",
            "testing": {
                "class": "Data_Extraction",
                "subclass": "load_from_csv",
                "subclass_id": 45,
                "predicted_subclass_probability": 0.93680686
            },
            "cluster": 3
        }, {
            "cell_id": 15,
            "code": "lr_tfidf = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', n_jobs = -1)\nlr_tfidf.fit(X_train_tfidf, y_train)\ny_predicted_lr = lr_tfidf.predict(X_test_tfidf)",
            "class": "Model Training",
            "desc": "This code initializes a Logistic Regression model with balanced class weights and fits it to the TF-IDF transformed training data, then uses the model to predict labels for the TF-IDF transformed test data using scikit-learn's `LogisticRegression`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.6878745
            },
            "cluster": 0
        }, {
            "cell_id": 21,
            "code": "pipeline = Pipeline([\n    ('clf', DecisionTreeClassifier(splitter='random', class_weight='balanced'))\n])\nparameters = {\n    'clf__max_depth':(150,160,165),\n    'clf__min_samples_split':(18,20,23),\n    'clf__min_samples_leaf':(5,6,7)\n}\n\ndf_tfidf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=-1, scoring='f1')\ndf_tfidf.fit(X_train_tfidf, y_train)\n\nprint(df_tfidf.best_estimator_.get_params())",
            "class": "Model Training",
            "desc": "This code sets up a machine learning pipeline with a Decision Tree classifier, defines a grid of hyperparameters for tuning, and performs a GridSearchCV to find the best hyperparameters based on the F1 scoring metric using scikit-learn.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_on_grid",
                "subclass_id": 6,
                "predicted_subclass_probability": 0.9916215
            },
            "cluster": 2
        }, {
            "cell_id": 22,
            "code": "y_predicted_dt = df_tfidf.predict(X_test_tfidf)",
            "class": "Model Training",
            "desc": "This code uses the best estimator found by the GridSearchCV to predict labels for the TF-IDF transformed test data set using the Decision Tree classifier.",
            "testing": {
                "class": "Model_Evaluation",
                "subclass": "predict_on_test",
                "subclass_id": 48,
                "predicted_subclass_probability": 0.99451977
            },
            "cluster": 1
        }, {
            "cell_id": 30,
            "code": "'''\nstarttime = time.time()\nword2vec_model = Word2Vec(data_list, size=300, iter=10, min_count=10)\nusedtime = time.time() - starttime\nprint('It took %.2fseconds to train word2vec' %usedtime)\n'''",
            "class": "Model Training",
            "desc": "This commented-out code is intended to train a Word2Vec model on the `data_list` with specified hyperparameters, and then print the time taken to train the model.",
            "testing": {
                "class": "Exploratory_Data_Analysis",
                "subclass": "commented",
                "subclass_id": 76,
                "predicted_subclass_probability": 0.9858438
            },
            "cluster": -1
        }, {
            "cell_id": 38,
            "code": "word2vec_lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', n_jobs = -1)\nword2vec_lr.fit(X_train_word2vec, y_train_word2vec)\ny_predicted_word2vec_lr = word2vec_lr.predict(X_test_word2vec)",
            "class": "Model Training",
            "desc": "This code initializes a Logistic Regression model with balanced class weights and fits it to the word2vec-transformed training data, then uses the model to predict labels for the word2vec-transformed test data using scikit-learn's `LogisticRegression`.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.57594216
            },
            "cluster": 0
        }, {
            "cell_id": 46,
            "code": "CNNmodel = Sequential()\nCNNmodel.add(Embedding(len(word_index)+1, embedding_dim, input_length = max_sequence_length))\nCNNmodel.add(Conv1D(filters=250, kernel_size=3, strides=1, padding='valid', activation = 'relu'))\nCNNmodel.add(MaxPooling1D(pool_size=3))\nCNNmodel.add(Flatten())\nCNNmodel.add(Dense(embedding_dim, activation='relu'))\nCNNmodel.add(Dropout(0.8))\nCNNmodel.add(Dense(cnn_label.shape[1], activation='sigmoid'))\n\nCNNmodel.summary()",
            "class": "Model Training",
            "desc": "This code constructs a Convolutional Neural Network (CNN) model using Keras, with layers including Embedding, Conv1D, MaxPooling1D, Flatten, Dense, and Dropout, and then outputs a summary of the model architecture.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.9960479
            },
            "cluster": 2
        }, {
            "cell_id": 47,
            "code": "CNNmodel.compile(optimizer='adam', loss=losses.binary_crossentropy, metrics=['accuracy'])\nhistory = CNNmodel.fit(X_cnn, y_cnn, epochs=3, validation_data=(X_val_cnn, y_val_cnn))",
            "class": "Model Training",
            "desc": "This code compiles the CNN model with the Adam optimizer and binary cross-entropy loss, and then fits the model to the training data for 3 epochs, using a validation set to monitor performance during training using Keras.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.925682
            },
            "cluster": 1
        }, {
            "cell_id": 51,
            "code": "embedding_layer = Embedding(len(word_index)+1,\n                           embedding_dim,\n                           weights = [embedding_matrix],\n                           input_length = max_sequence_length,\n                           trainable = False)",
            "class": "Model Training",
            "desc": "This code creates an embedding layer using Keras, initialized with pre-trained word vectors from the `embedding_matrix`, and configures it to not update these weights during training.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.99493694
            },
            "cluster": 0
        }, {
            "cell_id": 52,
            "code": "model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters=150, kernel_size=3, strides=1, padding='valid', activation = 'relu'))\nmodel.add(MaxPooling1D(pool_size=3))\nmodel.add(Flatten())\nmodel.add(Dense(embedding_dim, activation='relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Dense(cnn_label.shape[1], activation='sigmoid'))\n\nmodel.summary()",
            "class": "Model Training",
            "desc": "This code constructs a new Convolutional Neural Network (CNN) model using Keras, starting with the pre-trained embedding layer, followed by Conv1D, MaxPooling1D, Flatten, Dense, and Dropout layers, and then outputs a summary of the model architecture.",
            "testing": {
                "class": "Model_Train",
                "subclass": "choose_model_class",
                "subclass_id": 4,
                "predicted_subclass_probability": 0.99737453
            },
            "cluster": 2
        }, {
            "cell_id": 53,
            "code": "model.compile(optimizer='adam', loss=losses.binary_crossentropy, metrics=['accuracy'])\nhistory = model.fit(X_cnn, y_cnn, epochs=10, validation_data=(X_val_cnn, y_val_cnn))",
            "class": "Model Training",
            "desc": "This code compiles the new CNN model with the Adam optimizer and binary cross-entropy loss, and then fits the model to the training data for 10 epochs, using a validation set to monitor performance during training using Keras.",
            "testing": {
                "class": "Model_Train",
                "subclass": "train_model",
                "subclass_id": 7,
                "predicted_subclass_probability": 0.9875873
            },
            "cluster": 1
        }, {
            "cell_id": 60,
            "code": "def bert_encode(texts, bert_layer, max_len=128):\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    \n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        text = text[:max_len - 2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        input_ids = tokens + [0]* pad_len\n        all_tokens.append(input_ids)\n\n        masks = [1]*len(input_sequence) + [0]* pad_len\n        all_masks.append(masks)\n        \n        segments = [0]* max_len\n        all_segments.append(segments)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n    \ndef build_model(bert_layer, max_len = 128, lr = 1e-5):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"segment_ids\")\n        \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    dense_out = Dense(1,activation=\"relu\")(pooled_output)\n    drop_out = tf.keras.layers.Dropout(0.8)(dense_out)\n    out = Dense(1,activation=\"sigmoid\")(pooled_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    adam = tf.keras.optimizers.Adam(lr)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n        \n    return model\n\n\ndef plot_curve(history):\n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.ylim([0.5,1])\n    plt.legend()\n    plt.show()",
            "class": "Model Training",
            "desc": "This code defines three functions: `bert_encode` that tokenizes and encodes texts for BERT input with padding and truncation; `build_model` which constructs a BERT-based text classification model with dropout layers and sigmoid activation for binary classification; and `plot_curve` that plots the training and validation accuracy over epochs using matplotlib.",
            "testing": {
                "class": "Data_Transform",
                "subclass": "categorify",
                "subclass_id": 20,
                "predicted_subclass_probability": 0.9129451
            },
            "cluster": 0
        }, {
            "cell_id": 63,
            "code": "# train model\nmodel = build_model(bert_layer, max_len=128, lr = 1e-5)\nmodel.summary()\n\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)\n\nplot_curve(train_history)",
            "class": "Model Training",
            "desc": "This code initializes and summarizes the BERT-based model, sets up a model checkpoint to save the best model based on validation loss, fits the model to the training data for 3 epochs with a validation split of 20%, and plots the training and validation accuracy over epochs using Keras.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.61692107
            },
            "cluster": 1
        }, {
            "cell_id": 6,
            "code": "piedata = train_data['target']\nplt.figure(figsize=(6,6))\npiedata.value_counts().plot(kind = 'pie',autopct = '%.2f%%')",
            "class": "Visualization",
            "desc": "This code creates a pie chart to visualize the distribution of the 'target' values in the `train_data` DataFrame, including percentages, using matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.9974884
            },
            "cluster": 2
        }, {
            "cell_id": 7,
            "code": "num_words_0 = train_data[train_data['target']==0]['text'].apply(lambda x: len(x.split()))\nnum_words_1 = train_data[train_data['target']==1]['text'].apply(lambda x: len(x.split()))\nplt.figure(figsize=(12,6))\nsns.kdeplot(num_words_0, shade=True, color = 'b').set_title('Kernel distribution of number of words')\nsns.kdeplot(num_words_1, shade=True, color = 'r')\nplt.legend(labels=['0_no disaster', '1_disaster'])",
            "class": "Visualization",
            "desc": "This code calculates the number of words in the text for each entry in `train_data` with target values 0 and 1, and then plots the kernel density estimates (KDE) for these word counts using seaborn and matplotlib, distinguishing between non-disaster and disaster texts with different colors.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.97927135
            },
            "cluster": 0
        }, {
            "cell_id": 8,
            "code": "len_word_0 = train_data[train_data['target']==0]['text'].str.split().map(lambda x: [len(i) for i in x])\nave_len_0 = len_word_0.map(lambda x: np.mean(x))\nlen_word_1 = train_data[train_data['target']==1]['text'].str.split().map(lambda x: [len(i) for i in x])\nave_len_1 = len_word_1.map(lambda x: np.mean(x))\nplt.figure(figsize=(12,6))\nsns.kdeplot(ave_len_0, shade=True, color='b').set_title('Kernel distribution of average words lenth')\nsns.kdeplot(ave_len_1, shade=True, color='r')\nplt.legend(labels=['0_no disaster', '1_disaster'])",
            "class": "Visualization",
            "desc": "This code computes the average length of words in each text entry for non-disaster and disaster categories in `train_data`, and then plots the kernel density estimates (KDE) for these average word lengths using seaborn and matplotlib, differentiating between the two categories with distinct colors.",
            "testing": {
                "class": "Visualization",
                "subclass": "distribution",
                "subclass_id": 33,
                "predicted_subclass_probability": 0.98665464
            },
            "cluster": 0
        }, {
            "cell_id": 48,
            "code": "plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5,1])\nplt.legend()\nplt.show()",
            "class": "Visualization",
            "desc": "This code plots the training and validation accuracy over epochs to visually compare the model's performance on the training set with that on the validation set using matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9901933
            },
            "cluster": -1
        }, {
            "cell_id": 54,
            "code": "plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5,1])\nplt.legend()\nplt.show()",
            "class": "Visualization",
            "desc": "This code plots the training and validation accuracy over epochs to visually compare the model's performance on the training set with that on the validation set using matplotlib.",
            "testing": {
                "class": "Visualization",
                "subclass": "learning_history",
                "subclass_id": 35,
                "predicted_subclass_probability": 0.9901933
            },
            "cluster": -1
        }],
        "notebook_id": 19,
        "notebook_name": "nlp-getting-started.ipynb",
        "user": "nlp-getting-started.ipynb"
    }],
    "metadata": {
        "comp_name": "Disaster Tweets",
        "clusters": {
            "Data Transform": {
                "titles": {
                    "0": "Text Preprocessing and Vectorization with Python",
                    "1": "Text Preprocessing and Tokenization using Hugging Face, PyTorch, Scikit-learn, TensorFlow, BERT, spaCy, NLTK",
                    "2": "Data Preprocessing and NLP with NLTK",
                    "3": "Text Data Preprocessing with TensorFlow, Keras",
                    "4": "TensorFlow, Pandas, Tokenizer for Text Preprocessing",
                    "5": "Text Preprocessing with Pandas and Seaborn",
                    "-1": "Text Preprocessing with NLTK, SpaCy, TensorFlow"
                },
                "accuracy": {
                    "silhouette_score": 0.0043524821443575385,
                    "ch_index": 12.550880397052804,
                    "db_index": 2.827420737820396
                }
            },
            "Data Extraction": {
                "titles": {
                    "0": "Reading CSV Data with Pandas",
                    "1": "Data Handling and NLP with Pandas",
                    "-1": "Pandas Data Preparation and PyTorch DataLoader"
                },
                "accuracy": {
                    "silhouette_score": 0.10479981018965297,
                    "ch_index": 4.651146325187682,
                    "db_index": 2.866219041672828
                }
            },
            "Visualization": {
                "titles": {
                    "0": "Visualizing Twitter Data with Matplotlib, Seaborn",
                    "1": "Visualizing Model Performance using Matplotlib",
                    "2": "Data Visualization in Pandas, Matplotlib, Seaborn",
                    "3": "Matplotlib and Seaborn Data Visualizations",
                    "-1": "Seaborn and Matplotlib for Tweet Analysis"
                },
                "accuracy": {
                    "silhouette_score": 0.10131405526910806,
                    "ch_index": 5.319644526487969,
                    "db_index": 2.744727696437807
                }
            },
            "Model Training": {
                "titles": {
                    "0": "Machine Learning Models and Hyperparameter Tuning",
                    "1": "Model Training with BERT, LSTM, GRU, Keras",
                    "2": "Machine Learning Models with Keras, TensorFlow",
                    "-1": "BERT Models with Fastai and Keras"
                },
                "accuracy": {
                    "silhouette_score": 0.0798148992071514,
                    "ch_index": 11.478095920689434,
                    "db_index": 2.592581297976958
                }
            },
            "Model Evaluation": {
                "titles": {
                    "0": "Model Evaluation and Prediction with sklearn",
                    "1": "Model Evaluation in Sklearn and Octopus_ml",
                    "2": "Model Evaluation and Metrics in Python",
                    "3": "Model Evaluation and Predictions in Machine Learning"
                },
                "accuracy": {
                    "silhouette_score": 0.07609195565620122,
                    "ch_index": 6.476034110018519,
                    "db_index": 2.6328061000594434
                }
            },
            "Imports and Environment": {
                "titles": {
                    "0": "Data Processing and ML with Python Libraries",
                    "1": "Text Processing and Analysis with Transformers",
                    "-1": "SpaCy, TensorFlow, NLP, BERT Setup"
                },
                "accuracy": {
                    "silhouette_score": 0.09411676386995801,
                    "ch_index": 6.87766584622638,
                    "db_index": 2.962772896520569
                }
            },
            "Data Export": {
                "titles": {
                    "-1": "Pandas CSV Submissions Using Trained Models"
                },
                "accuracy": {
                    "silhouette_score": 0,
                    "ch_index": 0,
                    "db_index": 0
                }
            },
            "Exploratory Data Analysis": {
                "titles": {
                    "0": "Seaborn, BERT Tokenization, Dataset Analysis",
                    "1": "Pandas and Dataprep for Text Analysis",
                    "2": "Pandas DataFrame Head and Shape Analysis",
                    "3": "Data Inspection and Summarization in Pandas",
                    "4": "Data Inspection and Analysis in Pandas"
                },
                "accuracy": {
                    "silhouette_score": 0.013001418940470485,
                    "ch_index": 4.36574127364759,
                    "db_index": 3.3125775288013166
                }
            }
        },
        "clustering_accuracy": 0.32465543644716693
    }
}