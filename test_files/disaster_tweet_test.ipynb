{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Export",
    "desc": "This code snippet creates a DataFrame for submission by combining the PassengerId from the test dataset with the predicted survival values, and then writes this DataFrame to a CSV file named 'titanic.csv'.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9993875,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Data Export",
    "desc": "This code snippet adds the predicted survival values to the test DataFrame, and then exports the 'PassengerId' and 'Survived' columns to a CSV file named 'results-rf.csv' without the index.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99914086,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "df_test['Survived'] = test_y\n",
    "df_test[['PassengerId', 'Survived']] \\\n",
    "    .to_csv('results-rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Export",
    "desc": "This code defines a function to save the prediction results, including the `PassengerId` and the predicted `Survived` values, to a CSV file named with the specified classifier's name.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9993976354599,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "def save_result(y_pred, name):\n",
    "    pd.DataFrame({\n",
    "        \"PassengerId\": df_full.loc[df_full.Train == 0, :].index,\n",
    "        \"Survived\": y_pred\n",
    "        }).to_csv(\"predictions_{}.csv\".format(name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Export",
    "desc": "This code trains the ensemble voting classifier, generates predictions on the test set, and saves the results to a CSV file named \"predictions_vote_clf.csv\".",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.982008457183838,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "save_result(train_predict_classifier(eclf), \"vote_clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 57,
    "class": "Data Export",
    "desc": "The code snippet creates an ensemble learner using VotingClassifier with Logistic Regression, Random Forest, and AdaBoost classifiers, trains it on the 'titanic' dataset, predicts survival on both the training and test data, and exports the test predictions to a CSV file named \"titanic_submission.csv\".",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.47594857,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "predictions=[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\",\"NlengthD\",\n",
    "              \"FsizeD\", \"Title\",\"Deck\",\"NameLength\",\"TicketNumber\"]\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('lr', lr), ('rf', rf), ('adb', adb)], voting='soft')\n",
    "eclf1 = eclf1.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "predictions=eclf1.predict(titanic[predictors])\n",
    "predictions\n",
    "\n",
    "test_predictions=eclf1.predict(titanic_test[predictors])\n",
    "\n",
    "test_predictions=test_predictions.astype(int)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": test_predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"titanic_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Export",
    "desc": "This code snippet runs the logistic regression model on the test data, creates a submission DataFrame with the required format, prints the first few rows of this DataFrame, and exports it to a CSV file named \"Submission_lr.csv\".",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9991254,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "# run model and place values in test dataframe\n",
    "test[\"Survived\"] = LRmodel.predict(Xt)\n",
    "\n",
    "# produce submission format\n",
    "submission_lr = pd.DataFrame()\n",
    "\n",
    "submission_lr[\"PassengerId\"] = test[\"PassengerId\"]\n",
    "submission_lr[\"Survived\"] = test[\"Survived\"]\n",
    "\n",
    "print(\"Check format:\\n\")\n",
    "print(submission_lr.head())\n",
    "\n",
    "submission_lr.to_csv(\"Submission_lr.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Export",
    "desc": "This code snippet runs the RandomForestClassifier model on the test data, creates a submission DataFrame with the required format, prints the first few rows of this DataFrame, and exports it to a CSV file named \"Submission_rfc.csv\".",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.99906105,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "# run model and place values in test dataframe\n",
    "test[\"Survived\"] = RFCmodel.predict(Xt)\n",
    "\n",
    "# produce submission format\n",
    "submission_rfc = pd.DataFrame()\n",
    "\n",
    "submission_rfc[\"PassengerId\"] = test[\"PassengerId\"]\n",
    "submission_rfc[\"Survived\"] = test[\"Survived\"]\n",
    "\n",
    "print(\"Check format:\\n\")\n",
    "print(submission_rfc.head())\n",
    "\n",
    "submission_rfc.to_csv(\"Submission_rfc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Data Export",
    "desc": "This code snippet creates a DataFrame for the submission with \"PassengerId\" and \"Survived\" columns, then exports this DataFrame to a CSV file named 'titanic.csv'.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9993875,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "The code snippet uses the trained model to predict survival outcomes for the test dataset, creates a dataframe with the predictions and passenger IDs, and exports this dataframe to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9991854,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Export",
    "desc": "The code snippet predicts the 'Survived' values for the test dataset using the trained LogisticRegression model, creates a DataFrame with 'PassengerId' and 'Survived' columns, and exports it to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9991853833198548,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Export",
    "desc": "This code makes predictions on the test dataset, creates a DataFrame with the predictions, and then exports the results to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9991853833198548,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Export",
    "desc": "The code snippet generates predictions for the test dataset using the trained Logistic Regression model, creates a DataFrame with 'PassengerId' and 'Survived' columns, and exports this DataFrame to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9991854,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "This code snippet generates predictions for the test dataset using the trained model, creates a DataFrame with 'PassengerId' and 'Survived' columns, and saves the results to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99901414,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "print(test.shape)\n",
    "print(test.head())\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "This code snippet generates predictions on the test set, creates a DataFrame with passenger IDs and predicted survival statuses, and saves the results to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9991853833198548,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "This code snippet makes predictions on the test dataset, creates a DataFrame with the results including passenger IDs, and exports the predictions to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9991853833198548,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "This code snippet makes survival predictions on the test dataset using the trained model, creates a DataFrame with the predictions, and exports the results to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9991854,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Export",
    "desc": "This code snippet generates predictions for the test dataset using the trained model, combines them with passenger IDs, and exports the results to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9991853833198548,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "This code snippet generates predictions on the test set, creates a new DataFrame with 'PassengerId' and predicted 'Survived' values, and exports the result to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9991854,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "The snippet generates predictions for the test set, creates a DataFrame with passenger IDs and predicted survival values, and exports this DataFrame to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9991854,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Export",
    "desc": "This code snippet creates a DataFrame for the submission, containing \"PassengerId\" and the predicted \"Survived\" values, and exports it to a CSV file named 'titanic.csv'.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9993875,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Data Export",
    "desc": "This code snippet generates predictions for the test dataset using the trained model, creates a DataFrame with the predictions, and saves the results to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9991853833198548,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "This code generates predictions for the test set using the trained model, combines them with the 'PassengerId's, and exports the results to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9991854,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "The code snippet predicts survival on the test dataset, creates a DataFrame with passenger IDs and their predicted survival status, and saves the resulting DataFrame to a CSV file named 'titanic_pred_6.csv'.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9991262555122375,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred_6.csv' , index = False )\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Data Export",
    "desc": "This code predicts the survival outcomes for the test dataset, compiles the results with Passenger IDs into a DataFrame, and saves the predictions to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9991854,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Export",
    "desc": "This code snippet converts the test dataframe to a NumPy array, generates predictions using the trained RandomForestClassifier model, creates an output DataFrame with passenger IDs and their predicted survival status, sets the index to 'PassengerId', and exports the results to a CSV file.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9993220567703248,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_data = test_df.values\n",
    "X_test = test_data[:,1:]\n",
    "predictions = rf.predict(X_test)\n",
    "output = pandas.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived': predictions})\n",
    "output.set_index('PassengerId',inplace=True)\n",
    "output.to_csv('output.csv', header=True)\n",
    "\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "This code snippet uses the trained model to predict survival on the test data, constructs a DataFrame with the predictions and passenger IDs, and exports the results to a CSV file named 'titanic_pred.csv'.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9991854,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_Y = model.predict( test_X )\n",
    "passenger_id = full[891:].PassengerId\n",
    "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
    "test.shape\n",
    "test.head()\n",
    "test.to_csv( 'titanic_pred.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Export",
    "desc": "This code snippet generates predictions for the test data, updates the submission DataFrame with these predictions, and writes the DataFrame to a CSV file named \"submission.csv\".",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.999064,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(tv_test_reviews)\n",
    "submission.target = y_pred\n",
    "submission.to_csv(\"submission.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Export",
    "desc": "The code exports the preprocessed training and test datasets to CSV files named 'preprocess_train.csv' and 'preprocess_test.csv' respectively.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.999161,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "train.to_csv('preprocess_train.csv')\n",
    "test.to_csv('preprocess_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Data Export",
    "desc": "The code saves the NumPy array `store_train` containing the vector representations of the training dataset to a file named 'store_train.npy'.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9881634,
    "start_cell": false,
    "subclass": "save_model",
    "subclass_id": 50
   },
   "outputs": [],
   "source": [
    "np.save('store_train.npy',store_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Data Export",
    "desc": "The code saves the NumPy array `store_test` containing the vector representations of the test dataset to a file named 'store_test.npy'.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9880189,
    "start_cell": false,
    "subclass": "save_model",
    "subclass_id": 50
   },
   "outputs": [],
   "source": [
    "np.save('store_test.npy',store_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Data Export",
    "desc": "The code creates a DataFrame named `submission` containing the test dataset IDs and their predicted labels, prints the first 10 rows for verification, and then exports this DataFrame to a CSV file named 'submission_nlp_tweets.csv'.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99926823,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission=pd.DataFrame({'id': test['id'], 'target':test_label})\n",
    "print(submission.head(10))\n",
    "\n",
    "filename = 'submission_nlp_tweets.csv'\n",
    "\n",
    "submission.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Export",
    "desc": "This snippet generates predictions for the test data using the trained LSTM model, rounds and converts these predictions to integers, assigns them to the 'target' column of the submission DataFrame, and saves the DataFrame to a CSV file named \"submission.csv\".",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.99938893,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "test_pred_GloVe = model.predict(test)\n",
    "test_pred_GloVe_int = test_pred_GloVe.round().astype('int')\n",
    "\n",
    "submission['target'] = test_pred_GloVe_int\n",
    "submission.head(10)\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 51,
    "class": "Data Export",
    "desc": "This code snippet creates a new DataFrame for the final submission, assigns the 'id' column from the sample submission and the 'target' predictions, and then exports it as a CSV file named \"final_submission.csv\".",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9994616,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame()\n",
    "dataframe['id'] = sample_submission['id']\n",
    "dataframe['target'] = pred_final\n",
    "dataframe.to_csv(\"final_submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Export",
    "desc": "This code snippet, if uncommented, would create a DataFrame from the grid search results and save it to a CSV file named 'result.csv'.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.36818776,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "# X = []\n",
    "# V = []\n",
    "# A = []\n",
    "\n",
    "# for x,val_accuracy,accuracy in result:\n",
    "#     X.append(x)\n",
    "#     V.append(val_accuracy)\n",
    "#     A.append(accuracy)\n",
    "\n",
    "# df_acc = pd.DataFrame(data={'combination':X, 'val_accuracy':V, 'accuracy':A})\n",
    "# df_acc.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Export",
    "desc": "This code snippet creates a DataFrame containing the IDs and predicted labels for the evaluation dataset, saves it to a CSV file named 'my_submission_20200205.csv', and confirms that the submission was successfully saved.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9991793,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': data_eval_cleansed.id, 'target': y_eval})\n",
    "output.to_csv('my_submission_20200205.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Export",
    "desc": "This code saves the test dataframe to a TSV file called \"test.tsv\" and the training dataframe to a TSV file called \"train_eval.tsv,\" both without including the index or headers, and prints their shapes.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.99929094,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "# tsvファイルで保存する\n",
    "\n",
    "test_df.to_csv(\"test.tsv\", sep='\\t', index=False, header=None)\n",
    "print(test_df.shape)\n",
    "\n",
    "# 訓練&検証データとする\n",
    "train_val_df.to_csv(\"train_eval.tsv\", sep='\\t', index=False, header=None)\n",
    "print(train_val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Data Export",
    "desc": "This code reads a sample submission file, updates its 'target' column with the predicted labels from the test dataset, and displays the modified dataframe in preparation for submission.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9996031,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = ans_list\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Data Export",
    "desc": "This code saves the updated sample submission dataframe to a CSV file named \"submission_plus.csv\" without the index, for later submission to a competition or any other use case.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9992436,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission_plus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 87,
    "class": "Data Export",
    "desc": "This snippet assigns the integer predictions made by the BERT-based model on the test data to the 'target' column of the submission DataFrame and displays the first 10 rows, preparing the results for submission.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9664472,
    "start_cell": true,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "submission['target'] = test_pred_BERT_int\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 88,
    "class": "Data Export",
    "desc": "This snippet exports the submission DataFrame containing the model predictions to a CSV file named \"submission.csv\" with headers included, enabling the results to be submitted for evaluation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9992624,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Export",
    "desc": "This code exports the training, test, and development datasets to CSV files in the specified directory.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.99918073,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "train.to_csv(label_training_dir + \"/train.csv\", index=False)\n",
    "test.to_csv(label_training_dir + \"/test.csv\", index=False)\n",
    "dev.to_csv(label_training_dir + \"/dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Export",
    "desc": "This code creates a submission DataFrame with the 'id' and 'target' columns from the test DataFrame and counts the occurrences of each unique 'target' value.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9994636,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "submission = df_test[['id', 'target']]\n",
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Export",
    "desc": "This code exports the submission DataFrame to a CSV file at the specified path without including the DataFrame index.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.99898535,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Export",
    "desc": "This code snippet exports the `sub` DataFrame to a CSV file named 'submission.csv' without including the index.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9992668,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Export",
    "desc": "This code snippet displays the first few rows of the `sub` DataFrame to ensure correct formatting before final submission.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99974316,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Data Export",
    "desc": "This code snippet generates predictions on the test dataset using the trained model, rounds and reshapes the predictions, and then creates a submission file in CSV format with the predicted values.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.99940777,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "y_pre=model.predict(test)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
    "sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Export",
    "desc": "This code reads the sample submission file, predicts the target values for the test data using the trained Multinomial Naive Bayes model, and writes the results to a new CSV file named 'submission.csv'.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99921775,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = nb_model.predict(test)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Export",
    "desc": "This code reads a sample submission file, assigns the predicted labels to the 'target' column, and displays the updated submission DataFrame.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9996339,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "submission['target'] = preds\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Export",
    "desc": "This code saves the updated submission DataFrame to a CSV file named 'submission.csv' without including the index.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9992335,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Export",
    "desc": "This code snippet defines two functions for interacting with Google Cloud Storage: one for uploading a file to a specified bucket and another for downloading a file from a bucket to the working directory of the Kaggle notebook.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9701536,
    "start_cell": true,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket. https://cloud.google.com/storage/docs/ \"\"\"\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print('File {} uploaded to {}'.format(\n",
    "        source_file_name,\n",
    "        'gs://' + bucket_name + '/' + destination_blob_name))\n",
    "    \n",
    "def download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n",
    "    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n",
    "    os.makedirs(destination_directory, exist_ok = True)\n",
    "    full_file_path = os.path.join(destination_directory, file_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        blob.download_to_filename(full_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Export",
    "desc": "This code snippet selects the 'text' and 'target' columns from the training DataFrame and saves them to a CSV file named 'train.csv' without including the index and headers.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.99922717,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "# Select the text body and the target value, for sending to AutoML NL\n",
    "nlp_train_df[['text','target']].to_csv('train.csv', index=False, header=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Export",
    "desc": "This code snippet uploads the previously saved 'train.csv' file to a Google Cloud Storage bucket, specifying the path 'uploads/kaggle_getstarted/full_train.csv' within the bucket.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.83945614,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "training_gcs_path = 'uploads/kaggle_getstarted/full_train.csv'\n",
    "upload_blob(bucket_name, 'train.csv', training_gcs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Export",
    "desc": "This code snippet saves the submission DataFrame to a CSV file named \"submission.csv\" with included headers and no index.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.99938786,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Export",
    "desc": "Generates predictions on the test set, rounds them, reshapes them into the required format, and creates a DataFrame with these predictions and corresponding IDs for submission.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.8358635,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_tweet)\n",
    "y_pred = np.round(y_pred).astype(int).reshape(3263)\n",
    "sub = pd.DataFrame({'id':submission['id'].values.tolist(),'target':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Export",
    "desc": "Displays the DataFrame containing the final predictions and corresponding IDs that will be prepared for export or submission.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9996369,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Data Export",
    "desc": "Saves the submission DataFrame to a CSV file named 'submission.csv' without including the index.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9992668,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 44,
    "class": "Data Export",
    "desc": "The code generates predictions on the test data using the trained GloVe model, rounds them to nearest integers, updates the submission DataFrame with these predictions, and saves the results to a CSV file.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99928826,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "test_pred = model_glove.predict(X_test_seq)\n",
    "test_pred_int = test_pred.round().astype('int')\n",
    "submission['target'] = test_pred_int\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Export",
    "desc": "The code creates a DataFrame for submission, containing the test ids and the predicted target labels.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9941958,
    "start_cell": true,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_raw.id,\n",
    "    'target':y_hat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Export",
    "desc": "The code saves the submission DataFrame to a CSV file named \"my_submission_linear.csv\" without the index.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99924576,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"my_submission_linear.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 44,
    "class": "Data Export",
    "desc": "This code snippet generates predictions for the test set, processes them into the required format, and saves the predictions into a CSV file for submission.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9994172,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "y_pre=model.predict(test)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
    "sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
    "sub.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 45,
    "class": "Data Export",
    "desc": "This code snippet displays the first few rows of the generated submission DataFrame to verify its contents before submission.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99974316,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 43,
    "class": "Data Export",
    "desc": "This code snippet makes predictions on the test set, rounds and reshapes the predictions, and then creates and saves a submission DataFrame to a CSV file.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.999383,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "y_pre = model.predict(test)\n",
    "y_pre = np.round(y_pre).astype(int).reshape(3263)\n",
    "sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),\n",
    "                    'target':y_pre})\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Export",
    "desc": "This code snippet creates a DataFrame `output_df` that contains the 'id' from the test dataset and the corresponding predicted labels `y_pred` for output purposes.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9984261,
    "start_cell": true,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "output_df=pd.DataFrame({'id':id,'target':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Data Export",
    "desc": "This code snippet displays the `output_df` DataFrame, which contains the 'id' and 'target' columns, as a preview of the final output of the predictions.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99974567,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Data Export",
    "desc": "This code snippet exports the `output_df` DataFrame to a CSV file named \"submission2.csv\" without including the index.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99928904,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "#create submissoin file\n",
    "output_df.to_csv(\"submission2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Data Export",
    "desc": "The code saves the modified submission DataFrame to a CSV file named 'submit4.csv' without including the index, preparing it for submission.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9992329,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submit.to_csv('submit4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Export",
    "desc": "This code creates a DataFrame containing the test dataset IDs and the predicted target values.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.998738,
    "start_cell": true,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'id':test['id'], 'target':y_pred})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Export",
    "desc": "This code exports the DataFrame containing the test dataset IDs and predicted target values to a CSV file named 'mnb_submission.csv' without including the DataFrame index.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9992693,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "result.to_csv('mnb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Export",
    "desc": "This code loads the sample submission file, predicts the target labels for the test data using the trained model, updates the sample submission with the predictions, and saves the file as \"submission.csv\".",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99921703,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(test_vectors)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 49,
    "class": "Data Export",
    "desc": "This code snippet displays the first few rows of the submission DataFrame, providing a preview of its structure and content.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99975234,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# sample of submission\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 50,
    "class": "Data Export",
    "desc": "This code snippet adds a 'target' column to the submission DataFrame based on the model's predictions, converting probabilities to binary class labels.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.94987184,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# Add target column to submission file\n",
    "submission['target'] = (predictions > 0.5).astype(int)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 51,
    "class": "Data Export",
    "desc": "This code snippet saves the updated submission DataFrame to a CSV file named \"submission.csv\" with the header included.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9992624,
    "start_cell": false,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet reads the Titanic training and test datasets from CSV files into Pandas DataFrames and previews the first few rows of the Titanic training dataset.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9996555,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "titanic_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "# preview the data\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet reads the training data from a CSV file into a DataFrame and displays the first five rows.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99953246,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet displays the last five rows of the training data DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99975353,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Data Extraction",
    "desc": "This code snippet reads the test data from a CSV file into a DataFrame and displays the first five rows.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9994654,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../input/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code reads training and test datasets from CSV files, merges them into a master dataset, and prints the shapes of the resulting datasets.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.989820957183838,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv', index_col=['PassengerId'])\n",
    "df_test = pd.read_csv('../input/test.csv', index_col=['PassengerId'])\n",
    "\n",
    "# merge train set into master set\n",
    "df_full = df_train.copy()\n",
    "df_full['Train'] = 1\n",
    "df_full['Train'] = df_full.Train.astype(int)\n",
    "\n",
    "# merge test set into master set\n",
    "df_full = pd.concat([df_full, df_test])\n",
    "df_full.loc[df_full.Train.isnull(), 'Train'] = 0\n",
    "\n",
    "print(\"Training shape: \", df_train.shape)\n",
    "print(\"Test shape: \", df_test.shape)\n",
    "print(\"Full shape: \", df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Extraction",
    "desc": "This code selects specific features from the master dataset to create training and test matrices, and prints their shapes.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9951146841049194,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "#features = ['Age_group', 'Embarked', 'Fare', 'Sex', 'Pclass', 'Familiy_size', 'Mother', 'Deck']\n",
    "features = ['Age', 'Fare', 'Sex', 'Pclass', 'Embarked', 'Familiy_size']\n",
    "\n",
    "# copy only the relevant features from the master data set, to the prediction matrix.\n",
    "X_train = df_full.loc[df_full.Train == 1, features].copy()\n",
    "\n",
    "X_test = df_full.loc[df_full.Train == 0, features].copy()\n",
    "\n",
    "print(\"Shape X Train: \", X_train.shape)\n",
    "print(\"Shape X Test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "The code snippet reads a CSV file into a pandas DataFrame named 'titanic' and prints the first five rows.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99936324,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# This creates a pandas dataframe and assigns it to the titanic variable.\n",
    "titanic = pd.read_csv(\"../input/train.csv\")\n",
    "# Print the first 5 rows of the dataframe.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "The code snippet reads a CSV file into a pandas DataFrame named 'titanic_test', transposes the first five rows, and notes the absence of the 'Survived' column, which is the target variable for prediction.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9994337,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "titanic_test = pd.read_csv(\"../input/test.csv\")\n",
    "#transpose\n",
    "titanic_test.head().T\n",
    "#note their is no Survived column here which is our target varible we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet reads the training and test datasets from CSV files, combines them into a single dataset, and then prints some details about the datasets including the number of records, an example record, and the data types of each column.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9995623,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# import test and train and combine to full data set \n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "titanic = train.append(test, ignore_index = True)\n",
    "\n",
    "# printing the first row and structure\n",
    "print(\"The number of records are as follows:\")\n",
    "print(\"Training set: {}\".format(train[\"Age\"].count()))\n",
    "print(\"Test set: {}\".format(test[\"Age\"].count()))\n",
    "print(\"\\nBelow see an example record \\n\")\n",
    "print(titanic.iloc[1])\n",
    "print(\"\\nBelow see the data type for each column/variable \\n\")\n",
    "print(titanic.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet reads the Titanic dataset and test dataset into pandas DataFrames and displays the first few rows of the Titanic DataFrame.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9996555,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "titanic_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "# preview the data\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "The code snippet loads the Titanic training and test datasets into dataframes, combines them, and separates the training data, then deletes the original dataframes and prints their shapes.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.99972636,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "The code snippet splits the combined dataset into training, validation, and test sets for features and labels, and prints their shapes.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.99615127,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "The code snippet loads the Titanic dataset training and test CSV files into DataFrames, concatenates them, and then splits them back into the training set, while displaying the shapes of the combined dataset and the initial training set.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9997263550758362,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "The code snippet splits the combined feature set into training, validation, and test datasets, ensuring proper dimensions for each split and printing their shapes.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.977363109588623,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "#train_valid_y = train_valid_X.Survived\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code reads the Titanic training and test datasets, merges them into a single DataFrame, and prints the dimensions of the combined dataset as well as the training subset.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9997263550758362,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "This code splits the combined features into training, validation, and test datasets, prints their shapes, and assigns the 'Survived' column as the target variable for training and validation.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.977363109588623,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "#train_valid_y = train_valid_X.Survived\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "The code snippet reads the Titanic train and test datasets into DataFrames, combines them into one DataFrame, and then separates the combined DataFrame into the original Titanic dataset, printing the shapes of both DataFrames. ",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99972636,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "The code snippet splits the dataset into training, validation, and test sets, making sure the targets (Survived) are appropriately assigned, and prints the shapes of these datasets.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9773631,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "#train_valid_y = train_valid_X.Survived\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet loads the Titanic train and test datasets, combines them into a single DataFrame, and then splits it back into the original 'titanic' training set while removing the original DataFrames to free memory.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99972636,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "This code snippet creates the datasets needed for training, validation, and testing by splitting the 'full_X' DataFrame and 'titanic.Survived' Series into appropriate subsets, and then further splitting the training data into training and validation sets.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99615127,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet reads the Titanic dataset from CSV files into DataFrames, combines them into a single DataFrame, and then separates the training portion while deleting the original DataFrames.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9997263550758362,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "This code snippet splits the concatenated dataset into training, validation, and test sets, specifically creating `train_X`, `train_y`, `valid_X`, `valid_y` from the Titanic data, and prints the shapes of these sets.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9961512684822084,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet reads the Titanic dataset from CSV files into DataFrames, combines them into a single DataFrame, and then separates the training portion, before deleting the original individual datasets.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9997263550758362,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "This code snippet creates the datasets necessary for training, validation, and testing by splitting the original dataset into feature matrices and target vectors for these purposes, and prints their shapes.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9961512684822084,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet loads the Titanic dataset from CSV files, combines them into a single DataFrame, and then splits it back to extract the training set while deleting the original DataFrames.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.99972636,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "This code snippet prepares the datasets needed for training, validation, and testing by splitting the data into respective subsets and prints their shapes.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.99615127,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet reads Titanic dataset CSV files into DataFrames, merges them into one DataFrame, subsets the training data, and prints the shapes of the resulting DataFrames.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9997263550758362,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "This code snippet creates datasets for training, validation, and testing by splitting the concatenated feature dataset into appropriate subsets for model training, validation, and final testing, and then prints their shapes.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9940271377563475,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)\n",
    "print (train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet loads the Titanic dataset from CSV files, combines training and test sets into a single DataFrame, separates it back into the 'titanic' DataFrame containing the training data, and deletes the individual datasets.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99972636,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "This code snippet creates datasets for training, validation, and testing by splitting the combined feature set 'full_X' and the target variable 'Survived' from the 'titanic' dataset, and further splits the training data into training and validation sets.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99615127,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "The snippet reads the Titanic train and test CSV files into DataFrames, combines them, subsets the first 891 rows as the Titanic dataset, and prints the shapes of the resulting DataFrames.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99972636,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "The snippet splits the data into training, validation, and test sets, displaying the shapes of these resulting DataFrames for further model training and validation processes.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99615127,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet reads Titanic training and test datasets from CSV files into pandas DataFrames and displays the first few rows of the Titanic training dataset.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9996555,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "titanic_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "# preview the data\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet loads the Titanic dataset from CSV files into DataFrames, appends them into a single DataFrame, and then splits it back into training and test sets, printing their shapes.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9997263550758362,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Extraction",
    "desc": "This code snippet splits the preprocessed dataset into training, validation, and test sets, then prints the shapes of each resulting dataset to verify the splits.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9961512684822084,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code loads Titanic dataset CSV files into Pandas DataFrames, concatenates them for a full dataset, and then separates the original training set while deleting intermediate variables.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99972636,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "This code splits the combined dataset into training, validation, and test sets, with the training and validation sets further split into training and validation subsets for model training and evaluation.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99615127,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "The code snippet reads train and test CSV files into Pandas DataFrames and combines them into a single DataFrame while also creating a separate DataFrame for the Titanic dataset, displaying their shapes as well.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9997263550758362,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Extraction",
    "desc": "The code snippet splits the dataset into training and validation sets for both features and target variable, using 70% of the data for training, and prints the shapes of the full, training, validation, and test sets.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9961512684822084,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code loads the Titanic dataset from CSV files into DataFrames, combines them into one, extracts the training data, and outputs the shapes of the combined and training datasets.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99970263,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "print(\"lenthOfTrain:\", len(train), \"lenthOfTest:\",len(test))\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n",
    "del train , test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Extraction",
    "desc": "This code splits the data into training, validation, and test sets to prepare for model training, with 70% of the data allocated for training and the remaining 30% for validation.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99615127,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet reads train and test datasets from CSV files and displays information about the training dataset.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9997268319129944,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train_df = pandas.read_csv(\"../input/train.csv\")\n",
    "test_df = pandas.read_csv(\"../input/test.csv\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet reads the Titanic train and test CSV files into DataFrames, combines them into a single DataFrame, and then extracts the training portion while deleting the original train and test DataFrames, and prints the shapes of the resulting DataFrames.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99972636,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test    = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "full = train.append( test , ignore_index = True )\n",
    "titanic = full[ :891 ]\n",
    "\n",
    "del train , test\n",
    "\n",
    "print ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet loads the training, testing, and sample submission datasets from CSV files into pandas DataFrame objects.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9997285,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# načítanie dát\n",
    "train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test_df = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Extraction",
    "desc": "This code snippet splits the training dataset into training and validation sets for both the text data and target variable, with 20% of the data allocated to the validation set.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9982122,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# rozdelíme si dáta na treningovú a validačnú časť\n",
    "x_train_text, x_val_text, y_train, y_val = train_test_split(train_df.text, train_df.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "The code loads the training and test datasets from CSV files located in the specified directory into pandas DataFrames.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99975044,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This snippet reads three CSV files containing training data, test data, and sample submission data into pandas DataFrames. ",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.99961096,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "testset = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Extraction",
    "desc": "This snippet reads pre-trained GloVe word embeddings from a file and stores them in a dictionary, where each word is associated with its corresponding vector representation.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.7639546,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet reads the training and testing data from CSV files, prints the shape and columns of the datasets, and displays the first few rows of the training data.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99969697,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "print(train_data.shape)\n",
    "print(train_data.columns)\n",
    "print(test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 50,
    "class": "Data Extraction",
    "desc": "This code snippet reads the sample submission file and prints its column names.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9996171,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n",
    "print(sample_submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Extraction",
    "desc": "This code snippet reads the BERT vocabulary file, creates a dictionary mapping tokens to their indices, and initializes a tokenizer using this token dictionary.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.8598209,
    "start_cell": true,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Get bert model token\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "        \n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Extraction",
    "desc": "This code snippet reads training and evaluation datasets from CSV files into pandas DataFrames and displays the first few rows of the training dataset.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9996464,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "data_eval = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Extraction",
    "desc": "This code snippet defines a function to tokenize the text data using the BERT tokenizer and generate input arrays for the BERT model, then applies this function to the cleansed training data to obtain the training features and labels.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9990231,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "# Get train dataset\n",
    "def get_X(data, column_name = 'text'):\n",
    "    X1 = [tokenizer.encode(text, max_len=SEQ_LEN)[0]  for text in data[column_name] ]\n",
    "    X1 = np.array(X1)\n",
    "\n",
    "    X2 = np.zeros_like(X1)\n",
    "    X = [X1,X2]\n",
    "    return X\n",
    "\n",
    "X_train = get_X(data_cleansed)\n",
    "y_train = np.array(data_cleansed.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code reads the training and test datasets from CSV files using pandas and loads them into dataframes for further processing.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9997514,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Extraction",
    "desc": "This code reads the previously saved TSV files and converts them into torchtext TabularDataset objects, preparing them as datasets for training/evaluation and testing.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.40950394,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# 各tsvファイルを読み込み、datasetにする\n",
    "# 少し時間がかかる\n",
    "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(path='.', train='./train_eval.tsv', test='./test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Extraction",
    "desc": "This snippet reads the train, test, and sample submission CSV files into pandas DataFrame objects from specified file paths for use in the machine learning task.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9996797,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "tweet= pd.read_csv('../input/nlp-getting-started/train.csv') # train\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv') # test\n",
    "submission= pd.read_csv('../input/nlp-getting-started/sample_submission.csv') # submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Data Extraction",
    "desc": "This snippet concatenates the train and test DataFrames and outputs the shape of the resulting DataFrame, giving insights into its size for subsequent analysis or processing.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99860877,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "df=pd.concat([tweet,test])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 45,
    "class": "Data Extraction",
    "desc": "This snippet extracts the 'text' column and 'target' column from the train DataFrame subset into separate variables `X` and `y`, preparing the features and labels for model training.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9993303,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "X = df_train['text']\n",
    "y = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 46,
    "class": "Data Extraction",
    "desc": "This snippet splits the train data (`X` and `y`) into training and validation sets with a 90-10 split ratio and a defined random state for reproducibility, preparing the data for model training and validation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9982486,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 52,
    "class": "Data Extraction",
    "desc": "This snippet reads the GloVe pre-trained word vectors from a file and stores them in a dictionary with words as keys and their corresponding 100-dimensional embedding vectors as values, enabling the use of these embeddings in the model.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.5795064,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# 100D\n",
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word = values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 54,
    "class": "Data Extraction",
    "desc": "This snippet extracts and prints the number of unique words in the corpus by accessing the word index from the tokenizer, giving an insight into the vocabulary size of the dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.98261434,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 60,
    "class": "Data Extraction",
    "desc": "This snippet splits the training data and corresponding target values into training and validation sets with an 80-20 split ratio, then prints the shapes of these sets to provide insights into their dimensions for subsequent model training and validation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9958903,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.2)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 75,
    "class": "Data Extraction",
    "desc": "This snippet reads the train and test CSV files into pandas DataFrames from specified file paths, preparing the training and test datasets for further processing and analysis.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9997279,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# Load CSV files containing training data\n",
    "train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Extraction",
    "desc": "This code reads the train and test datasets from CSV files into pandas DataFrames with specified data types for certain columns.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.99974877,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv', dtype={'id': np.int16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Extraction",
    "desc": "This code splits the training DataFrame into training, testing, and development sets using an 80-10-10 split ratio.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.99713624,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, tbd_test = train_test_split(df_train, test_size=0.2)\n",
    "test, dev = train_test_split(tbd_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet loads a pre-trained Doc2Vec model from a specified file path using the Gensim library.  ",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9956636,
    "start_cell": true,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "Pretrained_Model=Doc2Vec.load('../input/pretrained-0923-2249/Pretrained_2249.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Extraction",
    "desc": "This code snippet reads training and test datasets from CSV files into pandas DataFrames named `tweet` and `test`.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99973994,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "tweet=pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Extraction",
    "desc": "This code snippet reads the sample submission CSV file into a DataFrame named `sub`.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99963653,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet reads training and test datasets from CSV files and displays the first three rows of the training dataset to get an initial glimpse of the data.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9995697,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "import os\n",
    "tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Extraction",
    "desc": "This code snippet reads pre-trained GloVe word embeddings from a text file and stores them in a dictionary where each key is a word, and the corresponding value is its embedding vector.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.7639546,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Data Extraction",
    "desc": "This code snippet reads a sample submission file from a CSV into a DataFrame for further use in the prediction and submission process.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9996909,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code reads the training and testing datasets from CSV files into pandas DataFrames.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9997478,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Extraction",
    "desc": "This code defines a `Dataset` class that loads and preprocesses text data from files into TorchText datasets, creates vocabularies and word embeddings, and sets up iterators for training, validation, and test datasets.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.5782217,
    "start_cell": true,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_iterator = None\n",
    "        self.test_iterator = None\n",
    "        self.val_iterator = None\n",
    "        self.vocab = []\n",
    "        self.word_embeddings = {}\n",
    "    \n",
    "    def parse_label(self, label):\n",
    "        '''\n",
    "        Get the actual labels from label string\n",
    "        Input:\n",
    "            label (string) : labels of the form '0 or 1'\n",
    "        Returns:\n",
    "            label (int) : integer value corresponding to label string\n",
    "        '''\n",
    "        return int(label)\n",
    "\n",
    "    def get_pandas_df(self, filename, isTest=False):\n",
    "        '''\n",
    "        Load the data into Pandas.DataFrame object\n",
    "        This will be used to convert data to torchtext object\n",
    "        '''\n",
    "        if isTest:\n",
    "            data = pd.read_csv(filename)\n",
    "            data_text = list(data.text)\n",
    "\n",
    "            full_df = pd.DataFrame({\"text\": data_text})\n",
    "        else:\n",
    "            data = pd.read_csv(filename)\n",
    "            data_text = list(data.text)\n",
    "            data_label = list(data.target)\n",
    "\n",
    "            full_df = pd.DataFrame({\"text\": data_text, \"label\": data_label})\n",
    "            \n",
    "        return clean_data(full_df)\n",
    "    \n",
    "    def load_data(self, w2v_file, train_file, test_file=None, val_file=None):\n",
    "        '''\n",
    "        Loads the data from files\n",
    "        Sets up iterators for training, validation and test data\n",
    "        Also create vocabulary and word embeddings based on the data\n",
    "        \n",
    "        Inputs:\n",
    "            w2v_file (String): path to file containing word embeddings (GloVe/Word2Vec)\n",
    "            train_file (String): path to training file\n",
    "            test_file (String): path to test file\n",
    "            val_file (String): path to validation file\n",
    "        '''\n",
    "\n",
    "        NLP = spacy.load('en')\n",
    "        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \"]\n",
    "        \n",
    "        # Creating Field for data\n",
    "        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n",
    "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "        datafields = [(\"text\",TEXT),(\"label\",LABEL)]\n",
    "        testfields = [(\"text\",TEXT)]\n",
    "        \n",
    "        # Load data from pd.DataFrame into torchtext.data.Dataset\n",
    "        train_df = self.get_pandas_df(train_file)\n",
    "        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
    "        train_data = data.Dataset(train_examples, datafields)\n",
    "        \n",
    "        if test_file:\n",
    "            test_df = self.get_pandas_df(test_file, isTest=True)\n",
    "            test_examples = [data.Example.fromlist(i, testfields) for i in test_df.values.tolist()]\n",
    "            test_data = data.Dataset(test_examples, testfields)\n",
    "        \n",
    "        # If validation file exists, load it. Otherwise get validation data from training data\n",
    "        if val_file:\n",
    "            val_df = self.get_pandas_df(val_file)\n",
    "            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n",
    "            val_data = data.Dataset(val_examples, datafields)\n",
    "        else:\n",
    "            train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "        \n",
    "        if w2v_file:\n",
    "            TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n",
    "        self.word_embeddings = TEXT.vocab.vectors\n",
    "        self.vocab = TEXT.vocab\n",
    "        \n",
    "        self.train_iterator = data.BucketIterator(\n",
    "            (train_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "        \n",
    "        self.val_iterator = data.BucketIterator(\n",
    "            (val_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=False)\n",
    "        \n",
    "        if test_file:\n",
    "            self.test_iterator = data.BucketIterator(\n",
    "            (test_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=False)\n",
    "        \n",
    "        print (\"Loaded {} training examples\".format(len(train_data)))\n",
    "        print (\"Loaded {} validation examples\".format(len(val_data)))\n",
    "        \n",
    "        if test_file:\n",
    "            print (\"Loaded {} test examples\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Extraction",
    "desc": "This code initializes the `Dataset` class with a configuration and loads the training and test data files, as well as the word embeddings file, into the dataset object.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.99949396,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "w2v_file = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\n",
    "train_file = \"../input/nlp-getting-started/train.csv\"\n",
    "test_file = \"../input/nlp-getting-started/test.csv\"\n",
    "\n",
    "config = Config()\n",
    "dataset = Dataset(config)\n",
    "dataset.load_data(w2v_file, train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Extraction",
    "desc": "This code reads the test data and ground truth labels for tweets, merges them on a common 'id' field, and extracts a DataFrame with 'id' and 'target' columns representing the ground truth labels.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9801941,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "\n",
    "gt_df = pd.read_csv(\"../input/disasters-on-social-media/socialmedia-disaster-tweets-DFE.csv\", encoding='latin_1')\n",
    "gt_df = gt_df[['choose_one', 'text']]\n",
    "gt_df['target'] = (gt_df['choose_one']=='Relevant').astype(int)\n",
    "gt_df['id'] = gt_df.index\n",
    "\n",
    "merged_df = pd.merge(test_data, gt_df, on='id')\n",
    "target_df = merged_df[['id', 'target']]\n",
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet traverses the directory `'/kaggle/input'` and prints the path of each file it finds, essentially listing all input data files available in that directory.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.999283,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Extraction",
    "desc": "This code snippet reads the training and testing datasets from CSV files into pandas DataFrames and defines a callback function that retrieves the result of an asynchronous operation.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9997203,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "nlp_train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "nlp_test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "def callback(operation_future):\n",
    "    result = operation_future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Extraction",
    "desc": "This code snippet checks if a dataset with the specified display name exists in AutoML, creates the dataset and imports training data from Google Cloud Storage if it does not exist, and prints timestamps before and after the dataset preparation.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.4550124,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "print(f'Getting dataset ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "if not amw.get_dataset_by_display_name(dataset_display_name):\n",
    "    print('dataset not found')\n",
    "    amw.create_dataset()\n",
    "    amw.import_gcs_data(training_gcs_path)\n",
    "\n",
    "amw.dataset\n",
    "print(f'Dataset ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "Iterates through directories and files in the specified path and prints the full path of each file found.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9990361,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Extraction",
    "desc": "Loads the training, testing, and sample submission datasets into pandas DataFrames from CSV files.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9997396,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Extraction",
    "desc": "Reads the pre-trained GloVe word embeddings from a text file and stores them in a dictionary where keys are words and values are the corresponding vectors.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.7639546,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "The code reads the training data from a CSV file into a pandas DataFrame and displays the first five rows.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9996433,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "The code reads the testing data from a CSV file into a pandas DataFrame and displays the first five rows.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9996711,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Extraction",
    "desc": "The code loads pre-trained GloVe word embeddings from a file into a dictionary, where keys are words and values are their corresponding embedding vectors.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.23091643,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Loading the embedding dictionary from file\n",
    "\n",
    "embedding_dict={}\n",
    "with open('../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word = values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "The code snippet reads CSV files containing training data, test data, and sample submission data into pandas DataFrames.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9997112,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "submission_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet lists the contents of the specified directory containing the GloVe word vectors, which are likely to be used later for embedding in the model.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.7258447,
    "start_cell": true,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.listdir('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet loads the training and test datasets from CSV files into pandas DataFrames and displays the first three rows of the training dataset.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9996824,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Extraction",
    "desc": "This code snippet reads GloVe word vectors from a text file and stores them in a dictionary where each word is mapped to its corresponding vector representation.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.7639546,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 43,
    "class": "Data Extraction",
    "desc": "This code snippet loads the sample submission CSV file into a pandas DataFrame, which is likely to be used for preparing the final submission.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9996909,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code snippet reads training and testing data from CSV files into Pandas DataFrames and displays the first three rows of the training DataFrame.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99969065,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Extraction",
    "desc": "This code snippet reads pre-trained GloVe word embeddings from a text file and stores them in a dictionary where keys are words and values are their corresponding embedding vectors.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.7639546,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Data Extraction",
    "desc": "This code snippet reads the sample submission CSV file into a Pandas DataFrame.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9996909,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Extraction",
    "desc": "This code snippet extracts the target variable from the training dataset and assigns it to the variable `y`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99899393,
    "start_cell": true,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "#extract dependent variable\n",
    "y=train_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Extraction",
    "desc": "This code snippet splits the dataset into training and testing sets with 20% of the data allocated for testing, using the `train_test_split` function and preserving the target variable `y`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9978635,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "#creating 20% of test data from our dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_train,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Extraction",
    "desc": "This code snippet extracts the 'id' column from the test dataset and assigns it to the variable `id`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9980877,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "id = test_data.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Extraction",
    "desc": "The code reads a CSV file containing training data into a Pandas DataFrame from a specified filepath.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99972755,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Data Extraction",
    "desc": "The code reads a CSV file containing test data into a Pandas DataFrame from a specified filepath, then selects and retains only the 'text' column.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9997379,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "test=test[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Data Extraction",
    "desc": "The code reads a CSV file containing a sample submission template into a Pandas DataFrame from a specified filepath.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9996799,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "submission_sample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code loads the training data from a CSV file into a pandas DataFrame and displays the first few rows.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.99962294,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Extraction",
    "desc": "This code splits the combined text data and the target labels into training and testing sets with a specified test size and random seed for reproducibility.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9980743,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X = train['combine']\n",
    "y = train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Extraction",
    "desc": "This code loads the training and test data from CSV files into pandas DataFrames and displays the first few rows of the test DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9997023,
    "start_cell": false,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Extraction",
    "desc": "This code extracts the combined text data for training and test datasets, as well as the target labels from the training dataset.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9993155,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "X_train = train['combine']\n",
    "y_train = train['target']\n",
    "\n",
    "X_test = test['combine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code loads training, testing, and sample submission data from CSV files into pandas DataFrames and displays the first few rows of the training data.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99940705,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "sub_sample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Extraction",
    "desc": "This code displays the first few rows of the test data DataFrame.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9997483,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Extraction",
    "desc": "This code displays the first few rows of the test data DataFrame.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9995821,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "#train.head()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Data Extraction",
    "desc": "This code displays the first 10 rows of the test data DataFrame.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.999762,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Extraction",
    "desc": "This code snippet loads the training, testing, and sample submission datasets from CSV files into pandas DataFrames.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9997086,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code snippet drops the columns 'PassengerId', 'Name', and 'Ticket' from the Titanic training dataset and 'Name' and 'Ticket' from the test dataset as they are deemed unnecessary for analysis and prediction.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.98214287,
    "start_cell": true,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns, these columns won't be useful in analysis and prediction\n",
    "titanic_df = titanic_df.drop(['PassengerId','Name','Ticket'], axis=1)\n",
    "test_df    = test_df.drop(['Name','Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "This code snippet handles missing values in the 'Embarked' column, visualizes its relationship with survival rates, and creates dummy variables for the 'Embarked' column before dropping the original categorical column from the Titanic datasets.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.99828136,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Embarked\n",
    "\n",
    "# only in titanic_df, fill the two missing values with the most occurred value, which is \"S\".\n",
    "titanic_df[\"Embarked\"] = titanic_df[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# plot\n",
    "sns.factorplot('Embarked','Survived', data=titanic_df,size=4,aspect=3)\n",
    "\n",
    "fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)\n",
    "# sns.factorplot('Survived',hue=\"Embarked\",data=titanic_df,kind='count',order=[1,0],ax=axis2)\n",
    "sns.countplot(x='Embarked', data=titanic_df, ax=axis1)\n",
    "sns.countplot(x='Survived', hue=\"Embarked\", data=titanic_df, order=[1,0], ax=axis2)\n",
    "\n",
    "# group by embarked, and get the mean for survived passengers for each value in Embarked\n",
    "embark_perc = titanic_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\n",
    "sns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)\n",
    "\n",
    "# Either to consider Embarked column in predictions,\n",
    "# and remove \"S\" dummy variable, \n",
    "# and leave \"C\" & \"Q\", since they seem to have a good rate for Survival.\n",
    "\n",
    "# OR, don't create dummy variables for Embarked column, just drop it, \n",
    "# because logically, Embarked doesn't seem to be useful in prediction.\n",
    "\n",
    "embark_dummies_titanic  = pd.get_dummies(titanic_df['Embarked'])\n",
    "embark_dummies_titanic.drop(['S'], axis=1, inplace=True)\n",
    "\n",
    "embark_dummies_test  = pd.get_dummies(test_df['Embarked'])\n",
    "embark_dummies_test.drop(['S'], axis=1, inplace=True)\n",
    "\n",
    "titanic_df = titanic_df.join(embark_dummies_titanic)\n",
    "test_df    = test_df.join(embark_dummies_test)\n",
    "\n",
    "titanic_df.drop(['Embarked'], axis=1,inplace=True)\n",
    "test_df.drop(['Embarked'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code snippet handles missing values in the 'Fare' column of the test dataset by filling it with the median fare, converts fare values to integers, analyzes fare statistics for survived and non-survived passengers, and visualizes the fare distribution and its relationship with survival.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.91612476,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# Fare\n",
    "\n",
    "# only for test_df, since there is a missing \"Fare\" values\n",
    "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
    "\n",
    "# convert from float to int\n",
    "titanic_df['Fare'] = titanic_df['Fare'].astype(int)\n",
    "test_df['Fare']    = test_df['Fare'].astype(int)\n",
    "\n",
    "# get fare for survived & didn't survive passengers \n",
    "fare_not_survived = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 0]\n",
    "fare_survived     = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 1]\n",
    "\n",
    "# get average and std for fare of survived/not survived passengers\n",
    "avgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\n",
    "std_fare      = DataFrame([fare_not_survived.std(), fare_survived.std()])\n",
    "\n",
    "# plot\n",
    "titanic_df['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n",
    "\n",
    "avgerage_fare.index.names = std_fare.index.names = [\"Survived\"]\n",
    "avgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "This code snippet handles missing values in the 'Age' column by filling them with random integers generated within one standard deviation of the mean age, and then converts the 'Age' values to integers while visualizing the distributions before and after imputation.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.67031723,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Age \n",
    "\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "axis1.set_title('Original Age values - Titanic')\n",
    "axis2.set_title('New Age values - Titanic')\n",
    "\n",
    "# axis3.set_title('Original Age values - Test')\n",
    "# axis4.set_title('New Age values - Test')\n",
    "\n",
    "# get average, std, and number of NaN values in titanic_df\n",
    "average_age_titanic   = titanic_df[\"Age\"].mean()\n",
    "std_age_titanic       = titanic_df[\"Age\"].std()\n",
    "count_nan_age_titanic = titanic_df[\"Age\"].isnull().sum()\n",
    "\n",
    "# get average, std, and number of NaN values in test_df\n",
    "average_age_test   = test_df[\"Age\"].mean()\n",
    "std_age_test       = test_df[\"Age\"].std()\n",
    "count_nan_age_test = test_df[\"Age\"].isnull().sum()\n",
    "\n",
    "# generate random numbers between (mean - std) & (mean + std)\n",
    "rand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, size = count_nan_age_titanic)\n",
    "rand_2 = np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)\n",
    "\n",
    "# plot original Age values\n",
    "# NOTE: drop all null values, and convert to int\n",
    "titanic_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n",
    "# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n",
    "\n",
    "# fill NaN values in Age column with random values generated\n",
    "titanic_df[\"Age\"][np.isnan(titanic_df[\"Age\"])] = rand_1\n",
    "test_df[\"Age\"][np.isnan(test_df[\"Age\"])] = rand_2\n",
    "\n",
    "# convert from float to int\n",
    "titanic_df['Age'] = titanic_df['Age'].astype(int)\n",
    "test_df['Age']    = test_df['Age'].astype(int)\n",
    "        \n",
    "# plot new Age Values\n",
    "titanic_df['Age'].hist(bins=70, ax=axis2)\n",
    "# test_df['Age'].hist(bins=70, ax=axis4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet drops the 'Cabin' column from both the Titanic training and test datasets due to its high number of missing values.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9949832,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Cabin\n",
    "# It has a lot of NaN values, so it won't cause a remarkable impact on prediction\n",
    "titanic_df.drop(\"Cabin\",axis=1,inplace=True)\n",
    "test_df.drop(\"Cabin\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet combines the 'Parch' and 'SibSp' columns into a single 'Family' column indicating whether a passenger had any family members aboard, removes the original 'Parch' and 'SibSp' columns, and visualizes the relationship between family presence and survival rate.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9953224,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Family\n",
    "\n",
    "# Instead of having two columns Parch & SibSp, \n",
    "# we can have only one column represent if the passenger had any family member aboard or not,\n",
    "# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\n",
    "titanic_df['Family'] =  titanic_df[\"Parch\"] + titanic_df[\"SibSp\"]\n",
    "titanic_df['Family'].loc[titanic_df['Family'] > 0] = 1\n",
    "titanic_df['Family'].loc[titanic_df['Family'] == 0] = 0\n",
    "\n",
    "test_df['Family'] =  test_df[\"Parch\"] + test_df[\"SibSp\"]\n",
    "test_df['Family'].loc[test_df['Family'] > 0] = 1\n",
    "test_df['Family'].loc[test_df['Family'] == 0] = 0\n",
    "\n",
    "# drop Parch & SibSp\n",
    "titanic_df = titanic_df.drop(['SibSp','Parch'], axis=1)\n",
    "test_df    = test_df.drop(['SibSp','Parch'], axis=1)\n",
    "\n",
    "# plot\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\n",
    "\n",
    "# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)\n",
    "sns.countplot(x='Family', data=titanic_df, order=[1,0], ax=axis1)\n",
    "\n",
    "# average of survived for those who had/didn't have any family member\n",
    "family_perc = titanic_df[[\"Family\", \"Survived\"]].groupby(['Family'],as_index=False).mean()\n",
    "sns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n",
    "\n",
    "axis1.set_xticklabels([\"With Family\",\"Alone\"], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet classifies passengers as 'child', 'male', or 'female' based on age and sex, replaces the 'Sex' column with the new 'Person' column, creates dummy variables for 'Person', and visualizes the distribution and survival rates of the different categories.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9766011,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Sex\n",
    "\n",
    "# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.\n",
    "# So, we can classify passengers as males, females, and child\n",
    "def get_person(passenger):\n",
    "    age,sex = passenger\n",
    "    return 'child' if age < 16 else sex\n",
    "    \n",
    "titanic_df['Person'] = titanic_df[['Age','Sex']].apply(get_person,axis=1)\n",
    "test_df['Person']    = test_df[['Age','Sex']].apply(get_person,axis=1)\n",
    "\n",
    "# No need to use Sex column since we created Person column\n",
    "titanic_df.drop(['Sex'],axis=1,inplace=True)\n",
    "test_df.drop(['Sex'],axis=1,inplace=True)\n",
    "\n",
    "# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers\n",
    "person_dummies_titanic  = pd.get_dummies(titanic_df['Person'])\n",
    "person_dummies_titanic.columns = ['Child','Female','Male']\n",
    "person_dummies_titanic.drop(['Male'], axis=1, inplace=True)\n",
    "\n",
    "person_dummies_test  = pd.get_dummies(test_df['Person'])\n",
    "person_dummies_test.columns = ['Child','Female','Male']\n",
    "person_dummies_test.drop(['Male'], axis=1, inplace=True)\n",
    "\n",
    "titanic_df = titanic_df.join(person_dummies_titanic)\n",
    "test_df    = test_df.join(person_dummies_test)\n",
    "\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)\n",
    "sns.countplot(x='Person', data=titanic_df, ax=axis1)\n",
    "\n",
    "# average of survived for each Person(male, female, or child)\n",
    "person_perc = titanic_df[[\"Person\", \"Survived\"]].groupby(['Person'],as_index=False).mean()\n",
    "sns.barplot(x='Person', y='Survived', data=person_perc, ax=axis2, order=['male','female','child'])\n",
    "\n",
    "titanic_df.drop(['Person'],axis=1,inplace=True)\n",
    "test_df.drop(['Person'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code snippet visualizes the relationship between passenger class (Pclass) and survival rates, creates dummy variables for the 'Pclass' column while dropping the class with the lowest survival rate, and removes the original 'Pclass' column from the datasets.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.99851733,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Pclass\n",
    "\n",
    "# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])\n",
    "sns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_df,size=5)\n",
    "\n",
    "# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n",
    "pclass_dummies_titanic  = pd.get_dummies(titanic_df['Pclass'])\n",
    "pclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\n",
    "pclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n",
    "\n",
    "pclass_dummies_test  = pd.get_dummies(test_df['Pclass'])\n",
    "pclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\n",
    "pclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n",
    "\n",
    "titanic_df.drop(['Pclass'],axis=1,inplace=True)\n",
    "test_df.drop(['Pclass'],axis=1,inplace=True)\n",
    "\n",
    "titanic_df = titanic_df.join(pclass_dummies_titanic)\n",
    "test_df    = test_df.join(pclass_dummies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Transform",
    "desc": "This code snippet defines the training and testing sets by separating the 'Survived' column as the target variable and removing the 'PassengerId' column from the test dataset.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9993105,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "# define training and testing sets\n",
    "\n",
    "X_train = titanic_df.drop(\"Survived\",axis=1)\n",
    "Y_train = titanic_df[\"Survived\"]\n",
    "X_test  = test_df.drop(\"PassengerId\",axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet creates a mapping dictionary that assigns unique numerical values to each unique gender in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.67645174,
    "start_cell": true,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "sexes = sorted(df_train['Sex'].unique())\n",
    "genders_mapping = dict(zip(sexes, range(0, len(sexes) + 1)))\n",
    "genders_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet maps the gender values in the training DataFrame to their corresponding numerical values and creates a new column 'Sex_Val' to store these integer values.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.97366714,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "df_train['Sex_Val'] = df_train['Sex'].map(genders_mapping).astype(int)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet creates a mapping dictionary that assigns unique numerical values to each unique port of embarkation in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.76496696,
    "start_cell": false,
    "subclass": "show_unique_values",
    "subclass_id": 57
   },
   "outputs": [],
   "source": [
    "# Get the unique values of Embarked\n",
    "embarked_locs = sorted(df_train['Embarked'].unique(), key=lambda x: str(x))\n",
    "\n",
    "embarked_locs_mapping = dict(zip(embarked_locs, \n",
    "                                 range(0, len(embarked_locs) + 1)))\n",
    "embarked_locs_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet maps the port of embarkation values in the training DataFrame to their corresponding numerical values and creates a new column 'Embarked_Val' to store these integer values.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.92997646,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "df_train['Embarked_Val'] = df_train['Embarked'] \\\n",
    "                               .map(embarked_locs_mapping) \\\n",
    "                               .astype(int)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet checks for any missing 'Embarked' values and replaces them in the 'Embarked_Val' column with the numerical value corresponding to the most common embarkation point 'S'.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99814606,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "if len(df_train[df_train['Embarked'].isnull()] > 0):\n",
    "    df_train.replace({'Embarked_Val' : \n",
    "                   { embarked_locs_mapping[np.nan] : embarked_locs_mapping['S'] \n",
    "                   }\n",
    "               }, \n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This code snippet adds one-hot encoded dummy variables for the 'Embarked_Val' column to the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.6991104,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, pd.get_dummies(df_train['Embarked_Val'], prefix='Embarked_Val')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "This code snippet creates a copy of the 'Age' column named 'AgeFill' and fills in the missing age values by using the median age within each group defined by gender and passenger class.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.938906,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# To keep Age in tact, make a copy of it called AgeFill \n",
    "# that we will use to fill in the missing ages:\n",
    "df_train['AgeFill'] = df_train['Age']\n",
    "\n",
    "# Populate AgeFill\n",
    "df_train['AgeFill'] = df_train['AgeFill'] \\\n",
    "                        .groupby([df_train['Sex_Val'], df_train['Pclass']]) \\\n",
    "                        .apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Transform",
    "desc": "This code snippet creates a new column 'FamilySize' in the training DataFrame by summing the values of the 'SibSp' and 'Parch' columns.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9988938,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df_train['FamilySize'] = df_train['SibSp'] + df_train['Parch']\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Transform",
    "desc": "This code snippet drops the 'Name', 'Sex', 'Ticket', 'Cabin', and 'Embarked' columns from the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99925166,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], \n",
    "                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Transform",
    "desc": "This code snippet drops additional columns 'Age', 'SibSp', 'Parch', 'PassengerId', and 'Embarked_Val' from the training DataFrame and then outputs the data types of the remaining columns.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99037737,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Age', 'SibSp', 'Parch', 'PassengerId', 'Embarked_Val'], axis=1)\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Transform",
    "desc": "This code snippet converts the training DataFrame into a NumPy array and assigns it to the variable `train_data`.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.38043734,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "train_data = df_train.values\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Data Transform",
    "desc": "This code snippet defines a function `clean_data` that processes a DataFrame by transforming categorical variables, filling missing values, creating new features, and dropping irrelevant columns.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.4435929,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def clean_data(df, drop_passenger_id):\n",
    "    \n",
    "    # Get the unique values of Sex\n",
    "    sexes = sorted(df['Sex'].unique())\n",
    "    \n",
    "    # Generate a mapping of Sex from a string to a number representation    \n",
    "    genders_mapping = dict(zip(sexes, range(0, len(sexes) + 1)))\n",
    "\n",
    "    # Transform Sex from a string to a number representation\n",
    "    df['Sex_Val'] = df['Sex'].map(genders_mapping).astype(int)\n",
    "    \n",
    "    # Get the unique values of Embarked\n",
    "    embarked_locs = sorted(df['Embarked'].unique(), key=lambda x: str(x))\n",
    "\n",
    "    # Generate a mapping of Embarked from a string to a number representation        \n",
    "    embarked_locs_mapping = dict(zip(embarked_locs, \n",
    "                                     range(0, len(embarked_locs) + 1)))\n",
    "    \n",
    "    # Transform Embarked from a string to dummy variables\n",
    "    df = pd.concat([df, pd.get_dummies(df['Embarked'], prefix='Embarked_Val')], axis=1)\n",
    "    \n",
    "    # Fill in missing values of Embarked\n",
    "    # Since the vast majority of passengers embarked in 'S': 3, \n",
    "    # we assign the missing values in Embarked to 'S':\n",
    "    if len(df[df['Embarked'].isnull()] > 0):\n",
    "        df.replace({'Embarked_Val' : \n",
    "                       { embarked_locs_mapping[np.nan] : embarked_locs_mapping['S'] \n",
    "                       }\n",
    "                   }, \n",
    "                   inplace=True)\n",
    "    \n",
    "    # Fill in missing values of Fare with the average Fare\n",
    "    if len(df[df['Fare'].isnull()] > 0):\n",
    "        avg_fare = df['Fare'].mean()\n",
    "        df.replace({ None: avg_fare }, inplace=True)\n",
    "    \n",
    "    # To keep Age in tact, make a copy of it called AgeFill \n",
    "    # that we will use to fill in the missing ages:\n",
    "    df['AgeFill'] = df['Age']\n",
    "\n",
    "    # Determine the Age typical for each passenger class by Sex_Val.  \n",
    "    # We'll use the median instead of the mean because the Age \n",
    "    # histogram seems to be right skewed.\n",
    "    df['AgeFill'] = df['AgeFill'] \\\n",
    "                        .groupby([df['Sex_Val'], df['Pclass']]) \\\n",
    "                        .apply(lambda x: x.fillna(x.median()))\n",
    "            \n",
    "    # Define a new feature FamilySize that is the sum of \n",
    "    # Parch (number of parents or children on board) and \n",
    "    # SibSp (number of siblings or spouses):\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch']\n",
    "    \n",
    "    # Drop the columns we won't use:\n",
    "    df = df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
    "    \n",
    "    # Drop the Age column since we will be using the AgeFill column instead.\n",
    "    # Drop the SibSp and Parch columns since we will be using FamilySize.\n",
    "    # Drop the PassengerId column since it won't be used as a feature.\n",
    "    df = df.drop(['Age', 'SibSp', 'Parch'], axis=1)\n",
    "    \n",
    "    if drop_passenger_id:\n",
    "        df = df.drop(['PassengerId'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Data Transform",
    "desc": "This code snippet applies the `clean_data` function to the test DataFrame to process and transform it, then converts the cleaned DataFrame to a NumPy array.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.68029535,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Data wrangle the test set and convert it to a numpy array\n",
    "df_test = clean_data(df_test, drop_passenger_id=False)\n",
    "test_data = df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 43,
    "class": "Data Transform",
    "desc": "This code snippet imports necessary modules for evaluation, splits the training data into an 80-20 train-test split using `train_test_split`, and prints the shapes of the resulting arrays.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.68926775,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split 80-20 train vs test data\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_features, \n",
    "                                                    train_target, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=0)\n",
    "print (train_features.shape, train_target.shape)\n",
    "print (train_x.shape, train_y.shape)\n",
    "print (test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code handles missing values in the 'Embarked' column by filling them with the most frequent value ('S') and creates a new column 'Surname' by extracting surnames from the 'Name' column.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.7275170087814331,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Handle missing data for attribute 'Embarked'\n",
    "df_full.Embarked.value_counts().plot(kind='bar');\n",
    "df_full['Surname'] = df_full.Name.str.replace('(,.*)', '')\n",
    "# the bar plot of 'Embarked' shows, that the most people embarked from 'Southampton' \n",
    "# and therefore I assume, that 'S' is a good replacement for the two missing values.\n",
    "df_full.loc[df_full.Embarked.isnull(), 'Embarked'] = 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "This code extracts titles from the 'Name' column into a new column 'Title' to aid in handling missing 'Age' values and visualizes the distribution of these titles using a bar plot.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9883108139038086,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Handle NAN values for column 'Age'\n",
    "# A good strategy, which will be suggested by other Kaggle user, \n",
    "# is to derive the ages by the help of the 'Title', contained in the 'Name' field.\n",
    "df_full[\"Title\"] = df_full.Name.str.replace('(.*, )|(\\\\..*)', '')\n",
    "\n",
    "# show Title distribution\n",
    "df_full[\"Title\"].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code imputes the missing 'Age' values with the mean ages based on the 'Title' groups and converts the 'Age' column to integer type.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.6081375479698181,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# assign the mean Ages per Title to the missing records\n",
    "df_full.loc[(df_full.Age.isnull()) & (df_full.Title == 'Mr'), 'Age'] = 32\n",
    "df_full.loc[(df_full.Age.isnull()) & (df_full.Title == 'Ms'), 'Age'] = 28\n",
    "df_full.loc[(df_full.Age.isnull()) & (df_full.Title == 'Mrs'), 'Age'] = 36\n",
    "df_full.loc[(df_full.Age.isnull()) & (df_full.Title == 'Miss'), 'Age'] = 21\n",
    "df_full.loc[(df_full.Age.isnull()) & (df_full.Title == 'Master'), 'Age'] = 5\n",
    "df_full.loc[(df_full.Age.isnull()) & (df_full.Title == 'Dr'), 'Age'] = 43\n",
    "\n",
    "# convert age column to int\n",
    "df_full.Age = df_full.Age.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code imputes the missing 'Fare' value for the passenger with Id=1044 by using the mean fare rate for passengers who embarked in 'Southampton'.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9158570170402528,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Handle missing data for attribute 'Fare'\n",
    "# Since there is only one record missing, and this passenger (Id=1044) embarked in 'Southampton'\n",
    "# we assign the mean Fare rate for this Port to the passenger\n",
    "df_full.loc[1044, 'Fare'] = df_full.groupby('Embarked').Fare.mean()['S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code imputes the missing 'Deck' values derived from the 'Cabin' column by first deriving the 'Deck' values using surnames where possible and then using a Logistic Regression classifier for prediction based on 'Embarked', 'Fare', and 'Pclass'.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.8633211255073547,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Handle missing 'Cabin' data\n",
    "df_full.loc[:, 'Deck'] = df_full.Cabin.str[0]\n",
    "\n",
    "# Remove the Deck 'T'\n",
    "df_full.loc[df_full.Deck == 'T', 'Deck'] = np.nan\n",
    "\n",
    "for row in df_full.loc[df_full.Deck.notnull(), ['Deck', 'Surname']].itertuples():\n",
    "    df_full.loc[(df_full.Deck.isnull()) & (df_full.Surname == row.Surname), 'Deck'] = row.Deck\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "X_deck_train = df_full.loc[df_full.Deck.notnull(), ['Embarked', 'Fare', 'Pclass']]\n",
    "X_deck_test = df_full.loc[df_full.Deck.isnull(), ['Embarked', 'Fare', 'Pclass']]\n",
    "X_deck_train = pd.get_dummies(X_deck_train)\n",
    "X_deck_test = pd.get_dummies(X_deck_test)\n",
    "y_deck_train = df_full.loc[df_full.Deck.notnull(), ['Deck']]\n",
    "lr_clf.fit(X_deck_train, y_deck_train)\n",
    "y_deck_pred = lr_clf.predict(X_deck_test)\n",
    "# Since a lot of data is missing for this attribute, we remove this column fully\n",
    "#df_full.drop('Cabin', axis=1, inplace=True)\n",
    "df_deck_pred = pd.DataFrame({\n",
    "        \"Deck\": y_deck_pred\n",
    "        }, index=X_deck_test.index)\n",
    "\n",
    "df_full.loc[df_full.Deck.isnull(), 'Deck'] = df_deck_pred.Deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code one-hot encodes the categorical features in both the training and test datasets and extracts the target variable 'Survived' for the training set, then prints the new shapes of the transformed datasets.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9954836368560792,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# scale all categorical features\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "y_train = df_full.loc[df_full.Train == 1, 'Survived']\n",
    "\n",
    "print(\"Shape X Train: \", X_train.info())\n",
    "print(\"Shape X Test: \", X_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This commented-out code snippet intends to standardize the training and test datasets using `StandardScaler`, and print their shapes after standardization.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9977586269378662,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#stdsc = StandardScaler()\n",
    "#X_train_std = stdsc.fit_transform(X_train)\n",
    "#X_test_std = stdsc.transform(X_test)\n",
    "\n",
    "#print(\"Shape Std X Train: \", X_train_std.shape)\n",
    "#print(\"Shape Std X Test: \", X_test_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "The code snippet fills the null values in the 'Embarked' column of the 'titanic' dataset with the value 'C'.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.5111096,
    "start_cell": true,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Data Transform",
    "desc": "The code snippet defines and applies a function that fills the missing 'Fare' values in the 'titanic_test' dataset by taking the median fare of passengers in the 3rd class who embarked from 'S'.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.74725676,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "#we can replace missing value in fare by taking median of all fares of those passengers \n",
    "#who share 3rd Passenger class and Embarked from 'S' \n",
    "def fill_missing_fare(df):\n",
    "    median_fare=df[(df['Pclass'] == 3) & (df['Embarked'] == 'S')]['Fare'].median()\n",
    "#'S'\n",
    "       #print(median_fare)\n",
    "    df[\"Fare\"] = df[\"Fare\"].fillna(median_fare)\n",
    "    return df\n",
    "\n",
    "titanic_test=fill_missing_fare(titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Data Transform",
    "desc": "The code snippet creates a new column 'Deck' in both the 'titanic' and 'titanic_test' datasets by extracting the first character of the 'Cabin' column, and then displays the unique values in the 'Deck' column of the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.95299536,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "titanic[\"Deck\"]=titanic.Cabin.str[0]\n",
    "titanic_test[\"Deck\"]=titanic_test.Cabin.str[0]\n",
    "titanic[\"Deck\"].unique() # 0 is for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Data Transform",
    "desc": "The code snippet fills the missing values in the 'Deck' column with 'Z' in both the 'titanic' and 'titanic_test' datasets, and then displays the unique values in the 'Deck' column of the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99315345,
    "start_cell": false,
    "subclass": "show_unique_values",
    "subclass_id": 57
   },
   "outputs": [],
   "source": [
    "titanic.Deck.fillna('Z', inplace=True)\n",
    "titanic_test.Deck.fillna('Z', inplace=True)\n",
    "titanic[\"Deck\"].unique() # Z is for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Data Transform",
    "desc": "The code snippet creates a new 'FamilySize' variable in both the 'titanic' and 'titanic_test' datasets by summing the 'SibSp' and 'Parch' values and adding 1 (to include the passenger themselves), and then prints the value counts of 'FamilySize' in the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.82772696,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "# Create a family size variable including the passenger themselves\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]+1\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]+1\n",
    "print(titanic[\"FamilySize\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Data Transform",
    "desc": "The code snippet discretizes the 'FamilySize' variable into categories ('singleton', 'small', 'large') based on the family size and assigns these categories to the new 'FsizeD' column in both the 'titanic' and 'titanic_test' datasets, and then prints the unique values and their counts in the 'FsizeD' column of the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9184033,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Discretize family size\n",
    "titanic.loc[titanic[\"FamilySize\"] == 1, \"FsizeD\"] = 'singleton'\n",
    "titanic.loc[(titanic[\"FamilySize\"] > 1)  &  (titanic[\"FamilySize\"] < 5) , \"FsizeD\"] = 'small'\n",
    "titanic.loc[titanic[\"FamilySize\"] >4, \"FsizeD\"] = 'large'\n",
    "\n",
    "titanic_test.loc[titanic_test[\"FamilySize\"] == 1, \"FsizeD\"] = 'singleton'\n",
    "titanic_test.loc[(titanic_test[\"FamilySize\"] >1) & (titanic_test[\"FamilySize\"] <5) , \"FsizeD\"] = 'small'\n",
    "titanic_test.loc[titanic_test[\"FamilySize\"] >4, \"FsizeD\"] = 'large'\n",
    "print(titanic[\"FsizeD\"].unique())\n",
    "print(titanic[\"FsizeD\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Transform",
    "desc": "The code snippet creates a feature 'NameLength' by calculating the length of each name in the 'titanic' and 'titanic_test' datasets, bins this length into categories ('short', 'okay', 'good', 'long') in the new 'NlengthD' column, and visualizes the survival rate by these name length categories while printing out the unique values of 'NlengthD' in the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.98443025,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "#Create feture for length of name \n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n",
    "\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))\n",
    "#print(titanic[\"NameLength\"].value_counts())\n",
    "\n",
    "bins = [0, 20, 40, 57, 85]\n",
    "group_names = ['short', 'okay', 'good', 'long']\n",
    "titanic['NlengthD'] = pd.cut(titanic['NameLength'], bins, labels=group_names)\n",
    "titanic_test['NlengthD'] = pd.cut(titanic_test['NameLength'], bins, labels=group_names)\n",
    "\n",
    "sns.factorplot(x=\"NlengthD\", y=\"Survived\", data=titanic)\n",
    "print(titanic[\"NlengthD\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Transform",
    "desc": "The code snippet creates a 'Title' feature by extracting the title from the 'Name' column in both the 'titanic' and 'titanic_test' datasets, combines titles with very low counts into a 'Rare Title' category, and standardizes some titles, then prints the frequency of each title in the datasets.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.7872781,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    #If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "#Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "\n",
    "#Add in the title column.\n",
    "titanic[\"Title\"] = titles\n",
    "\n",
    "# Titles with very low cell counts to be combined to \"rare\" level\n",
    "rare_title = ['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n",
    "                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\n",
    "\n",
    "# Also reassign mlle, ms, and mme accordingly\n",
    "titanic.loc[titanic[\"Title\"] == \"Mlle\", \"Title\"] = 'Miss'\n",
    "titanic.loc[titanic[\"Title\"] == \"Ms\", \"Title\"] = 'Miss'\n",
    "titanic.loc[titanic[\"Title\"] == \"Mme\", \"Title\"] = 'Mrs'\n",
    "titanic.loc[titanic[\"Title\"] == \"Dona\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Lady\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Countess\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Capt\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Col\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Don\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Major\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Rev\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Sir\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Jonkheer\", \"Title\"] = 'Rare Title'\n",
    "titanic.loc[titanic[\"Title\"] == \"Dr\", \"Title\"] = 'Rare Title'\n",
    "\n",
    "#titanic.loc[titanic[\"Title\"].isin(['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n",
    "#                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']), \"Title\"] = 'Rare Title'\n",
    "\n",
    "#titanic[titanic['Title'].isin(['Dona', 'Lady', 'Countess'])]\n",
    "#titanic.query(\"Title in ('Dona', 'Lady', 'Countess')\")\n",
    "\n",
    "titanic[\"Title\"].value_counts()\n",
    "\n",
    "\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "#Add in the title column.\n",
    "titanic_test[\"Title\"] = titles\n",
    "\n",
    "# Titles with very low cell counts to be combined to \"rare\" level\n",
    "rare_title = ['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n",
    "                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\n",
    "\n",
    "# Also reassign mlle, ms, and mme accordingly\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Mlle\", \"Title\"] = 'Miss'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Ms\", \"Title\"] = 'Miss'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Mme\", \"Title\"] = 'Mrs'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Dona\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Lady\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Countess\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Capt\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Col\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Don\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Major\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Rev\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Sir\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Jonkheer\", \"Title\"] = 'Rare Title'\n",
    "titanic_test.loc[titanic_test[\"Title\"] == \"Dr\", \"Title\"] = 'Rare Title'\n",
    "\n",
    "titanic_test[\"Title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Data Transform",
    "desc": "The code snippet extracts numerical parts of the 'Ticket' column into a new 'TicketNumber' column and converts these extracted values to numeric type in both the 'titanic' and 'titanic_test' datasets.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9652063,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "titanic[\"TicketNumber\"] = titanic[\"Ticket\"].str.extract('(\\d{2,})', expand=True)\n",
    "titanic[\"TicketNumber\"] = titanic[\"TicketNumber\"].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "titanic_test[\"TicketNumber\"] = titanic_test[\"Ticket\"].str.extract('(\\d{2,})', expand=True)\n",
    "titanic_test[\"TicketNumber\"] = titanic_test[\"TicketNumber\"].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Data Transform",
    "desc": "The code snippet fills the NaN values in the 'TicketNumber' column with the median value of the 'TicketNumber' column in both the 'titanic' and 'titanic_test' datasets.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.81652987,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "titanic.TicketNumber.fillna(titanic[\"TicketNumber\"].median(), inplace=True)\n",
    "titanic_test.TicketNumber.fillna(titanic_test[\"TicketNumber\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Data Transform",
    "desc": "The code snippet applies Label Encoding to convert categorical variables ('Embarked', 'Sex', 'Title', 'FsizeD', 'NlengthD', 'Deck') into numerical values in both the 'titanic' and 'titanic_test' datasets.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.999188,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "labelEnc=LabelEncoder()\n",
    "\n",
    "cat_vars=['Embarked','Sex',\"Title\",\"FsizeD\",\"NlengthD\",'Deck']\n",
    "for col in cat_vars:\n",
    "    titanic[col]=labelEnc.fit_transform(titanic[col])\n",
    "    titanic_test[col]=labelEnc.fit_transform(titanic_test[col])\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Data Transform",
    "desc": "The code snippet defines a function that uses a Random Forest Regressor to predict and fill the missing values in the 'Age' column based on other features in the dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99451596,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#predicting missing values in age using Random Forest\n",
    "def fill_missing_age(df):\n",
    "    \n",
    "    #Feature set\n",
    "    age_df = df[['Age','Embarked','Fare', 'Parch', 'SibSp',\n",
    "                 'TicketNumber', 'Title','Pclass','FamilySize',\n",
    "                 'FsizeD','NameLength',\"NlengthD\",'Deck']]\n",
    "    # Split sets into train and test\n",
    "    train  = age_df.loc[ (df.Age.notnull()) ]# known Age values\n",
    "    test = age_df.loc[ (df.Age.isnull()) ]# null Ages\n",
    "    \n",
    "    # All age values are stored in a target array\n",
    "    y = train.values[:, 0]\n",
    "    \n",
    "    # All the other values are stored in the feature array\n",
    "    X = train.values[:, 1::]\n",
    "    \n",
    "    # Create and fit a model\n",
    "    rtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n",
    "    rtr.fit(X, y)\n",
    "    \n",
    "    # Use the fitted model to predict the missing values\n",
    "    predictedAges = rtr.predict(test.values[:, 1::])\n",
    "    \n",
    "    # Assign those predictions to the full data set\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 43,
    "class": "Data Transform",
    "desc": "The code snippet applies the previously defined `fill_missing_age` function to the 'titanic' and 'titanic_test' datasets to predict and fill the missing values in the 'Age' column.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.5447028,
    "start_cell": false,
    "subclass": "correct_missing_values",
    "subclass_id": 17
   },
   "outputs": [],
   "source": [
    "titanic=fill_missing_age(titanic)\n",
    "titanic_test=fill_missing_age(titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 45,
    "class": "Data Transform",
    "desc": "The code snippet standardizes the 'Age' and 'Fare' columns in both the 'titanic' and 'titanic_test' datasets using the StandardScaler from sklearn, which scales the features to have mean 0 and standard deviation 1.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9951989,
    "start_cell": false,
    "subclass": "normalization",
    "subclass_id": 18
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(titanic[['Age', 'Fare']])\n",
    "titanic[['Age', 'Fare']] = std_scale.transform(titanic[['Age', 'Fare']])\n",
    "\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(titanic_test[['Age', 'Fare']])\n",
    "titanic_test[['Age', 'Fare']] = std_scale.transform(titanic_test[['Age', 'Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Transform",
    "desc": "This code snippet prints the total number of records and the count of missing values for each column in the training and test datasets. ",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9617819,
    "start_cell": true,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "# Return missing values in dataframe for training set\n",
    "print(\"Total Records: {}\\n\".format(titanic[\"Age\"].count()))\n",
    "\n",
    "print(\"Training set missing values\")\n",
    "print(train.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing set missing values\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code snippet fills missing 'Age' values in the training and test datasets with the mean age, and also fills missing 'Fare' values in the test set with the mean fare, then confirms that there are no longer any missing values.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.7620353,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# replace each Null Age with the mean\n",
    "train[\"Age\"].fillna(train[\"Age\"].mean(), inplace=True)\n",
    "\n",
    "# apply the same preprocessing to test set\n",
    "test[\"Age\"].fillna(test[\"Age\"].mean(), inplace=True)\n",
    "test[\"Fare\"].fillna(test[\"Fare\"].mean(), inplace=True)\n",
    "\n",
    "# confirm values are no longer null\n",
    "print(\"Training set missing values after pre-processing\")\n",
    "print(train.isnull().sum())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "This code snippet adds a new column 'Child' to both the training and test datasets, categorizing passengers as 'Child' if they are under 18 years old and 'Adult' otherwise.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.99586004,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# create Child column into dataframe for train and test using a list comprehension\n",
    "train[\"Child\"] = [\"Child\" if int(x) < 18 else \"Adult\" for x in train[\"Age\"]]\n",
    "test[\"Child\"] = [\"Child\" if int(x) < 18 else \"Adult\" for x in test[\"Age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This code snippet creates a new column 'Family_size' in both the training and test datasets by summing the number of parents/children and siblings/spouses a passenger has and adding 1, and prints an example record involving a child with family indicators.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9731016,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# Family size column\n",
    "train[\"Family_size\"] = train[\"Parch\"].astype(np.int64) + train[\"SibSp\"].astype(np.int64) + 1 \n",
    "test[\"Family_size\"] = test[\"Parch\"].astype(np.int64) + test[\"SibSp\"].astype(np.int64) + 1 \n",
    "\n",
    "print(\"See below a record for a child with sibling and parent, we know have a Child and Family Size indicator: \\n\")\n",
    "print(train.iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet extracts titles from passenger names in both the training and test datasets using a regular expression, then prints the count of each title in the training dataset.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.91893446,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Use regex and str.extract method to extract title from name for test and train\n",
    "train[\"Title\"] = train[\"Name\"].str.extract(\"\\,\\s(.*?)\\.\" , expand=True)\n",
    "train[\"Title\"].str.strip(\" \")\n",
    "test[\"Title\"] = test[\"Name\"].str.extract(\"\\,\\s(.*?)\\.\" , expand=True)\n",
    "test[\"Title\"].str.strip(\" \")\n",
    "\n",
    "# Print list of values and the count for that data frame series\n",
    "train[\"Title\"].value_counts(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to identify mothers based on age, sex, title, and number of parents/children and applies this function to both the training and test datasets, then prints an example record identified as a mother.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9970427,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# create mothers\n",
    "def mother(row):\n",
    "  if row[\"Child\"] == \"Adult\" and row[\"Sex\"] == \"female\" and row[\"Title\"] == \"Mrs\" and row[\"Parch\"] > 0:\n",
    "    return \"Mother\"\n",
    "  else:\n",
    "    return \"Not Mother\"\n",
    "\n",
    "train[\"Mother\"] = train.apply(mother, axis=1)\n",
    "test[\"Mother\"] = test.apply(mother, axis=1)\n",
    "\n",
    "print(\"See below and example record for who we believe to be a mother:\\n\")\n",
    "print(train.iloc[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet adds a blank 'Survived' column to the test dataset, formats the test data using the Patsy library, predicts survival using the trained logistic regression model, and prints the average chance of survival for passengers in the test data.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.94871336,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "# add Survived column to test dataframe (blank)\n",
    "test[\"Survived\"] = \"\"\n",
    "\n",
    "# format using patsy to get a matrix to pass into the LR model\n",
    "yt, Xt = dmatrices('Survived ~ Title + Age + Sex + Child + Family_size + Mother + Fare',\n",
    "                  test, return_type=\"dataframe\")\n",
    "# flatten y into a 1-D array\n",
    "yt = np.ravel(yt)\n",
    "\n",
    "# predict on the data\n",
    "test[\"Survived\"] = LRmodel.predict(Xt)\n",
    "\n",
    "print(\"Chance of Survival for Passengers in Test Data:\")\n",
    "print(test[\"Survived\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code snippet extracts titles from the 'Name' column, maps them to numeric categories, adds this information to both the Titanic and test DataFrames, and then visualizes the survival rate by title using a stacked bar plot.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.69169146,
    "start_cell": true,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    if '.' in name:\n",
    "        return name.split(',')[1].split('.')[0].strip()\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def title_map(title):\n",
    "    if title in ['Mr']:\n",
    "        return 1\n",
    "    elif title in ['Master']:\n",
    "        return 3\n",
    "    elif title in ['Ms','Mlle','Miss']:\n",
    "        return 4\n",
    "    elif title in ['Mme','Mrs']:\n",
    "        return 5\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "titanic_df['title'] = titanic_df['Name'].apply(get_title).apply(title_map)   \n",
    "test_df['title'] = test_df['Name'].apply(get_title).apply(title_map)\n",
    "title_xt = pd.crosstab(titanic_df['title'], titanic_df['Survived'])\n",
    "title_xt_pct = title_xt.div(title_xt.sum(1).astype(float), axis=0)\n",
    "\n",
    "title_xt_pct.plot(kind='bar', \n",
    "                  stacked=True, \n",
    "                  title='Survival Rate by title')\n",
    "plt.xlabel('title')\n",
    "plt.ylabel('Survival Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "This code snippet removes the 'PassengerId', 'Name', and 'Ticket' columns from the Titanic DataFrame and the 'Name' and 'Ticket' columns from the test DataFrame, as they are deemed unnecessary for analysis and prediction.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.98214287,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns, these columns won't be useful in analysis and prediction\n",
    "titanic_df = titanic_df.drop(['PassengerId','Name','Ticket'], axis=1)\n",
    "test_df    = test_df.drop(['Name','Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code snippet fills missing values in the 'Embarked' column of the Titanic DataFrame, creates dummy variables for the 'Embarked' column, joins these dummy variables back to the original DataFrames, and then drops the original 'Embarked' column.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9976432,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Embarked\n",
    "\n",
    "# only in titanic_df, fill the two missing values with the most occurred value, which is \"S\".\n",
    "titanic_df[\"Embarked\"] = titanic_df[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# plot\n",
    "#sns.factorplot('Embarked','Survived', data=titanic_df,size=4,aspect=3)\n",
    "\n",
    "#fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)\n",
    "# sns.factorplot('Survived',hue=\"Embarked\",data=titanic_df,kind='count',order=[1,0],ax=axis2)\n",
    "#sns.countplot(x='Embarked', data=titanic_df, ax=axis1)\n",
    "#sns.countplot(x='Survived', hue=\"Embarked\", data=titanic_df, order=[1,0], ax=axis2)\n",
    "\n",
    "# group by embarked, and get the mean for survived passengers for each value in Embarked\n",
    "#embark_perc = titanic_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\n",
    "#sns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)\n",
    "\n",
    "# Either to consider Embarked column in predictions,\n",
    "# and remove \"S\" dummy variable, \n",
    "# and leave \"C\" & \"Q\", since they seem to have a good rate for Survival.\n",
    "\n",
    "# OR, don't create dummy variables for Embarked column, just drop it, \n",
    "# because logically, Embarked doesn't seem to be useful in prediction.\n",
    "\n",
    "embark_dummies_titanic  = pd.get_dummies(titanic_df['Embarked'])\n",
    "embark_dummies_titanic.drop(['S'], axis=1, inplace=True)\n",
    "#print(embark_dummies_titanic)\n",
    "\n",
    "embark_dummies_test  = pd.get_dummies(test_df['Embarked'])\n",
    "embark_dummies_test.drop(['S'], axis=1, inplace=True)\n",
    "\n",
    "titanic_df = titanic_df.join(embark_dummies_titanic)\n",
    "#print(titanic_df)\n",
    "test_df    = test_df.join(embark_dummies_test)\n",
    "\n",
    "titanic_df.drop(['Embarked'], axis=1,inplace=True)\n",
    "test_df.drop(['Embarked'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "This code snippet handles missing values in the 'Fare' column in the test DataFrame, categorizes the 'Fare' values into discrete bins, converts them to integer type, and prepares the data for potential further analysis and modeling.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.998519,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "## Fare\n",
    "\n",
    "# only for test_df, since there is a missing \"Fare\" values\n",
    "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
    "\n",
    "titanic_df.loc[ titanic_df['Fare'] <= 7.91, 'Fare'] = 0\n",
    "titanic_df.loc[(titanic_df['Fare'] > 7.91) & (titanic_df['Fare'] <= 14.454), 'Fare'] = 1\n",
    "titanic_df.loc[(titanic_df['Fare'] > 14.454) & (titanic_df['Fare'] <= 31), 'Fare'] = 2\n",
    "titanic_df.loc[ titanic_df['Fare'] > 31, 'Fare'] = 3\n",
    "test_df.loc[ test_df['Fare'] <= 7.91, 'Fare'] = 0\n",
    "test_df.loc[(test_df['Fare'] > 7.91) & (test_df['Fare'] <= 14.454), 'Fare'] = 1\n",
    "test_df.loc[(test_df['Fare'] > 14.454) & (test_df['Fare'] <= 31), 'Fare'] = 2\n",
    "test_df.loc[test_df['Fare'] > 31, 'Fare'] = 3\n",
    "\n",
    "# convert from float to int\n",
    "titanic_df['Fare'] = titanic_df['Fare'].astype(int)\n",
    "test_df['Fare']    = test_df['Fare'].astype(int)\n",
    "\n",
    "\n",
    "# get fare for survived & didn't survive passengers \n",
    "#fare_not_survived = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 0]\n",
    "#fare_survived     = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 1]\n",
    "\n",
    "# get average and std for fare of survived/not survived passengers\n",
    "#avgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\n",
    "#std_fare      = DataFrame([fare_not_survived.std(), fare_survived.std()])\n",
    "\n",
    "# plot\n",
    "#titanic_df['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n",
    "\n",
    "#avgerage_fare.index.names = std_fare.index.names = [\"Survived\"]\n",
    "#avgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This code snippet imputes missing values in the 'Age' column for both Titanic and test DataFrames by filling them with the mean age of their respective passenger class ('Pclass').",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.45814142,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Age impute\n",
    "\n",
    "titanic_df['Age'] = titanic_df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.mean()))\n",
    "test_df['Age'] = test_df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet converts the 'Age' column to integers, categorizes the ages into discrete bins, and updates the 'Age' values accordingly for both the Titanic and test DataFrames.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.942179,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "# Age \n",
    "\n",
    "#fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "#axis1.set_title('Original Age values - Titanic')\n",
    "#axis2.set_title('New Age values - Titanic')\n",
    "\n",
    "# axis3.set_title('Original Age values - Test')\n",
    "# axis4.set_title('New Age values - Test')\n",
    "\n",
    "# get average, std, and number of NaN values in titanic_df\n",
    "#average_age_titanic   = titanic_df[\"Age\"].mean()\n",
    "#std_age_titanic       = titanic_df[\"Age\"].std()\n",
    "#count_nan_age_titanic = titanic_df[\"Age\"].isnull().sum()\n",
    "\n",
    "# get average, std, and number of NaN values in test_df\n",
    "#average_age_test   = test_df[\"Age\"].mean()\n",
    "#std_age_test       = test_df[\"Age\"].std()\n",
    "#count_nan_age_test = test_df[\"Age\"].isnull().sum()\n",
    "\n",
    "# generate random numbers between (mean - std) & (mean + std)\n",
    "#rand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, size = count_nan_age_titanic)\n",
    "#rand_2 = np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)\n",
    "\n",
    "# plot original Age values\n",
    "# NOTE: drop all null values, and convert to int\n",
    "#titanic_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n",
    "# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n",
    "\n",
    "# fill NaN values in Age column with random values generated\n",
    "#titanic_df[\"Age\"][np.isnan(titanic_df[\"Age\"])] = rand_1\n",
    "#test_df[\"Age\"][np.isnan(test_df[\"Age\"])] = rand_2\n",
    "\n",
    "# convert from float to int\n",
    "titanic_df['Age'] = titanic_df['Age'].astype(int)\n",
    "test_df['Age']    = test_df['Age'].astype(int)\n",
    "\n",
    "titanic_df.loc[ titanic_df['Age'] <= 16, 'Age'] = 0\n",
    "titanic_df.loc[(titanic_df['Age'] > 16) & (titanic_df['Age'] <= 32), 'Age'] = 1\n",
    "titanic_df.loc[(titanic_df['Age'] > 32) & (titanic_df['Age'] <= 48), 'Age'] = 2\n",
    "titanic_df.loc[(titanic_df['Age'] > 48) & (titanic_df['Age'] <= 64), 'Age'] = 3\n",
    "titanic_df.loc[(titanic_df['Age'] > 64), 'Age'] = 4\n",
    "\n",
    "test_df.loc[ test_df['Age'] <= 16, 'Age'] = 0\n",
    "test_df.loc[(test_df['Age'] > 16) & (test_df['Age'] <= 32), 'Age'] = 1\n",
    "test_df.loc[(test_df['Age'] > 32) & (test_df['Age'] <= 48), 'Age'] = 2\n",
    "test_df.loc[(test_df['Age'] > 48) & (test_df['Age'] <= 64), 'Age'] = 3\n",
    "test_df.loc[(test_df['Age'] > 64), 'Age'] = 4\n",
    "        \n",
    "# plot new Age Values\n",
    "#titanic_df['Age'].hist(bins=70, ax=axis2)\n",
    "# test_df['Age'].hist(bins=70, ax=axis4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet removes the 'Cabin' column from both the Titanic and test DataFrames due to its high amount of missing values.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9949832,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Cabin\n",
    "# It has a lot of NaN values, so it won't cause a remarkable impact on prediction\n",
    "titanic_df.drop(\"Cabin\",axis=1,inplace=True)\n",
    "test_df.drop(\"Cabin\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code snippet creates a new 'Family' column to indicate if passengers had any family members aboard, based on the 'Parch' and 'SibSp' columns, and then drops the original 'Parch' and 'SibSp' columns from both the Titanic and test DataFrames.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9953224,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Family\n",
    "\n",
    "# Instead of having two columns Parch & SibSp, \n",
    "# we can have only one column represent if the passenger had any family member aboard or not,\n",
    "# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\n",
    "titanic_df['Family'] =  titanic_df[\"Parch\"] + titanic_df[\"SibSp\"]\n",
    "titanic_df['Family'].loc[titanic_df['Family'] > 0] = 1\n",
    "titanic_df['Family'].loc[titanic_df['Family'] == 0] = 0\n",
    "\n",
    "test_df['Family'] =  test_df[\"Parch\"] + test_df[\"SibSp\"]\n",
    "test_df['Family'].loc[test_df['Family'] > 0] = 1\n",
    "test_df['Family'].loc[test_df['Family'] == 0] = 0\n",
    "\n",
    "# drop Parch & SibSp\n",
    "titanic_df = titanic_df.drop(['SibSp','Parch'], axis=1)\n",
    "test_df    = test_df.drop(['SibSp','Parch'], axis=1)\n",
    "\n",
    "# plot\n",
    "#fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\n",
    "\n",
    "# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)\n",
    "#sns.countplot(x='Family', data=titanic_df, order=[1,0], ax=axis1)\n",
    "\n",
    "# average of survived for those who had/didn't have any family member\n",
    "#family_perc = titanic_df[[\"Family\", \"Survived\"]].groupby(['Family'],as_index=False).mean()\n",
    "#sns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n",
    "\n",
    "#axis1.set_xticklabels([\"With Family\",\"Alone\"], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Transform",
    "desc": "This code snippet maps the 'Sex' column to integer values for both Titanic and test DataFrames using a dictionary of unique sex values.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9779068,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Sex\n",
    "\n",
    "# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.\n",
    "# So, we can classify passengers as males, females, and child\n",
    "#def get_person(passenger):\n",
    "    #age,sex = passenger\n",
    "    #return 'child' if age < 16 else sex\n",
    "    \n",
    "#titanic_df['Person'] = titanic_df[['Age','Sex']].apply(get_person,axis=1)\n",
    "#test_df['Person']    = test_df[['Age','Sex']].apply(get_person,axis=1)\n",
    "\n",
    "# No need to use Sex column since we created Person column\n",
    "#titanic_df.drop(['Sex'],axis=1,inplace=True)\n",
    "#test_df.drop(['Sex'],axis=1,inplace=True)\n",
    "\n",
    "# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers\n",
    "#person_dummies_titanic  = pd.get_dummies(titanic_df['Person'])\n",
    "#person_dummies_titanic.columns = ['Child','Female','Male']\n",
    "#person_dummies_titanic.drop(['Male'], axis=1, inplace=True)\n",
    "\n",
    "#person_dummies_test  = pd.get_dummies(test_df['Person'])\n",
    "#print(person_dummies_test)\n",
    "#person_dummies_test.columns = ['Child','Female','Male']\n",
    "#person_dummies_test.drop(['Male'], axis=1, inplace=True)\n",
    "\n",
    "#titanic_df = titanic_df.join(person_dummies_titanic)\n",
    "#test_df    = test_df.join(person_dummies_test)\n",
    "\n",
    "#fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)\n",
    "#sns.countplot(x='Person', data=titanic_df, ax=axis1)\n",
    "\n",
    "# average of survived for each Person(male, female, or child)\n",
    "#person_perc = titanic_df[[\"Person\", \"Survived\"]].groupby(['Person'],as_index=False).mean()\n",
    "#sns.barplot(x='Person', y='Survived', data=person_perc, ax=axis2, order=['male','female','child'])\n",
    "\n",
    "#titanic_df.drop(['Person'],axis=1,inplace=True)\n",
    "#test_df.drop(['Person'],axis=1,inplace=True)\n",
    "sexes = sorted(titanic_df['Sex'].unique())\n",
    "genders_mapping = dict(zip(sexes, range(0, len(sexes) + 1)))\n",
    "titanic_df['Sex'] = titanic_df['Sex'].map(genders_mapping).astype(int)\n",
    "test_df['Sex'] = test_df['Sex'].map(genders_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet creates a new feature 'age_class' by multiplying the 'Age' and 'Pclass' values for both Titanic and test DataFrames.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9978842,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Pclass\n",
    "\n",
    "# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])\n",
    "#sns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_df,size=5)\n",
    "\n",
    "# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n",
    "#pclass_dummies_titanic  = pd.get_dummies(titanic_df['Pclass'])\n",
    "#pclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\n",
    "#pclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n",
    "\n",
    "#pclass_dummies_test  = pd.get_dummies(test_df['Pclass'])\n",
    "#pclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\n",
    "#pclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n",
    "\n",
    "#titanic_df.drop(['Pclass'],axis=1,inplace=True)\n",
    "#test_df.drop(['Pclass'],axis=1,inplace=True)\n",
    "\n",
    "#titanic_df = titanic_df.join(pclass_dummies_titanic)\n",
    "#test_df    = test_df.join(pclass_dummies_test)\n",
    "titanic_df['age_class'] = titanic_df['Age'] * titanic_df['Pclass']\n",
    "test_df['age_class'] = test_df['Age'] * test_df['Pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "The code snippet converts the 'Sex' column in the dataset into a binary format, with male as 1 and female as 0.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "The code snippet creates one-hot encoded variables for the 'Embarked' column in the dataset, generating a new binary column for each unique value.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "The code snippet creates one-hot encoded variables for the 'Pclass' column in the dataset, generating a new binary column for each unique class.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.99897695,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "The code snippet creates a new dataframe named `imputed` and fills missing values in the 'Age' and 'Fare' columns with their respective mean values.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "The code snippet extracts titles from passenger names, maps them into a predefined dictionary of aggregated titles, and then creates one-hot encoded variables for these titles.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "The code snippet fills missing 'Cabin' values with 'U' for unknown, maps each cabin to its corresponding letter, and creates one-hot encoded variables for these cabin letters.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "The code snippet defines a function to clean and extract prefixes from ticket strings, then applies this function to create one-hot encoded variables for these prefixes.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "The code snippet creates a new feature representing family size and additional binary features categorizing families as single, small, or large based on the family size.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "The code snippet concatenates selected transformed feature dataframes including 'imputed', 'embarked', 'cabin', and 'sex' into a new dataframe called `full_X`.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.86217046,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "The code snippet transforms the 'Sex' column in the dataset into a binary variable, where 'male' is encoded as 1 and 'female' as 0.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.6640003323554993,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "The code snippet transforms the 'Embarked' column into multiple binary dummy variables, one for each unique value of 'Embarked', to prepare the data for machine learning.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9990243911743164,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "The code snippet transforms the 'Pclass' column into multiple binary dummy variables, one for each unique value of 'Pclass', to prepare the data for machine learning.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9989769458770752,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "The code snippet creates a new DataFrame where the missing values in the 'Age' and 'Fare' columns are filled with their respective mean values from the 'full' dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.6686704754829407,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "The code snippet extracts titles from the 'Name' column, maps them into broader categories using a dictionary, and creates binary dummy variables for each title category, preparing the data for machine learning.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.8323215842247009,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "The code snippet fills missing values in the 'Cabin' column with 'U' for unknown, maps each cabin value to its initial letter, and creates binary dummy variables for each cabin category.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.995426595211029,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "The code snippet defines a function to extract prefixes from ticket numbers, applies this function to the 'Ticket' column, and then creates binary dummy variables for each ticket prefix category.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9973102807998656,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "The code snippet creates a new DataFrame to calculate family size based on the 'Parch' and 'SibSp' columns, and introduces new binary features indicating if a family is single, small, or large.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.6389940977096558,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "The code snippet concatenates selected feature DataFrames ('imputed', 'embarked', 'pclass', and 'sex') into a single DataFrame to form the final feature set for modeling.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.8442047834396362,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , pclass , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code transforms the 'Sex' column in the dataset into a binary variable where 'male' is represented by 1 and 'female' is represented by 0.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.6640003323554993,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code creates new binary variables for each unique value of the 'Embarked' column using one-hot encoding.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9990243911743164,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code creates new binary variables for each unique value of the 'Pclass' column using one-hot encoding.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9989769458770752,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code creates a new DataFrame and fills missing values for the 'Age' and 'Fare' columns with their respective mean values.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.6686704754829407,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code extracts titles from the 'Name' column, maps them to a more aggregated title category, and then applies one-hot encoding to these titles.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.8323215842247009,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code fills missing 'Cabin' values with 'U' for unknown, maps each cabin to its initial letter, and then performs one-hot encoding on these cabin letters.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.995426595211029,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code defines a function to extract the prefix of each ticket, replaces the prefixes in the 'Ticket' column while handling missing prefixes by setting them to 'XXX', and then creates dummy variables for these prefixes using one-hot encoding.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9973102807998656,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code introduces a new feature 'FamilySize' by summing 'Parch' and 'SibSp' values and then creates additional features indicating whether a passenger is single, part of a small family, or part of a large family, using specific size thresholds.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.6389940977096558,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code combines the specified features—'imputed', 'embarked', 'pclass', and 'sex'—into a single DataFrame for further analyses.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.8442047834396362,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , pclass , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "The code snippet transforms the 'Sex' column into binary values, with 'male' converted to 1 and all other values converted to 0, creating a new Series named 'Sex'.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "The code snippet creates dummy (one-hot encoded) variables for each unique value in the 'Embarked' column, with the prefix 'Embarked', and displays the first few rows.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "The code snippet creates dummy (one-hot encoded) variables for each unique value in the 'Pclass' column, with the prefix 'Pclass', and displays the first few rows.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99897695,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "The code snippet creates a new DataFrame named 'imputed' and fills missing values in the 'Age' and 'Fare' columns with their respective mean values, then displays the first few rows.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "The code snippet extracts titles from the 'Name' column, maps them to more aggregated categories using a dictionary, and creates dummy (one-hot encoded) variables for each title category, displaying the first few rows.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "The code snippet fills missing values in the 'Cabin' column with 'U' (for Unknown), extracts the first letter of each cabin code, and creates dummy (one-hot encoded) variables for each cabin letter, displaying the first few rows.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "The code snippet defines a function to extract prefixes from the 'Ticket' column, replaces certain characters, and creates dummy (one-hot encoded) variables for each ticket prefix, displaying the first few rows.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "The code snippet creates new features based on family size, including 'FamilySize', 'Family_Single', 'Family_Small', and 'Family_Large' indicators, displaying the first few rows.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "The code snippet concatenates selected feature sets ('imputed', 'embarked', 'pclass', and 'sex') into a single DataFrame named 'full_X', and displays the first few rows.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.8442048,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , pclass , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet transforms the 'Sex' variable in the 'full' DataFrame into a binary format, where 'male' is 1 and all other values are 0, and stores the result in a new Series named 'Sex'.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet creates one-hot encoded variables for every unique value of the 'Embarked' column in the 'full' DataFrame and stores the result in a new DataFrame named 'embarked'.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet creates one-hot encoded variables for every unique value of the 'Pclass' column in the 'full' DataFrame and stores the result in a new DataFrame named 'pclass'.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99897695,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame named 'imputed' by filling the missing values of the 'Age' and 'Fare' columns in the 'full' DataFrame with their respective means.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet extracts titles from the 'Name' column of the 'full' DataFrame, maps them to a more aggregated form using a dictionary, and then one-hot encodes the titles into a new DataFrame named 'title'.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet fills missing 'Cabin' values in the 'full' DataFrame with 'U' for 'Unknown', maps each value to its first letter, and then one-hot encodes the results into a new DataFrame named 'cabin'.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to clean and extract prefixes from the 'Ticket' column of the 'full' DataFrame, replacing tickets with no prefix with 'XXX', and then one-hot encodes these prefixes into a new DataFrame named 'ticket'.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame named 'family' that includes the size of each passenger's family (including the passenger) and introduces additional binary features indicating whether the family size is single, small, or large.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet concatenates several previously transformed DataFrames (imputed, embarked, cabin, sex, family) to create a new DataFrame named 'full_X', which includes selected features for further machine learning tasks.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.90544254,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex, family ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet transforms the 'Sex' feature into binary values, encoding males as 1 and females as 0, and stores it in a new Series.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.6640003323554993,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet creates dummy variables for each unique value of the 'Embarked' feature, effectively one-hot encoding the feature, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9990243911743164,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet creates dummy variables for each unique value of the 'Pclass' feature, effectively one-hot encoding the feature, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9989769458770752,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame, 'imputed', and fills the missing values in the 'Age' and 'Fare' features with their respective mean values.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.6686704754829407,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet extracts titles from the 'Name' feature, maps them to a more aggregated set of titles according to a predefined dictionary, and then creates dummy variables for each of these titles.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.8323215842247009,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet replaces missing values in the 'Cabin' feature with 'U' (Unknown), extracts the first letter of each cabin as a new feature, and then creates dummy variables for each unique cabin letter.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.995426595211029,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract prefixes from ticket numbers, creates dummy variables for each unique ticket prefix, and then displays the first few rows and the shape of the resulting DataFrame.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9973102807998656,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet creates a new feature for family size by summing 'Parch', 'SibSp', and 1, and introduces additional binary features for single, small, and large families based on the family size.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.6389940977096558,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet concatenates selected features, specifically 'imputed', 'embarked', 'cabin', and 'sex', into a single DataFrame called 'full_X' that will be used for modeling.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.8621704578399658,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet transforms the 'Sex' column into binary values, with 'male' mapped to 1 and 'female' mapped to 0, and stores the result in a new Series.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.6640003323554993,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet creates dummy variables for each unique value in the 'Embarked' column, resulting in separate binary columns for each embarkation point.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9990243911743164,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet creates dummy variables for each unique value in the 'Pclass' column, resulting in separate binary columns for each passenger class.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9989769458770752,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame, fills missing values in the 'Age' and 'Fare' columns with their respective means, and displays the first few rows of the transformed dataset.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.6686704754829407,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet extracts titles from the 'Name' column, maps them to more aggregated categories using a predefined dictionary, and creates dummy variables for each unique title.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.8323215842247009,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet fills missing values in the 'Cabin' column with 'U' (for Unknown), maps each cabin to its initial letter, and creates dummy variables for each unique cabin letter.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.995426595211029,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract prefixes from ticket values, replaces digits with 'XXX', and creates dummy variables from these cleaned ticket prefixes.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9973102807998656,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet introduces a new 'FamilySize' feature by combining 'Parch' and 'SibSp', and creates additional binary features to categorize passengers as single, having a small family, or having a large family.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.6389940977096558,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet concatenates selected transformed features—'imputed', 'embarked', 'cabin', and 'sex'—into a single DataFrame to be used for modeling.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.8621704578399658,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet converts the 'Sex' column in the Titanic dataset into binary values, where 'male' is represented as 1 and 'female' as 0.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet creates one-hot encoded variables for each unique value in the 'Embarked' column of the Titanic dataset.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet generates one-hot encoded variables for each unique value in the 'Pclass' column of the Titanic dataset.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9989813,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Pclass\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame, filling missing values in the 'Age' and 'Fare' columns with their respective means.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet extracts titles from passengers' names, maps them to aggregated categories using a dictionary, and then creates one-hot encoded variables for each title category.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet fills missing 'Cabin' values with 'U', maps each cabin value to its initial letter, and then creates one-hot encoded variables for these cabin letters.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract and clean the prefix from the 'Ticket' column, assigns 'XXX' if no prefix exists, and then creates one-hot encoded variables for each unique ticket prefix.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet introduces new features based on family size, including the actual family size and binary indicators for single, small, and large families in the Titanic dataset.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame by concatenating selected features—'imputed', 'embarked', 'cabin', and 'sex'—for use in the dataset.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.86217046,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet converts the 'Sex' column of the full dataset into a binary variable, where 'male' is represented as 1 and 'female' as 0.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.6640003323554993,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet creates one-hot encoded variables for each unique value in the 'Embarked' column of the full dataset.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9990243911743164,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet generates one-hot encoded variables for each unique value in the 'Pclass' column of the full dataset.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9989769458770752,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame with missing 'Age' and 'Fare' values filled with their respective means from the full dataset.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.6686704754829407,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet extracts and standardizes titles from the 'Name' column in the full dataset, aggregates them into broader categories, and then one-hot encodes the resulting titles.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.8323215842247009,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet fills missing 'Cabin' values with 'U' (standing for Unknown), extracts the first letter of each cabin, and then performs one-hot encoding on the cabin letters.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.995426595211029,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract and clean ticket prefixes, applies it to the 'Ticket' column of the full dataset, and then one-hot encodes the ticket prefixes.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9973102807998656,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame to compute 'FamilySize' and additional features ('Family_Single', 'Family_Small', 'Family_Large') based on the size of each passenger's family.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.6389940977096558,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet concatenates selected feature DataFrames ('imputed', 'embarked', 'cabin', 'sex') into a single dataset called `full_X`.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9992916584014891,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "cabin\n",
    "full_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet transforms the 'Sex' variable in the 'full' dataset into binary values (0 for female and 1 for male) and stores it in a new pandas Series named 'Sex'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet creates dummy variables for each unique value in the 'Embarked' column of the 'full' dataset and stores them in a new DataFrame named 'embarked'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet creates dummy variables for each unique value in the 'Pclass' column of the 'full' dataset and stores them in a new DataFrame named 'pclass'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99897695,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame named 'imputed' and fills the missing values in the 'Age' and 'Fare' columns of the 'full' dataset with their respective means.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet extracts titles from the 'Name' column in the 'full' dataset, maps them to a new aggregated title dictionary, and creates dummy variables for each unique title, storing the result in a new DataFrame named 'title'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet fills missing 'Cabin' values in the 'full' dataset with 'U' for unknown, maps each 'Cabin' value to its first letter, and then creates dummy variables for each unique cabin letter, storing the result in a new DataFrame named 'cabin'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract prefixes from 'Ticket' entries, replaces numeric-only tickets with 'XXX', and then creates dummy variables for each unique ticket prefix, storing the result in a new DataFrame named 'ticket'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame named 'family', introduces a 'FamilySize' feature by combining 'Parch' and 'SibSp', and then creates additional binary features ('Family_Single', 'Family_Small', and 'Family_Large') based on the size of the family.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet concatenates selected features ('imputed', 'embarked', 'cabin', and 'sex') to form a new DataFrame named 'full_X', which will be used as the dataset for further analysis or modeling.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.86217046,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "The snippet converts the 'Sex' column in the combined dataset into binary values, where 'male' is represented by 1 and 'female' by 0, and stores it in a new Series named 'Sex'.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "The snippet creates dummy variables for every unique value in the 'Embarked' column and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "The snippet creates dummy variables for every unique value in the 'Pclass' column and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99897695,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "The snippet creates a new DataFrame, 'imputed', where missing values in the 'Age' and 'Fare' columns are filled with their respective column means, and displays the first few rows of this DataFrame.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "The snippet extracts titles from the 'Name' column, maps them to more aggregated categories using a predefined dictionary, creates dummy variables for each mapped title, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "The snippet replaces missing 'Cabin' values with 'U', maps each cabin to its first letter, creates dummy variables for each unique cabin letter, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "The snippet defines a function to extract ticket prefixes, applies this function to the 'Ticket' column, creates dummy variables for each unique ticket prefix, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "The snippet creates a new DataFrame to hold family-related features, calculates the family size for each passenger, and introduces binary features to indicate whether a passenger's family is single, small, or large, then displays the first few rows of the DataFrame.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "The snippet concatenates selected features including 'imputed', 'embarked', 'cabin', and 'sex' into a single DataFrame named 'full_X' and displays the first few rows of this DataFrame.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.86217046,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Data Transform",
    "desc": "This code snippet performs various data preprocessing steps including handling missing values, creating new features, and mapping categorical data to numeric values in preparation for model training.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9408006,
    "start_cell": true,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Copy original dataset in case we need it later when digging into interesting features\n",
    "# WARNING: Beware of actually copying the dataframe instead of just referencing it\n",
    "# \"original_train = train\" will create a reference to the train variable (changes in 'train' will apply to 'original_train')\n",
    "original_train = train.copy() # Using 'copy()' allows to clone the dataset, creating a different object with the same values\n",
    "\n",
    "# Feature engineering steps taken from Sina and Anisotropic, with minor changes to avoid warnings\n",
    "full_data = [train, test]\n",
    "\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "# Remove all NULLS in the Age column\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    # Next line has been improved to avoid warning\n",
    "    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Transform",
    "desc": "This code snippet removes unnecessary columns from the training and testing datasets to streamline the feature set for model training.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99877053,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Feature selection: remove variables no longer containing relevant information\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code snippet removes the 'PassengerId', 'Name', and 'Ticket' columns from the Titanic training DataFrame and the 'Name' and 'Ticket' columns from the test DataFrame, as they are deemed unnecessary for analysis and prediction.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.98214287,
    "start_cell": true,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns, these columns won't be useful in analysis and prediction\n",
    "titanic_df = titanic_df.drop(['PassengerId','Name','Ticket'], axis=1)\n",
    "test_df    = test_df.drop(['Name','Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "This code snippet fills missing values in the 'Embarked' column of the Titanic training dataset, visualizes the distribution and survival rates based on 'Embarked', creates dummy variables for 'Embarked', joins them with the original DataFrames, and then drops the original 'Embarked' column.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99828136,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Embarked\n",
    "\n",
    "# only in titanic_df, fill the two missing values with the most occurred value, which is \"S\".\n",
    "titanic_df[\"Embarked\"] = titanic_df[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# plot\n",
    "sns.factorplot('Embarked','Survived', data=titanic_df,size=4,aspect=3)\n",
    "\n",
    "fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)\n",
    "# sns.factorplot('Survived',hue=\"Embarked\",data=titanic_df,kind='count',order=[1,0],ax=axis2)\n",
    "sns.countplot(x='Embarked', data=titanic_df, ax=axis1)\n",
    "sns.countplot(x='Survived', hue=\"Embarked\", data=titanic_df, order=[1,0], ax=axis2)\n",
    "\n",
    "# group by embarked, and get the mean for survived passengers for each value in Embarked\n",
    "embark_perc = titanic_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\n",
    "sns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)\n",
    "\n",
    "# Either to consider Embarked column in predictions,\n",
    "# and remove \"S\" dummy variable, \n",
    "# and leave \"C\" & \"Q\", since they seem to have a good rate for Survival.\n",
    "\n",
    "# OR, don't create dummy variables for Embarked column, just drop it, \n",
    "# because logically, Embarked doesn't seem to be useful in prediction.\n",
    "\n",
    "embark_dummies_titanic  = pd.get_dummies(titanic_df['Embarked'])\n",
    "embark_dummies_titanic.drop(['S'], axis=1, inplace=True)\n",
    "\n",
    "embark_dummies_test  = pd.get_dummies(test_df['Embarked'])\n",
    "embark_dummies_test.drop(['S'], axis=1, inplace=True)\n",
    "\n",
    "titanic_df = titanic_df.join(embark_dummies_titanic)\n",
    "test_df    = test_df.join(embark_dummies_test)\n",
    "\n",
    "titanic_df.drop(['Embarked'], axis=1,inplace=True)\n",
    "test_df.drop(['Embarked'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code snippet fills missing 'Fare' values in the test DataFrame, converts the 'Fare' column values from float to integer, computes the mean and standard deviation of fares for survived and not survived passengers, and visualizes the fare distribution and average fare with error bars.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.91612476,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# Fare\n",
    "\n",
    "# only for test_df, since there is a missing \"Fare\" values\n",
    "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
    "\n",
    "# convert from float to int\n",
    "titanic_df['Fare'] = titanic_df['Fare'].astype(int)\n",
    "test_df['Fare']    = test_df['Fare'].astype(int)\n",
    "\n",
    "# get fare for survived & didn't survive passengers \n",
    "fare_not_survived = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 0]\n",
    "fare_survived     = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 1]\n",
    "\n",
    "# get average and std for fare of survived/not survived passengers\n",
    "avgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\n",
    "std_fare      = DataFrame([fare_not_survived.std(), fare_survived.std()])\n",
    "\n",
    "# plot\n",
    "titanic_df['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n",
    "\n",
    "avgerage_fare.index.names = std_fare.index.names = [\"Survived\"]\n",
    "avgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "This code snippet generates random values to fill missing 'Age' values in both the Titanic training and test DataFrames, converts the 'Age' column values from float to integer, and visualizes the original and new age distributions in the training dataset.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.67031723,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Age \n",
    "\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "axis1.set_title('Original Age values - Titanic')\n",
    "axis2.set_title('New Age values - Titanic')\n",
    "\n",
    "# axis3.set_title('Original Age values - Test')\n",
    "# axis4.set_title('New Age values - Test')\n",
    "\n",
    "# get average, std, and number of NaN values in titanic_df\n",
    "average_age_titanic   = titanic_df[\"Age\"].mean()\n",
    "std_age_titanic       = titanic_df[\"Age\"].std()\n",
    "count_nan_age_titanic = titanic_df[\"Age\"].isnull().sum()\n",
    "\n",
    "# get average, std, and number of NaN values in test_df\n",
    "average_age_test   = test_df[\"Age\"].mean()\n",
    "std_age_test       = test_df[\"Age\"].std()\n",
    "count_nan_age_test = test_df[\"Age\"].isnull().sum()\n",
    "\n",
    "# generate random numbers between (mean - std) & (mean + std)\n",
    "rand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, size = count_nan_age_titanic)\n",
    "rand_2 = np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)\n",
    "\n",
    "# plot original Age values\n",
    "# NOTE: drop all null values, and convert to int\n",
    "titanic_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n",
    "# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n",
    "\n",
    "# fill NaN values in Age column with random values generated\n",
    "titanic_df[\"Age\"][np.isnan(titanic_df[\"Age\"])] = rand_1\n",
    "test_df[\"Age\"][np.isnan(test_df[\"Age\"])] = rand_2\n",
    "\n",
    "# convert from float to int\n",
    "titanic_df['Age'] = titanic_df['Age'].astype(int)\n",
    "test_df['Age']    = test_df['Age'].astype(int)\n",
    "        \n",
    "# plot new Age Values\n",
    "titanic_df['Age'].hist(bins=70, ax=axis2)\n",
    "# test_df['Age'].hist(bins=70, ax=axis4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet drops the 'Cabin' column from both the Titanic training and test DataFrames due to its high number of NaN values, deeming it insignificant for prediction.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9949832,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Cabin\n",
    "# It has a lot of NaN values, so it won't cause a remarkable impact on prediction\n",
    "titanic_df.drop(\"Cabin\",axis=1,inplace=True)\n",
    "test_df.drop(\"Cabin\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet creates a new 'Family' column to indicate whether a passenger had family members aboard (by summing 'Parch' and 'SibSp'), drops the original 'Parch' and 'SibSp' columns, and visualizes both the count and survival rate of passengers based on the presence of family members.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9953224,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Family\n",
    "\n",
    "# Instead of having two columns Parch & SibSp, \n",
    "# we can have only one column represent if the passenger had any family member aboard or not,\n",
    "# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\n",
    "titanic_df['Family'] =  titanic_df[\"Parch\"] + titanic_df[\"SibSp\"]\n",
    "titanic_df['Family'].loc[titanic_df['Family'] > 0] = 1\n",
    "titanic_df['Family'].loc[titanic_df['Family'] == 0] = 0\n",
    "\n",
    "test_df['Family'] =  test_df[\"Parch\"] + test_df[\"SibSp\"]\n",
    "test_df['Family'].loc[test_df['Family'] > 0] = 1\n",
    "test_df['Family'].loc[test_df['Family'] == 0] = 0\n",
    "\n",
    "# drop Parch & SibSp\n",
    "titanic_df = titanic_df.drop(['SibSp','Parch'], axis=1)\n",
    "test_df    = test_df.drop(['SibSp','Parch'], axis=1)\n",
    "\n",
    "# plot\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\n",
    "\n",
    "# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)\n",
    "sns.countplot(x='Family', data=titanic_df, order=[1,0], ax=axis1)\n",
    "\n",
    "# average of survived for those who had/didn't have any family member\n",
    "family_perc = titanic_df[[\"Family\", \"Survived\"]].groupby(['Family'],as_index=False).mean()\n",
    "sns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n",
    "\n",
    "axis1.set_xticklabels([\"With Family\",\"Alone\"], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet creates a new 'Person' column to classify passengers as 'child', 'female', or 'male' based on their age and gender, drops the 'Sex' column, creates dummy variables for the 'Person' column, joins these to the original DataFrames, visualizes the count and survival rate by 'Person' category, and finally drops the 'Person' column from both DataFrames.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9766011,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Sex\n",
    "\n",
    "# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.\n",
    "# So, we can classify passengers as males, females, and child\n",
    "def get_person(passenger):\n",
    "    age,sex = passenger\n",
    "    return 'child' if age < 16 else sex\n",
    "    \n",
    "titanic_df['Person'] = titanic_df[['Age','Sex']].apply(get_person,axis=1)\n",
    "test_df['Person']    = test_df[['Age','Sex']].apply(get_person,axis=1)\n",
    "\n",
    "# No need to use Sex column since we created Person column\n",
    "titanic_df.drop(['Sex'],axis=1,inplace=True)\n",
    "test_df.drop(['Sex'],axis=1,inplace=True)\n",
    "\n",
    "# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers\n",
    "person_dummies_titanic  = pd.get_dummies(titanic_df['Person'])\n",
    "person_dummies_titanic.columns = ['Child','Female','Male']\n",
    "person_dummies_titanic.drop(['Male'], axis=1, inplace=True)\n",
    "\n",
    "person_dummies_test  = pd.get_dummies(test_df['Person'])\n",
    "person_dummies_test.columns = ['Child','Female','Male']\n",
    "person_dummies_test.drop(['Male'], axis=1, inplace=True)\n",
    "\n",
    "titanic_df = titanic_df.join(person_dummies_titanic)\n",
    "test_df    = test_df.join(person_dummies_test)\n",
    "\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)\n",
    "sns.countplot(x='Person', data=titanic_df, ax=axis1)\n",
    "\n",
    "# average of survived for each Person(male, female, or child)\n",
    "person_perc = titanic_df[[\"Person\", \"Survived\"]].groupby(['Person'],as_index=False).mean()\n",
    "sns.barplot(x='Person', y='Survived', data=person_perc, ax=axis2, order=['male','female','child'])\n",
    "\n",
    "titanic_df.drop(['Person'],axis=1,inplace=True)\n",
    "test_df.drop(['Person'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code snippet visualizes the survival rate based on passenger class ('Pclass'), creates dummy variables for the 'Pclass' column (dropping 'Class_3' as it has the lowest survival rate), drops the original 'Pclass' column, and joins the dummy variables to the Titanic training and test DataFrames.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99851733,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Pclass\n",
    "\n",
    "# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])\n",
    "sns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_df,size=5)\n",
    "\n",
    "# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n",
    "pclass_dummies_titanic  = pd.get_dummies(titanic_df['Pclass'])\n",
    "pclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\n",
    "pclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n",
    "\n",
    "pclass_dummies_test  = pd.get_dummies(test_df['Pclass'])\n",
    "pclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\n",
    "pclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n",
    "\n",
    "titanic_df.drop(['Pclass'],axis=1,inplace=True)\n",
    "test_df.drop(['Pclass'],axis=1,inplace=True)\n",
    "\n",
    "titanic_df = titanic_df.join(pclass_dummies_titanic)\n",
    "test_df    = test_df.join(pclass_dummies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Transform",
    "desc": "This code snippet defines the training and testing sets by separating the features (X) and target variable (Y) for the Titanic training dataset and preparing the test dataset by dropping the 'PassengerId' column.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9993105,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "# define training and testing sets\n",
    "\n",
    "X_train = titanic_df.drop(\"Survived\",axis=1)\n",
    "Y_train = titanic_df[\"Survived\"]\n",
    "X_test  = test_df.drop(\"PassengerId\",axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet transforms the 'Sex' column in the Titanic dataset into a binary variable, where male is encoded as 1 and female as 0.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.6640003323554993,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet displays the first few entries of the transformed 'Sex' column to verify the binary encoding.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9997509121894836,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "sex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet converts the 'Embarked' column in the Titanic dataset into dummy variables for each unique value and displays the first few rows.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9990243911743164,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet converts the 'Pclass' column in the Titanic dataset into dummy variables for each unique value and displays the first few rows.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9989769458770752,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame with missing values in the 'Age' and 'Fare' columns filled using their respective mean values.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.6686704754829407,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet extracts titles from passenger names in the Titanic dataset, maps them to aggregated categories, converts them into dummy variables, and displays the first few rows.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.8323215842247009,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet handles missing values in the 'Cabin' column by replacing them with 'U', maps each cabin value with its first letter, converts these letters into dummy variables, and displays the first few rows.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.995426595211029,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract prefixes from the 'Ticket' column, replaces missing prefixes with 'XXX', converts those prefixes into dummy variables, and displays the first few rows along with the shape of the resulting DataFrame.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9973102807998656,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet creates a 'FamilySize' variable based on the number of parents, children, siblings, and spouses, and then generates three new binary features indicating single, small, and large families, displaying the first few rows.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.6389940977096558,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This code snippet selects specific preprocessed features (imputed, embarked, cabin, sex) and concatenates them into a single DataFrame, displaying the first few rows.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.8621704578399658,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code transforms the 'Sex' column into a binary series where 'male' is encoded as 1 and 'female' as 0.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code creates dummy variables for each unique value in the 'Embarked' column, generating a new binary variable for each category with a prefix 'Embarked'.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code creates dummy variables for each unique value in the 'Pclass' column, generating new binary variables with a prefix 'Pclass'.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99897695,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code creates a new DataFrame 'imputed' and fills missing values in the 'Age' and 'Fare' columns with their respective mean values.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code extracts titles from the 'Name' column, maps them to aggregated titles using a dictionary, and creates dummy variables for each title, resulting in a one-hot encoded DataFrame.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code fills missing 'Cabin' values with 'U', maps each cabin value to its initial letter, and then dummy encodes these cabin letters into binary variables.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code defines a function to clean and extract ticket prefixes, then applies the function to the 'Ticket' column, and converts the prefixes into dummy variables.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code creates a new feature 'FamilySize' based on the sum of 'Parch' and 'SibSp' plus one, and further generates binary features for single, small, and large family sizes.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code concatenates selected feature sets (imputed, family, cabin, sex, pclass, embarked, ticket) into a single DataFrame 'full_X' for further use in the analysis or model training.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.83645827,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , family , cabin , sex , pclass , embarked , ticket ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "The code snippet converts the 'Sex' column in the dataset into binary values, where 'male' is represented by 1 and 'female' by 0, creating a new Pandas Series, and displays the first few values.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.3841752409934997,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )\n",
    "sex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "The code snippet creates dummy variables for each unique value in the 'Embarked' column, generating new variables prefixed with 'Embarked', and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9990243911743164,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "The code snippet creates dummy variables for each unique value in the 'Pclass' column, generating new variables prefixed with 'Pclass', and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9989769458770752,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "The code snippet creates a new DataFrame called `imputed` and fills the missing values in the 'Age' and 'Fare' columns with their respective mean values, displaying the first few rows of the resulting DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.6686704754829407,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "The code snippet extracts titles from the 'Name' column, maps each title to a more aggregated category based on a predefined dictionary, and creates dummy variables for each unique title, displaying the first few rows of the resulting DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.7844502925872803,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"plop\",\n",
    "                    \"Col\":        \"other\",\n",
    "                    \"Major\":      \"other\",\n",
    "                    \"Jonkheer\":   \"plop\",\n",
    "                    \"Don\":        \"plop\",\n",
    "                    \"Sir\" :       \"other\",\n",
    "                    \"Dr\":         \"plop\",\n",
    "                    \"Rev\":        \"plop\",\n",
    "                    \"the Countess\":\"other\",\n",
    "                    \"Dona\":       \"other\",\n",
    "                    \"Mme\":        \"other\",\n",
    "                    \"Mlle\":       \"other\",\n",
    "                    \"Ms\":         \"other\",\n",
    "                    \"Mr\" :        \"plop\",\n",
    "                    \"Mrs\" :       \"other\",\n",
    "                    \"Miss\" :      \"other\",\n",
    "                    \"Master\" :    \"other\",\n",
    "                    \"Lady\" :      \"other\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "The code snippet replaces missing values in the 'Cabin' column with 'U' for 'Unknown', extracts the first letter of each cabin value, and creates dummy variables for each unique cabin letter, displaying the first few rows of the resulting DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.995426595211029,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "The code snippet defines a function to extract prefixes from ticket numbers, replaces certain characters, filters out purely numeric tickets, and creates dummy variables for the ticket prefixes, displaying the first few rows and the shape of the resulting DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9973102807998656,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "The code snippet creates a new DataFrame that includes the size of families aboard by summing 'Parch' and 'SibSp' columns and introduces additional features indicating whether the family is single, small, or large, displaying the first few rows of the resulting DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.6389940977096558,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "The code snippet creates a new DataFrame `full_X` by concatenating specific features from the previously transformed datasets, such as imputed age, third class indicator, family size, and specific title, displaying the first few rows of the resulting DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.784508466720581,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed.Age, pclass.Pclass_3, family.FamilySize, title.plop ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code transforms the 'Sex' column into a binary format, where male is 1 and female is 0, for easier numerical analysis and modeling.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code creates binary dummy variables for each unique value in the 'Embarked' column, allowing categorical data to be used in machine learning algorithms.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code creates binary dummy variables for each unique value in the 'Pclass' column to facilitate the use of categorical class data in machine learning models.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99897695,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code creates a new DataFrame 'imputed' where missing values in the 'Age' and 'Fare' columns are filled with their respective means.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code extracts the title from the 'Name' column, maps them to more aggregated categories using a dictionary, and then creates dummy variables for each title category.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code fills missing 'Cabin' values with 'U' (Unknown), maps the 'Cabin' values to their initial letter, and then creates dummy variables for each cabin category.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code defines a function to extract ticket prefixes, applies it to the 'Ticket' column to clean and categorize tickets, and then creates dummy variables for each ticket prefix.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This code creates new features related to family size, namely the total family size, and binary indicators for whether the family size is single, small, or large.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "This code concatenates selected features—'imputed', 'embarked', 'cabin', and 'sex'—into a single DataFrame for further analysis or modeling.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.86217046,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This code snippet drops the 'Name', 'Ticket', and 'Embarked' columns from the training and test datasets and displays the first few rows of the modified training dataset.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9992044568061828,
    "start_cell": true,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Name','Ticket','Embarked'], axis = 1)\n",
    "test_df = test_df.drop(['Name','Ticket','Embarked'], axis =1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet converts the categorical 'Sex' column into numerical values by replacing 'male' with 1 and 'female' with 0 in both the training and test datasets.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9970383644104004,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "train_df['Sex'] = train_df['Sex'].replace({'male' : 1, 'female' : 0})\n",
    "test_df['Sex'] = test_df['Sex'].replace({'male' : 1, 'female' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet fills the missing values in the 'Age' column with the mean age for both the training and test datasets, and then displays information about the training dataset.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9338498711586,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())\n",
    "test_df['Age'] = test_df['Age'].fillna(test_df['Age'].mean())\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code snippet deletes the 'Cabin' column from both the training and test datasets.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9991588592529296,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "del train_df['Cabin']\n",
    "del test_df['Cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet converts the training dataframe to a NumPy array, separates the features (X_train) from the target variable (y_train), and prints their shapes.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.845933735370636,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "train_data = train_df.values\n",
    "X_train = train_data[:,2:]\n",
    "y_train = train_data[:,1]\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet fills the missing values in the 'Fare' column with the mean fare in the test dataset, displays information about the test dataset, and shows the first few rows for inspection.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.845858097076416,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "test_df['Fare'] = test_df['Fare'].fillna(test_df['Fare'].mean())\n",
    "test_df.info()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet converts the 'Sex' column of the 'full' DataFrame into a binary variable where 'male' is 1 and 'female' is 0, and stores it in a new pandas Series named 'Sex'.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.66400033,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Transform Sex into binary values 0 and 1\n",
    "sex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet creates dummy variables for each unique value in the 'Embarked' column of the 'full' DataFrame, with the prefix 'Embarked', and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9990244,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet creates dummy variables for each unique value in the 'Pclass' column of the 'full' DataFrame, with the prefix 'Pclass', and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9989813,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Pclass\n",
    "pclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\n",
    "pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame 'imputed', fills missing values in the 'Age' and 'Fare' columns of the 'full' DataFrame with their respective means, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.6686705,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "imputed = pd.DataFrame()\n",
    "\n",
    "# Fill missing values of Age with the average of Age (mean)\n",
    "imputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n",
    "\n",
    "# Fill missing values of Fare with the average of Fare (mean)\n",
    "imputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n",
    "\n",
    "imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet extracts and standardizes the 'Title' from each name in the 'full' DataFrame, maps them to more aggregated titles, converts these titles into dummy variables, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.8323216,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "title = pd.DataFrame()\n",
    "# we extract the title from each name\n",
    "title[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n",
    "\n",
    "# a map of more aggregated titles\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                    }\n",
    "\n",
    "# we map each title\n",
    "title[ 'Title' ] = title.Title.map( Title_Dictionary )\n",
    "title = pd.get_dummies( title.Title )\n",
    "#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n",
    "\n",
    "title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet replaces missing cabin values with 'U' for unknown, extracts the first letter of each cabin value, and converts these letters into dummy variables, displaying the first few rows of the resulting DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9954266,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract the prefix from the 'Ticket' column values (or return 'XXX' if no prefix is found), applies this function to the 'Ticket' column, converts the extracted prefixes into dummy variables, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9973103,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket( ticket ):\n",
    "    ticket = ticket.replace( '.' , '' )\n",
    "    ticket = ticket.replace( '/' , '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map( lambda t : t.strip() , ticket )\n",
    "    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n",
    "    if len( ticket ) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\n",
    "ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet creates a new DataFrame 'family' to introduce the 'FamilySize' feature (sum of 'Parch' and 'SibSp' plus one) and additional binary features that indicate whether the passenger is single, belongs to a small family, or belongs to a large family, and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.6389941,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "family = pd.DataFrame()\n",
    "\n",
    "# introducing a new feature : the size of families (including the passenger)\n",
    "family[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet combines the 'imputed', 'embarked', 'cabin', and 'sex' DataFrames horizontally into a single DataFrame named 'full_X' and displays the first few rows of the resulting DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.86217046,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "# Select which features/variables to include in the dataset from the list below:\n",
    "# imputed , embarked , pclass , sex , family , cabin , ticket\n",
    "\n",
    "full_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\n",
    "full_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This code snippet splits the combined dataset into training, validation, and test sets by selecting rows for training and testing, and then further splits the training set into training and validation subsets, printing the shapes of these datasets.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99615127,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train_valid_X = full_X[ 0:891 ]\n",
    "train_valid_y = titanic.Survived\n",
    "test_X = full_X[ 891: ]\n",
    "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n",
    "\n",
    "print (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "This code snippet fills missing values in the 'keyword' and 'location' columns of both the training and testing datasets with empty strings.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.31553853,
    "start_cell": true,
    "subclass": "rename_columns",
    "subclass_id": 61
   },
   "outputs": [],
   "source": [
    "# filling nan values in the columns. \n",
    "train_df.keyword.fillna('', inplace=True)\n",
    "train_df.location.fillna('', inplace=True)\n",
    "\n",
    "test_df.keyword.fillna('', inplace=True)\n",
    "test_df.location.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code snippet concatenates the 'text', 'keyword', and 'location' columns into a single 'text' column and then deletes the now redundant 'keyword', 'location', and 'id' columns from both the training and testing datasets.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9910278,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'] + ' ' + train_df['keyword'] + ' ' + train_df['location']\n",
    "test_df['text'] = test_df['text'] + ' ' + test_df['keyword'] + ' ' + test_df['location']\n",
    "\n",
    "del train_df['keyword']\n",
    "del train_df['location']\n",
    "del train_df['id']\n",
    "del test_df['keyword']\n",
    "del test_df['location']\n",
    "del test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet updates the set of English stopwords to include punctuation marks and prints out the resulting set of stopwords, which will be used for text preprocessing.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.8811513,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# treba odstrániť slová ako \"a\", \"that\" alebo \"there\", tzv. stopwords, ktoré nám nenapomáhajú pri rozlišovaní\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "stop.update(punctuations)\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet defines a set of functions for cleaning text data by removing numbers, URLs, HTML tags, and emojis to prepare the text for further processing.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.25493404,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Funkcie na očistenie textu od čísel, linkov\n",
    "def remove_numbers(text):\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code snippet applies the previously defined text cleaning functions (removing numbers, URLs, HTML, and emojis) to the 'text' column of the training dataset and then displays the first few rows of the cleaned dataset.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.982876,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(remove_numbers)\n",
    "train_df.text = train_df.text.apply(remove_URL)\n",
    "train_df.text = train_df.text.apply(remove_html)\n",
    "train_df.text = train_df.text.apply(remove_emoji)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Transform",
    "desc": "This code snippet applies the previously defined text cleaning functions (removing numbers, URLs, HTML, and emojis) to the 'text' column of the testing dataset and then displays the first few rows of the cleaned dataset.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.97290975,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "test_df.text = test_df.text.apply(remove_numbers)\n",
    "test_df.text = test_df.text.apply(remove_URL)\n",
    "test_df.text = test_df.text.apply(remove_html)\n",
    "test_df.text = test_df.text.apply(remove_emoji)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet defines a function that maps parts of speech (POS) tags to simpler POS categories (adjective, verb, noun, adverb) to facilitate text preprocessing tasks like lemmatization.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.44465184,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "# určenie slovného druhu\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet defines a function that cleans the text by tokenizing it, removing stopwords, lemmatizing the tokens according to their simplified parts of speech, and then joining the cleaned tokens back into a single string.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.3523756,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    clean_text = []\n",
    "    for w in word_tokenize(text):\n",
    "        if w.lower() not in stop:\n",
    "            pos = pos_tag([w])\n",
    "            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n",
    "            clean_text.append(new_w)\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet applies the `clean_text` function to the 'text' column of both the training and testing datasets to preprocess and clean the text data.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9978066,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(clean_text)\n",
    "test_df.text = test_df.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet separates the cleaned text from the training dataset into two groups: one for real disaster-related texts and one for fake (non-disaster-related) texts based on the target variable.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.86400545,
    "start_cell": false,
    "subclass": "filter",
    "subclass_id": 14
   },
   "outputs": [],
   "source": [
    "real = train_df.text[train_df.target[train_df.target==1].index]\n",
    "fake = train_df.text[train_df.target[train_df.target==0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet initializes a `TfidfVectorizer` to convert the text data into TF-IDF weighted term-document matrices, fits it to the training text data, transforms both the validation and test text data, and prints the shapes of the resulting matrices.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.835839,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "#niektoré slová, ktoré sa vyskytujú veľmi často môžeme ignorovať, keďže slová ktoré sa príliš často vyskytujú\n",
    "#nám nepomôžu pri predpovedaní - to isté platí pri slovách ktoré sú veľmi zriedkavé\n",
    "#na ignorovanie takýchto slov používame min_df a max_df\n",
    "tv=TfidfVectorizer(min_df=0,max_df=0.8,use_idf=True,ngram_range=(1,3))\n",
    "\n",
    "\n",
    "tv_train_reviews=tv.fit_transform(x_train_text)\n",
    "tv_val_reviews=tv.transform(x_val_text)\n",
    "tv_test_reviews=tv.transform(test_df.text)\n",
    "\n",
    "print('tfidf_train:',tv_train_reviews.shape)\n",
    "print('tfidf_validation:',tv_val_reviews.shape)\n",
    "print('tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Transform",
    "desc": "The code converts all text in the 'text' and 'keyword' columns of both the training and test datasets to lowercase to standardize the text data for further analysis.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9862394,
    "start_cell": true,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train['text']=train['text'].apply(lambda x : x.lower())\n",
    "test['text']=test['text'].apply(lambda x : x.lower())\n",
    "\n",
    "def lower_keywords(keywords):\n",
    "    if keywords == keywords:\n",
    "        keywords=keywords.lower()\n",
    "    return keywords\n",
    "        \n",
    "train['keyword']=train['keyword'].apply(lambda x : lower_keywords(x))\n",
    "test['keyword']=test['keyword'].apply(lambda x : lower_keywords(x))\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "The code transforms the 'keyword' column in both the training and test datasets by splitting the keywords into lists, with special handling for NaN values.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9967776,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def keywords_to_list(keywords):\n",
    "    if keywords!=keywords: # nan value is not equal to itself\n",
    "        return []\n",
    "    else:\n",
    "        return keywords.split('%20')\n",
    "            \n",
    "train['keyword']=train['keyword'].apply(lambda x : keywords_to_list(x))\n",
    "test['keyword']=test['keyword'].apply(lambda x : keywords_to_list(x))\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "The code extracts hashtags from the 'text' column in both the training and test datasets and stores them in a new column called 'hashtag'.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99900657,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "import re\n",
    "train['hashtag'] = train['text'].apply(lambda x: re.findall(r'#(\\w+)', x))\n",
    "test['hashtag'] = test['text'].apply(lambda x: re.findall(r'#(\\w+)', x))\n",
    "\n",
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "The code cleans the text in the 'text', 'hashtag', and 'keyword' columns of both the training and test datasets using the `preprocessor.clean` function to remove unwanted content.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.64284086,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: preprocessor.clean(x))\n",
    "test['text'] = test['text'].apply(lambda x: preprocessor.clean(x))\n",
    "\n",
    "def clear_list(lista):\n",
    "    try:\n",
    "        for i,ele in enumerate(lista):\n",
    "            lista[i]=preprocessor.clean(ele)\n",
    "        return lista\n",
    "    except:\n",
    "        print(lista)\n",
    "\n",
    "train['hashtag'] = train['hashtag'].apply(lambda x: clear_list(x))\n",
    "test['hashtag'] = test['hashtag'].apply(lambda x: clear_list(x))\n",
    "\n",
    "train['keyword'] = train['keyword'].apply(lambda x: clear_list(x))\n",
    "test['keyword'] = test['keyword'].apply(lambda x: clear_list(x))\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "The code defines a function to expand common English contractions and applies it to the 'text' column in both the training and test datasets to ensure consistency in the text data.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.87677044,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: decontracted(x))\n",
    "test['text'] = test['text'].apply(lambda x: decontracted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "The code removes punctuation and underscores from the 'text', 'hashtag', and 'keyword' columns in both the training and test datasets to further clean and standardize the text data.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9966815,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punc(lista):\n",
    "    for i,ele in enumerate(lista):\n",
    "        lista[i] = re.sub(r'[^\\w\\s]', '', ele)\n",
    "        lista[i] = re.sub('_', ' ', lista[i]) # the previous row doesn't remove underscore\n",
    "    return lista\n",
    "\n",
    "train['text']=train['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "train['text']=train['text'].apply(lambda x: re.sub('_', ' ', x)) # the previous row doesn't remove underscore\n",
    "test['text']=test['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "test['text']=test['text'].apply(lambda x: re.sub('_', ' ', x)) # the previous row doesn't remove underscore\n",
    "\n",
    "train['hashtag']=train['hashtag'].apply(lambda x: remove_punc(x))\n",
    "test['hashtag']=test['hashtag'].apply(lambda x: remove_punc(x))\n",
    "\n",
    "train['keyword']=train['keyword'].apply(lambda x: remove_punc(x))\n",
    "test['keyword']=test['keyword'].apply(lambda x: remove_punc(x))\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "The code tokenizes the text in the 'text' column of both the training and test datasets using NLTK's `word_tokenize` function to convert sentences into lists of words.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.8696794,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "train['text']=train['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "test['text']=test['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "The code removes stop words, including some numbers and the letter 'u', from the 'text', 'hashtag', and 'keyword' columns of both the training and test datasets using a custom function.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9192129,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "stop_words.append('u') # 'i love u' is the semantically the same as 'i love you'\n",
    "stop_words.append('one') # want to remove numbers\n",
    "stop_words.append('two')\n",
    "stop_words.append('three')\n",
    "stop_words.append('four')\n",
    "stop_words.append('five')\n",
    "stop_words.append('six')\n",
    "stop_words.append('seven')\n",
    "stop_words.append('eight')\n",
    "stop_words.append('nine')\n",
    "stop_words.append('ten')\n",
    "\n",
    "def remove_stop_words(lista):\n",
    "    pt=0 # don't use a for loop because len(lista) keeps changing as we remove stop words.\n",
    "    while pt<len(lista):\n",
    "        if lista[pt] in stop_words:\n",
    "            lista.remove(lista[pt])\n",
    "        else:\n",
    "            pt+=1\n",
    "    return lista\n",
    "\n",
    "train['text']=train['text'].apply(lambda x : remove_stop_words(x))\n",
    "train['hashtag']=train['hashtag'].apply(lambda x : remove_stop_words(x))\n",
    "train['keyword']=train['keyword'].apply(lambda x : remove_stop_words(x))\n",
    "\n",
    "test['text']=test['text'].apply(lambda x : remove_stop_words(x))\n",
    "test['hashtag']=test['hashtag'].apply(lambda x : remove_stop_words(x))\n",
    "test['keyword']=test['keyword'].apply(lambda x : remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "The code lemmatizes the words in the 'text', 'hashtag', and 'keyword' columns of both the training and test datasets by applying the `WordNetLemmatizer` to each element in these columns.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.83677053,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def lemmatize_list(lista):\n",
    "    for i, ele in enumerate(lista):\n",
    "        lista[i]=WordNetLemmatizer().lemmatize(ele)\n",
    "    return lista\n",
    "\n",
    "train['text']=train['text'].apply(lambda x : lemmatize_list(x))\n",
    "train['hashtag']=train['hashtag'].apply(lambda x : lemmatize_list(x))\n",
    "train['keyword']=train['keyword'].apply(lambda x : lemmatize_list(x))\n",
    "\n",
    "test['text']=test['text'].apply(lambda x : lemmatize_list(x))\n",
    "test['hashtag']=test['hashtag'].apply(lambda x : lemmatize_list(x))\n",
    "test['keyword']=test['keyword'].apply(lambda x : lemmatize_list(x))\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "The code initializes a NumPy array to store vector representations of words from the 'text', 'hashtag', and 'keyword' columns in the training dataset, iterating over each example to fill the array using `spaCy` embeddings.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.55836535,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "m=train.shape[0]\n",
    "store_train=np.zeros((m,23+13+2,300))\n",
    "for i in range(m): # m\n",
    "    if i % 100 == 99:\n",
    "        print(i)\n",
    "    for j in range(len(train['text'][i])): # length of the list ['love','peace','compassion','wisdom']        \n",
    "        store_train[i,j,:]=nlp(train['text'][i][j])[0].vector\n",
    "    for j in range(len(train['hashtag'][i])):\n",
    "        try:\n",
    "            store_train[i,23+j,:]=nlp(train['hashtag'][i][j])[0].vector\n",
    "        except:\n",
    "            store_train[i,23+j,:]=nlp(train['hashtag'][i][j]).vector # in the case when hashtag is [''] instead of ['some','word']\n",
    "    for j in range(len(train['keyword'][i])):\n",
    "        store_train[i,36+j,:]=nlp(train['keyword'][i][j])[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Data Transform",
    "desc": "The code initializes a NumPy array to store vector representations of words from the 'text', 'hashtag', and 'keyword' columns in the test dataset, iterating over each example to fill the array using `spaCy` embeddings.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.95844316,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "m=test.shape[0]\n",
    "store_test=np.zeros((m,23+13+2,300))\n",
    "for i in range(m): # m\n",
    "    if i % 100 == 99:\n",
    "        print(i)\n",
    "    for j in range(len(test['text'][i])): # length of the list ['love','peace','compassion','wisdom']        \n",
    "        store_test[i,j,:]=nlp(test['text'][i][j])[0].vector\n",
    "    for j in range(len(test['hashtag'][i])):\n",
    "        try:\n",
    "            store_test[i,23+j,:]=nlp(test['hashtag'][i][j])[0].vector\n",
    "        except:\n",
    "            store_test[i,23+j,:]=nlp(test['hashtag'][i][j]).vector # in the case when hashtag is [''] instead of ['some','word']\n",
    "    for j in range(len(test['keyword'][i])):\n",
    "        store_test[i,36+j,:]=nlp(test['keyword'][i][j])[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Data Transform",
    "desc": "The code segments the `store_train` array into three separate arrays (`store_train_text`, `store_train_hashtag`, and `store_train_keyword`) based on the calculated maximum lengths of the text, hashtag, and keyword sequences.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.48342198,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "store_train_text=store_train[:,:maxi_text,:]\n",
    "store_train_hashtag=store_train[:,maxi_text:maxi_text+maxi_hashtag,:]\n",
    "store_train_keyword=store_train[:,-maxi_keyword:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Transform",
    "desc": "The code prints the shapes of the segmented arrays (`store_train_text`, `store_train_hashtag`, and `store_train_keyword`) to verify the correct partitioning of the `store_train` array.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9991648,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "print(store_train_text.shape)\n",
    "print(store_train_hashtag.shape)\n",
    "print(store_train_keyword.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Data Transform",
    "desc": "The code segments the `store_train` array (instead of `store_test` array, which seems to be an error) into three separate arrays (`store_test_text`, `store_test_hashtag`, and `store_test_keyword`) based on the calculated maximum lengths of the text, hashtag, and keyword sequences.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.63455486,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "store_test_text=store_train[:,:maxi_text,:]\n",
    "store_test_hashtag=store_train[:,maxi_text:maxi_text+maxi_hashtag,:]\n",
    "store_test_keyword=store_train[:,-maxi_keyword:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Transform",
    "desc": "The code loads the stored training data from 'store_train.npy' and creates a one-hot encoded matrix `train_Y` for the target labels in the training dataset.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99184144,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "store_train=np.load('store_train.npy')\n",
    "\n",
    "m=store_train.shape[0]\n",
    "train_Y=np.zeros((m,2))\n",
    "for i in range(m):\n",
    "    train_Y[i,train.iloc[i]['target']]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Data Transform",
    "desc": "The code sets a random seed, then shuffles the `store_train` array and corresponding `train_Y` labels to ensure that the data and labels stay aligned for training.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.24489856,
    "start_cell": false,
    "subclass": "randomize_order",
    "subclass_id": 15
   },
   "outputs": [],
   "source": [
    "sed=13\n",
    "np.random.seed(sed)\n",
    "np.random.shuffle(store_train)\n",
    "np.random.seed(sed)\n",
    "np.random.shuffle(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This function creates a corpus by iterating through the split text of each tweet and appending each word to a list which is then returned.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9449553,
    "start_cell": true,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "def create_corpus():\n",
    "    corpus=[]\n",
    "    \n",
    "    for x in tweet['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This function identifies and returns the top n most frequent bigrams from a given corpus of tweets by using CountVectorizer to create and count the bigrams.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.7835433,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This snippet defines two functions: one to build a vocabulary dictionary from a corpus with word frequencies and another to check the coverage of these words in a given word embeddings dictionary, returning out-of-vocabulary words, vocabulary coverage, and text coverage.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.60241115,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def build_vocab(X):\n",
    "    \n",
    "    tweets = X.apply(lambda s: s.split()).values      \n",
    "    vocab = {}\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1                \n",
    "    return vocab\n",
    "\n",
    "def check_embeddings_coverage(X, embeddings):\n",
    "    \n",
    "    vocab = build_vocab(X)    \n",
    "    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This function cleans and preprocesses input text by converting it to lowercase, removing punctuations, URLs, HTML tags, and specific unwanted substrings or characters.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.77874374,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text):\n",
    "        ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "     \n",
    "    ## clean urls \n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url.sub(r'',text)\n",
    "    \n",
    "    url = re.compile(r'http?://\\S+|www\\.\\S+')\n",
    "    text = url.sub(r'',text)\n",
    "    \n",
    "    ## remove html \n",
    "    html=re.compile(r'<.*?>') \n",
    "    html.sub(r'',text)\n",
    "        \n",
    "        \n",
    "    text = re.sub(r'mh370','flight crash',text)     \n",
    "    text = re.sub(r'û_','',text)     \n",
    "    text = re.sub(r'ûò','',text) \n",
    "    text = re.sub(r'typhoondevastated','typhoon devastated',text)      \n",
    "    text = re.sub(r'irandeal','iran deal',text)      \n",
    "    text = re.sub(r'worldnews','world news',text)      \n",
    "    text = re.sub(r'animalrescue','animal rescue',text)      \n",
    "    text = re.sub(r'viralspell','viral spell',text)      \n",
    "    text = re.sub(r'griefûª','grief',text)      \n",
    "    text = re.sub(r'pantherattack','panther attack',text)      \n",
    "    text = re.sub(r'injuryi495','injury in 495',text) \n",
    "    text = re.sub(r'explosionproof','explosion proof',text) \n",
    "    text = re.sub(r'americaûªs','americans',text) \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This snippet combines the training and test datasets into one DataFrame and outputs its shape.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9981925,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "df=pd.concat([tweet,testset])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Transform",
    "desc": "This snippet applies the `utils_preprocess_text` function to the 'text' column of the combined dataset to create a new column 'text_clean' with the cleaned text.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9958597,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(lambda x: utils_preprocess_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This snippet assigns the 'text_clean' column of the combined dataset to the variable `corpus`.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9991335,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "corpus = df[\"text_clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This snippet tokenizes the cleaned text corpus, converts the text to sequences of integers, and pads these sequences to a maximum length of 50 tokens.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.5715023,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This snippet creates an embedding matrix where each row corresponds to a word's vector representation from the GloVe embeddings, initializing it with zeros and filling it with vectors for words present in the GloVe embedding dictionary.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.5574094,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i < num_words:\n",
    "        emb_vec=embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i]=emb_vec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This snippet splits the padded sequences into training and test sets based on their original indices in the combined dataset.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.781005,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This snippet splits the training data into training and validation sets along with their corresponding target values, with a validation size of 20%, and prints their respective shapes.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9958903,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.2)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Transform",
    "desc": "This code snippet removes the 'id' column from both the training and testing datasets.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99910396,
    "start_cell": true,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop('id',axis = 1)\n",
    "test_data = test_data.drop('id',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code snippet fills any missing values in the training and testing datasets with empty strings.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.5468173,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "test_data = test_data.fillna('')\n",
    "train_data = train_data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to correct keywords by replacing '%20' with a space in the input string.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9173236,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def keyword_correction(x):\n",
    "    try:\n",
    "        x = x.split(\"%20\")\n",
    "        x = ' '.join(x)\n",
    "        return x\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "This code snippet applies the 'keyword_correction' function to the 'keyword' column in both the training and testing datasets to correct the keywords.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9410188,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train_data['keyword'] = train_data['keyword'].apply(lambda x: keyword_correction(x))\n",
    "test_data['keyword'] = test_data['keyword'].apply(lambda x: keyword_correction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet defines multiple functions to clean text data by removing unwanted characters, correcting misspellings, normalizing numbers, and finally applies these cleaning steps to specific columns in the provided DataFrame.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9857341,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "def text_cleaning(text):\n",
    "    forbidden_words = set(stopwords.words('english'))\n",
    "    if text:\n",
    "        text = ' '.join(text.split('.'))\n",
    "        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z0-9]', ' ', text.strip().lower())).strip()\n",
    "        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n",
    "        text = [word for word in text.split() if word not in forbidden_words]\n",
    "        return text\n",
    "    return []\n",
    "#clean data\n",
    "#this following cleaning is taken from https://www.kaggle.com/nxhong93/tweet-predict1\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"havent\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"thats\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"theres\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"theyre\":  \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\"}\n",
    "\n",
    "puncts = puncts + list(string.punctuation)\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x).replace(\"\\n\",\"\")\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('\\d+', ' ', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "\n",
    "    def replace(match):\n",
    "        return mispell_dict[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def remove_space(string):\n",
    "    string = BeautifulSoup(string).text.strip().lower()\n",
    "    string = re.sub(r'((http)\\S+)', 'http', string)\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    return string\n",
    "\n",
    "\n",
    "def clean_data(df, columns: list):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: remove_space(x).lower())        \n",
    "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
    "        df[col] = df[col].apply(lambda x: clean_text(x))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet applies the `text_cleaning` function to the 'location' and 'text' columns of both training and testing datasets, and then applies the `clean_data` function to clean the 'keyword' and 'text' columns in the datasets.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.406117,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "for col in ['location','text']:\n",
    "    train_data[col] = train_data[col].apply(lambda x: ' '.join(text_cleaning(x)))\n",
    "    test_data[col] = test_data[col].apply(lambda x: ' '.join(text_cleaning(x)))\n",
    "train_data = clean_data(train_data,['keyword','text'])\n",
    "test_data = clean_data(test_data,['keyword','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to detect named entities in the input text using the spaCy model and returns 1 if any entities are found, otherwise 0.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.40232253,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def location_detection(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent)\n",
    "    if len(entities)>0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet adds a new column 'original_locations' to both the training and testing datasets, indicating whether the 'location' column contains any named entities.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.8857917,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train_data['original_locations'] = train_data['location'].apply(lambda x: location_detection(x))\n",
    "test_data['original_locations'] = test_data['location'].apply(lambda x: location_detection(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to check if the input text contains any words from a predefined list of \"spam\" locations, returning 1 if a word is found and 0 otherwise.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.90423363,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "spam_locations = ['place','room','home','somewhere','nowhere','everywhere','location',\n",
    "                  'dope','kidding','moon','wherever','dimension','world','fvck','fuck','beside']\n",
    "def is_location_spammy(text):\n",
    "    for word in spam_locations:\n",
    "        if word in text:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet adds a new column 'Is_location_spam' to both the training and testing datasets, indicating whether the 'location' column contains any spammy words from the predefined list.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9345971,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train_data['Is_location_spam'] = train_data['location'].apply(lambda x: is_location_spammy(x))\n",
    "test_data['Is_location_spam'] = test_data['location'].apply(lambda x: is_location_spammy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet defines a function that counts the number of tokens in the input text that contain digits using the spaCy model.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.42822075,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "def digit_counter(text):\n",
    "    \"\"\"detects any digit in any token and counts\n",
    "       once par token.\"\"\"\n",
    "    sum_number = 0\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        sum_number += bool(re.search(r'\\d', token.text))*1\n",
    "    return sum_number    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet adds a new column 'digit_count_location' to both the training and testing datasets, reflecting the count of tokens containing digits in the 'location' column.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99510044,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train_data['digit_count_location'] = train_data['location'].apply(lambda x: digit_counter(x))\n",
    "test_data['digit_count_location'] = test_data['location'].apply(lambda x: digit_counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet concatenates all 'text' values from the training dataset into two separate strings, one for tweets labeled as disasters and one for non-disasters.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.77438766,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "disaster_tweets =' '.join(train_data[train_data['target'] == 1]['text'].tolist())\n",
    "non_disaster_tweets = ' '.join(train_data[train_data['target'] == 0]['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to return the top `n` most common words in a given text, excluding stop words, by tokenizing the input text and calculating word frequencies using NLTK.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.8723762,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def return_top_words(text,words = 10):\n",
    "    allWords = nltk.tokenize.word_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in allWords if w not in stopwords)    \n",
    "    mostCommontuples= allWordExceptStopDist.most_common(words)\n",
    "    mostCommon = [tupl[0] for tupl in mostCommontuples]\n",
    "    return mostCommon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet calculates the top 50 most common words for both disaster and non-disaster tweets using the previously defined `return_top_words` function.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9773226,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "top_50_disaster_words = return_top_words(disaster_tweets,50)\n",
    "top_50_nondisaster_words = return_top_words(non_disaster_tweets,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "This code snippet calculates the top 400 most common words for both disaster and non-disaster tweets and then identifies words that are exclusively found in disaster tweets and non-disaster tweets, respectively.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9461599,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "top_200_disaster_words = return_top_words(disaster_tweets,400)\n",
    "top_200_nondisaster_words = return_top_words(non_disaster_tweets,400)\n",
    "top_disaster_exclusive = list(set(top_200_disaster_words).difference(set(top_200_nondisaster_words)))\n",
    "top_nondisaster_exclusive = list(set(top_200_nondisaster_words).difference(set(top_200_disaster_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Data Transform",
    "desc": "This code snippet creates a combined list of words that are exclusively found in either disaster tweets or non-disaster tweets, forming the total vocabulary.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9986142,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "total_vocab = top_disaster_exclusive + top_nondisaster_exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Data Transform",
    "desc": "This code snippet creates new columns in both the training and testing datasets to indicate the presence of each word in the `total_vocab` within the 'text' column, marking presence with a 1 and absence with a 0.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9986064,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "for word in total_vocab:\n",
    "    train_data['Is_'+word+'_present'] = train_data['text'].apply(lambda x: (word in x)*1)\n",
    "    test_data['Is_'+word+'_present'] = test_data['text'].apply(lambda x: (word in x)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Transform",
    "desc": "This code snippet utilizes the `TfidfVectorizer` from scikit-learn to convert the 'text', 'keyword', and 'location' columns of both the training and testing datasets into TF-IDF representations while limiting the number of features and specifying n-gram ranges.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.93943185,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         max_features = 5000,\n",
    "                         smooth_idf=False)\n",
    "X_train_tfidf = tf_idf.fit_transform(train_data['text'])\n",
    "X_test_tfidf = tf_idf.transform(test_data['text'])\n",
    "tf_kw = TfidfVectorizer(ngram_range = (1,2),\n",
    "                        binary = True,\n",
    "                        max_features = 1500,\n",
    "                        smooth_idf = False)\n",
    "kw_train_tfidf = tf_kw.fit_transform(train_data['keyword'])\n",
    "kw_test_tfidf = tf_kw.transform(test_data['keyword'])\n",
    "tf_location = TfidfVectorizer(ngram_range = (1,2),\n",
    "                              binary = True,\n",
    "                              max_features = 1500,\n",
    "                              smooth_idf = False)\n",
    "location_train_tfidf = tf_location.fit_transform(train_data['location'])\n",
    "location_test_tfidf = tf_location.transform(test_data['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Data Transform",
    "desc": "This code snippet concatenates the TF-IDF feature representations of the 'text', 'keyword', and 'location' columns to the existing training and testing datasets, adding new columns to each dataset to reflect these TF-IDF features.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99309033,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data,pd.DataFrame(X_train_tfidf.toarray(),\n",
    "                                                columns = ['text_contains_'+ str(text) for text in tf_idf.get_feature_names()]),\n",
    "                        pd.DataFrame(kw_train_tfidf.toarray(),\n",
    "                                     columns = ['keyword_contains_'+str(text) for text in tf_kw.get_feature_names()]),\n",
    "                        pd.DataFrame(location_train_tfidf.toarray(),\n",
    "                                     columns = ['location_contains_'+str(text) for text in tf_location.get_feature_names()])],axis = 1)\n",
    "test_data = pd.concat([test_data,pd.DataFrame(X_test_tfidf.toarray(),\n",
    "                                              columns = ['text_contains_'+ str(text) for text in tf_idf.get_feature_names()]),\n",
    "                       pd.DataFrame(kw_test_tfidf.toarray(),\n",
    "                                    columns = ['keyword_contains_'+str(text) for text in tf_kw.get_feature_names()]),\n",
    "                       pd.DataFrame(location_test_tfidf.toarray(),\n",
    "                                    columns = ['location_contains_'+str(text) for text in tf_location.get_feature_names()])],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Data Transform",
    "desc": "This code snippet generates 300-dimensional word vectors for the 'text' column in both training and testing datasets using spaCy's language model, and then concatenates these vectors to the original datasets as new columns.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.85527724,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "def create_vec(dataframe):\n",
    "    texts = dataframe['text'].tolist()\n",
    "    vectors = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        vectors.append(list(doc.vector))\n",
    "    df = pd.DataFrame(vectors,columns = ['vec_'+str(i) for i in range(300)])\n",
    "    return df\n",
    "vec_train = create_vec(train_data)\n",
    "vec_test = create_vec(test_data)\n",
    "train_data = pd.concat([train_data,vec_train],axis = 1)\n",
    "test_data = pd.concat([test_data,vec_test],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Transform",
    "desc": "This code snippet removes the 'keyword', 'location', and 'text' columns from both the training and testing datasets.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.999161,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['keyword','location','text'],axis = 1)\n",
    "test_data = test_data.drop(['keyword','location','text'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Transform",
    "desc": "This code snippet separates the training dataset into features (`X_train`) and the target variable (`Y_train`) and checks if the 'target' column exists in the testing dataset, printing the result.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9993099,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "X_train = train_data.drop('target',axis = 1)\n",
    "Y_train = train_data['target']\n",
    "print('target' in test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Data Transform",
    "desc": "This code snippet, if uncommented, would create reduced versions of the training features (`X_train`) and testing dataset (`test_data`) by dropping columns identified as having low importance.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9929066,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#X_train_reduced = X_train.drop(bad_features,axis = 1)\n",
    "#test_data_reduced = test_data.drop(bad_features,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This code snippet defines functions to remove URLs, HTML tags, emojis, punctuation, tags, and extra spaces from text, and then applies these transformations to cleanse a given dataset.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.6653787,
    "start_cell": true,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_tag(text):\n",
    "    url = re.compile(r'(?<=\\@)(.*?)(?= )')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_space(text):\n",
    "    return text.replace(r'%20',' ')\n",
    "\n",
    "def cleanse_data(data_in):\n",
    "    data = data_in.copy()\n",
    "    data['text'] = data['text'].apply(lambda x:remove_URL(x))\n",
    "    data['text'] = data['text'].apply(lambda x:remove_html(x))\n",
    "    data['text'] = data['text'].apply(lambda x:remove_emoji(x))\n",
    "    data['text'] = data['text'].apply(lambda x:remove_tag(x))\n",
    "    data['text'] = data['text'].apply(lambda x:remove_punct(x))\n",
    "    \n",
    "    data['keyword'].fillna('Nothing', inplace=True)\n",
    "    data['keyword'] = data['keyword'].apply(lambda x:remove_space(x))\n",
    "    data['keyword'] = data['keyword'].apply(lambda x:remove_punct(x))\n",
    "    \n",
    "    data['location'].fillna('Nothing', inplace=True)\n",
    "    data['location'] = data['location'].apply(lambda x:remove_punct(x))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet applies the previously defined data cleansing functions to the dataset to produce a cleansed version of the training data.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9975368,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Cleanse data\n",
    "data_cleansed = cleanse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet cleanses the evaluation dataset using previously defined functions and prepares it for evaluation by tokenizing the text data.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.68808633,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "data_eval_cleansed = cleanse_data(data_eval)\n",
    "X_eval = get_X(data_eval_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code selects specific columns from the training and test dataframes and adds a 'target' column with default values to the test dataframe for uniformity.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9986106,
    "start_cell": true,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train_val_df = train_val_df.loc[:,[\"text\",\"target\"]]\n",
    "test_df = test_df.loc[:,[\"text\"]]\n",
    "test_df[\"target\"] = [0]*len(test_df[\"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code defines several functions to preprocess text data by replacing typical misspellings, removing spaces, stripping HTML tags, and deleting emojis, and then applies these transformations to the text columns in both the training and test dataframes.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9977969,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"havent\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"thats\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"theres\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"theyre\":  \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\"}\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    text = text.lower()\n",
    "    mispellings_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "\n",
    "    def replace(match):\n",
    "        return mispell_dict[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "train_val_df['text'] = train_val_df['text'].apply(lambda x : replace_typical_misspell(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x : replace_typical_misspell(x))\n",
    "\n",
    "def remove_space(string):\n",
    "    string = BeautifulSoup(string).text.strip().lower()\n",
    "    string = re.sub(r'((http)\\S+)', 'http', string)\n",
    "    string = re.sub(r'@\\w*', '', string)\n",
    "    string = re.sub(r'#', '', string)\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    return string\n",
    "\n",
    "train_val_df['text'] = train_val_df['text'].apply(lambda x : remove_space(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x : remove_space(x))\n",
    "\n",
    "# 絵文字の削除\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "train_val_df['text'] = train_val_df['text'].apply(lambda x: remove_emoji(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_emoji(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This code shuffles the rows of the training dataframe randomly to ensure that the data is mixed and not in any specific order, and then resets the index while keeping the new order.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9995983,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# 順番をシャッフルする\n",
    "train_val_df = train_val_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "train_val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code defines a tokenizer function that tokenizes input text into a fixed length of 50 tokens using the tokenizer from the transformers library and sets up `TEXT` and `LABEL` fields for input data and labels, respectively, using torchtext for later model training.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.38500202,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "\n",
    "\n",
    "def tokenizer_50(input_text):\n",
    "    \"\"\"torchtextのtokenizerとして扱えるように、50単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
    "    return tokenizer.encode(input_text, max_length=50, return_tensors='pt')[0]\n",
    "\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_50, use_vocab=False, lower=False,\n",
    "                            include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
    "\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code splits the unified training/evaluation dataset into separate training and evaluation datasets based on a specified split ratio and prints their lengths to verify the distribution.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.26750895,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
    "\n",
    "dataset_train, dataset_eval = dataset_train_eval.split(\n",
    "    split_ratio=1.0 - 1800/7613, random_state=random.seed(1234))\n",
    "\n",
    "# datasetの長さを確認してみる\n",
    "print(dataset_train.__len__())\n",
    "print(dataset_eval.__len__())\n",
    "print(dataset_test.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code creates DataLoader objects for the training, evaluation, and test datasets with a specified batch size, and combines the training and evaluation DataLoaders into a dictionary for easier access during model training and evaluation.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9920397,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# DataLoaderを作成\n",
    "batch_size = 32  # BERTでは16、32あたりを使用する\n",
    "\n",
    "dl_train = torchtext.data.Iterator(\n",
    "    dataset_train, batch_size=batch_size, train=True)\n",
    "\n",
    "dl_eval = torchtext.data.Iterator(\n",
    "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "dl_test = torchtext.data.Iterator(\n",
    "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This snippet defines a function that returns the length of a given text, which could be used for feature engineering or data preprocessing.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99316376,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "def length(text):    \n",
    "    '''a function which returns the length of text'''\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This snippet creates a new column in the train DataFrame that stores the length of each tweet's text by applying the previously defined function, aiding in feature engineering.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9992126,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "tweet['length'] = tweet['text'].apply(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This snippet defines a function that creates a corpus by extracting and appending all words from the tweets in the train DataFrame based on the specified target label, enabling further text analysis and preprocessing.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.51645607,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "def create_corpus(target):\n",
    "    corpus=[]\n",
    "    \n",
    "    for x in tweet[tweet['target']==target]['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This snippet creates a corpus of words from tweets labeled as 'Not', counts the occurrences of stop words in this corpus using a defaultdict, and then extracts the top 10 most common stop words in these tweets.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.96934545,
    "start_cell": false,
    "subclass": "sort_values",
    "subclass_id": 9
   },
   "outputs": [],
   "source": [
    "corpus=create_corpus(0)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "        \n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This snippet constructs the corpus of words from tweets labeled as 'Not', computes the 40 most common words excluding stop words, and stores these words and their counts in separate lists for further use in visualization or analysis.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.95793825,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "x=[]\n",
    "y=[]\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop) :\n",
    "        x.append(word)\n",
    "        y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "This snippet defines a function that extracts and returns the most frequent bigrams from a given corpus of text, using a CountVectorizer with a specified n-gram range and sorting the bigrams by frequency.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.7835433,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Data Transform",
    "desc": "This snippet defines a function to remove URLs from text using regular expressions and demonstrates its application on a sample tweet, illustrating how text data can be cleaned for further processing.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9513789,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "remove_URL(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Transform",
    "desc": "This snippet applies the previously defined URL removal function to the 'text' column of the concatenated DataFrame, cleaning the text data by removing any URLs.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99688476,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Data Transform",
    "desc": "This snippet defines a function to remove HTML tags from text using regular expressions and demonstrates its application by printing the cleaned version of a sample HTML string.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.5649151,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "print(remove_html(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Data Transform",
    "desc": "This snippet applies the previously defined HTML removal function to the 'text' column of the concatenated DataFrame, cleaning the text data by stripping out any HTML tags.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9979342,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Transform",
    "desc": "This snippet defines a function to remove emojis from text using a regular expression and demonstrates its application by cleaning a sample string containing emojis.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.5422814,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake 😔😔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Transform",
    "desc": "This snippet applies the previously defined emoji removal function to the 'text' column of the concatenated DataFrame, cleaning the text data by stripping out any emojis.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9978229,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Transform",
    "desc": "This snippet defines a function to remove punctuation from text using translation and demonstrates its application by cleaning a sample string containing punctuation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.70283914,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "example=\"I am a #king\"\n",
    "print(remove_punct(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Data Transform",
    "desc": "This snippet applies the previously defined punctuation removal function to the 'text' column of the concatenated DataFrame, cleaning the text data by stripping out any punctuation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9982992,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Data Transform",
    "desc": "This snippet defines a function to vectorize text data using CountVectorizer, applies this function to the train and test text data split from the concatenated DataFrame, and transforms the text data into count-based feature vectors for subsequent modeling.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9714689,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "list_corpus = df[\"text\"].tolist()\n",
    "list_labels = df[\"target\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=random_state_split)\n",
    "\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Data Transform",
    "desc": "This snippet defines a function to vectorize text data using TfidfVectorizer, applies this function to the train and test text data split from the concatenated DataFrame, transforming the text data into TF-IDF feature vectors for subsequent modeling.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99001664,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 43,
    "class": "Data Transform",
    "desc": "This snippet splits the concatenated DataFrame back into train and test DataFrames based on their original row indices, ensuring that subsequent analyses can be conducted on the appropriate subsets.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9967955,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "df_train = df[:7612]\n",
    "df_test = df[7613:10875]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 47,
    "class": "Data Transform",
    "desc": "This snippet initializes and fits a TfidfVectorizer with specific parameters on the text data from the entire concatenated DataFrame, then prints the length of the resulting vocabulary, preparing the data for TF-IDF transformation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.96011144,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(ngram_range=(1,3),min_df=3,strip_accents='unicode', use_idf=1,smooth_idf=1, sublinear_tf=1,max_features=None)\n",
    "vectorizer.fit(list(df['text']))\n",
    "print('vocab length',len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 48,
    "class": "Data Transform",
    "desc": "This snippet transforms the training and validation text data into dense TF-IDF vectors using the previously fitted TfidfVectorizer, converting the text data into numerical representations for model training.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9983187,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "X_train_onehot = vectorizer.transform(X_train).todense()\n",
    "X_val_onehot = vectorizer.transform(X_val).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 50,
    "class": "Data Transform",
    "desc": "This snippet defines a function to create a new corpus by tokenizing each tweet's text into lowercase words and compiling them into a list, enabling further text preprocessing and analysis.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.3999448,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "def create_corpus_new(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet)]\n",
    "        corpus.append(words)\n",
    "    return corpus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 51,
    "class": "Data Transform",
    "desc": "This snippet applies the previously defined function to create a corpus from the text data in the concatenated DataFrame, resulting in a list of tokenized, lowercase words for each tweet.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.40998268,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "corpus=create_corpus_new(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 53,
    "class": "Data Transform",
    "desc": "This snippet initializes a tokenizer, fits it on the created corpus, converts the corpus into sequences of integers, and then pads these sequences to a maximum length of 50, standardizing the input for subsequent neural network training.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.5715023,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 55,
    "class": "Data Transform",
    "desc": "This snippet creates an embedding matrix where each row corresponds to the GloVe embeddings for words in the tokenizer's word index, initializing non-existent embeddings to zeros and facilitating their use in the neural network model.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.52565557,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i < num_words:\n",
    "        emb_vec=embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i]=emb_vec           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 59,
    "class": "Data Transform",
    "desc": "This snippet splits the padded sequences back into training and test sets based on the original shape of the tweet DataFrame, ensuring the appropriate subsets are used for model training and evaluation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.781005,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 66,
    "class": "Data Transform",
    "desc": "This snippet defines a function to encode texts using BERT tokenizer by tokenizing each text, creating input sequences with special tokens, padding them to a maximum length, and returning the tokens, masks, and segment IDs as NumPy arrays for input into a BERT model.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9458959,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 68,
    "class": "Data Transform",
    "desc": "This snippet defines a function to clean tweets by removing non-ASCII characters and URLs from the text, ensuring that the data is sanitized for further processing and modeling.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.31128287,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def clean_tweets(tweet):\n",
    "    \"\"\"Removes links and non-ASCII characters\"\"\"\n",
    "    \n",
    "    tweet = ''.join([x for x in tweet if x in string.printable])\n",
    "    \n",
    "    # Removing URLs\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 69,
    "class": "Data Transform",
    "desc": "This snippet redefines the function to remove emojis from text using a regular expression, ensuring that the text is cleaned of any emoji characters for further processing and analysis.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.42551854,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 70,
    "class": "Data Transform",
    "desc": "This snippet defines a function to separate punctuations from text by adding spaces around them, ensuring that the punctuations are treated as distinct tokens during subsequent text processing.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.46570426,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def remove_punctuations(text):\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    \n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "\n",
    "    text = text.replace('...', ' ... ')\n",
    "    \n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 72,
    "class": "Data Transform",
    "desc": "This snippet defines a function to convert abbreviations in a word to their expanded forms by looking them up in a predefined dictionary of abbreviations, aiding in the normalization of text data.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.92899597,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 73,
    "class": "Data Transform",
    "desc": "This snippet defines a function to apply abbreviation conversion on an entire text by tokenizing it into words, converting each word using the abbreviation dictionary, and then joining the tokens back into a normalized text string.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.61446923,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 76,
    "class": "Data Transform",
    "desc": "This snippet applies a series of text cleaning functions—removing non-ASCII characters, URLs, emojis, separating punctuations, and converting abbreviations—on the text data in the train and test DataFrames, contingent upon the `target_big_corrected` flag being set to True.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.95746833,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "if target_big_corrected:\n",
    "    train[\"text\"] = train[\"text\"].apply(lambda x: clean_tweets(x))\n",
    "    test[\"text\"] = test[\"text\"].apply(lambda x: clean_tweets(x))\n",
    "    \n",
    "    train[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "    test[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "    \n",
    "    train[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuations(x))\n",
    "    test[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuations(x))\n",
    "    \n",
    "    train[\"text\"] = train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n",
    "    test[\"text\"] = test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 78,
    "class": "Data Transform",
    "desc": "This snippet encodes the text data from the train and test DataFrames into BERT input formats (tokens, masks, and segment IDs) with a maximum sequence length of 160, and extracts the target labels from the train DataFrame for subsequent model training.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99541265,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "This function cleans tweet text by removing links and non-ASCII characters.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.35914353,
    "start_cell": true,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def clean_tweets(tweet):\n",
    "    \"\"\"Removes links and non-ASCII characters\"\"\"\n",
    "    \n",
    "    tweet = ''.join([x for x in tweet if x in string.printable])\n",
    "    \n",
    "    # Removing URLs\n",
    "    tweet = re.sub(r\"http\\S+\", \"_url_\", tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This function removes emojis from the given text by substituting them with a placeholder.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.42551854,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'_emoji_', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "This function removes and replaces punctuations in the given text, ensuring they are surrounded by spaces for better tokenization.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.46570426,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def remove_punctuations(text):\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    \n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "\n",
    "    text = text.replace('...', ' ... ')\n",
    "    \n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This code snippet defines a dictionary of common abbreviations and their expanded forms to be used for text preprocessing.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.99585694,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This function converts a word to its expanded form if it is found in the predefined abbreviations dictionary; otherwise, it returns the original word.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.92899597,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This function tokenizes a given text, converts any abbreviations in the text using the `convert_abbrev` function, and then joins the tokens back into a single string.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.61446923,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet sequentially applies text preprocessing functions—`clean_tweets`, `remove_emoji`, `remove_punctuations`, and `convert_abbrev_in_text`—to the train and test DataFrame columns, and converts the text to lowercase if `to_lower` is set to True.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9856442,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lambda x: clean_tweets(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda x: clean_tweets(x))\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lambda x: remove_punctuations(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda x: remove_punctuations(x))\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n",
    "\n",
    "if to_lower:\n",
    "    df_train[\"text\"] = df_train[\"text\"].apply(lambda x: x.lower())\n",
    "    df_test[\"text\"] = df_test[\"text\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code snippet corrects the target labels for specific tweets in the training data based on their IDs if the `target_corrected` flag is set to True.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.998466,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - \n",
    "# author of this kernel read tweets in training data and figure out that some of them have errors:\n",
    "if target_corrected:\n",
    "    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "    df_train.loc[df_train['id'].isin(ids_with_target_error),'target'] = 0\n",
    "    df_train[df_train['id'].isin(ids_with_target_error)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "This code snippet concatenates the training and test datasets into a single DataFrame named `df`.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9994925,
    "start_cell": true,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "df=pd.concat([tweet,test],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code snippet defines functions to remove URLs, HTML tags, emojis, and punctuation from text, and convert text to lowercase, then applies these functions to the 'text' column in the DataFrame `df`.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9691914,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "df['text']=df['text'].apply(lambda x: remove_URL(x))\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "df['text']=df['text'].apply(lambda x: remove_html(x))\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "df['text']=df['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "def string_lower(text):\n",
    "    return text.lower()\n",
    "df['text']=df['text'].apply(lambda x: string_lower(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "This code snippet splits the combined DataFrame `df` back into the original training (`tweet`) and test (`test`) DataFrames based on their initial lengths.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9945739,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "tweet=df[:len(tweet)]\n",
    "test=df[len(tweet):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet uses the pre-trained Doc2Vec model to infer vectors for each document in the `tweet` DataFrame, constructs a new DataFrame `train` from these vectors, and displays the first few rows.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.51562935,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "newvecs=[Pretrained_Model.infer_vector(tweet['text'][i].split()) for i in range(len(tweet))]\n",
    "train=pd.DataFrame(data=newvecs)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet converts the 'target' column of the `tweet` DataFrame to integers and stores the result in the Series `Y_train`, then displays the first few values.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99583447,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "Y_train=tweet['target'].apply(lambda x:int(x))\n",
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet uses the pre-trained Doc2Vec model to infer vectors for each document in the `test` DataFrame, storing them in the list `Test_newvecs`.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.96810883,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "Test_newvecs=[Pretrained_Model.infer_vector(test['text'][i].split()) for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet converts the list of inferred vectors `Test_newvecs` into a DataFrame named `TEST`.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9989673,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "TEST=pd.DataFrame(data=Test_newvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet rounds the model predictions to the nearest integer, converts them to integer type, and reshapes the resulting array to have 3263 elements, matching the number of test samples.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9539579,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "predict=np.round(predict).astype(int).reshape(3263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet updates the 'target' column in the `sub` DataFrame with the rounded predictions from the neural network model.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.7176085,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "sub['target']=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Transform",
    "desc": "This code snippet concatenates the training and test datasets into a single DataFrame and prints the shape of the resulting DataFrame to understand its dimensions.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9992186,
    "start_cell": true,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "df=pd.concat([tweet,test], sort = False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove URLs from a given text using regular expressions.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.76660377,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_URL` function to the 'text' column of the DataFrame to remove any URLs from the text data.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.99688476,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove HTML tags from a given text using regular expressions.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.20691286,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_html` function to the 'text' column of the DataFrame to clean the text data by removing any HTML tags.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9979342,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove various emoji characters from a given text using regular expressions.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.62529504,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_emoji` function to the 'text' column of the DataFrame to clean the text data by removing any emoji characters.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9978229,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove punctuation characters from a given text by using the `str.translate` method and a translation table.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.39670715,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_punct` function to the 'text' column of the DataFrame to clean the text data by removing any punctuation characters.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9982992,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to correct the misspellings in a given text by identifying misspelled words and replacing them with their corrected versions using the `SpellChecker` class.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.96809155,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "#spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to create a corpus by tokenizing the text data, converting words to lowercase, removing stopwords, and keeping only alphabetic words from the 'text' column of the DataFrame.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.3279341,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet calls the `create_corpus` function to generate a processed corpus of text data from the DataFrame `df`.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.3502663,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "corpus=create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet initializes a tokenizer, fits it on the corpus to create sequences of word indices, and then pads these sequences to a maximum length of 50.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.5715023,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet creates an embedding matrix by mapping each word's index in the word index to its corresponding GloVe embedding vector, resulting in a dense representation of the vocabulary for use in neural network models.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.52054274,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "This code snippet separates the padded sequences back into training and test datasets based on the original size of the training data.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.781005,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "This code snippet splits the training data and corresponding target values into training and validation sets, with 15% of the data allocated for validation, and prints the shapes of these sets.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9958483,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code concatenates the training and testing DataFrames into a single DataFrame for combined processing.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9995054,
    "start_cell": true,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code applies multiple text preprocessing steps to the 'text' column of the training DataFrame, including converting text to lowercase, removing punctuation, URLs, HTML tags, and emojis.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9963546,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# lower case\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Punctional\n",
    "train['text'] = train['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Removing URL\n",
    "def remove_URL(text):    \n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "train['text']=train['text'].apply(lambda x : remove_URL(x))\n",
    "\n",
    "# Removing HTML Tags\n",
    "def remove_html(text):    \n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "train['text']= train['text'].apply(lambda x : remove_html(x))\n",
    "\n",
    "# Removing Emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "train['text']= train['text'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code removes the 1,000 least frequent words from the 'text' column of the training DataFrame.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.5893947,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "delete = pd.Series(' '.join(train['text']).split()).value_counts()[-1000:]\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code downloads the English stopwords from the nltk library and removes these stopwords from the 'text' column of the training DataFrame.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.94620574,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# StopWords\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "train[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code assigns the 'text' column of the training DataFrame to the variable `x` and the 'target' column to the variable `y` for later use in modeling.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9992976,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "x = train[\"text\"]\n",
    "y = train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code splits the preprocessed text data and target labels into training and testing sets with 33% of the data reserved for testing, using a fixed random state for reproducibility.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9977597,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code uses a LabelEncoder from sklearn to convert the target labels in the training and testing sets into numerical format.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9992428,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code utilizes the CountVectorizer from sklearn to convert the text data in the training and testing sets into a matrix of token counts.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.8321505,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(X_train)\n",
    "test_vectors = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "This code transforms the 'text' column of the testing DataFrame into a matrix of token counts using the previously fitted CountVectorizer.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9827654,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "test = count_vectorizer.transform(test[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code defines several functions to clean tweet text data by removing special characters, URLs, HTML tags, emojis, punctuation, and normalizing text, and then applies these transformations to a DataFrame column.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.99880517,
    "start_cell": true,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean(tweet): \n",
    "            \n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
    "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
    "            \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
    "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
    "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
    "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
    "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
    "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
    "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
    "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
    "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
    "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
    "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
    "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
    "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
    "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
    "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
    "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
    "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
    "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
    "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
    "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
    "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
    "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
    "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
    "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
    "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
    "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
    "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
    "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
    "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
    "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
    "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
    "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
    "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
    "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
    "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
    "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
    "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
    "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
    "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
    "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
    "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
    "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
    "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
    "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
    "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
    "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
    "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
    "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
    "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
    "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
    "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
    "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
    "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
    "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
    "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
    "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
    "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
    "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
    "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
    "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
    "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
    "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
    "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
    "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
    "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
    "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
    "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
    "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
    "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
    "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
    "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
    "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
    "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
    "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
    "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
    "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
    "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
    "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
    "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
    "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
    "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
    "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
    "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
    "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
    "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
    "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
    "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
    "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
    "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
    "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
    "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
    "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
    "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
    "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
    "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
    "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
    "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
    "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
    "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
    "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
    "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
    "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
    "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
    "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
    "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
    "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
    "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
    "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
    "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
    "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
    "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
    "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
    "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
    "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
    "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
    "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
    "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
    "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
    "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
    "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
    "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
    "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
    "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
    "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
    "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
    "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
    "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
    "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
    "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
    "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
    "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
    "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
    "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
    "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
    "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
    "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
    "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
    "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
    "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
    "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
    "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
    "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
    "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
    "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
    "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
    "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
    "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
    "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
    "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
    "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
    "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
    "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
    "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
    "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
    "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
    "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
    "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
    "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
    "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
    "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
    "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
    "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
    "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
    "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
    "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
    "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
    "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
    "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
    "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
    "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
    "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
    "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
    "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
    "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
    "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
    "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
    "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
    "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
    "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
    "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
    "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
    "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
    "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
    "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
    "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
    "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
    "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
    "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
    "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
    "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
    "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
    "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
    "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
    "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
    "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
    "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
    "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
    "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
    "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
    "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
    "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
    "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
    "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
    "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
    "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
    "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
    "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
    "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
    "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
    "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
    "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
    "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
    "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
    "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
    "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
    "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
    "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
    "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
    "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
    "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
    "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
    "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
    "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
    "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
    "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
    "    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n",
    "    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n",
    "    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n",
    "    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n",
    "    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n",
    "    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n",
    "    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n",
    "    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n",
    "    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n",
    "    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n",
    "    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n",
    "    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n",
    "    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n",
    "    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n",
    "    tweet = re.sub(r\"andword\", \"and word\", tweet)\n",
    "    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n",
    "    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n",
    "    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
    "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
    "    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n",
    "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
    "    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n",
    "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
    "    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n",
    "    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n",
    "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
    "    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n",
    "    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n",
    "    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n",
    "    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n",
    "    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
    "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
    "    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n",
    "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
    "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
    "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
    "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
    "    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n",
    "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
    "    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n",
    "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
    "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
    "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
    "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
    "    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n",
    "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
    "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
    "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
    "    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n",
    "    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n",
    "    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n",
    "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
    "    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n",
    "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
    "    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n",
    "    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n",
    "    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n",
    "    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n",
    "    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n",
    "    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n",
    "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
    "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
    "    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n",
    "    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
    "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
    "    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n",
    "    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n",
    "    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n",
    "    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n",
    "    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n",
    "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
    "    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n",
    "    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n",
    "    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n",
    "    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n",
    "    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n",
    "    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n",
    "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
    "    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n",
    "    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n",
    "    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n",
    "    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n",
    "    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n",
    "    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n",
    "    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n",
    "    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n",
    "    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n",
    "    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n",
    "    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n",
    "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
    "    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n",
    "    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n",
    "    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n",
    "    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n",
    "    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n",
    "    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n",
    "    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n",
    "    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n",
    "    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n",
    "    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n",
    "    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n",
    "    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n",
    "    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n",
    "    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n",
    "    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n",
    "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
    "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
    "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
    "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
    "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
    "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
    "    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n",
    "    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n",
    "    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n",
    "    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
    "    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n",
    "    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n",
    "    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n",
    "    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n",
    "    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n",
    "    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n",
    "    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n",
    "    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n",
    "    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n",
    "    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n",
    "    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n",
    "    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n",
    "    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n",
    "    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n",
    "    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n",
    "    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n",
    "    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n",
    "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
    "    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
    "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
    "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
    "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
    "    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n",
    "    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n",
    "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
    "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
    "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
    "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
    "    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n",
    "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
    "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
    "    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n",
    "    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n",
    "    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n",
    "    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n",
    "    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n",
    "    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n",
    "    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n",
    "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
    "    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n",
    "    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n",
    "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
    "    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n",
    "    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n",
    "    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n",
    "    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n",
    "    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n",
    "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
    "    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n",
    "    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n",
    "    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n",
    "    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n",
    "    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n",
    "    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n",
    "    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n",
    "    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n",
    "    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n",
    "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
    "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
    "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
    "    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n",
    "    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n",
    "    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n",
    "    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n",
    "    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n",
    "    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n",
    "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
    "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
    "    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n",
    "    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n",
    "    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n",
    "    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n",
    "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
    "    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n",
    "    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n",
    "    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n",
    "    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n",
    "    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n",
    "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
    "    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n",
    "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
    "    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n",
    "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
    "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
    "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
    "    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n",
    "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
    "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
    "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
    "    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n",
    "    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n",
    "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
    "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
    "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
    "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
    "    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n",
    "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
    "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
    "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
    "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
    "    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n",
    "    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n",
    "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
    "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
    "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
    "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
    "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
    "    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n",
    "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
    "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
    "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
    "    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n",
    "    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n",
    "    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n",
    "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
    "    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n",
    "    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n",
    "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
    "    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n",
    "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
    "    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n",
    "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
    "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
    "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
    "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
    "    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n",
    "    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n",
    "    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n",
    "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
    "    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n",
    "    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n",
    "    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n",
    "    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n",
    "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
    "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
    "    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n",
    "    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n",
    "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
    "    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n",
    "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
    "    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n",
    "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
    "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
    "    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n",
    "    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n",
    "    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n",
    "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
    "    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n",
    "    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n",
    "    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n",
    "    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n",
    "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
    "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
    "    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n",
    "    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n",
    "    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n",
    "    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n",
    "    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n",
    "    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n",
    "    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n",
    "    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n",
    "    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n",
    "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
    "    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n",
    "    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n",
    "    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n",
    "    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n",
    "    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n",
    "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
    "    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n",
    "    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n",
    "    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n",
    "    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n",
    "    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n",
    "    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n",
    "    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n",
    "    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n",
    "    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n",
    "    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n",
    "    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n",
    "    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n",
    "    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n",
    "    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n",
    "    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n",
    "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
    "    df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "    df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
    "    df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
    "    df['text']=df['text'].apply(lambda x : clean(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet creates a DataFrame for submission by combining the 'id' column from the test DataFrame with the 'class' column from the predictions DataFrame, and displays the first few rows.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9992551,
    "start_cell": true,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "submission_df = pd.concat([nlp_test_df['id'], predictions_df['class']], axis=1)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This code snippet renames the 'class' column in the submission DataFrame to 'target' and displays the first few rows to reflect the change.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9837615,
    "start_cell": false,
    "subclass": "rename_columns",
    "subclass_id": 61
   },
   "outputs": [],
   "source": [
    "submission_df = submission_df.rename(columns={'class':'target'})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "Initializes an instance of the `NLPlot` object with the training dataset, specifying the 'text' column as the target column for further natural language processing and visualization.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9980926,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "npt = nlplot.NLPlot(train, target_col='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "Generates and displays a list of stopwords based on the most frequently occurring words in the text column of the dataset that appear at least a specified minimum number of times.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.95914584,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "stopwords = npt.get_stopword(top_n=30, min_freq=0)\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "Constructs a co-occurrence graph based on the text data in the dataset, excluding the specified stopwords and only including edges (word pairs) that meet a minimum frequency threshold.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.6888755,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# ビルド（データ件数によっては処理に時間を要します）\n",
    "npt.build_graph(stopwords=stopwords, min_edge_frequency=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "Concatenates the training and testing datasets into a single DataFrame and outputs the shape of the combined DataFrame to verify its new dimensions.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.99849164,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train,test])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "Defines several functions for cleaning text data by removing URLs, HTML tags, emojis, punctuation, numbers, and standardizing acronyms, and then combines these functions in a comprehensive text-cleaning function.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.736169,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    num = re.compile(r'\\w*\\d\\w*')\n",
    "    return num.sub(r'',text)\n",
    "\n",
    "\n",
    "def acronyms(text):\n",
    "    # cf. https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert#4.-Embeddings-and-Text-Cleaning\n",
    "    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)\n",
    "    text = re.sub(r\"mÌ¼sica\", \"music\", text)\n",
    "    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n",
    "    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)\n",
    "    text = re.sub(r\"gawx\", \"Georgia Weather\", text)\n",
    "    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)\n",
    "    text = re.sub(r\"cawx\", \"California Weather\", text)\n",
    "    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n",
    "    text = re.sub(r\"azwx\", \"Arizona Weather\", text)\n",
    "    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n",
    "    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)\n",
    "    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text)\n",
    "    text = re.sub(r\"Suruc\", \"Sanliurfa\", text)\n",
    "\n",
    "    # Grouping same words without embeddings\n",
    "    text = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", text)\n",
    "    text = re.sub(r\"SOUDELOR\", \"Soudelor\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = acronyms(text)\n",
    "    text = text.lower()\n",
    "    text = remove_URL(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_punct(text)\n",
    "    text = remove_number(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "Applies the text-cleaning function to the 'text' column of the combined DataFrame using the `progress_apply` method to show progress, thereby preprocessing the text data.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.99770904,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].progress_apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "Defines a function to create a corpus by tokenizing each tweet in the text column, converting words to lowercase, and excluding non-alphabetic words and stopwords, then storing the cleaned words in a list.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.59735614,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# curpusの生成\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "Generates the corpus by applying the `create_corpus` function to the text column of the combined DataFrame.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.22817008,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "Initializes a tokenizer, fits it on the created corpus to build the word index, converts the text corpus into sequences of integers, and then pads these sequences to a fixed maximum length.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.7466092,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences = tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "Creates a word index from the fitted tokenizer and prints the number of unique words in the corpus.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.98261434,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "Creates an embedding matrix where each row corresponds to a word's vector representation, obtained from the pre-trained embeddings dictionary, and fills in the matrix for words present in the word index.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.60035264,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "Splits the padded tweet sequences into training and testing sets based on the original sizes of the training and testing datasets.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.70438695,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "train_tweet = tweet_pad[:train.shape[0]]\n",
    "test_tweet = tweet_pad[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Data Transform",
    "desc": "Splits the training tweet data and target labels into training and validation sets, with 10% of the data allocated for validation, and prints their shapes.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9922604,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train_tweet, train['target'].values,test_size=0.1)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "The code defines a function to clean and preprocess tweets by removing HTML, non-ASCII characters, punctuation, and stopwords, and then applies this function to the 'text' column of the training data to create a new column 'prep_text'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.27900088,
    "start_cell": true,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Text Cleaning\n",
    "import re, string # Regular Expressions, String\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "from nltk.stem.porter import PorterStemmer # for word stemming\n",
    "from nltk.stem import WordNetLemmatizer # for word lemmatization\n",
    "import unicodedata\n",
    "import html\n",
    "\n",
    "# set of stopwords to be removed from text\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# update stopwords to have punctuation too\n",
    "stop.update(list(string.punctuation))\n",
    "\n",
    "def clean_tweets(text):\n",
    "    \n",
    "    # Remove unwanted html characters\n",
    "    re1 = re.compile(r'  +')\n",
    "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "    '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    text = re1.sub(' ', html.unescape(x1))\n",
    "    \n",
    "    # remove non-ascii characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # strip html\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # remove between square brackets\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # remove twitter tags\n",
    "    text = text.replace(\"@\", \"\")\n",
    "    \n",
    "    # remove hashtags\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    \n",
    "    # remove all non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    \n",
    "    # remove stopwords from text\n",
    "    final_text = []\n",
    "    for word in text.split():\n",
    "        if word.strip().lower() not in stop:\n",
    "            final_text.append(word.strip().lower())\n",
    "    \n",
    "    text = \" \".join(final_text)\n",
    "    \n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n",
    "    \n",
    "    # replace all numbers with \"num\"\n",
    "    text = re.sub(\"\\d\", \"num\", text)\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "train_data['prep_text'] = train_data['text'].apply(clean_tweets)\n",
    "train_data['prep_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "The code applies the previously defined `clean_tweets` function to the 'text' column of the testing data to preprocess the text.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9986481,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "test_data['text'] = test_data['text'].apply(clean_tweets)\n",
    "test_data['text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "The code sets up a Keras `Tokenizer` for text tokenization with a vocabulary size of 1000 and fits it on the concatenated preprocessed text data from both training and testing datasets.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9994017,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer # Text tokenization\n",
    "\n",
    "# Setting up the tokenizer\n",
    "vocab_size = 1000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\n",
    "tokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "The code converts the preprocessed text data into one-hot encoded sequences using the tokenizer and prints the shapes of the resulting training and testing input matrices along with the target variable array.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.5156552,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# Representing texts as one hot encoded sequence\n",
    "\n",
    "X_train_ohe = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'binary')\n",
    "X_test_ohe = tokenizer.texts_to_matrix(test_data['text'], mode = 'binary')\n",
    "y_train = np.array(train_data['target']).astype(int)\n",
    "\n",
    "print(f\"X_train shape: {X_train_ohe.shape}\")\n",
    "print(f\"X_test shape: {X_test_ohe.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "The code splits the training data into training and validation sets using an 80-20 split and prints the shapes of the resulting matrices for input and target variables.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.961794,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ohe, X_val_ohe, y_train, y_val = train_test_split(X_train_ohe, y_train, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(f\"X_train shape: {X_train_ohe.shape}\")\n",
    "print(f\"X_val shape: {X_val_ohe.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "The code converts the preprocessed text data into count-based sequences using the tokenizer and prints the shapes of the resulting training and testing input matrices along with the target variable array.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.61748403,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "X_train_wc = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'count')\n",
    "X_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\n",
    "y_train = np.array(train_data['target']).astype(int)\n",
    "\n",
    "print(f\"X_train shape: {X_train_wc.shape}\")\n",
    "print(f\"X_test shape: {X_test_wc.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "The code splits the count-based training data into training and validation sets using an 80-20 split and prints the shapes of the resulting matrices for input and target variables.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.80229384,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(f\"X_train shape: {X_train_wc.shape}\")\n",
    "print(f\"X_val shape: {X_val_wc.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "The code converts the preprocessed text data into frequency-based sequences using the tokenizer and prints the shapes of the resulting training and testing input matrices along with the target variable array.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.74439836,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "X_train_freq = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'freq')\n",
    "X_test_freq = tokenizer.texts_to_matrix(test_data['text'], mode = 'freq')\n",
    "y_train = np.array(train_data['target']).astype(int)\n",
    "\n",
    "print(f\"X_train shape: {X_train_freq.shape}\")\n",
    "print(f\"X_test shape: {X_test_freq.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "The code splits the frequency-based training data into training and validation sets using an 80-20 split and prints the shapes of the resulting matrices for input and target variables.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.84670115,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train_freq, X_val_freq, y_train, y_val = train_test_split(X_train_freq, y_train, test_size = 0.2, random_state = 42)\n",
    "print(f\"X_train shape: {X_train_freq.shape}\")\n",
    "print(f\"X_val shape: {X_val_freq.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Data Transform",
    "desc": "The code initializes a TF-IDF vectorizer, fits it on the concatenated preprocessed text data from both training and testing datasets, and transforms the text data into TF-IDF weighted matrices for training and testing, while also printing their shapes along with the target variable array.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.57227826,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # Term Frequency - Inverse Document Frequency\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = vocab_size)\n",
    "vectorizer.fit(list(train_data['prep_text']) + list(test_data['text']))\n",
    "\n",
    "# Fitting on training and testing data\n",
    "X_train_tfidf = vectorizer.transform(list(train_data['prep_text'])).toarray() \n",
    "X_test_tfidf = vectorizer.transform(list(test_data['text'])).toarray()\n",
    "\n",
    "y_train = np.array(train_data['target']).astype(int)\n",
    "\n",
    "print(f\"X_train shape {X_train_tfidf.shape}\")\n",
    "print(f\"X_test shape {X_test_tfidf.shape}\")\n",
    "print(f\"y_train shape {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Transform",
    "desc": "The code splits the TF-IDF based training data into training and validation sets using an 80-20 split and prints the shapes of the resulting matrices for input and target variables.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.70971876,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size = 0.2, random_state = 42)\n",
    "print(f\"X_train shape: {X_train_tfidf.shape}\")\n",
    "print(f\"X_val shape: {X_val_tfidf.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Transform",
    "desc": "The code tokenizes the preprocessed text data into sequences with a specified vocabulary size, followed by truncation and padding to a maximum length, and prints the shapes of the resulting training and testing input matrices along with the target variable array.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.7396719,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Sequences creation, truncation and padding\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Setting up the tokenizer\n",
    "vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\n",
    "tokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))\n",
    "\n",
    "max_len = 15\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\n",
    "y_train = np.array(train_data['target']).astype(int)\n",
    "\n",
    "print(f\"X_train shape: {X_train_seq.shape}\")\n",
    "print(f\"X_test shape: {X_test_seq.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Data Transform",
    "desc": "The code splits the tokenized, truncated, and padded training sequences into training and validation sets using an 80-20 split and prints the shapes of the resulting matrices for input and target variables.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9086002,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\n",
    "print(f\"X_train shape: {X_train_seq.shape}\")\n",
    "print(f\"X_val shape: {X_val_seq.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Data Transform",
    "desc": "The code creates an embedding matrix by mapping words from the tokenizer's word index to their corresponding GloVe embedding vectors, initialized for the size of the word index.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.91961074,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Applying GloVE representations on our corpus\n",
    "\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if i < num_words:\n",
    "        emb_vec = embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i] = emb_vec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Data Transform",
    "desc": "The code snippet processes the training data by removing stopwords, punctuation, and duplicate tweets using spaCy, lemmatizes the words, and stores the cleaned text along with their labels.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9552063,
    "start_cell": true,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# remove stopwords,punct\n",
    "# remove duplicate tweet\n",
    "texts = []\n",
    "labels = []\n",
    "texts_md5 = set()\n",
    "for target, doc in zip(train_raw.target, nlp.pipe(train_raw.text)):\n",
    "    tokens = [token.lemma_ for token in doc if token.is_stop is False and token.is_punct is False and token.is_space is False]\n",
    "    temp_text = ' '.join(tokens)\n",
    "    # remove duplicate\n",
    "    md5 = hashlib.md5()\n",
    "    md5.update(temp_text.encode('utf-8'))\n",
    "    text_md5 = md5.hexdigest()\n",
    "    if text_md5 not in texts_md5:\n",
    "        texts.append(temp_text)\n",
    "        labels.append(target)\n",
    "        texts_md5.add(text_md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "The code processes the test data by removing stopwords, punctuation, and lemmatizing the words using spaCy, then stores the cleaned text.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9769126,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "tests = []\n",
    "for doc in nlp.pipe(test_raw.text):\n",
    "    tokens = [token.lemma_ for token in doc if token.is_stop is False and token.is_punct is False and token.is_space is False]\n",
    "    tests.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Data Transform",
    "desc": "The code creates a TF-IDF vectorizer with a maximum of 10,000 features, fits it to the cleaned training texts, and then transforms both the training and test texts into TF-IDF features.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9555307,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(max_features=10000).fit(texts)\n",
    "train = tf_idf.transform(texts)\n",
    "test = tf_idf.transform(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "The code splits the transformed training data and labels into training and validation sets, with 30% of the data allocated to the validation set.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99791616,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to create a corpus of words by combining all the words from tweets that belong to a specified target category (disaster or non-disaster).",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.51645607,
    "start_cell": true,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "def create_corpus(target):\n",
    "    corpus=[]\n",
    "    \n",
    "    for x in tweet[tweet['target']==target]['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "This code snippet generates a corpus of words from non-disaster tweets, counts the occurrences of stopwords in that corpus, and identifies the top ten most frequent stopwords.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9748024,
    "start_cell": false,
    "subclass": "sort_values",
    "subclass_id": 9
   },
   "outputs": [],
   "source": [
    "corpus=create_corpus(0)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "        \n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet counts the frequency of words in the non-disaster tweets corpus, excluding stopwords, and stores the top 40 most common words along with their counts.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.40664467,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "x=[]\n",
    "y=[]\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop) :\n",
    "        x.append(word)\n",
    "        y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract and count the top n most common bigrams (two-word combinations) from a given text corpus.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.7835433,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "This code snippet concatenates the training and test datasets into a single DataFrame and outputs its shape.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99860877,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "df=pd.concat([tweet,test])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove URLs from a given text string and demonstrates its functionality on an example tweet.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9513789,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "remove_URL(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet applies the URL removal function to the 'text' column of the combined DataFrame to clean the text data by removing any URLs.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99688476,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove HTML tags from a given text string and demonstrates its functionality on an example HTML string.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.5649151,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "print(remove_html(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "This code snippet applies the HTML removal function to the 'text' column of the combined DataFrame to clean the text data by removing any HTML tags.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9979342,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove emojis from a given text string using a regular expression and demonstrates its functionality on an example sentence.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.5422814,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake 😔😔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Data Transform",
    "desc": "This code snippet applies the emoji removal function to the 'text' column of the combined DataFrame to clean the text data by removing any emojis.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9971827,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove punctuation from a given text string using a translation table and demonstrates its functionality on an example sentence.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.70283914,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "example=\"I am a #king\"\n",
    "print(remove_punct(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Data Transform",
    "desc": "This code snippet applies the punctuation removal function to the 'text' column of the combined DataFrame to clean the text data by removing any punctuation.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9982992,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to correct misspelled words in a given text string using the `pyspellchecker` library and demonstrates its functionality on an example sentence.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.96504265,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "text = \"corect me plese\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to create a corpus of lowercased, alphabetic words from the 'text' column of the DataFrame, excluding stopwords, using tokenization.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.3624271,
    "start_cell": false,
    "subclass": "remove_duplicates",
    "subclass_id": 19
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Transform",
    "desc": "This code snippet generates a cleaned and tokenized corpus of words from the 'text' column of the combined DataFrame using the previously defined function.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.3502663,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "corpus=create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Transform",
    "desc": "This code snippet tokenizes the words in the corpus, converts the text sequences to numerical sequences, and then pads these sequences to a maximum length of 50 for uniform input size.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.5715023,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Data Transform",
    "desc": "This code snippet creates an embedding matrix that maps each word in the word index to its corresponding GloVe word vector, filling in the vectors for the words found in the embedding dictionary.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.517325,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Data Transform",
    "desc": "This code snippet splits the padded sequences into training and test sets based on the original sizes of the training and test datasets.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.781005,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Data Transform",
    "desc": "This code snippet splits the training data into training and validation sets, and prints their shapes to confirm the split.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9958483,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to create a corpus of words from tweets based on their target classification by concatenating all the words from tweets of a given class into a list.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.45771617,
    "start_cell": true,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "def create_corpus(target):\n",
    "    corpus = []\n",
    "    \n",
    "    for x in tweet[tweet['target']==target]['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code snippet counts the occurrences of each word in the corpus, excluding stopwords, and stores the 40 most common words and their counts in separate lists.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.41688988,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "counter = Counter(corpus)\n",
    "most = counter.most_common()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for word, count in most[:40]:\n",
    "    if (word not in stop):\n",
    "        x.append(word)\n",
    "        y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to extract the top `n` most frequent bigrams from a given text corpus using a CountVectorizer.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.5759071,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range = (2,2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet concatenates the training and testing DataFrames into a single DataFrame and outputs its shape.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9983077,
    "start_cell": false,
    "subclass": "concatenate",
    "subclass_id": 11
   },
   "outputs": [],
   "source": [
    "df = pd.concat([tweet, test])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove URLs from a given text and demonstrates its usage with an example string.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9544977,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "remove_URL(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_URL` function to the 'text' column of the combined DataFrame to remove URLs from all tweets.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9961047,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove HTML tags from a given text and demonstrates its usage with an example string.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.5656171,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "print(remove_html(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_html` function to the 'text' column of the combined DataFrame to remove HTML tags from all tweets.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9977336,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove emojis from a given text and demonstrates its usage with an example string.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.62529504,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake 😔😔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_emoji` function to the 'text' column of the combined DataFrame to remove emojis from all tweets.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9978229,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to remove punctuation from a given text and demonstrates its usage with an example string.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.7528076,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "example=\"I am a #king\"\n",
    "print(remove_punct(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_punct` function to the 'text' column of the combined DataFrame to remove punctuation from all tweets.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9982992,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to correct misspellings in a given text using the `pyspellchecker` package and demonstrates its usage with an example string.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.94864124,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "text = 'corect me please'\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Data Transform",
    "desc": "This code snippet defines a function to create a corpus by tokenizing and lowercasing words from the 'text' column of the given DataFrame, while excluding stopwords and non-alphabetic tokens.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.46042022,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    corpus = []\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words = [word.lower() for word in word_tokenize(tweet)\n",
    "                if ((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Data Transform",
    "desc": "This code snippet applies the previously defined `create_corpus` function to the combined DataFrame to generate a corpus of cleaned, tokenized text data.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.3502663,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "corpus=create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Transform",
    "desc": "This code snippet tokenizes the corpus, converts the text sequences into numerical sequences, and pads them to a maximum length of 50 using truncating or padding as necessary.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9373977,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences = tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, \n",
    "                         truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Data Transform",
    "desc": "This code snippet creates an embedding matrix where each row corresponds to a word's GloVe embedding vector, ensuring it matches the index from the tokenizer's word index.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.42767188,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec = embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Data Transform",
    "desc": "This code snippet splits the padded sequences into training and testing sets based on the original shape of the training DataFrame.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.59589547,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "train = tweet_pad[:tweet.shape[0]]\n",
    "test = tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Data Transform",
    "desc": "This code snippet splits the training data and corresponding target values into training and validation sets using a 85-15 split ratio and prints their shapes.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9869506,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train,\n",
    "                                                    tweet['target'].values,\n",
    "                                                    test_size=0.15)\n",
    "\n",
    "print('shape of train', X_train.shape)\n",
    "print('shape of validation', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Data Transform",
    "desc": "This code snippet extracts the 'text' column from both the training and test datasets and assigns them to `train_text` and `test_text` variables, respectively.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.51321733,
    "start_cell": true,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "#we will be considering the text columns only \n",
    "train_text=train_data.text\n",
    "test_text=test_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code snippet defines a function `clean_text` that converts text to lowercase, removes hash symbols (#), and retains only alphabetic characters and spaces, serving to preprocess and clean textual data.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9233298,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "#Write some basic function to clean our texts\n",
    "def clean_text(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('#','',text)\n",
    "    text=re.sub('[^a-zA-Z ]','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Data Transform",
    "desc": "This code snippet applies the `clean_text` function to both `train_text` and `test_text` to clean and preprocess the textual data in the training and test datasets.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.65748864,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "#apply that cleaning function to our data\n",
    "train_text=train_text.apply(clean_text)\n",
    "test_text=test_text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Data Transform",
    "desc": "This code snippet tokenizes, lemmatizes, removes stopwords from each text in `train_text`, and appends the processed sentences to a list called `train_sequence`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.97941816,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "#lemmatize the words in each text,remove some unnecessary keywords like this,the,an etc and append the final sentence in a list called train_sequence\n",
    "train_sequence=[]\n",
    "for i in range(len(train_text)):\n",
    "    words=nltk.word_tokenize(train_text.iloc[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sent=' '.join(words)\n",
    "    train_sequence.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "This code snippet tokenizes, lemmatizes, removes stopwords from each text in `test_text`, and appends the processed sentences to a list called `test_sequence`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9541493,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "#do the same procedure for test data\n",
    "test_sequence=[]\n",
    "for i in range(len(test_text)):\n",
    "    words=nltk.word_tokenize(test_text.iloc[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sent=' '.join(words)\n",
    "    test_sequence.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet creates a `TfidfVectorizer` object configured to produce up to 10,000 features, considering unigrams, bigrams, and trigrams, with a minimum document frequency of 2.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.77589417,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "#create tfidf object which produces 10000 features\n",
    "tfidf=TfidfVectorizer(min_df=2,ngram_range=(1,3),max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code snippet fits the `TfidfVectorizer` to the `train_sequence` data and transforms the text into TF-IDF feature vectors stored in `vectorized_train`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.5069272,
    "start_cell": false,
    "subclass": "normalization",
    "subclass_id": 18
   },
   "outputs": [],
   "source": [
    "# fit and transform train data\n",
    "vectorized_train=tfidf.fit_transform(train_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "This code snippet transforms the `test_sequence` data into TF-IDF feature vectors using the already fitted `TfidfVectorizer`, storing the result in `vectorized_test`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.7695273,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "#transform test data\n",
    "vectorized_test=tfidf.transform(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Data Transform",
    "desc": "This code snippet converts the sparse TF-IDF feature matrices `vectorized_train` and `vectorized_test` into dense arrays.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9027112,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "#convert vectorized sparse matrix into an array\n",
    "vectorized_train=vectorized_train.toarray()\n",
    "vectorized_test=vectorized_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "The code snippet selects and retains only the 'id', 'text', and 'target' columns from the training DataFrame.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9917567,
    "start_cell": true,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "train=train[['id','text', 'target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "The code defines a function to expand contractions in a tweet by splitting words that contain \"n't\" into two parts - the root word and \"not\".",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.50917083,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def expand_tweet(tweet):\n",
    "    expanded_tweet = []\n",
    "    for word in tweet:\n",
    "        if re.search(\"n't\", word):\n",
    "            expanded_tweet.append(word.split(\"n't\")[0])\n",
    "            expanded_tweet.append(\"not\")\n",
    "        else:\n",
    "            expanded_tweet.append(word)\n",
    "    return expanded_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Data Transform",
    "desc": "The code defines a function to clean tweet data by removing user handles, numbers, special characters, URLs, and single characters, tokenizing the text, removing stopwords, expanding contractions, lemmatizing, stemming, and finally, combining the words back into a cleaned tweet string.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9932033,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_tweet(data, wordNetLemmatizer, porterStemmer):\n",
    "    data['text'] = data['text']\n",
    "    print(colored(\"Removing user handles starting with @\", \"yellow\"))\n",
    "    data['text'] = data['text'].str.replace(\"@[\\w]*\",\"\")\n",
    "    print(colored(\"Removing numbers and special characters\", \"yellow\"))\n",
    "    data['text'] = data['text'].str.replace(\"[^a-zA-Z' ]\",\"\")\n",
    "    print(colored(\"Removing urls\", \"yellow\"))\n",
    "    data['text'] = data['text'].replace(re.compile(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\"), \"\")\n",
    "    print(colored(\"Removing single characters\", \"yellow\"))\n",
    "    data['text'] = data['text'].replace(re.compile(r\"(^| ).( |$)\"), \" \")\n",
    "    print(colored(\"Tokenizing\", \"yellow\"))\n",
    "    data['text'] = data['text'].str.split()\n",
    "    print(colored(\"Removing stopwords\", \"yellow\"))\n",
    "    data['text'] = data['text'].apply(lambda text: [word for word in text if word not in STOPWORDS])\n",
    "    print(colored(\"Expanding not words\", \"yellow\"))\n",
    "    data['text'] = data['text'].apply(lambda text: expand_tweet(text))\n",
    "    print(colored(\"Lemmatizing the words\", \"yellow\"))\n",
    "    data['text'] = data['text'].apply(lambda text: [wordNetLemmatizer.lemmatize(word) for word in text])\n",
    "    print(colored(\"Stemming the words\", \"yellow\"))\n",
    "    data['text'] = data['text'].apply(lambda text: [porterStemmer.stem(word) for word in text])\n",
    "    print(colored(\"Combining words back to tweets\", \"yellow\"))\n",
    "    data['text'] = data['text'].apply(lambda text: ' '.join(text))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "The code applies the previously defined `clean_tweet` function to the training DataFrame to preprocess and clean the text data.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.85564166,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "train = clean_tweet(train, wordNetLemmatizer, porterStemmer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Data Transform",
    "desc": "The code initializes a Keras `Tokenizer` with a specified vocabulary size, fits it on the 'text' column of the training DataFrame to build the word index, and includes a commented out line to tokenize the text data.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9580604,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train['text'].values)\n",
    "#train['text'] = train['text'].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Data Transform",
    "desc": "The code snippet, currently commented out, is intended to split the 'text' and 'target' data into training and validation sets using an 75-25 split ratio with a fixed random state for reproducibility.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.59494233,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(train['text'], train['target'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "The code assigns the 'text' column of the training DataFrame to `X_train` and the 'target' column to `y_train`, preparing the data for further processing or model training.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9993168,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "X_train=train['text']\n",
    "y_train=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "The code converts the text data in `X_train` to sequences of integers using the tokenizer, and then pads these sequences to a fixed length defined by `MAX_LEN`, preparing the data for model input.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.8254797,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "x_train_seq = tokenizer.texts_to_sequences(X_train.values)\n",
    "#x_val_seq = tokenizer.texts_to_sequences(X_val.values)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=MAX_LEN, padding=\"post\", value=0)\n",
    "#x_val = sequence.pad_sequences(x_val_seq, maxlen=MAX_LEN, padding=\"post\", value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Data Transform",
    "desc": "The code applies the `clean_tweet` function to preprocess the test data, fits the tokenizer on the test text, converts the cleaned text to sequences of integers, and pads these sequences to a fixed length, preparing the test data for model input.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9630493,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "test=clean_tweet(test, wordNetLemmatizer, porterStemmer)\n",
    "tokenizer.fit_on_texts(test['text'].values)\n",
    "X_test=test['text']\n",
    "x_test_seq = tokenizer.texts_to_sequences(X_test.values)\n",
    "x_test = sequence.pad_sequences(x_test_seq, maxlen=MAX_LEN, padding=\"post\", value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Data Transform",
    "desc": "The code creates a copy of the sample submission DataFrame to use as the final submission DataFrame, which will be modified with prediction results.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.6151267,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "submit = submission_sample.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Data Transform",
    "desc": "The code assigns binary predictions to the 'target' column of the submission DataFrame, labeling entries as 1 if the prediction is greater than 0.5 and 0 otherwise.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.6646889,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "submit.target = np.where(prediction > 0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Data Transform",
    "desc": "This code defines functions to clean and normalize text data by removing punctuation, links, and unnecessary words, and then applies these functions to a column in the DataFrame, saving the results in a new column.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.93467706,
    "start_cell": true,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "### function to clean text (remove punctuation, links, lowercase all letters, etc)\n",
    "def clean_text(text):\n",
    "    temp = text.lower()\n",
    "    temp = re.sub('\\n', \" \", temp)\n",
    "    temp = re.sub('\\'', \"\", temp)\n",
    "    temp = re.sub('-', \" \", temp)\n",
    "    temp = re.sub(r\"(http|https|pic.)\\S+\",\" \",temp)\n",
    "    temp = re.sub(r'[^\\w\\s]',' ',temp)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "### list of stop words that need to be removed\n",
    "stop_words = ['as', 'in', 'of', 'is', 'are', 'were', 'was', 'it', 'for', 'to', 'from', 'into', 'onto', \n",
    "              'this', 'that', 'being', 'the','those', 'these', 'such', 'a', 'an']\n",
    "### function to remove unnecessary words\n",
    "def remove_stopwords(text):\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    temp = [word for word in tokenized_words if word not in stop_words]\n",
    "    temp = ' '.join(temp)\n",
    "    return temp\n",
    "\n",
    "### We save the cleaned and normalized texts in the new column, called 'clean'\n",
    "train['clean'] = train['text'].apply(clean_text)\n",
    "train['clean'] = train['clean'].apply(remove_stopwords)\n",
    "train['clean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Data Transform",
    "desc": "This code defines a function to combine text, location, and keyword attributes into a single string and applies this function to create a new column in the DataFrame, filling any missing values with empty strings before the operation.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.8933411,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def combine_attributes(text, location, keyword):\n",
    "    var_list = [text, location, keyword]\n",
    "    combined = ' '.join(x for x in var_list if x)\n",
    "    return combined\n",
    "\n",
    "train.fillna('', inplace=True)\n",
    "train['combine'] = train.apply(lambda x: combine_attributes(x['clean'], x['location'], x['keyword']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Data Transform",
    "desc": "This code initializes a TF-IDF vectorizer, fits it to the training text data, and transforms both the training and testing text data into TF-IDF feature matrices.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.58706194,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Data Transform",
    "desc": "This code applies text preprocessing functions to clean and remove stopwords from the 'text' column in both the training and test data, saving the results in a new 'clean' column for each DataFrame.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.97276026,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "### apply preprocessing on train data\n",
    "train['clean'] = train['text'].apply(clean_text)\n",
    "train['clean'] = train['clean'].apply(remove_stopwords)\n",
    "\n",
    "### apply preprocessing on test data\n",
    "test['clean'] = test['text'].apply(clean_text)\n",
    "test['clean'] = test['clean'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Data Transform",
    "desc": "This code fills any missing values with empty strings and then applies a function to combine the 'clean', 'location', and 'keyword' columns into a single string, storing the result in a new 'combine' column for both the training and test data.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.95965916,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "train.fillna('', inplace=True)\n",
    "train['combine'] = train.apply(lambda x: combine_attributes(x['clean'], x['location'], x['keyword']), axis=1)\n",
    "\n",
    "test.fillna('', inplace=True)\n",
    "test['combine'] = test.apply(lambda x: combine_attributes(x['clean'], x['location'], x['keyword']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Data Transform",
    "desc": "This code initializes a TF-IDF vectorizer, fits it to the combined training text data, and transforms both the training and test combined text data into TF-IDF feature matrices.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.94159025,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Data Transform",
    "desc": "This code standardizes various ambiguous location names in the training data and then creates a bar plot to show the top 5 most frequent updated locations.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.96501577,
    "start_cell": true,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Replacing the ambigious locations name with Standard names\n",
    "train['location'].replace({'United States':'USA',\n",
    "                           'New York':'USA',\n",
    "                            \"London\":'UK',\n",
    "                            \"Los Angeles, CA\":'USA',\n",
    "                            \"Washington, D.C.\":'USA',\n",
    "                            \"California\":'USA',\n",
    "                             \"Chicago, IL\":'USA',\n",
    "                             \"Chicago\":'USA',\n",
    "                            \"New York, NY\":'USA',\n",
    "                            \"California, USA\":'USA',\n",
    "                            \"FLorida\":'USA',\n",
    "                            \"Nigeria\":'Africa',\n",
    "                            \"Kenya\":'Africa',\n",
    "                            \"Everywhere\":'Worldwide',\n",
    "                            \"San Francisco\":'USA',\n",
    "                            \"Florida\":'USA',\n",
    "                            \"United Kingdom\":'UK',\n",
    "                            \"Los Angeles\":'USA',\n",
    "                            \"Toronto\":'Canada',\n",
    "                            \"San Francisco, CA\":'USA',\n",
    "                            \"NYC\":'USA',\n",
    "                           \"Seattle\":'USA',\n",
    "                            \"Earth\":'Worldwide',\n",
    "                            \"Ireland\":'UK',\n",
    "                            \"London, England\":'UK',\n",
    "                            \"New York City\":'USA',\n",
    "                            \"Texas\":'USA',\n",
    "                            \"London, UK\":'UK',\n",
    "                            \"Atlanta, GA\":'USA',\n",
    "                            \"Mumbai\":\"India\"},inplace=True)\n",
    "\n",
    "sns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Data Transform",
    "desc": "This code fills any missing values in the 'keyword' and 'location' columns of both the training and test data with the strings 'no_keyword' and 'no_location', respectively.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.608941,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "for col in ['keyword', 'location']:\n",
    "    train[col] = train[col].fillna(f'no_{col}')\n",
    "for col in ['keyword', 'location']:\n",
    "    test[col] = test[col].fillna(f'no_{col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Data Transform",
    "desc": "This code defines a function to clean text by removing various unwanted elements (e.g., URLs, punctuation, digits) and applies this function to the 'text' column in both the training and test data, modifying the text accordingly.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.8696903,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Applying a first round of text cleaning techniques\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'<.*?>',' ' ,text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to both test and training datasets\n",
    "train['text'] = train['text'].apply(lambda x: clean_text(x))\n",
    "test['text'] = test['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This code tokenizes the 'text' column in both the training and test data by splitting the text into individual words using a regular expression tokenizer and modifies the text accordingly.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.86087424,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenizing the training and the test set\n",
    "train['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "test['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "train['text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "This code defines a function to remove English stopwords from the tokenized text and then applies this function to the 'text' column in both the training and test data.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9867591,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removing stopwords belonging to english language\n",
    "    \n",
    "    \"\"\"\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "\n",
    "    train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\n",
    "    test['text'] = test['text'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Data Transform",
    "desc": "This code defines a function to remove emojis from text by using a regular expression pattern that matches various emoji ranges and replaces them with an empty string.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.62529504,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Data Transform",
    "desc": "This code defines a function to stem words using the Porter Stemmer and applies this function to the 'text' column in both the training and test data.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.7123636,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def stemming(words):\n",
    "     ps=PorterStemmer()\n",
    "     return [ps.stem(word) for word in words]\n",
    "train['text']=train['text'].apply(lambda x: stemming(x))\n",
    "test['text']=test['text'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Data Transform",
    "desc": "This code defines a function to lemmatize words by using the WordNet Lemmatizer and applies this function to the 'text' column in both the training and test data.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9204686,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "def lemmatizing(words):\n",
    "            lemmatizer =WordNetLemmatizer()\n",
    "            return [lemmatizer.lemmatize(word) for word in words]\n",
    "train['text']=train['text'].apply(lambda x: lemmatizing(x))\n",
    "test['text']=test['text'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Data Transform",
    "desc": "This code defines a function to join tokenized words back into a single string and applies this function to the 'text' column in both the training and test data, displaying the first 10 rows of the transformed training data.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9932473,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "def final_text(words):\n",
    "     return ' '.join(words)\n",
    "train['text']=train['text'].apply(lambda x:final_text(x))\n",
    "test['text']=test['text'].apply(lambda x:final_text(x))\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Transform",
    "desc": "This code initializes a CountVectorizer, fits it to the 'text' column of the training data to transform it into a document-term matrix, and then transforms the 'text' column of the test data using the same vectorizer.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9961116,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train['text'])\n",
    "test_vectors = count_vectorizer.transform(test[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Data Transform",
    "desc": "This code initializes a TfidfVectorizer with specific parameters and fits it to the 'text' column of the training data to transform it into a TF-IDF matrix, and then transforms the 'text' column of the test data using the same vectorizer.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99765307,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_vectors = tfidf.fit_transform(train['text'])\n",
    "test_vectors = tfidf.transform(test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Data Transform",
    "desc": "This code snippet creates two new features in the training dataset: the length of each tweet in characters and the number of words in each tweet.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99920976,
    "start_cell": true,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Create a new feature with text lenght, or number of characters\n",
    "train_df['length'] = train_df['text'].str.len()\n",
    "# Create a new feature with number of words\n",
    "train_df['num_words'] = train_df['text'].str.split().map(lambda x: len(x))\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Data Transform",
    "desc": "This code snippet defines a function `counter_word` which calculates the frequency of each unique word in the given text data using the `Counter` class.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9223718,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "# Reference https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud#kln-160\n",
    "# Function to compute many unique words have this text\n",
    "def counter_word (text):\n",
    "    count = Counter()\n",
    "    for i in text.values:\n",
    "        for word in i.split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Data Transform",
    "desc": "This code snippet defines a function `remove_emoji` that removes emojis and other specific Unicode characters from a given string using a compiled regular expression pattern.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.21526304,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Reference https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_emoji` function to the text data in both the training and testing datasets to remove any emojis present.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99650013,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Apply removing emoji function to dataset\n",
    "train_df['text'] = train_df['text'].apply(lambda x: remove_emoji(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Data Transform",
    "desc": "This code snippet defines a function `remove_url` that removes URLs from a given text using a compiled regular expression pattern.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.29311776,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Reference https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud#5.-Cleaning-the-text\n",
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "def remove_url(text):\n",
    "    no_url = pattern.sub(r'',text)\n",
    "    return no_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_url` function to the text data in both the training and testing datasets to remove any URLs present.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99200153,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Apply removing URL function to dataset\n",
    "train_df['text'] = train_df['text'].apply(lambda x: remove_url(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Data Transform",
    "desc": "This code snippet defines a function `remove_html` that removes HTML tags from a given text using a compiled regular expression pattern.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.45646903,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Reference https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove#Data-Cleaning\n",
    "pattern = re.compile('<.*?>')\n",
    "def remove_html(text):\n",
    "    no_html = pattern.sub(r'',text)\n",
    "    return no_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Data Transform",
    "desc": "This code snippet applies the `remove_html` function to the text data in both the training and testing datasets to remove any HTML tags present.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9854784,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Apply removing HTML tags function to dataset\n",
    "train_df['text'] = train_df['text'].apply(lambda x: remove_html(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Data Transform",
    "desc": "This code snippet defines a dictionary `abbreviations` that maps common abbreviations and shorthand notations to their expanded forms for use in text preprocessing.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99783856,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Dictionary of abbreviations\n",
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Data Transform",
    "desc": "This code snippet defines functions `convert_abbrev` and `convert_abbrev_in_text` to replace abbreviations in the text with their expanded forms using the provided abbreviations dictionary.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.5417522,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
    "\n",
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Data Transform",
    "desc": "This code snippet applies the `convert_abbrev_in_text` function to the text data in both the training and testing datasets to replace any abbreviations with their expanded forms.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.647331,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Appy abbreviation to text\n",
    "train_df['text'] = train_df['text'].apply(lambda x: convert_abbrev_in_text(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: convert_abbrev_in_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Data Transform",
    "desc": "This code snippet defines a function `clean_text` that preprocesses text by removing non-alphabetic characters, converting to lowercase, and removing stopwords.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.98651797,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# Reference https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud#kln-160\n",
    "# This function remove stopwords, turn text to lower and add a delimiter\n",
    "def clean_text(text):\n",
    " \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)  \n",
    "    text = text.lower()  \n",
    "    # split to array(default delimiter is \" \") \n",
    "    text = text.split()      \n",
    "    text = [w for w in text if not w in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)    \n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Data Transform",
    "desc": "This code snippet applies the `clean_text` function to the text data in both the training and testing datasets to preprocess the text by removing non-alphabetic characters, converting to lowercase, and removing stopwords.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.73203254,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Apply clean text\n",
    "train_df['text'] = train_df['text'].apply(lambda x : clean_text(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Data Transform",
    "desc": "This code snippet creates training and testing subsets from the training dataset, dividing the text and target columns based on an 80% training data split.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.5059886,
    "start_cell": false,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "# Create training subsets\n",
    "training_sentences = train_df.text[0:training_size]\n",
    "training_labels = train_df.target[0:training_size]\n",
    "# Create testing subsets\n",
    "testing_sentences  = train_df.text[training_size:]\n",
    "testing_labels = train_df.target[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Data Transform",
    "desc": "This code snippet initializes a tokenizer with the specified vocabulary size and out-of-vocabulary token, fits it on the training sentences, and creates a dictionary mapping words to their respective indices.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.7487316,
    "start_cell": false,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "# Fitting the training dataset\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "# Create a words dictionary\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Data Transform",
    "desc": "This code snippet converts the training and testing sentences into sequences of tokens using the fitted tokenizer.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.8546356,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# Create sequences of tokens that represent each sentence \n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Data Transform",
    "desc": "This code snippet creates padded sequences for both training and testing sentences to ensure they have a uniform length, using the specified maximum length, padding type, and truncation type.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9054655,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Create padding sequences for training sentences \n",
    "training_padded = pad_sequences(training_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "# Create padding sequences for testing sentences \n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Data Transform",
    "desc": "This code snippet converts the training and testing padded sequences and their corresponding labels into NumPy arrays to ensure compatibility with TensorFlow 2.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.7567072,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# Convert the sets to array to get it to work with TensorFlow 2\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 47,
    "class": "Data Transform",
    "desc": "This code snippet converts the submission dataset's text into sequences of tokens and then pads these sequences to ensure they have a uniform length.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.89899194,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# Sequencing\n",
    "submission_sequences = tokenizer.texts_to_sequences(test_df.text)\n",
    "# Padding\n",
    "submission_padded = pad_sequences(submission_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Debug",
    "desc": "This code checks for any missing values in the variables used for training the logistic regression model that predicts the 'Deck' of a passenger.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9985806941986084,
    "start_cell": true,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "X_deck_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Debug",
    "desc": "This code provides a concise summary of the transformed training dataset, including the number of non-null entries and data types for each feature.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9993711113929749,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Debug",
    "desc": "The code sets a random seed, shuffles two arrays (`lis1` and `lis2`), and prints them to verify that the shuffling was done correctly and consistently for debugging purposes.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.4769763,
    "start_cell": true,
    "subclass": "define_search_space",
    "subclass_id": 5
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "lis1=np.array([[1,1],[2,2],[3,3],[4,4],[5,5],[6,6],[7,7],[8,8],[9,9]])\n",
    "lis2=np.array([1,2,3,4,5,6,7,8,9])\n",
    "np.random.shuffle(lis1)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(lis2)\n",
    "print(lis1)\n",
    "print(lis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Debug",
    "desc": "This code retrieves and prints a batch of data from the evaluation DataLoader to verify the functionality of the DataLoader and to check the shapes of the text and label tensors within the batch.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9715512,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "# DataLoaderの動作確認\n",
    "\n",
    "batch = next(iter(dl_eval))\n",
    "print(batch)\n",
    "print(batch.Text[0].shape)\n",
    "print(batch.Label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Debug",
    "desc": "This code snippet splits the text of specific rows in the `tweet` DataFrame into words and then rejoins them into strings, likely for the purpose of inspecting or debugging preprocessing steps.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99561995,
    "start_cell": true,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "words1=tweet['text'][5].split()\n",
    "words2=tweet['text'][3].split()\n",
    "' '.join(words1)\n",
    "' '.join(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Debug",
    "desc": "This code snippet is a commented-out line indicating an attempt to apply the `correct_spellings` function to the 'text' column of the DataFrame, with a note that it is not working and needs checking.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.99219877,
    "start_cell": true,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "#Not working - Need to check\n",
    "#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Debug",
    "desc": "This code snippet contains commented-out lines likely used for debugging purposes to view the first ten entries of the 'class' column in the predictions DataFrame and the 'id' column in the test DataFrame.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.99561393,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# predictions_df['class'].iloc[:10]\n",
    "# nlp_test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Debug",
    "desc": "This code snippet executes a shell command to list the details of the \"submission.csv\" file, likely to verify its creation and properties.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.4902688,
    "start_cell": false,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "! ls -l submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Debug",
    "desc": "This commented code snippet signifies an attempt to apply a spelling correction function to each row in the 'text' column of the DataFrame, but it notes that the operation takes too long to complete.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9949934,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(lambda x: correct_spellings(x))\n",
    "# this code takes too long time to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Debug",
    "desc": "This code snippet tests the `remove_emoji` function by removing emojis from a sample string containing emoji characters.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9979081,
    "start_cell": true,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# removing emoji test\n",
    "remove_emoji(\"Oh no, a hurricane is happening here 🌪️🚫😫\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Debug",
    "desc": "This code snippet tests the `remove_url` function by removing a URL from a sample string containing a URL.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9817778,
    "start_cell": false,
    "subclass": "drop_column",
    "subclass_id": 10
   },
   "outputs": [],
   "source": [
    "# Removing URL example\n",
    "example = \"Hey, look at this Data Science platform: https://www.kaggle.com\"\n",
    "remove_url(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Debug",
    "desc": "This code snippet tests the `remove_html` function by removing HTML tags from a sample string containing HTML content.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.94336337,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Removing HTML tags example\n",
    "example = \"\"\"<div>\n",
    "<h1>NLP TensorFlow</h1>\n",
    "<p>Predicting sarcastic sentences by Wesley Galvão </p>\n",
    "<a href=\"https://www.kaggle.com/galvaowesley/nlp-tensorflow-predicting-sarcastic-sentences\">Check it out</a>\n",
    "</div>\"\"\"\n",
    "remove_html(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries and modules for data manipulation, visualization, and machine learning. ",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9993529,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries for data handling and visualization, sets up inline plotting, and configures default plot sizes.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9962999,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%pylab inline\n",
    "\n",
    "# Set the global default size of matplotlib figures\n",
    "plt.rc('figure', figsize=(10, 5))\n",
    "# Size of matplotlib figures that contain subplots\n",
    "fizsize_with_subplots = (10, 10)\n",
    "# Size of matplotlib histogram bins\n",
    "bin_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Environment",
    "desc": "This code snippet imports the `RandomForestClassifier` from `sklearn.ensemble` and initializes it with 100 estimators.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99534625,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code imports various libraries and modules necessary for data manipulation, visualization, machine learning algorithms, and model validation.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9985221028327942,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "import numpy as np\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code snippet imports the pandas library for data manipulation and the numpy library for numerical computations.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9993017,
    "start_cell": true,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "# We can use the pandas library in python to read in the csv file.\n",
    "import pandas as pd\n",
    "#for numerical computaions we can use numpy library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports several libraries and modules necessary for data processing, machine learning, and visualization, and provides a way to list files in the input directory. ",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.97734624,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "# Numpy and Pandas\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Machine Learning\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import metrics\n",
    "\n",
    "# graphical modules\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries and packages for data manipulation, visualization, and machine learning tasks.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9993173,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code imports necessary libraries and configures settings for handling data, modeling, and visualization in a Jupyter notebook environment.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code snippet imports various libraries for data handling, machine learning model implementation, and visualization, and sets up the environment to ignore warnings and configure visualization styles.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9993361830711364,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code imports necessary libraries for handling data, modelling, and visualization, and configures the visualization settings. ",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9993361830711364,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code snippet imports the necessary libraries and configures settings for data handling, modeling, and visualization. ",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries for data manipulation, modeling, and visualization and sets up the environment configurations for visualizations. ",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries for data handling, modeling, preprocessing, and visualization, and configures the visualization style settings.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9993361830711364,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports the necessary libraries for data handling, modeling, and visualization, and configures the visualization settings. ",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9993361830711364,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code imports necessary libraries for handling data, modeling algorithms, modeling helpers, and visualization, and configures visual settings for plots. ",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet sets up the environment by importing essential libraries for data handling, modeling, and visualization, and suppressing warnings.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9993361830711364,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries for data handling, modeling, preprocessing, and visualization, and configures visualization styles. ",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The snippet suppresses warnings, imports essential libraries for handling data, modeling, and visualization, and configures visualization settings to use the 'ggplot' style with specific figure sizes.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries and loads the training and testing datasets, displaying the first 3 rows of the training set to provide an overview. ",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.93880355,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "# Imports needed for the script\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from IPython.display import Image as PImage\n",
    "from subprocess import check_call\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Loading the data\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "# Store our test passenger IDs for easy access\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "# Showing overview of the train dataset\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries and modules essential for data manipulation (pandas, numpy), visualization (matplotlib, seaborn), and machine learning (scikit-learn classifiers: LogisticRegression, SVC, LinearSVC, RandomForestClassifier, KNeighborsClassifier, GaussianNB).",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9993529,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries for data manipulation, visualization, and machine learning, and sets visualization configurations. ",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9993361830711364,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code imports necessary libraries for data manipulation, machine learning algorithms, preprocessing, and visualization and sets up visualizations to be displayed inline with a specific style. ",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code snippet imports necessary libraries for data handling, modeling, and visualization, sets up configurations to ignore warnings, and configures the style for visualizations to be inline and use specific aesthetics.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9993361830711364,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code imports necessary libraries for handling data, modeling algorithms, preprocessing, cross-validation, feature selection, and visualization, and configures the visualization settings. ",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports the necessary libraries for data manipulation, visualization, and ensures that plots are displayed inline in a Jupyter notebook.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9992682337760924,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries for handling warnings, data manipulation, modeling algorithms, modeling helpers, and visualization, and configures some visualization settings.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9993362,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8 , 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries and modules required for data manipulation, visualization, natural language processing, and machine learning tasks in the notebook.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.5645129,
    "start_cell": true,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from wordcloud import WordCloud\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code sets up the Python environment by importing necessary libraries (NumPy and pandas) and lists all files in the input directory to verify available data files.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99921954,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Environment",
    "desc": "The code installs the `tweet-preprocessor` package and then imports it for use in preprocessing tweets.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9878071,
    "start_cell": false,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "!pip install tweet-preprocessor\n",
    "import preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Environment",
    "desc": "The code imports the `WordNetLemmatizer` class from the NLTK library, which is typically used for lemmatizing words to their base forms.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99927074,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Environment",
    "desc": "The code installs the `spaCy` library and downloads the 'en_core_web_lg' language model for natural language processing tasks.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99370664,
    "start_cell": false,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "!pip3 install spacy\n",
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Environment",
    "desc": "The code imports the `spaCy` library and loads the 'en_core_web_lg' model for use in natural language processing tasks.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99426717,
    "start_cell": false,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Environment",
    "desc": "The code imports necessary modules and functions from the TensorFlow and Keras libraries, which are commonly used for building and training deep learning models.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99935824,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dropout, GRU, BatchNormalization, TimeDistributed, Reshape, Dense, Conv1D, Concatenate\n",
    "from keras import Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries and modules required for data manipulation, text processing, visualization, and building a machine learning model.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.99650407,
    "start_cell": true,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet sets up the environment by importing necessary libraries (NumPy and Pandas) and listing all files in the input directory for data access. ",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99921954,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Environment",
    "desc": "This code snippet imports the spaCy library and loads the 'en_core_web_lg' language model for natural language processing tasks.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99470055,
    "start_cell": false,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries and packages for data processing, visualization, and text analysis, and lists all files under the input directory.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.999231,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import pandas_profiling\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This code snippet installs the `keras-bert` and `keras-rectified-adam` packages.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.99072194,
    "start_cell": false,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "# Install keras-bert packages\n",
    "!pip install keras-bert\n",
    "!pip install keras-rectified-adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Environment",
    "desc": "This code snippet imports various libraries and modules related to TensorFlow, Keras, progress tracking, and text encoding for later use in the notebook.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9993438,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from chardet import detect\n",
    "import keras\n",
    "from keras_radam import RAdam\n",
    "from keras import backend as K\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "import codecs\n",
    "from keras_bert import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Environment",
    "desc": "This code snippet downloads and unzips a pre-trained BERT model from the provided URL.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.59883535,
    "start_cell": false,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "# Download bert model\n",
    "!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip -o uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Environment",
    "desc": "This code snippet sets the sequence length and learning rate parameters for BERT, and defines the paths for the BERT pretrained model configuration, checkpoint, and vocabulary files.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.99740463,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Set parameters from bert\n",
    "SEQ_LEN = 128\n",
    "LR = 1e-4\n",
    "\n",
    "# Set path of bert model\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Environment",
    "desc": "This code snippet loads the pre-trained BERT model with the specified configuration and checkpoint for training purposes.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.88544095,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Load bert model\n",
    "bert_model = load_trained_model_from_checkpoint(\n",
    "      config_path,\n",
    "      checkpoint_path,\n",
    "      training=True,\n",
    "      trainable=True,\n",
    "      seq_len=SEQ_LEN,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Environment",
    "desc": "This code snippet initializes a Keras session and explicitly initializes any uninitialized TensorFlow variables.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.26596707,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "# Initialize keras session\n",
    "sess = K.get_session()\n",
    "uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "init_op = tf.variables_initializer(\n",
    "    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n",
    ")\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code installs specific versions of the 'transformers' and 'pyspellchecker' libraries necessary for the machine learning task.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9702134,
    "start_cell": true,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "## old version\n",
    "# ライブラリのインストール\n",
    "!pip install transformers==3.5.1\n",
    "\n",
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This code imports necessary libraries and modules, such as pandas, transformers, and PyTorch, which are essential for data manipulation, model building, and training in the machine learning task.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9993318,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import torchtext  # torchtextを使用\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertConfig\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Environment",
    "desc": "This code loads a pre-trained BERT tokenizer using the 'bert-base-cased' model, which will be used for tokenizing input text data.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9913293,
    "start_cell": false,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "# Tokenizerの読み込み\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Environment",
    "desc": "This code initializes a pre-trained BERT model using the 'bert-base-cased' configuration from the transformers library, enabling transfer learning for the specific task.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9901598,
    "start_cell": false,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "from transformers.modeling_bert import BertModel\n",
    "\n",
    "# BERTの学習済みパラメータのモデル\n",
    "model = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This snippet imports various libraries and modules required for data manipulation, visualization, natural language processing, machine learning, and neural network construction and training, setting up the environment for the machine learning task.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99879694,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Environment",
    "desc": "This snippet sets the resolution for saved figures in Matplotlib to 1200 dots per inch (dpi) to enhance the image quality.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9859579,
    "start_cell": false,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "# enhance image resoulution\n",
    "import matplotlib as mpl\n",
    "mpl.rc(\"savefig\", dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Environment",
    "desc": "This snippet imports the ticker module from Matplotlib, which provides tools for configuring the tick marks on plots, aiding in customizations for data visualizations.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99929714,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Environment",
    "desc": "This snippet imports additional sklearn modules for logistic regression, train-test splitting, and various evaluation metrics, setting up the necessary components for model training and evaluation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9993144,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, accuracy_score,make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 64,
    "class": "Environment",
    "desc": "This commented-out snippet indicates the intention to download Google's official BERT tokenization script from a specified GitHub repository, setting up the necessary tools for BERT-based text preprocessing.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.8908539,
    "start_cell": false,
    "subclass": "load_from_url",
    "subclass_id": 42
   },
   "outputs": [],
   "source": [
    "# Google official tokenization\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 65,
    "class": "Environment",
    "desc": "This snippet imports TensorFlow and Keras modules for building and training neural network models, along with TensorFlow Hub for accessing pre-trained models, and the tokenization module for text preprocessing, enhancing the environment setup for advanced NLP tasks.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99932015,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 74,
    "class": "Environment",
    "desc": "This snippet loads the BERT model from TensorFlow Hub, specifically the uncased variant with 24 layers, and sets it as trainable to allow fine-tuning during model training.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.87859446,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Load BERT from the Tensorflow Hub\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 77,
    "class": "Environment",
    "desc": "This snippet creates a BERT tokenizer using the vocabulary file and lowercase setting obtained from the loaded BERT layer, enabling tokenization of text data in a manner consistent with the pre-trained BERT model.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.56229645,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code installs the 'adaptnlp' library using pip.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.990258,
    "start_cell": true,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "# Don't forget to make sure your Internet connection is turned on\n",
    "! pip install adaptnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This code imports the 'numpy' and 'pandas' libraries and sets up some configuration variables.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.998589,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "target_corrected = True\n",
    "to_lower = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Environment",
    "desc": "This code sets up the model type and directory, then attempts to create a directory for the model, handling any exceptions that occur.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.99615115,
    "start_cell": false,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "MODEL = 'bert'\n",
    "MODEL_DIR = '/kaggle/' + MODEL + '-working'\n",
    "try:\n",
    "    os.mkdir(MODEL_DIR)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Environment",
    "desc": "This code imports `EasyDocumentEmbeddings` and `SequenceClassifierTrainer` from the `adaptnlp` library.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9992964,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "from adaptnlp import EasyDocumentEmbeddings, SequenceClassifierTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Environment",
    "desc": "This code snippet sets up a directory for labeled training data and then attempts to create this directory, handling any exceptions that may occur.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9951199,
    "start_cell": false,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "label_training_dir = MODEL_DIR + '/label_training'\n",
    "try:\n",
    "    os.mkdir(label_training_dir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Environment",
    "desc": "This code snippet sets the paths for the corpus, fine-tuned model directory, and output directory, and initializes the `EasyDocumentEmbeddings` with the BERT model and RNN method.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9634659,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "corpus = label_training_dir # Or path to directory of train.csv, test.csv, dev.csv files at \"Path/to/data/directory\" \n",
    "FINETUNED_MODEL_DIR = MODEL_DIR\n",
    "OUTPUT_DIR = label_training_dir\n",
    "doc_embeddings = EasyDocumentEmbeddings(\"bert-base-uncased\", methods = [\"rnn\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Environment",
    "desc": "This code defines the configurations for the sequence classifier trainer, including the corpus, encoder, column mappings, and predictive head, and then initializes the `SequenceClassifierTrainer` with these configurations.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.8980685,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "sc_configs = {\n",
    "              \"corpus\": corpus,\n",
    "              \"encoder\": doc_embeddings,\n",
    "              \"column_name_map\": {3: \"text\", 4: \"label\"},\n",
    "              \"corpus_in_memory\": True,\n",
    "              \"predictive_head\": \"flair\",\n",
    "             }\n",
    "sc_trainer = SequenceClassifierTrainer(**sc_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports libraries to enhance Jupyter notebook interactivity, handle datetime with timezone, and defines a utility function to print out method arguments or object attributes. ",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.3470069,
    "start_cell": true,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "datetime.now(timezone('Asia/Tokyo')).strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "def refer_args(x):\n",
    "    if type(x) == 'method':\n",
    "        print(*x.__code__.co_varnames.split(), sep='\\n')\n",
    "    else:\n",
    "        print(*[x for x in dir(x) if not x.startswith('__')], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This code snippet imports a variety of libraries and modules for data manipulation, visualization, natural language processing, machine learning, and neural network tasks, sets a plot style, defines a set of stopwords, and configures pandas display options.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99893683,
    "start_cell": false,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from wordcloud import WordCloud\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "plt.style.use('ggplot')\n",
    "stop = set(stopwords.words('english')) | {\n",
    "            'i','im','you','youre','they','theyre','he','hes','she','shes','we','our','us','were','arent',\\\n",
    "            'can','cant','could','couldnt','will','wont','would','wouldnt','should','shouldnt','may',\\\n",
    "            'dont','didnt','doesnt'}\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Environment",
    "desc": "This code snippet imports necessary modules and classes (Sequential, Dense, Input, Adam) from the TensorFlow Keras library to build and optimize neural network models.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9993243,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries and modules necessary for data manipulation, visualization, text processing, machine learning model creation, and optimization. ",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.99868137,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Environment",
    "desc": "This code snippet is a commented-out command for installing the \"pyspellchecker\" package, which is used for spell checking in text processing tasks.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9968748,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Environment",
    "desc": "This code snippet is a commented-out import statement for bringing in the `SpellChecker` class from the `spellchecker` library, which is typically used for spelling correction in text data.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9951976,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code imports necessary libraries and lists all files available in the input directory for the Kaggle environment.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99923015,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This code imports various modules from the `sklearn` library and the `re` module for feature extraction, model training, preprocessing, and regular expression operations.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9993718,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code loads essential libraries, lists input data files available in a specific directory, and provides context for file storage capabilities. ",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.99923015,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This code imports various libraries and modules necessary for building and evaluating a machine learning model using PyTorch and TorchText, as well as for data manipulation and evaluation with scikit-learn.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9993242,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports necessary libraries and modules for data processing, model training, and Google Cloud services.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.99931204,
    "start_cell": true,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import automl_v1beta1 as automl\n",
    "\n",
    "# workaround to fix gapic_v1 error\n",
    "from google.api_core.gapic_v1.client_info import ClientInfo\n",
    "\n",
    "from automlwrapper import AutoMLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This code snippet sets up the project-specific variables and initializes the Google Cloud Storage and AutoML clients, while also printing a start time message for the AutoML notebook.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.30418834,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Set your own values for these. bucket_name should be the project_id + '-lcm'.\n",
    "PROJECT_ID = 'cloudml-demo'\n",
    "bucket_name = 'cloudml-demo-lcm'\n",
    "\n",
    "region = 'us-central1' # Region must be us-central1\n",
    "dataset_display_name = 'kaggle_tweets'\n",
    "model_display_name = 'kaggle_starter_model1'\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# adding ClientInfo here to get the gapic_v1 call in place\n",
    "client = automl.AutoMlClient(client_info=ClientInfo())\n",
    "\n",
    "print(f'Starting AutoML notebook at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Environment",
    "desc": "This code snippet checks if a specified Google Cloud Storage bucket exists, and if not, creates it in the specified region.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.6219822,
    "start_cell": false,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "bucket = storage.Bucket(storage_client, name=bucket_name)\n",
    "if not bucket.exists():\n",
    "    bucket.create(location=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Environment",
    "desc": "This code snippet initializes an instance of the `AutoMLWrapper` class with the specified Google Cloud project parameters and the previously defined client, bucket name, region, dataset display name, and model display name.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.74450374,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "amw = AutoMLWrapper(client=client, \n",
    "                    project_id=PROJECT_ID, \n",
    "                    bucket_name=bucket_name, \n",
    "                    region='us-central1', \n",
    "                    dataset_display_name=dataset_display_name, \n",
    "                    model_display_name=model_display_name)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Environment",
    "desc": "This code snippet calls a method to undeploy the model, likely to free up resources in the AutoML system after evaluation or deployment.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.28704625,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "amw.undeploy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "Installs the `nlplot` library, which is commonly used for visualizing and analyzing natural language processing tasks. ",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.99194145,
    "start_cell": true,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "! pip install nlplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "Imports various libraries and modules essential for data processing, natural language processing, machine learning, and visualization, and sets display options for pandas.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.7842885,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "import emoji\n",
    "import nlplot\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop=set(stopwords.words('english'))\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report,accuracy_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,BatchNormalization,TimeDistributed,Dropout,Bidirectional,Flatten,GlobalMaxPool1D\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code imports essential libraries for data processing and visualization, and lists all files in the Kaggle input directory.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99922085,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Environment",
    "desc": "The code installs the BeautifulSoup4 library, which is used for parsing HTML and XML documents.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9954203,
    "start_cell": false,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code initializes the environment by loading necessary libraries and listing all files under the \"/kaggle/input\" directory for data access and manipulation purposes.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99921954,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Environment",
    "desc": "The code imports various Python libraries and modules needed for natural language processing, machine learning model building, and evaluation.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99932015,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "import spacy\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Environment",
    "desc": "The code loads the English language model for the spaCy library, which is used for natural language processing tasks.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9950819,
    "start_cell": false,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries and sets the style for matplotlib plots, preparing the environment for data manipulation, visualization, and model building.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9986696,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Environment",
    "desc": "This code snippet installs the `pyspellchecker` Python package using pip, which is likely to be used for spell-checking in text data preprocessing.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99582124,
    "start_cell": false,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries and modules necessary for text processing, data visualization, neural network construction, and machine learning tasks.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9985905,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "plt.style.use('ggplot')\n",
    "stop = set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "This code snippet imports the `os` module, which provides functionalities to interact with the operating system.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99917585,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Environment",
    "desc": "This code snippet installs the `pyspellchecker` package, which is used for spell checking text.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99582124,
    "start_cell": false,
    "subclass": "install_modules",
    "subclass_id": 87
   },
   "outputs": [],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code imports necessary libraries (numpy, pandas, and nltk) and reads train and test datasets from CSV files using pandas.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99963045,
    "start_cell": true,
    "subclass": "load_from_csv",
    "subclass_id": 45
   },
   "outputs": [],
   "source": [
    "#Import numpy,pandas and nltk\n",
    "#Download necessary nltk Components \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "train_data=pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test_data=pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Environment",
    "desc": "This code snippet imports the `re` library, which is used for working with regular expressions in Python.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99916375,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Environment",
    "desc": "This code snippet imports the WordNetLemmatizer for lemmatizing words and stopwords from the NLTK library, and instantiates a lemmatizer object for processing text.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.93402934,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "#import stopwords and lemmatizer for lemmatization and remove some unnecessary keywords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Environment",
    "desc": "This code snippet imports the TfidfVectorizer from the sklearn library, which will be used for transforming text data into TF-IDF feature vectors.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9993125,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "#Import tfidf vectorizer from sklearn library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Environment",
    "desc": "This code snippet imports the `LogisticRegression` class from the `sklearn.linear_model` module to be used for training a logistic regression model.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99928147,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "#import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Environment",
    "desc": "This code snippet imports the `train_test_split` function from the `sklearn.model_selection` module, which will be used to split the training data into training and validation sets.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99873096,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "#split our train data into train and test set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Environment",
    "desc": "This code snippet initializes a `LogisticRegression` classifier with a regularization parameter `C` set to 3.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9981187,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "classifier=LogisticRegression(C=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "The code imports essential libraries like NumPy, Pandas, and os, then lists all files in the specified input directory to ensure data accessibility and environment setup.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99929106,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Environment",
    "desc": "The code imports various libraries for plotting (matplotlib), text preprocessing (nltk), and visualization (WordCloud), and suppresses deprecation warnings to set up the necessary environment for the notebook.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99930036,
    "start_cell": false,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from termcolor import colored\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Environment",
    "desc": "The code initializes the list of English stopwords, a WordNet lemmatizer, and a Porter stemmer to be used for text preprocessing activities.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.7952712,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "STOPWORDS= stopwords.words(\"english\")\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "porterStemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Environment",
    "desc": "The code imports necessary modules from the Keras library for neural network construction, text preprocessing, and tokenization, as well as an additional tokenizer from the NLTK library to set up tools for model building and text data handling.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99932766,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalMaxPooling1D, Flatten, Conv1D, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Environment",
    "desc": "The code imports necessary modules from the TensorFlow Keras library for text preprocessing, data conversion, model building, and sequence padding to support the development of a neural network model.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9993382,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional,Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code imports necessary libraries for data manipulation, machine learning model training and text processing, and lists the files in a specific directory.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.99932766,
    "start_cell": true,
    "subclass": "list_files",
    "subclass_id": 88
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import os, re\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code imports libraries and modules needed for linear algebra, data processing, plotting, text processing, natural language processing, and machine learning. ",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9993235,
    "start_cell": true,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np \n",
    "# data processing\n",
    "import pandas as pd \n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "# text processing libraries\n",
    "import string\n",
    "# regular expression\n",
    "import re\n",
    "# natural language processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "# scikit-learn\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Environment",
    "desc": "This code snippet imports various libraries required for data processing, visualization, preprocessing, and text analysis in the machine learning task.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99932945,
    "start_cell": true,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data processing\n",
    "import numpy as np  # linear algebra\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and libraries for preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Preprocessing libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import gensim\n",
    "import string\n",
    "# Counter function\n",
    "from collections import Counter\n",
    "\n",
    "#WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "# create text with Markdown from within code cells¶\n",
    "from IPython.display import Markdown as md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet prints the summary information of the Titanic training and test datasets, including the data types and non-null counts for each column.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.99917716,
    "start_cell": true,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic_df.info()\n",
    "print(\"----------------------------\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet outputs the data types of each column in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9968425,
    "start_cell": true,
    "subclass": "show_data_types",
    "subclass_id": 70
   },
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet provides a concise summary of the training DataFrame, including the number of non-null entries and data types of each column.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9992316,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet generates descriptive statistics of the numeric columns in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9994429,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet creates and displays a cross-tabulation of the passenger class against the survival status in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99839824,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "pclass_xt = pd.crosstab(df_train['Pclass'], df_train['Survived'])\n",
    "pclass_xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet prints the count of male and female passengers for each unique passenger class in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.8531439,
    "start_cell": false,
    "subclass": "show_unique_values",
    "subclass_id": 57
   },
   "outputs": [],
   "source": [
    "# Get the unique values of Pclass:\n",
    "passenger_classes = sorted(df_train['Pclass'].unique())\n",
    "\n",
    "for p_class in passenger_classes:\n",
    "    print('M: ', p_class, len(df_train[(df_train['Sex'] == 'male') & \n",
    "                             (df_train['Pclass'] == p_class)]))\n",
    "    print('F: ', p_class, len(df_train[(df_train['Sex'] == 'female') & \n",
    "                             (df_train['Pclass'] == p_class)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet filters and displays rows from the training DataFrame where the 'Embarked' value is missing (null).",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.7257105,
    "start_cell": false,
    "subclass": "filter",
    "subclass_id": 14
   },
   "outputs": [],
   "source": [
    "df_train[df_train['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet retrieves and displays the sorted unique values from the 'Embarked_Val' column in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9614523,
    "start_cell": false,
    "subclass": "show_unique_values",
    "subclass_id": 57
   },
   "outputs": [],
   "source": [
    "embarked_locs = sorted(df_train['Embarked_Val'].unique())\n",
    "embarked_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet filters and displays the first few rows of the training DataFrame where the 'Age' value is missing, specifically showing the 'Sex', 'Pclass', and 'Age' columns.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9996885,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "df_train[df_train['Age'].isnull()][['Sex', 'Pclass', 'Age']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet counts and outputs the number of rows in the training DataFrame where the 'AgeFill' column still has missing (null) values.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.7509087,
    "start_cell": false,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "len(df_train[df_train['AgeFill'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet lists the columns in the training DataFrame that have data types classified as objects.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.3351488,
    "start_cell": false,
    "subclass": "show_data_types",
    "subclass_id": 70
   },
   "outputs": [],
   "source": [
    "df_train.dtypes[df_train.dtypes.map(lambda x: x == 'object')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Exploratory Data Analysis",
    "desc": "This code prints the count of missing (NAN) values for each column in the combined dataset.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9960766434669496,
    "start_cell": true,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "# Print all column names and their count of NAN values\n",
    "for col in df_full.columns:\n",
    "    print(\"Found {} NAN values for column: {}\".format(df_full[col].isnull().sum(), col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This code counts and prints the number of rows where 'Age' is missing but 'Title' is present, grouped by the different titles.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9981630444526672,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "# how many rows exists, where Age is NaN and Title is given\n",
    "print(df_full.loc[(df_full.Age.isnull()) & (df_full.Ticket.notnull()), :].Title.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code counts and prints the number of rows where both 'Age' and 'Title' are missing, grouped by the different titles.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.998459815979004,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "# how many rows exists, where Age is NaN and also Title is NaN\n",
    "print(df_full.loc[(df_full.Age.isnull()) & (df_full.Ticket.isnull()), :].Title.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "This code calculates and displays the mean 'Age' for each group of 'Title' in the dataset.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9446871280670166,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# determine the mean Age grouped by Title\n",
    "df_full.groupby(['Title']).Age.mean().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Exploratory Data Analysis",
    "desc": "This code provides a concise summary of the DataFrame including the number of non-null entries, column data types, and memory usage.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9993414282798768,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet uses the shape command to display the number of rows and columns in the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9991304,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "#shape command will give number of rows/samples/examples and number of columns/features/predictors in dataset\n",
    "#(rows,columns)\n",
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet uses the describe method to provide statistical information about the numerical columns in the 'titanic' dataset and highlights that the 'Age' column has missing values.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9993962,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "#Describe gives statistical information about numerical columns in the dataset\n",
    "titanic.describe()\n",
    "#you can check from count if there are missing vales in columns, here age has got missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet uses the info method to provide information about the 'titanic' dataset, including the total values in each column, whether they are null or not, data types, and memory usage.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9987674,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "#info method provides information about dataset like \n",
    "#total values in each column, null/not null, datatype, memory occupied etc\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet identifies columns with missing values in the 'titanic' dataset and sums up the total number of missing values for each of those columns.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.5850566,
    "start_cell": false,
    "subclass": "show_columns",
    "subclass_id": 71
   },
   "outputs": [],
   "source": [
    "#lets see if there are any more columns with missing values \n",
    "null_columns=titanic.columns[titanic.isnull().any()]\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet sums up the total number of missing values in each column of the 'titanic_test' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9986224,
    "start_cell": false,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "#how about test set??\n",
    "titanic_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet calculates and displays the correlation of each feature in the 'titanic' dataset with the target variable 'Survived'.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99903846,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "#correlation of features with target variable\n",
    "titanic.corr()[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet identifies and displays the rows in the 'titanic' dataset where the 'Embarked' column has null values.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.91445994,
    "start_cell": false,
    "subclass": "filter",
    "subclass_id": 14
   },
   "outputs": [],
   "source": [
    "#Lets check which rows have null Embarked column\n",
    "titanic[titanic['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet uses the describe method to provide statistical information about the numerical columns in the 'titanic_test' dataset, including identifying any columns with empty values.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9994411,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "#there is an empty fare column in test set\n",
    "titanic_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet identifies and displays the rows in the 'titanic_test' dataset where the 'Fare' column has null values.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.8052727,
    "start_cell": false,
    "subclass": "filter",
    "subclass_id": 14
   },
   "outputs": [],
   "source": [
    "titanic_test[titanic_test['Fare'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet displays the last five entries of the 'Ticket' column in the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99973696,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "titanic[\"Ticket\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet identifies and displays the rows in the 'titanic' dataset where the 'TicketNumber' column has NaN values due to the absence of numeric values in the original 'Ticket' column.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.7099305,
    "start_cell": false,
    "subclass": "filter",
    "subclass_id": 14
   },
   "outputs": [],
   "source": [
    "#some rows in ticket column dont have numeric value so we got NaN there\n",
    "titanic[titanic[\"TicketNumber\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 46,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet calculates and displays the correlation of each feature in the 'titanic' dataset with the target variable 'Survived'.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9992786,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.corr()[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet calculates and prints the mean of the response variable `y` to understand the percentage of records that had the target attribute (Survived in this case).",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9968941,
    "start_cell": true,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "# what percentage had affairs?\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the summary information of the Titanic and test DataFrames, including the data types and non-null count of each feature.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.99917716,
    "start_cell": true,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic_df.info()\n",
    "print(\"----------------------------\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of both the Titanic and test DataFrames to review the current state of the data after transformations.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9997528,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "titanic_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet displays the first few rows of the Titanic dataset to examine the available variables and their initial values.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet provides summary statistics of the numerical variables in the Titanic dataset to understand their distributions and central tendencies.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.99945647,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet displays the first few rows of the Titanic dataset to provide an initial view of the variables and their data.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9879770874977112,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code outputs the first few rows of the Titanic dataset to provide an initial look at the data.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9879770874977112,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code provides descriptive statistics for all columns in the Titanic dataset.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.99943345785141,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe( include = 'all' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet displays the first few rows of the Titanic dataset to get an initial look at the variables and data structure.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet generates a descriptive statistical summary of all the columns in the Titanic dataset, including both numeric and categorical variables.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99943346,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe( include = 'all' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the 'titanic' DataFrame for an initial examination of the dataset's variables and structure.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet generates descriptive statistics of the 'titanic' DataFrame, providing insights into the central tendency, dispersion, and shape of the dataset’s distribution.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99945647,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the Titanic dataset to provide an initial look at the variables and data.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9879770874977112,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet generates summary statistics of the Titanic dataset to give an overview of the central tendency, dispersion, and shape of the dataset's distribution.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9994564652442932,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the Titanic dataset to provide an initial view of the variables and their values.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9879770874977112,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet generates descriptive statistics for the Titanic dataset, summarizing the central tendency, dispersion, and shape of the dataset’s distributions.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9994564652442932,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the Titanic dataset to provide an initial glimpse of the variables and data structure.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet provides a statistical summary of the Titanic dataset, including measures such as mean, standard deviation, min, and max for numerical features.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.99945647,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the Titanic dataset to get an initial understanding of the variables and their values.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9879770874977112,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet generates descriptive statistics of the Titanic dataset, providing insights into the distribution, central tendency, and spread of the features.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9994564652442932,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the 'titanic' dataset to provide an initial look at the variables.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet provides summary statistics for the numerical variables in the 'titanic' dataset.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99945647,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "The snippet displays the first few rows of the Titanic dataset to provide an initial look at the data.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "The snippet generates descriptive statistics for the Titanic dataset, including count, mean, standard deviation, min, and max for each numeric column.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99945647,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first 3 rows of the transformed training dataset to provide an overview of the data after preprocessing.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99976355,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet calculates and displays the mean survival rate, total observations, and total number of people who survived, grouped by the \"Title\" feature in the training dataset.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.7890924,
    "start_cell": false,
    "subclass": "groupby",
    "subclass_id": 60
   },
   "outputs": [],
   "source": [
    "train[['Title', 'Survived']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n",
    "# Since \"Survived\" is a binary class (0 or 1), these metrics grouped by the Title feature represent:\n",
    "    # MEAN: survival rate\n",
    "    # COUNT: total observations\n",
    "    # SUM: people survived\n",
    "\n",
    "# title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet calculates and displays the mean survival rate, total observations, and total number of people who survived, grouped by the \"Sex\" feature in the training dataset.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9318028,
    "start_cell": false,
    "subclass": "groupby",
    "subclass_id": 60
   },
   "outputs": [],
   "source": [
    "train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).agg(['mean', 'count', 'sum'])\n",
    "# Since Survived is a binary feature, this metrics grouped by the Sex feature represent:\n",
    "    # MEAN: survival rate\n",
    "    # COUNT: total observations\n",
    "    # SUM: people survived\n",
    "    \n",
    "# sex_mapping = {{'female': 0, 'male': 1}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet creates a table showing the distribution of the \"Sex\" feature (as a binary variable) grouped by the \"Title\" feature in the original training dataset, providing the mean percentage of men, total observations, and number of men.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.6058313,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# Let's use our 'original_train' dataframe to check the sex distribution for each title.\n",
    "# We use copy() again to prevent modifications in out original_train dataset\n",
    "title_and_sex = original_train.copy()[['Name', 'Sex']]\n",
    "\n",
    "# Create 'Title' feature\n",
    "title_and_sex['Title'] = title_and_sex['Name'].apply(get_title)\n",
    "\n",
    "# Map 'Sex' as binary feature\n",
    "title_and_sex['Sex'] = title_and_sex['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "# Table with 'Sex' distribution grouped by 'Title'\n",
    "title_and_sex[['Title', 'Sex']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n",
    "\n",
    "# Since Sex is a binary feature, this metrics grouped by the Title feature represent:\n",
    "    # MEAN: percentage of men\n",
    "    # COUNT: total observations\n",
    "    # SUM: number of men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the summary information of the Titanic training and test DataFrames, such as the number of non-null entries, data types, and memory usage.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99917716,
    "start_cell": true,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic_df.info()\n",
    "print(\"----------------------------\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the Titanic dataset to provide an initial view of the data structure and its variables.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9879770874977112,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet provides summary statistics for the numerical variables in the Titanic dataset to give an overview of its central tendencies and dispersion.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9994564652442932,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few entries and the shape of the target variable ('Survived') from the training and validation set to verify its structure and size.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.866479754447937,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train_valid_y.head()\n",
    "train_valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code displays the first few rows of the Titanic dataset to get an initial look at the variables and their values.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code provides descriptive statistics for the numerical columns in the Titanic dataset to summarize their central tendency, dispersion, and shape.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99945647,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet displays the first few rows of the Titanic dataset to provide a quick overview of the data's structure and contents.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9879770874977112,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet calls the `describe_more` function on the Titanic dataset, which provides a detailed summary of each variable, including the number of unique levels and data types.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9993897676467896,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "describe_more(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code displays the first few rows of the Titanic dataset to give an overview of the variables and their values.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code provides a concise summary of the Titanic dataset, including the number of non-null entries and the data types of each column.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9993418,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This code generates descriptive statistics of the Titanic dataset, including both numeric and categorical features.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9994118,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code generates summary statistics for the numeric features in the Titanic dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99945647,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Exploratory Data Analysis",
    "desc": "This code outputs the column names of the DataFrame 'full_X' to review the features included in the dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9985097,
    "start_cell": false,
    "subclass": "show_columns",
    "subclass_id": 71
   },
   "outputs": [],
   "source": [
    "full_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first five rows of the training dataset.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9997615218162536,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet provides descriptive statistics for the categorical features in the training dataset.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.999426007270813,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "train_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet imports the preprocessing module from scikit-learn, defines a list of fields to preprocess, and groups the training dataset by 'Parch' to count the number of occurrences for each unique value in the 'Parch' column.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9936479926109314,
    "start_cell": false,
    "subclass": "groupby",
    "subclass_id": 60
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "fields = ['Sex',]\n",
    "train_df.groupby('Parch')['Parch'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the training dataset for inspection.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9997544884681702,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the 'titanic' DataFrame to allow inspection of its variables and values.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9879771,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Run the code to see the variables, then read the variable description below to understand them.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet generates descriptive statistics for the 'titanic' DataFrame, summarizing its central tendency, dispersion, and shape of the data distribution.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99945647,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the training dataset to give an initial glimpse of its structure and contents.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9997545,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the testing dataset to give an initial glimpse of its structure and contents.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9997558,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the transformed training dataset to review the changes made and confirm that the columns were concatenated and deleted correctly.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9997545,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the transformed testing dataset to review the changes made and confirm that the columns were concatenated and deleted correctly.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9997558,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Exploratory Data Analysis",
    "desc": "The code calculates and outputs the maximum length of the tokenized text sequences in both the training and test datasets.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.6832335,
    "start_cell": true,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "maxi=0\n",
    "\n",
    "for ele in train['text']:\n",
    "    maxi=max(maxi,len(ele))\n",
    "    \n",
    "for ele in test['text']:\n",
    "    maxi=max(maxi,len(ele))\n",
    "    \n",
    "maxi_text=maxi\n",
    "maxi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Exploratory Data Analysis",
    "desc": "The code calculates and outputs the maximum length of the hashtag sequences in both the training and test datasets.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.90138,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "maxi=0\n",
    "\n",
    "for ele in train['hashtag']:\n",
    "    maxi=max(maxi,len(ele))\n",
    "    \n",
    "for ele in test['hashtag']:\n",
    "    maxi=max(maxi,len(ele))\n",
    "    \n",
    "maxi\n",
    "maxi_hashtag=maxi\n",
    "maxi_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Exploratory Data Analysis",
    "desc": "The code calculates and outputs the maximum length of the keyword sequences in both the training and test datasets.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9374175,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "maxi=0\n",
    "\n",
    "for ele in train['keyword']:\n",
    "    maxi=max(maxi,len(ele))\n",
    "    \n",
    "for ele in test['keyword']:\n",
    "    maxi=max(maxi,len(ele))\n",
    "    \n",
    "maxi_keyword=maxi\n",
    "maxi_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet calculates and prints the coverage of GloVe word embeddings for vocabulary and text in both the training and test datasets using the `check_embeddings_coverage` function.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.61129916,
    "start_cell": true,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(tweet[\"text\"], embedding_dict)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(testset['text'], embedding_dict)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet calculates and prints the coverage of GloVe word embeddings for vocabulary and text in the cleaned, combined dataset using the `check_embeddings_coverage` function.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.19538902,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "df_glove_oov, df_glove_vocab_coverage, df_glove_text_coverage = check_embeddings_coverage(df[\"text_clean\"], embedding_dict)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(df_glove_vocab_coverage, df_glove_text_coverage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet outputs the number of unique words in the `word_index` dictionary created by the tokenizer.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.98261434,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would generate a list of unique keywords from the 'keyword' column in the training data and print them.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9977023,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#keywords = list(train_data['keyword'].unique())\n",
    "#print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display the unique values in the 'keyword' column of the training dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9964826,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#train_data['keyword'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would generate and display a list of unique values from the 'location' column in the training dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99648947,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#list(train_data['location'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would generate and display a list of unique values from the 'location' column in the training dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99648947,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#list(train_data['location'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display the top 50 most common words found in disaster tweets.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99137735,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#top_50_disaster_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display the top 50 most common words found in non-disaster tweets.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9912341,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#top_50_nondisaster_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display the list of words that are exclusively found in disaster tweets.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9930703,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#top_disaster_exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display the list of words that are exclusively found in non-disaster tweets.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9916366,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#top_nondisaster_exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display descriptive statistics of the 'text' column in the training dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9956852,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#for col in train_data.columns:\n",
    "#    if col == 'text':\n",
    "#        print(train_data[col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if executed, would return the number of columns in the training dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9806949,
    "start_cell": false,
    "subclass": "show_columns",
    "subclass_id": 71
   },
   "outputs": [],
   "source": [
    "len(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display the shape of the reduced training feature set (`X_train_reduced`).",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99557626,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#X_train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 47,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display the total number of missing values in the testing dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99711645,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#test_data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 48,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet, if uncommented, would display the shape of the testing dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99622947,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code prints the contents of the training and test dataframes, as well as creates a copy of the training dataframe for backup or comparison purposes during analysis.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.95230055,
    "start_cell": true,
    "subclass": "prepare_output",
    "subclass_id": 55
   },
   "outputs": [],
   "source": [
    "print(train_val_df)\n",
    "print(test_df.head())\n",
    "original_df = train_val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code prints the 20th row of both the transformed training dataframe and the original training dataframe for comparison and verification of the preprocessing steps.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.82359093,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# データの確認\n",
    "print(train_val_df.loc[20])\n",
    "print(original_df.loc[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Exploratory Data Analysis",
    "desc": "This code prints the lengths of the training/evaluation and test datasets to confirm the number of samples contained in each.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9964072,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "# datasetの長さを確認\n",
    "print(dataset_train_eval.__len__())\n",
    "print(dataset_test.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Exploratory Data Analysis",
    "desc": "This code retrieves and prints the first item in the training dataset to inspect the tokenized text and its length, as well as the associated label.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.33225554,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# datasetの中身を確認してみる\n",
    "item = next(iter(dataset_train))\n",
    "print(item.Text)\n",
    "print(\"長さ：\", len(item.Text))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
    "print(\"ラベル：\", item.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Exploratory Data Analysis",
    "desc": "This code converts the token IDs of the first item in the training dataset back to tokens (words) using the tokenizer and prints them to inspect the original text, along with its label as an integer.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.8763242,
    "start_cell": false,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "# datasetの中身を文章に戻し、確認\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(item.Text.tolist()))  # 文章\n",
    "print(int(item.Label))# id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet prints the number of rows and columns in the train and test DataFrames to understand the size of the datasets.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99756986,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet displays the first 10 rows of the train DataFrame to give an overview of the initial data in the dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99975616,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "tweet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet calculates the number of examples in the train DataFrame for each class (target values 0 and 1), providing insights into the class distribution.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9593875,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# extracting the number of examples of each class\n",
    "Real_len = tweet[tweet['target'] == 1].shape[0]\n",
    "Not_len = tweet[tweet['target'] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet displays the array of stopwords used in the analysis to better understand which words are being filtered out during preprocessing.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9659018,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# displaying the stopwords\n",
    "np.array(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet outputs the shape of the concatenated DataFrame to provide insights into its current dimensions, which can be useful for understanding the dataset's size after preprocessing steps.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9995491,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 44,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet displays the first few rows of the test DataFrame subset to provide an overview of its initial data, aiding in visual inspection and verification.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99975866,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 56,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet displays the first padded sequence of the transformed text data to provide a glimpse of how the text data looks after tokenization and padding, useful for verification and understanding the data structure.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99935824,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "tweet_pad[0][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 86,
    "class": "Exploratory Data Analysis",
    "desc": "This snippet displays the first few rows of the DataFrame containing the predictions made by the BERT-based model on the test data, providing an initial look at the predicted values.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99974257,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the `tweet` DataFrame for examination.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.9997403,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the `TEST` DataFrame for examination.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99975115,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "TEST.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet retrieves the word index from the tokenizer object and prints the number of unique words in the corpus.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.98261434,
    "start_cell": true,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the generated submission DataFrame to review the output before final submission.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.99974316,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code displays a summary of the training DataFrame, including the data types and non-null values for each column.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9993617,
    "start_cell": true,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code outputs the dimensions (number of rows and columns) of the training DataFrame.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99950194,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This code generates descriptive statistics for the numerical columns in the training DataFrame and transposes the result for better readability.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9994435,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "train.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code displays the first five rows of the training DataFrame to provide a quick glance at the data.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9997507,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "This code retrieves and displays the text of the second instance in the training DataFrame where the target value is 1.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.88786155,
    "start_cell": false,
    "subclass": "filter",
    "subclass_id": 14
   },
   "outputs": [],
   "source": [
    "train[train[\"target\"] == 1][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Exploratory Data Analysis",
    "desc": "This code retrieves and displays the text of the first instance in the training DataFrame where the target value is 0.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.79435223,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train[train[\"target\"] == 0][\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Exploratory Data Analysis",
    "desc": "This code displays the first five rows of the processed training DataFrame to observe the changes made during preprocessing.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9997507,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Exploratory Data Analysis",
    "desc": "This code displays the first five rows of the testing DataFrame for initial inspection.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9997483,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the last few rows of the training dataset to provide a preview of its structure and contents.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9997632,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "nlp_train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet filters and displays rows from the training dataset where the 'text' column contains the word 'fire', irrespective of case.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9910484,
    "start_cell": false,
    "subclass": "filter",
    "subclass_id": 14
   },
   "outputs": [],
   "source": [
    "nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet counts the occurrences of each unique value in the 'target' column for rows where the 'text' column contains the word 'fire'.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9993672,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the training DataFrame, showing only the 'id', 'text', and 'target' columns, to provide a quick look at the data.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9997446,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "nlp_train_df[['id','text','target']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few rows of the testing DataFrame to provide an initial look at the test data.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9997614,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "nlp_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "Prints the number of rows and columns in the training and testing datasets to understand their size.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9979109,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\n",
    "print('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "Displays the first few rows of the training and testing datasets to provide an initial look at the data.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9997433,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "display(train.head(), test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Exploratory Data Analysis",
    "desc": "Displays the first few rows of the combined DataFrame to inspect the changes made by the text-cleaning process.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9997553,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "The code provides a concise summary of the training data, including the number of non-null entries and data types for each column.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99936634,
    "start_cell": true,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "The code provides a concise summary of the testing data, including the number of non-null entries and data types for each column.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9993579,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Exploratory Data Analysis",
    "desc": "The code displays the first five rows of the training dataset.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9997532,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Exploratory Data Analysis",
    "desc": "The code prints the number of unique words in the tokenizer's word index obtained from the preprocessed text data.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9865096,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "num_words = len(tokenizer.word_index)\n",
    "print(f\"Number of unique words: {num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet prints the number of rows and columns in both the training and test datasets to give an overview of the dataset dimensions.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99756986,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet prints the number of unique words in the corpus by accessing the word index created by the tokenizer.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.98261434,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet prints the number of rows and columns in the training and testing DataFrames.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9982999,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "print('there are {} rows and {} columns in train'.format(tweet.shape[0], tweet.shape[1]))\n",
    "print('there are {} rows and {} columns in test'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet retrieves and prints the number of unique words in the corpus based on the tokenizer's word index.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9840506,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer_obj.word_index\n",
    "print('number of unique words: ', len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet checks and displays the number of records and features in both the training and test datasets.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9995427,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "#Check the number of records and number of features\n",
    "train_data.shape,test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few records of the training dataset to give an initial look at the data.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9951192,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "#Checkout the train dataframe\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few records of the test dataset to provide an initial look at the test data.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99893194,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "#Checkout the test dataframe\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet checks the training dataset for any NULL values and provides the count of missing values for each feature.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99871135,
    "start_cell": false,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "#Check for NULL values in Train data\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet checks the test dataset for any NULL values and provides the count of missing values for each feature.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9988362,
    "start_cell": false,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "#Check for NULL values in test data\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet retrieves and displays the unique values from the 'keyword' column of the training dataset to understand the distinct keywords present.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.997072,
    "start_cell": false,
    "subclass": "show_unique_values",
    "subclass_id": 57
   },
   "outputs": [],
   "source": [
    "#Lets explore keyword column now\n",
    "train_data.keyword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few records of the `train_text` data before applying any cleaning transformations.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9997465,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "#check train data before cleaning\n",
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first few records of the cleaned `train_text` data to verify the results of the applied cleaning function.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99975175,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "#Check data after cleaning\n",
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet checks and displays the length of the `train_sequence` list to ensure that all training texts have been processed and appended correctly.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99865425,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "#Check length of train_sequence list\n",
    "len(train_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet checks and displays the length of the `test_sequence` list to ensure that all test texts have been processed and appended correctly.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9991165,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "len(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet checks and displays the shape of the `vectorized_train` to understand the dimensions of the TF-IDF features matrix for the training data.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99954563,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "vectorized_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet checks and displays the shape of the `vectorized_test` to understand the dimensions of the TF-IDF features matrix for the test data.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9995852,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "vectorized_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first TF-IDF feature vector from the `vectorized_train` array to provide insight into the processed feature values for the first training instance.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99969673,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "vectorized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "The code displays the first few rows of the training DataFrame to give an initial overview of the dataset structure and content.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9997507,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet returns the dimensions of the training DataFrame, providing information about the number of rows and columns.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99950194,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet counts the occurrences of each unique value in the 'target' column, giving an overview of the distribution of the target variable.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99950993,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet returns the dimensions of the `y_train` array, providing information about the number of target values in the training set.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9996031,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Exploratory Data Analysis",
    "desc": "The code displays the first few rows of the `X_train` series to give an initial overview of the preprocessed text data in the training set.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99975497,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Exploratory Data Analysis",
    "desc": "The code displays the first few entries of the `y_train` series to give an initial look at the target values in the training set.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9997508,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Exploratory Data Analysis",
    "desc": "The code prints the text of the third tweet (index 2) in the `train` DataFrame to show an example of the data before additional preprocessing.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.8827919,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "print('First sample before preprocessing: \\n', train['text'].values[2], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Exploratory Data Analysis",
    "desc": "The code prints the tokenized and padded sequence of the third tweet (index 2) in the `x_train` array to show an example of the data after preprocessing.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.8363421,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "print('First sample after preprocessing: \\n', x_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Exploratory Data Analysis",
    "desc": "The code displays the first few rows of the test DataFrame to provide an initial overview of the test dataset structure and content.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9997483,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet returns the dimensions of the test DataFrame, providing information about the number of rows and columns in the test dataset.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9995401,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Exploratory Data Analysis",
    "desc": "The code displays the first few rows of the sample submission DataFrame to provide an initial overview of its structure and content.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99975115,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "submission_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Exploratory Data Analysis",
    "desc": "The code snippet returns the dimensions of the sample submission DataFrame, providing information about the number of rows and columns in the template.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9995115,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "submission_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code prints the shape (number of rows and columns) of the training data DataFrame.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.5846203,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "print('Training Data Shape',train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code prints the shape (number of rows and columns) of the test data DataFrame.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9990657,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "print('Testing Data Shape',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Exploratory Data Analysis",
    "desc": "This code prints the shape (number of rows and columns) of the sample submission DataFrame and displays its first few rows.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99701464,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "print('Sub Sample Data Shape',sub_sample.shape)\n",
    "sub_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code counts and displays the frequency of each unique value in the 'target' column of the training data DataFrame.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9995309,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Exploratory Data Analysis",
    "desc": "This code groups the training data by the 'target' column, counts the number of 'text' entries for each target, and labels each row as 'Disaster Tweet' or 'Non Disaster Tweet' based on the target value.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9951746,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "temp = train.groupby('target').count()['text'].reset_index()\n",
    "temp['label'] = temp['target'].apply(lambda x : 'Disaster Tweet' if x==1 else 'Non Disaster Tweet')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Exploratory Data Analysis",
    "desc": "This code calculates and displays the number of missing values in each column of the training data DataFrame.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9989716,
    "start_cell": false,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Exploratory Data Analysis",
    "desc": "This code calculates and displays the number of missing values in each column of the test data DataFrame.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99903905,
    "start_cell": false,
    "subclass": "count_missing_values",
    "subclass_id": 39
   },
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Exploratory Data Analysis",
    "desc": "This code calculates and displays the number of unique values in the 'keyword' column of both the training and test data DataFrames.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.97303426,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "train.keyword.nunique(),test.keyword.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Exploratory Data Analysis",
    "desc": "This code calculates and displays the number of unique values in the 'location' column of both the training and test data DataFrames.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9729524,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "train.location.nunique(),test.location.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet displays the first four rows of the training dataset to provide a preview of its structure and content.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9997639,
    "start_cell": true,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "# Head\n",
    "train_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 3,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet prints the number of rows and columns in both the training and testing datasets to understand their dimensions.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99787784,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "# Datasets shape\n",
    "print('Train dataset:\\n{} rows\\n{} columns'.format(train_df.shape[0], train_df.shape[1]))\n",
    "print('\\nTest dataset:\\n{} rows\\n{} columns'.format(test_df.shape[0], test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet counts and displays the number of disaster and non-disaster tweets in the training dataset using a Markdown formatted message.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9994272,
    "start_cell": false,
    "subclass": "count_values",
    "subclass_id": 72
   },
   "outputs": [],
   "source": [
    "# Count target values by its factor\n",
    "x = train_df.target.value_counts()\n",
    "md(\"The amount disaster tweets is {}. And the amount for not disaster is {}.\".\n",
    "     format(x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet calculates the proportion of disaster and non-disaster tweets as a percentage of the total and displays these percentages using a Markdown formatted message.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.4878066,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "proportion = x/train_df.shape[0] # Compute the tweets proportion by target\n",
    "md(\"The percentual of disaster tweets is {}%, and {}% for not disaster.\".\n",
    "     format(round(proportion[1]*100,0),round(proportion[0]*100, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet generates summary statistics (count, mean, std, min, 25%, 50%, 75%, max) for the length of tweets, grouped by the target variable (disaster or not).",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9994091,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "# Text length summary by target\n",
    "train_df.groupby(['target']).length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet generates summary statistics (count, mean, std, min, 25%, 50%, 75%, max) for the number of words in tweets, grouped by the target variable (disaster or not).",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9993376,
    "start_cell": false,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "# number of words summary by target\n",
    "train_df.groupby(['target']).num_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Exploratory Data Analysis",
    "desc": "This code snippet calculates the number of unique words in the training dataset's text and displays this count using a Markdown formatted message.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.86991686,
    "start_cell": false,
    "subclass": "count_unique_values",
    "subclass_id": 54
   },
   "outputs": [],
   "source": [
    "# Unique words\n",
    "text_values = train_df[\"text\"]\n",
    "\n",
    "counter = counter_word(text_values)\n",
    "md(\"The training dataset has {} unique words\".format(len(counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet sets up a ShuffleSplit cross-validator and defines a function to evaluate a classifier's accuracy using cross-validation, printing the mean and standard deviation of the accuracy scores.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.7686682939529419,
    "start_cell": true,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "shuffle_validator = cross_validation.ShuffleSplit(len(X_train), n_iter=20, test_size=0.2, random_state=0)\n",
    "def test_classifier(clf):\n",
    "    scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=shuffle_validator)\n",
    "    print(\"Accuracy: %0.4f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Hyperparam Tuning",
    "desc": "This code defines a range of hyperparameters, specifying the number of neighbors and weights to be considered for tuning the k-nearest neighbors classifier.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9968416690826416,
    "start_cell": false,
    "subclass": "define_search_space",
    "subclass_id": 5
   },
   "outputs": [],
   "source": [
    "k_range = range(1, 40)\n",
    "param_grid = dict(n_neighbors=list(k_range),weights = [\"uniform\", \"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Hyperparam Tuning",
    "desc": "This code sets up a k-nearest neighbors classifier with a grid search for hyperparameter optimization, fits it to the training data, and prints the best accuracy score and corresponding parameters.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9888269901275636,
    "start_cell": false,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "print(\"Best params: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet performs hyperparameter tuning on the minimum samples per leaf for a RandomForestClassifier, evaluates the model on both training and test datasets for various values, and prints the top 5 configurations with the highest test accuracy.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.92607117,
    "start_cell": true,
    "subclass": "find_best_params",
    "subclass_id": 2
   },
   "outputs": [],
   "source": [
    "leafsizes = []\n",
    "\n",
    "for x in range(1,110,2):\n",
    "    RFCmodel = RandomForestClassifier(n_estimators =100, min_samples_leaf= x)\n",
    "    \n",
    "    RFCmodel = RFCmodel.fit(X_train, y_train)\n",
    "    RFC_acc = (RFCmodel.score(X_train, y_train))\n",
    "    \n",
    "    predicted = RFCmodel.predict(X_test)\n",
    "    RFC_test_acc = metrics.accuracy_score(y_test, predicted)\n",
    "    \n",
    "    diff_mag = (((RFC_acc - RFC_test_acc)**2)**0.5)\n",
    "    \n",
    "    leafsizes.append((\"leaf {0}\".format(x), RFC_acc, RFC_test_acc, diff_mag))\n",
    "\n",
    "leafsizes = list(reversed(sorted(leafsizes, key=lambda tup: tup[2])))\n",
    "    \n",
    "for i in range(5):\n",
    "    print(leafsizes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet, though commented out, indicates the intention to perform hyperparameter tuning using GridSearchCV to find the best parameters for a RandomForestClassifier based on a predefined grid of parameters and cross-validation.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.49064222,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "#grid_1 = { \"n_estimators\"      : [100],\n",
    "#               \"criterion\"         : [\"gini\", \"entropy\"],\n",
    "#               \"max_features\"      : ['sqrt','log2',0.2,0.5,0.8],\n",
    "#               \"max_depth\"         : [3,4,6,10],\n",
    "#               \"min_samples_split\" : [2, 5, 20,50] }\n",
    "#RF=RandomForestClassifier()\n",
    "#grid_search = sklearn.model_selection.GridSearchCV(RF, grid_1, n_jobs=-1, cv=5)\n",
    "#grid_search.fit(X_train, Y_train)\n",
    "#grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet, though commented out, indicates the intention to perform hyperparameter tuning using GridSearchCV to find the best parameters for a GradientBoostingClassifier based on a predefined grid of parameters and cross-validation.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.7487272,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#grid_2 = { \"loss\"          : [\"deviance\",\"exponential\"],\n",
    " #              \"n_estimators\"  : [100],\n",
    " #             \"max_features\"      : ['sqrt','log2',0.2,0.5,0.8]}\n",
    "#GB=GradientBoostingClassifier()\n",
    "#grid_search = sklearn.model_selection.GridSearchCV(GB, grid_2, n_jobs=-1, cv=5)\n",
    "#grid_search.fit(X_train, Y_train)\n",
    "#grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet, though commented out, indicates the intention to perform hyperparameter tuning using GridSearchCV to find the best parameters for an ExtraTreesClassifier based on a predefined grid of parameters and cross-validation.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.8742281,
    "start_cell": false,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "#grid_3 = { \"n_estimators\" : [100],\n",
    "              # \"criterion\"         : [\"gini\", \"entropy\"],\n",
    "             #  \"max_features\"      : ['sqrt','log2',0.2,0.5,0.8],\n",
    "             #  \"max_depth\"         : [4,7,10],\n",
    "            #   \"min_samples_split\" : [2, 5, 10] }\n",
    "#ET=ExtraTreesClassifier()\n",
    "#grid_search = sklearn.model_selection.GridSearchCV(ET, grid_3, n_jobs=-1, cv=5)\n",
    "#grid_search.fit(X_train, Y_train)\n",
    "#grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet, though commented out, indicates the intention to perform hyperparameter tuning using GridSearchCV to find the best parameters for an AdaBoostClassifier based on a predefined grid of parameters and cross-validation.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9901468,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#grid_4 = { \"n_estimators\"      : [100,150,200],\n",
    "               #\"algorithm\"  : ['SAMME','SAMME.R'] }\n",
    "#AB=AdaBoostClassifier()\n",
    "#grid_search = sklearn.model_selection.GridSearchCV(AB, grid_4, n_jobs=-1, cv=5)\n",
    "#grid_search.fit(X_train, Y_train)\n",
    "#grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet, though commented out, indicates the intention to perform hyperparameter tuning using GridSearchCV to find the best parameters for a KNeighborsClassifier based on a predefined grid of parameters and cross-validation.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.5392715,
    "start_cell": false,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "#grid_5 = { \"n_neighbors\"      : [3,5,7],\n",
    "               #\"weights\"  : ['uniform','distance'] }\n",
    "#KNN=sklearn.neighbors.KNeighborsClassifier()\n",
    "#grid_search = sklearn.model_selection.GridSearchCV(KNN, grid_5, n_jobs=-1, cv=5)\n",
    "#grid_search.fit(X_train, Y_train)\n",
    "#grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet performs Recursive Feature Elimination with Cross-Validation (RFECV) to select the optimal number of features for the model, while the commented lines are intended to display the score and a plot of cross-validation scores versus the number of features selected.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.57596904,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet applies Recursive Feature Elimination with Cross-Validation (RFECV) using the LogisticRegression model to select the optimal number of features and possibly visualize the cross-validation scores against the number of features (if the commented parts are uncommented).",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.5759690403938293,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Hyperparam Tuning",
    "desc": "This code initializes a `LogisticRegression` model. ",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9948827028274536,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Hyperparam Tuning",
    "desc": "This code performs recursive feature elimination with cross-validation (RFECV) to select the optimal number of features for the `LogisticRegression` model by evaluating cross-validation scores, and includes commented plotting of the results.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.5759690403938293,
    "start_cell": false,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Hyperparam Tuning",
    "desc": "The commented out code snippet uses Recursive Feature Elimination with Cross-Validation (RFECV) to identify the optimal number of features for the Logistic Regression model and plots the number of features against cross-validation scores.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.57596904,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes and fits an RFECV (Recursive Feature Elimination with Cross-Validation) to optimize the number of features for the given model, evaluating the feature's importance through cross-validation.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.57596904,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet performs Recursive Feature Elimination with Cross-Validation (RFECV) to select the optimal number of features for the model, aiming to improve accuracy, and includes commented-out code for evaluating and visualizing the results.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.5759690403938293,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet uses Recursive Feature Elimination with Cross-Validation (RFECV) to select optimal features for the model, which involves recursively removing features and using cross-validation to evaluate model accuracy.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.5759690403938293,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet performs recursive feature elimination with cross-validation (RFECV) to select the optimal number of features for the model, and includes commented-out code to display the results and plot the cross-validation scores against the number of features.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.57596904,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a RandomForestClassifier with 150 estimators for subsequent training.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.998266577720642,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a Support Vector Classifier (SVC) with a regularization parameter \\( C \\) set to 100 for subsequent model training.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.997304916381836,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC(C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a GradientBoostingClassifier without specifying any hyperparameters for subsequent model training.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.997496783733368,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a KNeighborsClassifier with the number of neighbors set to 3 for subsequent model training.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.998189389705658,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a Gaussian Naive Bayes (GaussianNB) classifier for subsequent model training.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9967920184135436,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet performs recursive feature elimination with cross-validation (RFECV) to identify the optimal number of features, fits the model using this approach, and includes code (currently commented out) to print scores and plot the number of features versus cross-validation scores.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.57596904,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "The snippet initializes and fits an RFECV object with StratifiedKFold cross-validation to perform recursive feature elimination, optimizing the number of features for the model based on cross-validation accuracy.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.57596904,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet performs cross-validation to evaluate the accuracy of a Decision Tree model with varying `max_depth` parameters, and it prints out the averaged accuracy for each depth to assist in selecting the optimal maximum depth for the model.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.7882641,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10)            # Desired number of Cross Validation folds\n",
    "accuracies = list()\n",
    "max_attributes = len(list(test))\n",
    "depth_range = range(1, max_attributes + 1)\n",
    "\n",
    "# Testing max_depths from 1 to max attributes\n",
    "# Uncomment prints for details about each Cross Validation pass\n",
    "for depth in depth_range:\n",
    "    fold_accuracy = []\n",
    "    tree_model = tree.DecisionTreeClassifier(max_depth = depth)\n",
    "    # print(\"Current max depth: \", depth, \"\\n\")\n",
    "    for train_fold, valid_fold in cv.split(train):\n",
    "        f_train = train.loc[train_fold] # Extract train data with cv indices\n",
    "        f_valid = train.loc[valid_fold] # Extract valid data with cv indices\n",
    "\n",
    "        model = tree_model.fit(X = f_train.drop(['Survived'], axis=1), \n",
    "                               y = f_train[\"Survived\"]) # We fit the model with the fold train data\n",
    "        valid_acc = model.score(X = f_valid.drop(['Survived'], axis=1), \n",
    "                                y = f_valid[\"Survived\"])# We calculate accuracy with the fold validation data\n",
    "        fold_accuracy.append(valid_acc)\n",
    "\n",
    "    avg = sum(fold_accuracy)/len(fold_accuracy)\n",
    "    accuracies.append(avg)\n",
    "    # print(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\n",
    "    # print(\"Average accuracy: \", avg)\n",
    "    # print(\"\\n\")\n",
    "    \n",
    "# Just to show results conveniently\n",
    "df = pd.DataFrame({\"Max Depth\": depth_range, \"Average Accuracy\": accuracies})\n",
    "df = df[[\"Max Depth\", \"Average Accuracy\"]]\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a RandomForestClassifier with 100 estimators as the base model for subsequent training.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9982681274414062,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a Support Vector Classifier (SVC) model for subsequent training.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.68790203332901,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a Gradient Boosting Classifier model for subsequent training.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.997496783733368,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a KNeighborsClassifier model with 3 neighbors as a base configuration for subsequent training steps.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.998189389705658,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a Gaussian Naive Bayes model for subsequent training.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9967920184135436,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet initializes a Logistic Regression model for subsequent training.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9948827028274536,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "This code performs Recursive Feature Elimination with Cross-Validation (RFECV) to determine the optimal number of features and their importance using the given model and a stratified K-fold cross-validation approach.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.57596904,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet initializes a RandomForestClassifier model with 100 decision trees (n_estimators).",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9982681274414062,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet initializes a Support Vector Classifier (SVC) model with default hyperparameters.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.68790203332901,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet initializes a GradientBoostingClassifier model with default hyperparameters.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.997496783733368,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet initializes a KNeighborsClassifier model with the number of neighbors set to 3.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.998189389705658,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet initializes a Gaussian Naive Bayes (GaussianNB) model with default hyperparameters.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9967920184135436,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet initializes a LogisticRegression model with default hyperparameters.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9948827028274536,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet performs Recursive Feature Elimination with Cross-Validation (RFECV) to optimize the feature set for the model, prints the model scores for the training and validation sets, the optimal number of features, and plots the cross-validation scores against the number of features selected.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9189477562904358,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "#Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel( \"Number of features selected\" )\n",
    "plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Hyperparam Tuning",
    "desc": "This code uses Recursive Feature Elimination with Cross-Validation (RFECV) to identify the optimal number of features for the model and evaluates its performance on both training and validation datasets.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.720703,
    "start_cell": true,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "print( \"Optimal number of features : %d\" % rfecv.n_features_ )rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "#plt.figure()\n",
    "#plt.xlabel( \"Number of features selected\" )\n",
    "#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet employs Recursive Feature Elimination with Cross-Validation (RFECV) to select the optimal number of features for the model by fitting it on the training data, scoring it on both training and validation datasets, and plotting the number of features versus cross-validation scores.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9381272,
    "start_cell": true,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel( \"Number of features selected\" )\n",
    "plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet defines a function that calculates the average AUC score from cross-validation using StratifiedKFold on the provided training features and target labels, returning the mean AUC score.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9433515,
    "start_cell": true,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "#taken from https://www.kaggle.com/vishalsiram50/fine-tuning-bert-88-accuracy\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_auc_CV(model,X_train,Y_train):\n",
    "    \"\"\"\n",
    "    Return the average AUC score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    # Get AUC scores\n",
    "    auc = cross_val_score(\n",
    "        model, X_train, Y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 43,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet, if uncommented, would evaluate the Multinomial Naive Bayes classifier's performance with different values of the smoothing parameter `alpha`, calculating the average AUC score through cross-validation, fitting the model, and printing the classification report for each `alpha` value.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.2399163,
    "start_cell": false,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "#from sklearn.metrics import classification_report\n",
    "#for alpha in [0.001,0.1,1]:\n",
    "#    print(alpha)\n",
    "#    clf = MNB(alpha = alpha)\n",
    "#    auc = get_auc_CV(clf,X_train,Y_train)\n",
    "#    print(auc)\n",
    "#    clf.fit(X_train,Y_train)\n",
    "#    Y_pred_train = clf.predict(X_train)\n",
    "#    print(classification_report(Y_train,Y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet outlines a procedure for performing a grid search to determine the best batch size and number of neurons for training the model, by iterating over random configurations and recording the validation accuracy.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9034748,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# # Grid search the best configuration\n",
    "# list_batch_size = [2,4,8,16,32,64,128,256]\n",
    "# list_neuron = [8,16,32,64,128,256,512,1024,2048]\n",
    "\n",
    "# result = []\n",
    "# test_run = 10\n",
    "\n",
    "# for i in range(test_run):\n",
    "    \n",
    "#     batch_size = random.choice(list_batch_size)\n",
    "#     neuron = random.choice(list_neuron)\n",
    "#     epochs = 8\n",
    "    \n",
    "#     print('-----------------------------------------------------------------')\n",
    "#     print('[current config]:',[batch_size,neuron])\n",
    "#     #val_accuracy = []\n",
    "    \n",
    "#     for j in range(2):\n",
    "#         print('[current iteration]:',j+1)\n",
    "#         model = get_model(neuron)\n",
    "#         history = model.fit(\n",
    "#                             X_train, \n",
    "#                             y_train, \n",
    "#                             batch_size=batch_size, \n",
    "#                             epochs=epochs, \n",
    "#                             verbose=1,\n",
    "#                             validation_split=0.1\n",
    "#                             )\n",
    "\n",
    "#         for ep in range(epochs):\n",
    "#             result.append([\n",
    "#                             [batch_size,\n",
    "#                             ep+1,\n",
    "#                             neuron],\n",
    "#                             history.history.get('val_acc')[ep],\n",
    "#                             history.history.get('acc')[ep]\n",
    "#                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet defines hyperparameters and configurations for fine-tuning, pooling, and RNN settings to be used in the model training process.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.7919986,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "configs = {\"pool_configs\": {\"fine_tune_mode\": \"linear\", \"pooling\": \"mean\"},\n",
    "\"rnn_configs\": {\"hidden_size\": 512,\n",
    "                                   \"rnn_layers\": 1,\n",
    "                                   \"reproject_words\": True,\n",
    "                                   \"reproject_words_dimension\": 256,\n",
    "                                   \"bidirectional\": False,\n",
    "                                   \"dropout\": 0.4,\n",
    "                                   \"word_dropout\": 0.0,\n",
    "                                   \"locked_dropout\": 0.0,\n",
    "                                   \"rnn_type\": \"GRU\",\n",
    "                                   \"fine_tune\": True, }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Hyperparam Tuning",
    "desc": "This code snippet defines the hyperparameters and configurations for finding the optimal learning rate and then uses the `SequenceClassifierTrainer` to determine the learning rate.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.8091228,
    "start_cell": false,
    "subclass": "init_hyperparams",
    "subclass_id": 59
   },
   "outputs": [],
   "source": [
    "sc_lr_configs = {\n",
    "        \"output_dir\": OUTPUT_DIR,\n",
    "        \"file_name\": \"learning_rate.tsv\",\n",
    "        \"start_learning_rate\": 1e-8,\n",
    "        \"end_learning_rate\": 10,\n",
    "        \"iterations\": 100,\n",
    "        \"mini_batch_size\": 32,\n",
    "        \"stop_early\": True,\n",
    "        \"smoothing_factor\": 0.8,\n",
    "        \"plot_learning_rate\": True,\n",
    "}\n",
    "learning_rate = sc_trainer.find_learning_rate(**sc_lr_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Hyperparam Tuning",
    "desc": "This code defines a configuration class with various hyperparameters for a neural network model, including settings for embedding size, hidden layers, learning rate, batch size, and dropout rate.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.8460937,
    "start_cell": true,
    "subclass": "init_hyperparams",
    "subclass_id": 59
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    embed_size = 300\n",
    "    hidden_layers = 2\n",
    "    hidden_size = 128\n",
    "    bidirectional = True\n",
    "    output_size = 1\n",
    "    max_epochs = 30\n",
    "    lr = 0.05\n",
    "    batch_size = 128\n",
    "    dropout_keep = 0.2\n",
    "    max_sen_len = None # Sequence length for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Hyperparam Tuning",
    "desc": "Defines a function to build a Bidirectional LSTM neural network model with several layers including embedding, spatial dropout, global max pooling, batch normalization, dense layers with ReLU activation, and a final sigmoid activation for binary classification.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.99327105,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "def build_BLSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=embedding_matrix.shape[0], \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights = [embedding_matrix], \n",
    "                        input_length=MAX_LEN))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(LSTM(MAX_LEN, return_sequences = True, recurrent_dropout=0.2)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(MAX_LEN, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(MAX_LEN, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Hyperparam Tuning",
    "desc": "Builds the Bidirectional LSTM model using the previously defined function and displays a summary of the model architecture, including layers and parameters.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9933675,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model = build_BLSTM()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Hyperparam Tuning",
    "desc": "Sets up model checkpoints to save the best model based on validation loss and a learning rate reduction strategy that lowers the learning rate when the validation loss plateaus.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.98103344,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5', \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', \n",
    "    factor = 0.2, \n",
    "    verbose = 1, \n",
    "    patience = 5,                        \n",
    "    min_lr = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Hyperparam Tuning",
    "desc": "The code defines a grid of hyperparameters for tuning a Support Vector Classifier (SVC), which includes different values for \"gamma\", \"C\", and \"kernel\" options.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99403256,
    "start_cell": true,
    "subclass": "define_search_space",
    "subclass_id": 5
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"gamma\" : [0.001,0.01,1,10,100],\n",
    "    \"C\":[0.001,0.01,1,10,100],\n",
    "    'kernel' : ['poly', \"linear\", 'sigmoid', 'rbf']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Hyperparam Tuning",
    "desc": "The code initializes a Support Vector Classifier (SVC) and uses GridSearchCV to perform cross-validated hyperparameter tuning on the training data using the defined parameter grid, eventually finding and outputting the best parameters based on the F1 score.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99040365,
    "start_cell": false,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "grid_searcher = GridSearchCV(svc, param_grid, cv=5, scoring='f1')\n",
    "grid_searcher.fit(X_train, y_train)\n",
    "grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet defines and sets hyperparameters for batch size, number of epochs, vocabulary size, maximum sequence length, and embedding dimension to be used in the machine learning model.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99455345,
    "start_cell": true,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "VOCAB_SIZE = 25000\n",
    "MAX_LEN =90\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Hyperparam Tuning",
    "desc": "The code snippet, currently commented out, is intended to set model parameters including the number of filters, kernel size, and hidden layer dimensions for a neural network.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.991325,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Model Parameters \n",
    "\n",
    "NUM_FILTERS = 250\n",
    "KERNEL_SIZE = 4\n",
    "HIDDEN_DIMS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Model Evaluation",
    "desc": "This code snippet extracts the features from the test data and uses the trained RandomForestClassifier to predict the survival values, converting the predictions to integers.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99282104,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "# Get the test data features, skipping the first column 'PassengerId'\n",
    "test_x = test_data[:, 1:]\n",
    "\n",
    "# Predict the Survival values for the test data\n",
    "test_y = map(int, clf.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 44,
    "class": "Model Evaluation",
    "desc": "This code snippet retrains the RandomForestClassifier on the training split, makes predictions on the test split, and prints the accuracy of the model using the `accuracy_score` metric.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.4565435,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "clf = clf.fit(train_x, train_y)\n",
    "predict_y = clf.predict(test_x)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (\"Accuracy = %.2f\" % (accuracy_score(test_y, predict_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 45,
    "class": "Model Evaluation",
    "desc": "This code snippet calculates and prints the model score, generates a confusion matrix for the predictions, and prints the confusion matrix in a formatted manner to show the comparison between actual and predicted values.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.997529,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "model_score = clf.score(test_x, test_y)\n",
    "print (\"Model Score %.2f \\n\" % (model_score))\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(test_y, predict_y)\n",
    "print (\"Confusion Matrix \", confusion_matrix)\n",
    "\n",
    "print (\"          Predicted\")\n",
    "print (\"         |  0  |  1  |\")\n",
    "print (\"         |-----|-----|\")\n",
    "print (\"       0 | %3d | %3d |\" % (confusion_matrix[0, 0],\n",
    "                                   confusion_matrix[0, 1]))\n",
    "print (\"Actual   |-----|-----|\")\n",
    "print (\"       1 | %3d | %3d |\" % (confusion_matrix[1, 0],\n",
    "                                   confusion_matrix[1, 1]))\n",
    "print (\"         |-----|-----|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 46,
    "class": "Model Evaluation",
    "desc": "This code snippet prints the classification report, which includes precision, recall, f1-score, and support for each class ('Not Survived' and 'Survived') based on the predictions.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.8918813,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, \n",
    "                            predict_y, \n",
    "                            target_names=['Not Survived', 'Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Evaluation",
    "desc": "This code tests the Logistic Regression classifier using the previously defined `test_classifier` function, which evaluates the model using cross-validation and prints the accuracy scores.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.981886625289917,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "lrcls = LogisticRegression()\n",
    "test_classifier(lrcls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Evaluation",
    "desc": "This code tests the Support Vector Classifier (SVC) using the previously defined `test_classifier` function, which evaluates the model using cross-validation and prints the accuracy scores.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.3318977355957031,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "svc = svm.SVC()\n",
    "test_classifier(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Model Evaluation",
    "desc": "This code defines an ensemble voting classifier combining Decision Tree, Random Forest, and Gradient Boosting classifiers, and evaluates its accuracy using the `test_classifier` function.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.8984527587890625,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "eclf = ske.VotingClassifier([('dt', clf_dt), ('rf', clf_rf), ('gb', clf_gb)])\n",
    "test_classifier(eclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 49,
    "class": "Model Evaluation",
    "desc": "The code snippet concatenates all the predictions from the cross-validation, maps the predictions to binary outcomes (1 or 0), and calculates the accuracy of the predictions by comparing them to the actual 'Survived' values in the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.52186865,
    "start_cell": true,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "\n",
    "accuracy=sum(titanic[\"Survived\"]==predictions)/len(titanic[\"Survived\"])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 50,
    "class": "Model Evaluation",
    "desc": "The code snippet initializes a Logistic Regression model, sets up ShuffleSplit cross-validation with 10 splits, computes F1 scores for each cross-validation split using the provided predictor variables, and prints the mean F1 score.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.8850502,
    "start_cell": false,
    "subclass": "import_modules",
    "subclass_id": 22
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\",\"Deck\",\"Age\",\n",
    "              \"FsizeD\", \"NlengthD\",\"Title\",\"Parch\"]\n",
    "\n",
    "# Initialize our algorithm\n",
    "lr = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=50)\n",
    "\n",
    "scores = cross_val_score(lr, titanic[predictors], \n",
    "                                          titanic[\"Survived\"],scoring='f1', cv=cv)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 51,
    "class": "Model Evaluation",
    "desc": "The code snippet initializes a Random Forest Classifier with specified parameters, sets up K-Fold cross-validation and ShuffleSplit, generates cross-validated predictions using the provided predictor variables, calculates F1 scores for each fold, and prints the mean F1 score.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9762013,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "import numpy as np\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\",\n",
    "              \"Fare\",\"NlengthD\",\"NameLength\", \"FsizeD\", \"Title\",\"Deck\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, \n",
    "                            min_samples_leaf=1)\n",
    "kf = KFold(titanic.shape[0], n_folds=5, random_state=1)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=50)\n",
    "\n",
    "predictions = cross_validation.cross_val_predict(rf, titanic[predictors],titanic[\"Survived\"],cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "scores = cross_val_score(rf, titanic[predictors], titanic[\"Survived\"],\n",
    "                                          scoring='f1', cv=kf)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 55,
    "class": "Model Evaluation",
    "desc": "The code snippet initializes a Logistic Regression model, sets up ShuffleSplit cross-validation, computes F1 scores for each cross-validation fold using the specified predictor variables, and prints the mean F1 score.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.38884497,
    "start_cell": false,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\",\"NlengthD\",\n",
    "              \"FsizeD\", \"Title\",\"Deck\"]\n",
    "\n",
    "# Initialize our algorithm\n",
    "lr = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  \n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=50)\n",
    "scores = cross_val_score(lr, titanic[predictors], titanic[\"Survived\"], scoring='f1',cv=cv)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 56,
    "class": "Model Evaluation",
    "desc": "The code snippet initializes an AdaBoostClassifier, trains it on the 'titanic' dataset using the specified predictor variables, sets up ShuffleSplit cross-validation, and computes the F1 scores for each fold, printing the mean F1 score.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.77291787,
    "start_cell": false,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\",\"NlengthD\",\n",
    "              \"FsizeD\", \"Title\",\"Deck\",\"TicketNumber\"]\n",
    "adb=AdaBoostClassifier()\n",
    "adb.fit(titanic[predictors],titanic[\"Survived\"])\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=50)\n",
    "scores = cross_val_score(adb, titanic[predictors], titanic[\"Survived\"], scoring='f1',cv=cv)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Model Evaluation",
    "desc": "This code snippet uses the trained logistic regression model to predict the outcomes on the test set and then calculates and prints the accuracy score of these predictions.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9677555,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "# check the accuracy on the test set\n",
    "predicted = LRmodel.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Model Evaluation",
    "desc": "This code snippet evaluates the logistic regression model using 10-fold cross-validation, prints the accuracy scores for each fold, and calculates the mean accuracy.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.96577716,
    "start_cell": false,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\n",
    "\n",
    "print(\"List of Scores for CV Folds:\")\n",
    "[print(score) for score in scores]\n",
    "\n",
    "print(\"\\nMean Accuracy\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Evaluation",
    "desc": "This code snippet trains a RandomForestClassifier with the chosen `min_samples_leaf` parameter, evaluates the model using 10-fold cross-validation, prints the accuracy scores for each fold and the mean accuracy, and prints the feature importances.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.58878803,
    "start_cell": false,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "# assign RFC model with the chosen  min_samples_leaf\n",
    "RFCmodel = RandomForestClassifier(n_estimators =100, min_samples_leaf= 15)\n",
    "RFCmodel = RFCmodel.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(RandomForestClassifier(n_estimators =100, min_samples_leaf= 15), X, y, scoring='accuracy', cv=10)\n",
    "\n",
    "print(\"List of Scores for CV Folds:\")\n",
    "[print(score) for score in scores]\n",
    "\n",
    "print(\"\\nMean Accuracy\")\n",
    "print(scores.mean())\n",
    "\n",
    "print(RFCmodel.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "The code snippet evaluates the trained model by printing the accuracy scores on both the training and validation sets.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Evaluation",
    "desc": "The code snippet evaluates the LogisticRegression model by printing its accuracy scores on both the training and validation datasets.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9979725480079652,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Evaluation",
    "desc": "This code evaluates the performance of the trained `LogisticRegression` model by printing its accuracy on both the training and validation datasets.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9979725480079652,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Evaluation",
    "desc": "The code snippet evaluates the Logistic Regression model by printing its accuracy score on both the training and validation datasets.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "This code snippet evaluates the trained model by printing its accuracy scores on both the training and validation datasets.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "This code snippet prints the accuracy scores of the trained model on the training data and the validation data.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9979725480079652,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "This code snippet prints the accuracy scores of the trained model on both the training dataset and the validation dataset.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9979725480079652,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "This code snippet evaluates the trained model by printing its accuracy scores on both the training and validation datasets.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Evaluation",
    "desc": "This code snippet evaluates the trained GradientBoostingClassifier by printing its accuracy scores on both the training and validation datasets.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9979725480079652,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Model Evaluation",
    "desc": "This code snippet performs Recursive Feature Elimination with Cross-Validation (RFECV) on the trained model to determine the optimal number of features, evaluates the model's performance, and visualizes the relationship between the number of features selected and cross-validation scores.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9381272196769714,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel( \"Number of features selected\" )\n",
    "plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "This code snippet prints the accuracy scores of the model on both the training and validation datasets to evaluate its performance.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "The snippet evaluates the trained model's accuracy on both the training and validation datasets.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Model Evaluation",
    "desc": "This code snippet calculates the accuracy of the trained Decision Tree model on the training dataset and displays it as a percentage.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9983393,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Model Evaluation",
    "desc": "This code snippet evaluates the trained Logistic Regression model by printing its accuracy score on both the training and validation datasets.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9979725480079652,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "This code prints the training and validation accuracy scores of the fitted machine learning model to evaluate its performance.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "The code snippet evaluates the trained model by printing its accuracy score on both the training set and the validation set.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9979725480079652,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Model Evaluation",
    "desc": "This code calculates and prints the accuracy of the model on both the training and validation datasets to evaluate its performance.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "This code snippet prints the accuracy score of the model on both the training and validation datasets.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99797255,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Model Evaluation",
    "desc": "This code snippet generates predictions on the validation set using the trained neural network model, computes and visualizes the confusion matrix using a heatmap to evaluate the model's performance.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.95411265,
    "start_cell": true,
    "subclass": "heatmap",
    "subclass_id": 80
   },
   "outputs": [],
   "source": [
    "model_val_predict = model.predict_classes(tv_val_reviews)\n",
    "cm = confusion_matrix(y_val, model_val_predict)\n",
    "cm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Model Evaluation",
    "desc": "The code evaluates the trained model on the last 500 samples of the training data and labels to assess its performance, outputting the loss and accuracy metrics.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.87624604,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "model.evaluate(store_train[-500:,:,:], train_Y[-500:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Model Evaluation",
    "desc": "The code uses the trained model to predict labels for the test dataset `store_test`, then converts these predictions into binary labels based on a threshold of 0.5 and stores them in the `test_label` list.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99062234,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "test_Y=model.predict(store_test)\n",
    "\n",
    "test_label=[]\n",
    "\n",
    "for i in range(test_Y.shape[0]):\n",
    "    if test_Y[i,1]>=0.5:\n",
    "        test_label.append(1)\n",
    "    else:\n",
    "        test_label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Evaluation",
    "desc": "This snippet generates predictions on the training data using the trained LSTM model, rounds these predictions to the nearest integer, and converts them to integer type.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9891784,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "train_pred_GloVe = model.predict(train)\n",
    "train_pred_GloVe_int = train_pred_GloVe.round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 46,
    "class": "Model Evaluation",
    "desc": "This code snippet calculates the average AUC score for the trained XGBoost classifier using cross-validation on the training dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.56113243,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "get_auc_CV(xgb,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 49,
    "class": "Model Evaluation",
    "desc": "This code snippet, if uncommented, would use the trained Multinomial Naive Bayes classifier to make predictions on the testing dataset.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9974981,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#test_prediction = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Model Evaluation",
    "desc": "This code snippet predicts the training set outcomes, rounds and reshapes the predictions, then compares them with actual results, and identifies the incorrectly predicted instances.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.90300536,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "# Compare training set pred with the actual result \n",
    "y_pred = model.predict(X_train, verbose=True).flatten()\n",
    "y_pred = np.round(y_pred).astype(int).reshape(X_train[0].shape[0])\n",
    "\n",
    "pred = pd.DataFrame(data=y_pred, columns=['pred'])\n",
    "pred_df = pd.concat([data['text'], data_cleansed['text'], data['target'],pred], axis=1)\n",
    "incorrect = pred_df[pred_df.pred!=pred_df.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Model Evaluation",
    "desc": "This code snippet displays the DataFrame of incorrectly predicted instances created in the previous step for further inspection.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.999166,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Evaluation",
    "desc": "This code snippet predicts the outcomes for the evaluation dataset, flattens and rounds the predictions, and reshapes them to match the format of the evaluation data.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9884036,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "y_eval = model.predict(X_eval, verbose=True)\n",
    "y_eval = y_eval.flatten()\n",
    "y_eval = np.round(y_eval).astype(int).reshape(X_eval[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Model Evaluation",
    "desc": "This code snippet creates a DataFrame for the predictions of the evaluation dataset and concatenates it with the original evaluation data and the cleansed text data for further analysis.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9971635,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(data=y_eval, columns=['pred'])\n",
    "pd.concat([data_eval, data_eval_cleansed['text'], pred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Evaluation",
    "desc": "This code evaluates the trained model on the test dataset, predicts labels for each batch without calculating gradients, and stores the predictions in a list while keeping track of correct predictions for further analysis.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.7715116,
    "start_cell": true,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "ans_list = []\n",
    "\n",
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # BertForTwitterに入力\n",
    "        outputs = net_trained(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        for i in preds:\n",
    "            ans_list.append(i.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Evaluation",
    "desc": "This code prints the list of predicted labels obtained from evaluating the trained model on the test dataset.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.99965227,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "print(ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 83,
    "class": "Model Evaluation",
    "desc": "This snippet loads the saved weights of the trained BERT-based model, makes predictions on the encoded test data, and converts the predicted probabilities to integer class labels by rounding them.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.991269,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "model_BERT.load_weights('model_BERT.h5')\n",
    "test_pred_BERT = model_BERT.predict(test_input)\n",
    "test_pred_BERT_int = test_pred_BERT.round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 84,
    "class": "Model Evaluation",
    "desc": "This snippet uses the trained BERT-based model to make predictions on the encoded training data and converts the predicted probabilities into integer class labels by rounding them.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.990164,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "train_pred_BERT = model_BERT.predict(train_input)\n",
    "train_pred_BERT_int = train_pred_BERT.round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 89,
    "class": "Model Evaluation",
    "desc": "This snippet defines a function to plot a confusion matrix with percentage annotations and raw counts for the given true and predicted labels, using Seaborn's heatmap for visualization, aiding in the evaluation of the model's performance.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.39753675,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Showing Confusion Matrix #https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n",
    "def plot_cm(y_true, y_pred, title, figsize=(5,5)):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 91,
    "class": "Model Evaluation",
    "desc": "This snippet generates and displays a confusion matrix for the GloVe model predictions on the training data, visualizing the model's performance in terms of actual versus predicted classifications.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9731088,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Showing Confusion Matrix for GloVe model\n",
    "plot_cm(train_pred_GloVe_int, train['target'].values, 'Confusion matrix for GloVe model', figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 92,
    "class": "Model Evaluation",
    "desc": "This snippet generates and displays a confusion matrix for the BERT model predictions on the training data, visualizing the model's performance in terms of actual versus predicted classifications.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9722401,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Showing Confusion Matrix for BERT model - Train Data\n",
    "plot_cm(train_pred_BERT_int, train['target'].values, 'Confusion matrix for BERT model', figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Model Evaluation",
    "desc": "This code snippet sets up an `EasySequenceClassifier` using the trained model, and then uses it to predict the labels for example text, printing the predicted labels and their associated scores.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.8449867,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "from adaptnlp import EasySequenceClassifier\n",
    "# Set example text and instantiate tagger instance\n",
    "example_text = [\"that was a really bad storm! but I suppose it could have been worse\"]\n",
    "MODEL_PATH = OUTPUT_DIR + '/final-model.pt'\n",
    "\n",
    "classifier = EasySequenceClassifier()\n",
    "\n",
    "# Example prediction\n",
    "sentences = classifier.tag_text(example_text, model_name_or_path=MODEL_PATH)\n",
    "print(\"Label output:\\n\")\n",
    "for sentence in sentences:\n",
    "    print(sentence.labels)\n",
    "    print(sentence.labels[0].value)\n",
    "    print(sentence.labels[0].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Model Evaluation",
    "desc": "This code snippet uses the trained classifier to predict the 'target' labels for each text entry in the test DataFrame and stores the predicted labels in the 'target' column.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9984242,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "df_test['target'] = df_test['text'].apply(lambda x: classifier.tag_text(x, model_name_or_path=MODEL_PATH)[0].labels[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Model Evaluation",
    "desc": "This code snippet calculates the similarity between two unseen documents (represented by `words1` and `words2`) using the pre-trained Doc2Vec model. ",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.66025394,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "Pretrained_Model.docvecs.similarity_unseen_docs(Pretrained_Model,words1,words2,alpha=1,min_alpha=0.0001,steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Model Evaluation",
    "desc": "This code snippet generates predictions on the `TEST` DataFrame using the trained neural network model.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99290776,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "predict=model.predict(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Model Evaluation",
    "desc": "This code evaluates the trained Random Forest model using 10-fold cross-validation on the training data and prints the mean accuracy.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.8122686,
    "start_cell": true,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "accuracy = model_selection.cross_val_score(rf_model, \n",
    "                                           train_vectors, \n",
    "                                           y_train, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Model Evaluation",
    "desc": "This code evaluates the trained Multinomial Naive Bayes model using 10-fold cross-validation on the training data and prints the mean accuracy.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.95603037,
    "start_cell": false,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "accuracy = model_selection.cross_val_score(nb_model, \n",
    "                                           train_vectors, \n",
    "                                           y_train, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Model Evaluation",
    "desc": "This code defines a function to evaluate a model by predicting labels for batches of input data, collecting the predictions, and calculating the accuracy score.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.5542958,
    "start_cell": true,
    "subclass": "find_best_params",
    "subclass_id": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, iterator):\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "        y_pred = model(x)\n",
    "        predicted = torch.round(y_pred.cpu().data)\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_y.extend(batch.label.numpy())\n",
    "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Model Evaluation",
    "desc": "This code defines a function to generate predictions from a model by feeding it batches of data, computing predicted labels, and collecting these predictions for all batches in the iterator.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9858849,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "def predict_model(model, iterator):\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "        y_pred = model(x)\n",
    "        predicted = torch.round(y_pred.cpu().data).tolist()\n",
    "        all_preds.extend(predicted)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Model Evaluation",
    "desc": "This code uses the `predict_model` function to generate predictions for the test dataset, converts the predicted labels to integers, and computes the length of the prediction list.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.99494225,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "preds = predict_model(model, dataset.test_iterator)\n",
    "preds = [int(p[0]) for p in preds]\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Model Evaluation",
    "desc": "This code adds the predicted labels from the submission DataFrame to the target DataFrame, and then calculates and prints the classification metrics, such as precision, recall, and F1-score, to evaluate the model's performance.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.69623107,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "target_df[\"predict\"] = list(submission.target)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(target_df.target, target_df.predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Model Evaluation",
    "desc": "This code snippet initializes a prediction client for AutoML, sets it for the AutoMLWrapper instance, and retrieves predictions for the test DataFrame based on the 'text' column, printing timestamps before and after obtaining the predictions.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.97024757,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "print(f'Begin getting predictions at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "\n",
    "# Create client for prediction service.\n",
    "prediction_client = automl.PredictionServiceClient()\n",
    "amw.set_prediction_client(prediction_client)\n",
    "\n",
    "predictions_df = amw.get_predictions(nlp_test_df, \n",
    "                                     input_col_name='text', \n",
    "#                                      ground_truth_col_name='target', # we don't have ground truth in our test set\n",
    "                                     limit=None, \n",
    "                                     threshold=0.5,\n",
    "                                     verbose=False)\n",
    "\n",
    "print(f'Finished getting predictions at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Evaluation",
    "desc": "This code snippet displays the first few rows of the predictions DataFrame to provide a preview of the model's output on the test data.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9997571,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Evaluation",
    "desc": "Defines two functions: one for plotting training history metrics such as loss and accuracy, and another for calculating and printing various performance metrics, including F1-score, precision, recall, accuracy, and a classification report.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.98977226,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot(history, arr):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    for idx in range(2):\n",
    "        ax[idx].plot(history.history[arr[idx][0]])\n",
    "        ax[idx].plot(history.history[arr[idx][1]])\n",
    "        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n",
    "        ax[idx].set_xlabel('A ',fontsize=16)\n",
    "        ax[idx].set_ylabel('B',fontsize=16)\n",
    "        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)\n",
    "        \n",
    "        \n",
    "def metrics(pred_tag, y_test):\n",
    "    print(\"F1-score: \", f1_score(pred_tag, y_test))\n",
    "    print(\"Precision: \", precision_score(pred_tag, y_test))\n",
    "    print(\"Recall: \", recall_score(pred_tag, y_test))\n",
    "    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n",
    "    print(\"-\"*50)\n",
    "    print(classification_report(pred_tag, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "Plots the training and validation loss and accuracy over epochs to visualize the model's performance during training and validation.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.98810965,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "plot(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Evaluation",
    "desc": "Evaluates the trained model on the validation set, printing the loss and accuracy to measure its performance.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9961479,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Model Evaluation",
    "desc": "Generates predictions on the validation set using the trained model, and computes and prints evaluation metrics such as F1-score, precision, recall, accuracy, and a classification report.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9395055,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "preds = model.predict_classes(X_test)\n",
    "metrics(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Model Evaluation",
    "desc": "The code evaluates the trained model on the validation data and computes its accuracy.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99220455,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "_, accuracy = model.evaluate(X_val_ohe, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Model Evaluation",
    "desc": "The code evaluates the trained model on the count-based validation data and computes its accuracy.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9896338,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "_, accuracy = model.evaluate(X_val_wc, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Model Evaluation",
    "desc": "The code uses the trained Support Vector Classifier to predict the labels for the test dataset.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99424076,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "y_hat = svc.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Evaluation",
    "desc": "This code snippet evaluates the trained Logistic Regression model by calculating its accuracy score on the test set `X_test` and `y_test`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.99776566,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "#evaluating our model\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Model Evaluation",
    "desc": "This code snippet makes predictions on the `vectorized_test` data using the trained Logistic Regression classifier and stores the predicted labels in `y_pred`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9942477,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "#prediction of out test data\n",
    "y_pred=classifier.predict(vectorized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Model Evaluation",
    "desc": "The code uses the trained neural network model to make predictions on the preprocessed test data.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9943821,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Model Evaluation",
    "desc": "The code will output the predictions made by the trained model on the test dataset, showing the raw prediction values.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99975973,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Model Evaluation",
    "desc": "This code calculates and returns the accuracy score of the model predictions on the test data compared to the true labels.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9956963,
    "start_cell": true,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Model Evaluation",
    "desc": "This code computes and returns the confusion matrix for the model's predictions on the test data.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9949639,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Evaluation",
    "desc": "This code initializes a Logistic Regression model with specified hyperparameters and evaluates its performance using 7-fold cross-validation on the training data, calculating the F1 score for each fold.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.96752954,
    "start_cell": true,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=0.9,max_iter=1000,penalty='l2')\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=7, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 43,
    "class": "Model Evaluation",
    "desc": "This code snippet defines and uses a function `plot_graphs` to plot the training and validation accuracy and loss over epochs, allowing for the evaluation of the model's performance.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9954209,
    "start_cell": true,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 44,
    "class": "Model Evaluation",
    "desc": "This code snippet generates class predictions for the testing dataset using the trained model, as part of evaluating its performance on unseen data.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99458236,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "# predict_ clases because is classification problem with the split test\n",
    "predictions = model.predict_classes(testing_padded)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 45,
    "class": "Model Evaluation",
    "desc": "This code snippet defines a function `plot_cm` to generate and display a confusion matrix with percentage annotations and absolute counts, aiding in the evaluation of the model's classification performance.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.27290073,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Reference [2]\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "# Showing Confusion Matrix\n",
    "def plot_cm(y_true, y_pred, title, figsize=(5,4)):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 46,
    "class": "Model Evaluation",
    "desc": "This code snippet generates and displays the confusion matrix for the model's predictions on the testing dataset, providing a visual representation of the classification performance.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.97454166,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# Showing Confusion Matrix\n",
    "plot_cm(testing_labels,predictions, 'Confution matrix of Tweets', figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 48,
    "class": "Model Evaluation",
    "desc": "This code snippet generates predictions using the trained model on the padded sequences from the submission dataset.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9938507,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "# Apply model prediction on submission sequences\n",
    "predictions = model.predict(submission_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Model Interpretation",
    "desc": "This code snippet creates a DataFrame to display the correlation coefficients of each feature as estimated by the trained Logistic Regression model, providing insight into the importance of each feature.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.75249034,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# get Correlation Coefficient for each feature using Logistic Regression\n",
    "coeff_df = DataFrame(titanic_df.columns.delete(0))\n",
    "coeff_df.columns = ['Features']\n",
    "coeff_df[\"Coefficient Estimate\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "# preview\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Interpretation",
    "desc": "This code defines a function to print the feature importance scores of a trained classifier, sorted in descending order of importance.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9096144437789916,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "def print_feature_importance(clf):\n",
    "    imp_feat = clf.feature_importances_\n",
    "    df_feat = pd.DataFrame({ 'feature' : X_test.columns, 'importance' : imp_feat})\n",
    "    df_feat = df_feat.sort_values(by=['importance'], ascending=False)\n",
    "    print(df_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 53,
    "class": "Model Interpretation",
    "desc": "The code snippet calculates feature importances from the trained Random Forest model, sorts these importances, and visualizes them in a bar plot with error bars to show which features are most influential in the model.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99852186,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "importances=rf.feature_importances_\n",
    "std = np.std([rf.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "sorted_important_features=[]\n",
    "for i in indices:\n",
    "    sorted_important_features.append(predictors[i])\n",
    "#predictors=titanic.columns\n",
    "plt.figure()\n",
    "plt.title(\"Feature Importances By Random Forest Model\")\n",
    "plt.bar(range(np.size(predictors)), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(np.size(predictors)), sorted_important_features, rotation='vertical')\n",
    "\n",
    "plt.xlim([-1, np.size(predictors)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 54,
    "class": "Model Interpretation",
    "desc": "The code snippet uses the SelectKBest method for feature selection based on ANOVA F-value, transforms the p-values into scores for each feature, sorts these scores, and visualizes the feature importances in a bar plot to show which features are most relevant according to SelectKBest.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.42064977,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.cross_validation import KFold\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\",\n",
    " #             \"FsizeD\", \"Embarked\", \"NlengthD\",\"Deck\",\"TicketNumber\"]\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\",\n",
    "              \"Fare\",\"NlengthD\", \"FsizeD\",\"NameLength\",\"Deck\",\"Embarked\"]\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "indices = np.argsort(scores)[::-1]\n",
    "\n",
    "sorted_important_features=[]\n",
    "for i in indices:\n",
    "    sorted_important_features.append(predictors[i])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature Importances By SelectKBest\")\n",
    "plt.bar(range(np.size(predictors)), scores[indices],\n",
    "       color=\"seagreen\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(np.size(predictors)), sorted_important_features, rotation='vertical')\n",
    "\n",
    "plt.xlim([-1, np.size(predictors)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Interpretation",
    "desc": "This code snippet, though partially commented out, generates the final predictions by using the predictions from the Random Forest model.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.94054234,
    "start_cell": true,
    "subclass": "data_type_conversions",
    "subclass_id": 16
   },
   "outputs": [],
   "source": [
    "#Y_pred = np.rint((Y_pred_1 + Y_pred_2 + Y_pred_3)/3).astype(int)\n",
    "Y_pred = Y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Interpretation",
    "desc": "This code snippet outputs the final predictions stored in `Y_pred`, which were generated using the Random Forest model.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.999729,
    "start_cell": false,
    "subclass": "show_table",
    "subclass_id": 41
   },
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "The code snippet plots the feature importances of a DecisionTreeClassifier trained on the training dataset to interpret which features are most influential in predicting the target variable.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.51051027,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "The code snippet aims to plot the feature importances of the trained model using the training dataset to provide insights into which features are most important.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9941099,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Interpretation",
    "desc": "The code snippet calls a function to visualize the importance of variables in the trained DecisionTreeClassifier model, providing insight into which features are most influential in the model's predictions.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.510510265827179,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Interpretation",
    "desc": "This code visualizes the importance of features in the training dataset using a decision tree classifier to provide insights into which features are most influential.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.510510265827179,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Interpretation",
    "desc": "The code snippet calls the `plot_variable_importance` function to display the importance of features in the trained model, aiding in the interpretation of what variables most influence the model's predictions.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.51051027,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "This code snippet calls a previously defined function to plot the importance of variables in the training set and evaluates the model's performance score using a DecisionTreeClassifier.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9599136,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "##plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "This code snippet calls the `plot_model_var_imp` function to visualize the importance of variables in the training dataset as determined by the trained model.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.8397475,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "This code snippet plots the importance of features in a decision tree classifier trained on the training dataset and prints the model's accuracy score.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.510510265827179,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "This code snippet plots the importance of features in the trained model and prints the model's accuracy score.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.8397474884986877,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "This code snippet plots the feature importances of the trained model and prints the model's accuracy score on the training data.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.8397474884986877,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "This code snippet plots the variable importance of features in the training dataset using a decision tree classifier and outputs the model's accuracy score.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.51051027,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "This code snippet visualizes the feature importance of the trained model on the training dataset and prints the model's accuracy score.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.8397475,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "This code snippet generates a plot to visualize the importance of different features in the training dataset using a Decision Tree classifier.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.510510265827179,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Model Interpretation",
    "desc": "This code snippet visualizes the feature importance of the trained GradientBoostingClassifier and prints its accuracy score on the training dataset.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.8397474884986877,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "This code snippet plots the variable importance of features in the training set using a DecisionTreeClassifier and displays the model's accuracy on the training data.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.51051027,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "This code snippet plots the variable importance of features in the training set, as determined by the currently fitted model, and prints the model's score on the training dataset.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9941099,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "The snippet calls a previously defined function to fit a DecisionTreeClassifier on the training data, then plots and displays the feature importances of the trained model.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.51051027,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "The snippet calls a previously defined function to plot and display the feature importances of the trained model and its performance score on the training data.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9941099,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Model Interpretation",
    "desc": "This code snippet calculates and displays the correlation coefficients for each feature in the Titanic training dataset using the coefficients from the trained Logistic Regression model, aiding in the interpretation of feature importance.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.75249034,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "# get Correlation Coefficient for each feature using Logistic Regression\n",
    "coeff_df = DataFrame(titanic_df.columns.delete(0))\n",
    "coeff_df.columns = ['Features']\n",
    "coeff_df[\"Coefficient Estimate\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "# preview\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Interpretation",
    "desc": "This code snippet generates a plot to show the importance of different features in the training dataset, based on a Decision Tree Classifier model, and prints the model's accuracy score.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.510510265827179,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Model Interpretation",
    "desc": "This code snippet generates a plot to visualize the importance of different features in the training dataset according to the Logistic Regression model and prints the model's accuracy score on the training set.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.8397474884986877,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Model Interpretation",
    "desc": "This code snippet uses Recursive Feature Elimination with Cross-Validation (RFECV) to select the optimal number of features for the Logistic Regression model, prints the accuracy scores for both training and validation sets, and visualizes the cross-validation scores against the number of features selected.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9381272196769714,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\n",
    "rfecv.fit( train_X , train_y )\n",
    "\n",
    "print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel( \"Number of features selected\" )\n",
    "plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "This code evaluates and plots the importance of features in the training set using a DecisionTreeClassifier to help interpret which variables are most influential for the model's predictions.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.51051027,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "This code prints the feature importance scores and visualizes the top 10 most important features in the training set for the given model.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.8397475,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "The code snippet plots the variable importance of features within the training dataset using a DecisionTreeClassifier and displays the model's score.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.510510265827179,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "The code snippet plots the variable importance of features within the training dataset using the previously trained model and displays the model's score on the training data.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.8397474884986877,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Interpretation",
    "desc": "This code plots the feature importance derived from a DecisionTreeClassifier model fitted on the training data to interpret which features are most influential in predicting the target variable.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.51051027,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 35,
    "class": "Model Interpretation",
    "desc": "This code plots the feature importance of the trained model using the training data to understand which features contribute most to the predictions.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.8397475,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Model Interpretation",
    "desc": "This code prints the model's accuracy on training and validation datasets, along with the optimal number of features, and plots the number of features versus cross-validation scores to visualize the feature selection process.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.8090923,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n",
    "print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n",
    "#Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel( \"Number of features selected\" )\n",
    "plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n",
    "plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "This code snippet calls the `plot_variable_importance()` function to plot the importance of features in the training dataset using a Decision Tree Classifier model.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.51051027,
    "start_cell": true,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "This code snippet calls the `plot_model_var_imp()` function to plot the feature importance of the trained model and output its accuracy on the training data.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.8397475,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "plot_model_var_imp(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Model Interpretation",
    "desc": "The code outputs the shape of the vector representation of the first word in the `doc` object, indicating the dimensionality of the word's embedding.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99947315,
    "start_cell": true,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "doc[0].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Interpretation",
    "desc": "The code calculates and prints the Euclidean distance and cosine similarity between the vector representations of the words \"father\" and \"grandfather\" in the `doc` object to measure their semantic similarity.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99583673,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "print(distance(doc[0].vector, doc[1].vector)) # the smaller the more similar\n",
    "print(doc[0].similarity(doc[1])) # the larger the more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Interpretation",
    "desc": "This snippet outputs a summary of the defined LSTM model, showing the layers, output shapes, and the number of parameters.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.99612254,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Model Interpretation",
    "desc": "This code snippet, if uncommented, would create a DataFrame containing the feature importances from the trained RandomForestClassifier, sort the features by their importance in descending order, and print the resulting DataFrame.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9903812,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "#features = list(X_train.columns)\n",
    "#feature_importances = forest.feature_importances_\n",
    "#data = pd.DataFrame()\n",
    "#data['features'] = features\n",
    "#data['feature_importances'] = feature_importances\n",
    "#data = data.sort_values(by = 'feature_importances',ascending = False)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Model Interpretation",
    "desc": "This code snippet, if uncommented, would identify and list features from the model that have an importance score of less than 0.001.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.998028,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#bad_features = data[data['feature_importances']<0.001]['features'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 58,
    "class": "Model Interpretation",
    "desc": "This snippet outputs a summary of the constructed neural network model, detailing the layers, their configurations, and the overall number of parameters, which is essential for understanding the model architecture and complexity.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99612254,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Model Interpretation",
    "desc": "This code snippet generates and displays a summary of the neural network model, including the architecture, number of parameters, and layer details.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.99612254,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Model Interpretation",
    "desc": "This code snippet generates and displays a summary of the neural network model, including the architecture, layer types, output shapes, and the number of parameters in each layer.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.99612254,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Model Interpretation",
    "desc": "The code defines a function to plot the training and validation loss over the epochs and then calls this function to visualize the loss trends in training and validation phases.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9967168,
    "start_cell": true,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history): \n",
    "\n",
    "    history_dict = history.history\n",
    "    history_dict.keys()\n",
    "\n",
    "\n",
    "    acc = history.history['binary_accuracy']\n",
    "    val_acc = history.history['val_binary_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Model Interpretation",
    "desc": "The code visualizes the training and validation loss over the epochs for the model trained on count-based sequences by calling the `plot_history` function.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9654706,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Interpretation",
    "desc": "The code visualizes the training and validation loss over the epochs for the model trained on frequency-based sequences by calling the `plot_history` function.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9654706,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Interpretation",
    "desc": "The code visualizes the training and validation loss over the epochs for the model trained on TF-IDF based sequences by calling the `plot_history` function.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9654706,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Model Interpretation",
    "desc": "The code visualizes the training and validation loss over the epochs for the model trained with GloVe embeddings by calling the `plot_history` function.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9654706,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Model Interpretation",
    "desc": "This code snippet outputs a summary of the neural network model architecture, including the layers, their shapes, and the number of parameters.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99612254,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Model Interpretation",
    "desc": "This code snippet prints the summary of the defined model, detailing its architecture, layers, and parameters.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99612254,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Model Interpretation",
    "desc": "This code snippet generates a visual representation of the model architecture, showing the layers and their shapes, and saves it to a file.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9234821,
    "start_cell": true,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Model Train",
    "desc": "This code snippet trains a Logistic Regression model using the training data, makes predictions on the test data, and prints the accuracy score of the model on the training data.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.308305,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_test)\n",
    "\n",
    "logreg.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Model Train",
    "desc": "This code snippet is setting up the training of a Support Vector Machine (SVM) model using the training data, making predictions on the test data, and calculating the accuracy score on the training data, but it is currently commented out.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9889135,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# svc = SVC()\n",
    "\n",
    "# svc.fit(X_train, Y_train)\n",
    "\n",
    "# Y_pred = svc.predict(X_test)\n",
    "\n",
    "# svc.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Model Train",
    "desc": "This code snippet trains a Random Forest classifier with 100 estimators using the training data, makes predictions on the test data, and prints the accuracy score on the training data.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.977218,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "\n",
    "random_forest.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Model Train",
    "desc": "This code snippet is setting up the training of a K-Nearest Neighbors (KNN) classifier with 3 neighbors using the training data, making predictions on the test data, and calculating the accuracy score on the training data, but it is currently commented out.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.99430627,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "# knn.fit(X_train, Y_train)\n",
    "\n",
    "# Y_pred = knn.predict(X_test)\n",
    "\n",
    "# knn.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Model Train",
    "desc": "This code snippet is setting up the training of a Gaussian Naive Bayes classifier using the training data, making predictions on the test data, and calculating the accuracy score on the training data, but it is currently commented out.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9824242,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "# gaussian = GaussianNB()\n",
    "\n",
    "# gaussian.fit(X_train, Y_train)\n",
    "\n",
    "# Y_pred = gaussian.predict(X_test)\n",
    "\n",
    "# gaussian.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Model Train",
    "desc": "This code snippet separates the features and target variable from the training data, trains a RandomForestClassifier on the training data, computes the mean accuracy, and outputs it as a formatted string.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9713821,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Training data features, skip the first column 'Survived'\n",
    "train_features = train_data[:, 1:]\n",
    "\n",
    "# 'Survived' column values\n",
    "train_target = train_data[:, 0]\n",
    "\n",
    "# Fit the model to our training data\n",
    "clf = clf.fit(train_features, train_target)\n",
    "score = clf.score(train_features, train_target)\n",
    "\"Mean accuracy of Random Forest: {0}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code defines a function to train a classifier on the training dataset and return the predictions for the test dataset.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.98023784160614,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "def train_predict_classifier(clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Train",
    "desc": "This code trains and tests a Decision Tree classifier with a maximum depth of 10, evaluates its accuracy using the `test_classifier` function, generates predictions on the test set, and prints the feature importances.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.332506000995636,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "clf_dt = tree.DecisionTreeClassifier(max_depth=10)\n",
    "test_classifier(clf_dt)\n",
    "train_predict_classifier(clf_dt)\n",
    "print_feature_importance(clf_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Train",
    "desc": "This code trains and evaluates a Gradient Boosting classifier with 50 estimators, tests its accuracy using the `test_classifier` function, makes predictions on the test set, and saves the results to a CSV file.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.8849647045135498,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "clf_gb = ske.GradientBoostingClassifier(n_estimators=50)\n",
    "test_classifier(clf_gb)\n",
    "save_result(train_predict_classifier(clf_gb), \"grad_clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Model Train",
    "desc": "This code trains and evaluates a Random Forest classifier with 50 estimators, tests its accuracy using the `test_classifier` function, generates predictions on the test set, and prints the feature importances.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.3171769082546234,
    "start_cell": false,
    "subclass": "features_selection",
    "subclass_id": 86
   },
   "outputs": [],
   "source": [
    "clf_rf = ske.RandomForestClassifier(n_estimators=50)\n",
    "test_classifier(clf_rf)\n",
    "train_predict_classifier(clf_rf)\n",
    "print_feature_importance(clf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 47,
    "class": "Model Train",
    "desc": "The code snippet imports necessary classes for linear regression and cross-validation, defines the set of predictor variables and the target variable 'Survived', initializes the LinearRegression algorithm, and sets up K-Fold cross-validation on the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9814153,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\",\"SibSp\", \"Parch\", \"Fare\",\n",
    "              \"Embarked\",\"NlengthD\", \"FsizeD\", \"Title\",\"Deck\"]\n",
    "target=\"Survived\"\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 48,
    "class": "Model Train",
    "desc": "The code snippet iterates over the K-Fold cross-validation splits, trains the LinearRegression model on the training folds, makes predictions on the test folds, and stores these predictions.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9695041,
    "start_cell": false,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[target].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 52,
    "class": "Model Train",
    "desc": "The code snippet initializes a Random Forest Classifier with specified hyperparameters, trains the model on the 'titanic' dataset using the given predictor variables, generates cross-validated predictions, and calculates the mean F1 score for model evaluation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.7261861,
    "start_cell": false,
    "subclass": "find_best_model_class",
    "subclass_id": 3
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\",\n",
    "              \"Fare\",\"NlengthD\",\"NameLength\", \"FsizeD\", \"Title\",\"Deck\",\"TicketNumber\"]\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=50, max_depth=9,min_samples_split=6, min_samples_leaf=4)\n",
    "rf.fit(titanic[predictors],titanic[\"Survived\"])\n",
    "kf = KFold(titanic.shape[0], n_folds=5, random_state=1)\n",
    "predictions = cross_validation.cross_val_predict(rf, titanic[predictors],titanic[\"Survived\"],cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "scores = cross_val_score(rf, titanic[predictors], titanic[\"Survived\"],scoring='f1', cv=kf)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Model Train",
    "desc": "This code snippet formats the data using the Patsy library for logistic regression, splits the data into training and testing sets, instantiates and fits a logistic regression model, then checks the model's accuracy on the training set.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9645951,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# format using patsy to get a matrix to pass into the LR model\n",
    "y, X = dmatrices('Survived ~ Title + Age + Sex + Child + Family_size + Mother + Fare',\n",
    "                  train, return_type=\"dataframe\")\n",
    "# flatten y into a 1-D array\n",
    "y = np.ravel(y)\n",
    "\n",
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# instantiate a logistic regression model, and fit with X and y\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel = LRmodel.fit(X_train, y_train)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "LRmodel.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Model Train",
    "desc": "This code snippet instantiates a RandomForestClassifier, trains it on the training data, prints the training data accuracy, and evaluates the test data accuracy.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.44751683,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# instantiate Random Forest Classifier and train\n",
    "RFCmodel = RandomForestClassifier(n_estimators =1000)\n",
    "RFCmodel = RFCmodel.fit(X_train, y_train)\n",
    "\n",
    "# print score for training set\n",
    "print(\"Train data accuracy:\")\n",
    "print(RFCmodel.score(X_train, y_train))\n",
    "\n",
    "# check the accuracy on the test set\n",
    "predicted = RFCmodel.predict(X_test)\n",
    "print(\"\\nTest data accuracy:\")\n",
    "print(metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Model Train",
    "desc": "This code snippet defines the training and testing sets by separating the features ('X_train' and 'X_test') from the target variable ('Y_train') for the Titanic dataset.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9993105,
    "start_cell": true,
    "subclass": "prepare_x_and_y",
    "subclass_id": 21
   },
   "outputs": [],
   "source": [
    "# define training and testing sets\n",
    "\n",
    "X_train = titanic_df.drop(\"Survived\",axis=1)\n",
    "Y_train = titanic_df[\"Survived\"]\n",
    "X_test  = test_df.drop(\"PassengerId\",axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Model Train",
    "desc": "This code snippet, though commented out, indicates the intention to initialize a Logistic Regression model, fit it with the training data, predict outcomes on the test data, and evaluate the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.30714077,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "#logreg = LogisticRegression()\n",
    "\n",
    "#logreg.fit(X_train, Y_train)\n",
    "\n",
    "#Y_pred = logreg.predict(X_test)\n",
    "\n",
    "#logreg.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Model Train",
    "desc": "This code snippet, though commented out, indicates the intention to initialize a Support Vector Machine (SVM) model, fit it with the training data, predict outcomes on the test data, and evaluate the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.4006321,
    "start_cell": false,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "##Support Vector Machines\n",
    "\n",
    "#svc = SVC()\n",
    "\n",
    "#svc.fit(X_train, Y_train)\n",
    "\n",
    "#Y_pred_4 = svc.predict(X_test)\n",
    "\n",
    "#svc.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Train",
    "desc": "This code snippet initializes a RandomForestClassifier with 100 estimators, fits it to the training data, predicts outcomes on the test data, and evaluates the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.29851466,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "#random_forest = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=10, max_features='sqrt', min_samples_split=5)\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_1 = random_forest.predict(X_test)\n",
    "\n",
    "random_forest.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Model Train",
    "desc": "This code snippet initializes a GradientBoostingClassifier with 100 estimators, fits it to the training data, predicts outcomes on the test data, and evaluates the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.5448585,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#gradient_boost = GradientBoostingClassifier(n_estimators=100,loss='exponential',max_features='log2')\n",
    "gradient_boost = GradientBoostingClassifier(n_estimators=100)\n",
    "gradient_boost.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_2 = gradient_boost.predict(X_test)\n",
    "\n",
    "gradient_boost.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Train",
    "desc": "This code snippet initializes an ExtraTreesClassifier with 100 estimators, fits it to the training data, predicts outcomes on the test data, and evaluates the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.7597736,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#extra_tree = ExtraTreesClassifier(n_estimators=100,criterion='gini',max_depth=10,max_features='log2',min_samples_split=10)\n",
    "extra_tree = ExtraTreesClassifier(n_estimators=100)\n",
    "extra_tree.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_3 = extra_tree.predict(X_test)\n",
    "\n",
    "extra_tree.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code snippet, though commented out, indicates the intention to initialize an AdaBoostClassifier with specific parameters, fit it to the training data, predict outcomes on the test data, and evaluate the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9810052,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#ada_boost = AdaBoostClassifier(n_estimators=100,algorithm='SAMME')\n",
    "\n",
    "#ada_boost.fit(X_train, Y_train)\n",
    "\n",
    "#Y_pred_4 = ada_boost.predict(X_test)\n",
    "\n",
    "#ada_boost.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code snippet, though commented out, indicates the intention to initialize a KNeighborsClassifier with specific parameters, fit it to the training data, predict outcomes on the test data, and evaluate the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9849932,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 7,weights='distance')\n",
    "#knn.fit(X_train, Y_train)\n",
    "#Y_pred_5 = knn.predict(X_test)\n",
    "#acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "#acc_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code snippet, though commented out, indicates the intention to initialize a KNeighborsClassifier with 3 neighbors, fit it to the training data, predict outcomes on the test data, and evaluate the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.995522,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "##knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "#knn.fit(X_train, Y_train)\n",
    "\n",
    "#Y_pred = knn.predict(X_test)\n",
    "\n",
    "#knn.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Train",
    "desc": "This code snippet, though commented out, indicates the intention to initialize a Gaussian Naive Bayes model, fit it to the training data, predict outcomes on the test data, and evaluate the model's accuracy on the training data.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9165068,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes\n",
    "\n",
    "#gaussian = GaussianNB()\n",
    "\n",
    "#gaussian.fit(X_train, Y_train)\n",
    "\n",
    "#Y_pred = gaussian.predict(X_test)\n",
    "\n",
    "#gaussian.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "The code snippet initializes a RandomForestClassifier model with 100 decision trees for training.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9982681,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "The code snippet initializes a Support Vector Classifier (SVC) model for training purposes.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.68790203,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "The code snippet initializes a Gradient Boosting Classifier model for training.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9974968,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "The code snippet initializes a KNeighborsClassifier model with 3 neighbors for training.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9981894,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "The code snippet initializes a Gaussian Naive Bayes (GaussianNB) model for training.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.996792,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "The code snippet initializes a Logistic Regression model for training.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9948827,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "The code snippet fits the initialized model to the training data using the provided feature matrix and target vector.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "The code snippet trains the initialized LogisticRegression model using the training data (train_X and train_y).",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9997127652168274,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code trains the `LogisticRegression` model using the training features and target variable.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9997127652168274,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Train",
    "desc": "The commented out code snippet indicates an intention to instantiate a RandomForestClassifier model with 100 estimators.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99804485,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "The commented out code snippet indicates an intention to instantiate a Support Vector Classifier (SVC) model.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99710995,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "The commented out code snippet indicates an intention to instantiate a Gradient Boosting Classifier model.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99756634,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "The commented out code snippet indicates an intention to instantiate a K-Nearest Neighbors Classifier model with 3 neighbors.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99827373,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "The commented out code snippet indicates an intention to instantiate a Gaussian Naive Bayes classifier model.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9971566,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "The code snippet instantiates a Logistic Regression model.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9948827,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "The code snippet trains the instantiated Logistic Regression model on the training data.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "This code snippet initializes a RandomForestClassifier model with 100 trees (estimators) for subsequent training.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9982681,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code snippet initializes a support vector classifier (SVC) model for subsequent training.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99786985,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "##model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "This code snippet initializes a GradientBoostingClassifier model for subsequent training.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99756634,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code snippet initializes a KNeighborsClassifier model with 3 neighbors for subsequent training.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99827373,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gaussian Naive Bayes (GaussianNB) model for subsequent training.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99804723,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "##model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code snippet initializes a LogisticRegression model for subsequent training.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99735606,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code snippet trains the previously defined machine learning model on the training dataset (train_X and train_y).",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "This code snippet initializes a RandomForestClassifier with 100 decision trees for training a machine learning model.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9982681274414062,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code snippet initializes a Support Vector Classifier (SVC) for training a machine learning model.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.68790203332901,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gradient Boosting Classifier for training a machine learning model.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.997496783733368,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code snippet initializes a KNeighborsClassifier with 3 neighbors for training a machine learning model.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.998189389705658,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gaussian Naive Bayes classifier for training a machine learning model.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9967920184135436,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code snippet initializes a Logistic Regression classifier for training a machine learning model.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9948827028274536,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code snippet trains the initialized model using the training data `train_X` and `train_y`.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9997127652168274,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "This code snippet initializes a RandomForestClassifier model with 100 trees to be used for training.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9982681274414062,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code snippet initializes a Support Vector Classifier (SVC) model to be used for training.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.68790203332901,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "This code snippet initializes a GradientBoostingClassifier model to be used for training.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.997496783733368,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code snippet initializes a KNeighborsClassifier model with 3 neighbors to be used for training.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.998189389705658,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gaussian Naive Bayes (GaussianNB) model to be used for training.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9967920184135436,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code snippet initializes a LogisticRegression model to be used for training.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9948827028274536,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code snippet trains the initialized model on the training dataset using the features 'train_X' and the target 'train_y'.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9997127652168274,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "This code snippet initializes a RandomForestClassifier model with 100 estimators.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9982681,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code snippet initializes a Support Vector Classifier (SVC) model with default parameters.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.68790203,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gradient Boosting Classifier model with default parameters.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9974968,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code snippet initializes a K-Nearest Neighbors Classifier model with 3 neighbors.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9981894,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gaussian Naive Bayes model with default parameters.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.996792,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code snippet initializes a Logistic Regression model with default parameters.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9948827,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code snippet trains the initialized machine learning model on the training dataset comprising features (`train_X`) and target labels (`train_y`).",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code snippet initializes a LogisticRegression model and prints the training dataset `train_X` for model training.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9963364601135254,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code snippet outputs the shape of the training dataset `train_X` to provide insights into the dimensions of the data used for training the Logistic Regression model.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9995459914207458,
    "start_cell": false,
    "subclass": "show_shape",
    "subclass_id": 58
   },
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Train",
    "desc": "This code snippet trains a GradientBoostingClassifier with specified parameters (learning rate and number of estimators) using the `train_X` and `train_y` datasets.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9988875985145568,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#params_grid ={\n",
    " #   'learning_rate':[0.1,0.01],\n",
    "#}\n",
    "#clf = GridSearchCV(GradientBoostingClassifier(),params_grid)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = GradientBoostingClassifier(learning_rate=0.05, n_estimators=500)\n",
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "This code snippet initializes a RandomForestClassifier model with 100 estimators for training.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9982681,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code snippet initializes a Support Vector Classifier (SVC) model for training.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.68790203,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gradient Boosting Classifier model for training.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9974968,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code snippet initializes a K-Nearest Neighbors (KNN) classifier with 3 neighbors for training.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9981894,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gaussian Naive Bayes classifier model for training.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.996792,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code snippet initializes a Logistic Regression model for training.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9948827,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code snippet trains the specified model using the training dataset (`train_X` and `train_y`).",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "The snippet initializes a RandomForestClassifier with 100 trees, setting up the model for subsequent training.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9982681,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "The snippet initializes a Support Vector Classifier (SVC) model, preparing it for subsequent training.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.68790203,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "The snippet initializes a GradientBoostingClassifier model, preparing it for subsequent training.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9974968,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "The snippet initializes a KNeighborsClassifier model with 3 neighbors, preparing it for subsequent training.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9981894,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "The snippet initializes a Gaussian Naive Bayes model, preparing it for subsequent training.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.996792,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "The snippet initializes a LogisticRegression model, preparing it for subsequent training.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9948827,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "The snippet trains the previously initialized machine learning model using the training data (train_X and train_y).",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Model Train",
    "desc": "This code snippet trains a Decision Tree model with a maximum depth of 3 using the preprocessed training data, makes predictions on the test dataset, saves the predictions to a CSV file, exports the trained model as a .dot file, converts the .dot file to a .png, annotates the image, and displays the annotated Decision Tree.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99933004,
    "start_cell": true,
    "subclass": "save_to_csv",
    "subclass_id": 25
   },
   "outputs": [],
   "source": [
    "# Create Numpy arrays of train, test and target (Survived) dataframes to feed into our models\n",
    "y_train = train['Survived']\n",
    "x_train = train.drop(['Survived'], axis=1).values \n",
    "x_test = test.values\n",
    "\n",
    "# Create Decision Tree with max_depth = 3\n",
    "decision_tree = tree.DecisionTreeClassifier(max_depth = 3)\n",
    "decision_tree.fit(x_train, y_train)\n",
    "\n",
    "# Predicting results for test dataset\n",
    "y_pred = decision_tree.predict(x_test)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": PassengerId,\n",
    "        \"Survived\": y_pred\n",
    "    })\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Export our trained model as a .dot file\n",
    "with open(\"tree1.dot\", 'w') as f:\n",
    "     f = tree.export_graphviz(decision_tree,\n",
    "                              out_file=f,\n",
    "                              max_depth = 3,\n",
    "                              impurity = True,\n",
    "                              feature_names = list(train.drop(['Survived'], axis=1)),\n",
    "                              class_names = ['Died', 'Survived'],\n",
    "                              rounded = True,\n",
    "                              filled= True )\n",
    "        \n",
    "#Convert .dot to .png to allow display in web notebook\n",
    "check_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n",
    "\n",
    "# Annotating chart with PIL\n",
    "img = Image.open(\"tree1.png\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', 26)\n",
    "draw.text((10, 0), # Drawing offset (position)\n",
    "          '\"Title <= 1.5\" corresponds to \"Mr.\" title', # Text to draw\n",
    "          (0,0,255), # RGB desired color\n",
    "          font=font) # ImageFont object with desired font\n",
    "img.save('sample-out.png')\n",
    "PImage(\"sample-out.png\")\n",
    "\n",
    "# Code to check available fonts and respective paths\n",
    "# import matplotlib.font_manager\n",
    "# matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Model Train",
    "desc": "This code snippet trains a Logistic Regression model using the training features (X_train) and target variable (Y_train), makes predictions on the test dataset (X_test), and calculates the accuracy score of the model on the training data.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.308305,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_test)\n",
    "\n",
    "logreg.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Model Train",
    "desc": "This code snippet, currently commented out, is intended to train a Support Vector Machine (SVM) model using the training features (X_train) and target variable (Y_train), make predictions on the test dataset (X_test), and calculate the accuracy score of the model on the training data.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9889135,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# svc = SVC()\n",
    "\n",
    "# svc.fit(X_train, Y_train)\n",
    "\n",
    "# Y_pred = svc.predict(X_test)\n",
    "\n",
    "# svc.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Model Train",
    "desc": "This code snippet trains a Random Forest classifier with 100 estimators using the training features (X_train) and target variable (Y_train), makes predictions on the test dataset (X_test), and calculates the accuracy score of the model on the training data.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.977218,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "\n",
    "random_forest.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Model Train",
    "desc": "This code snippet, currently commented out, is intended to train a k-Nearest Neighbors (kNN) classifier with 3 neighbors using the training features (X_train) and target variable (Y_train), make predictions on the test dataset (X_test), and calculate the accuracy score of the model on the training data.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99430627,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "# knn.fit(X_train, Y_train)\n",
    "\n",
    "# Y_pred = knn.predict(X_test)\n",
    "\n",
    "# knn.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Model Train",
    "desc": "This code snippet, currently commented out, is intended to train a Gaussian Naive Bayes classifier using the training features (X_train) and target variable (Y_train), make predictions on the test dataset (X_test), and calculate the accuracy score of the model on the training data.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9824242,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "# gaussian = GaussianNB()\n",
    "\n",
    "# gaussian.fit(X_train, Y_train)\n",
    "\n",
    "# Y_pred = gaussian.predict(X_test)\n",
    "\n",
    "# gaussian.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Train",
    "desc": "This code snippet trains the previously initialized Logistic Regression model using the training data (train_X) and target labels (train_y).",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9997127652168274,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "This code initializes a RandomForestClassifier model with 100 decision trees for use in training on the dataset.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9982681,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code initializes a Support Vector Classifier (SVC) model for use in training on the dataset.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.68790203,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "This code initializes a GradientBoostingClassifier model for use in training on the dataset.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9974968,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code initializes a KNeighborsClassifier model with 3 neighbors for use in training on the dataset.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9981894,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "This code initializes a Gaussian Naive Bayes (GaussianNB) model for use in training on the dataset.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.996792,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code initializes a LogisticRegression model for use in training on the dataset.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9948827,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code fits the initialized machine learning model to the training data consisting of 'train_X' features and 'train_y' target variable.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "The code snippet trains the previously initialized model using the training dataset (train_X and train_y).",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9997127652168274,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code initializes a RandomForestClassifier model with 100 decision trees in preparation for training on the dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9982681,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "This code initializes a Support Vector Classifier (SVC) model in preparation for training on the dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.68790203,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code initializes a GradientBoostingClassifier model in preparation for training on the dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9974968,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code initializes a KNeighborsClassifier model with 3 neighbors in preparation for training on the dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9981894,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Train",
    "desc": "This code initializes a Gaussian Naive Bayes (GaussianNB) model in preparation for training on the dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.996792,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Train",
    "desc": "This code initializes a LogisticRegression model in preparation for training on the dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9948827,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Model Train",
    "desc": "This code fits the initialized model to the training data using the feature matrix 'train_X' and target vector 'train_y'.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Model Train",
    "desc": "This code snippet instantiates a RandomForestClassifier and fits it to the training data (X_train and y_train).",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9992470741271972,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "This code snippet initializes a RandomForestClassifier model with 100 estimators.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9982681,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code snippet initializes a Support Vector Classifier (SVC) model with default parameters.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.68790203,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gradient Boosting Classifier model with default parameters.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9974968,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "This code snippet initializes a KNeighborsClassifier model with the number of neighbors set to 3.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9981894,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Model Train",
    "desc": "This code snippet initializes a Gaussian Naive Bayes (GaussianNB) model with default parameters.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.996792,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "This code snippet initializes a Logistic Regression model with default parameters.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9948827,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "This code snippet fits the previously initialized model to the training data (`train_X` and `train_y`).",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99971277,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit( train_X , train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Train",
    "desc": "This code snippet constructs, compiles, and displays the summary of a Sequential neural network model with multiple Dense layers, dropout layers for regularization, and uses the 'nadam' optimizer along with the 'binary_crossentropy' loss function for binary classification.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.99696594,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 512 , activation = 'relu' , input_dim = tv_train_reviews.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 256 , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 100 , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 10 , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'nadam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Model Train",
    "desc": "This code snippet trains the previously defined neural network model on the TF-IDF transformed training data, using the validation data to assess performance after each epoch, over a total of 5 epochs with a batch size of 128.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.9996747,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history = model.fit(tv_train_reviews, y_train, validation_data=(tv_val_reviews, y_val), batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Train",
    "desc": "The code defines a neural network architecture using the Keras library, which includes various layers like GRU, Dropout, BatchNormalization, TimeDistributed Dense, and Conv1D, but it is commented out and not currently executed.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9867454,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# input_text=Input(shape=(store_train_text.shape[1],store_train_text.shape[2]))\n",
    "# input_hashtag=Input(shape=(store_train_hashtag.shape[1],store_train_hashtag.shape[2]))\n",
    "# input_keyword=Input(shape=(store_train_keyword.shape[1],store_train_keyword.shape[2]))\n",
    "\n",
    "# mid1=GRU(units=128, return_sequences=True)(input_text)\n",
    "# mid1=Dropout(0.8)(mid1)\n",
    "# mid1=BatchNormalization()(mid1)  \n",
    "\n",
    "# mid1=GRU(units=16, return_sequences=True)(mid1)\n",
    "# mid1=Dropout(0.8)(mid1)\n",
    "# mid1=BatchNormalization()(mid1)  \n",
    "\n",
    "# mid1=GRU(units=1, return_sequences=False)(mid1)\n",
    "# #mid1=Dropout(0.8)(mid1)\n",
    "# #mid1=BatchNormalization()(mid1)\n",
    "# print(mid1.shape)\n",
    "\n",
    "# # mid1=Dropout(0.8)(mid1)\n",
    "# # mid1=TimeDistributed(Dense(1, activation = \"relu\"))(mid1)\n",
    "# # mid1=Reshape((mid1.shape[1],))(mid1)\n",
    "# # # mid1 has shape (m,23)\n",
    "\n",
    "# mid2=TimeDistributed(Dense(128, activation = \"relu\"))(input_hashtag)\n",
    "# # kernel_size=1 makes the conv1d the same as TimeDistributed(Dense)\n",
    "# print(mid2.shape)\n",
    "# mid2=Conv1D(1, kernel_size=1, strides=1, padding='valid')(mid2)\n",
    "# mid2=Reshape((mid2.shape[1],))(mid2)\n",
    "# # now mid2 has shape (m,13)\n",
    "# #print(mid2.shape)\n",
    "\n",
    "# # mid3=Conv1D(30, kernel_size=1, strides=1, padding='valid')(input_keyword)\n",
    "# # # kernel_size=1 makes the conv1d the same as TimeDistributed(Dense)\n",
    "# # mid3=Conv1D(1, kernel_size=1, strides=1, padding='valid')(mid3)\n",
    "# # mid3=Reshape((mid3.shape[1],))(mid3)\n",
    "\n",
    "# # mid=Concatenate(axis=-1)([mid1,mid2,mid3])\n",
    "# # #print(mid1.shape,mid2.shape,mid3.shape)\n",
    "# # output=Dense(2, activation=\"softmax\")(mid)\n",
    "# # #print(output.shape)\n",
    "\n",
    "# # model=Model(inputs=[input_text,input_hashtag, input_keyword], outputs=outputs)\n",
    "# # model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Model Train",
    "desc": "The code defines a neural network model using Keras, consisting of several GRU layers, Dropout, BatchNormalization, TimeDistributed Dense, and a final dense layer with softmax activation to output class probabilities.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.2440908,
    "start_cell": false,
    "subclass": "define_search_model",
    "subclass_id": 82
   },
   "outputs": [],
   "source": [
    "inp=Input(shape=(store_train.shape[1],store_train_text.shape[2]))\n",
    "\n",
    "mid=GRU(units=300, return_sequences=True)(inp)\n",
    "mid=Dropout(0.6)(mid)\n",
    "mid=BatchNormalization()(mid)  \n",
    "\n",
    "mid=GRU(units=300, return_sequences=True)(mid)\n",
    "mid=Dropout(0.6)(mid)\n",
    "mid=BatchNormalization()(mid)  \n",
    "\n",
    "mid=GRU(units=300, return_sequences=True)(mid)\n",
    "mid=Dropout(0.6)(mid)\n",
    "mid=BatchNormalization()(mid)  \n",
    "\n",
    "mid=Dropout(0.6)(mid)\n",
    "mid=TimeDistributed(Dense(1,activation='relu'))(mid)\n",
    "mid=Reshape((mid.shape[1],))(mid)\n",
    "mid=Dropout(0.6)(mid)\n",
    "mid=BatchNormalization()(mid) \n",
    "outp=Dense(2,activation='softmax')(mid)\n",
    "\n",
    "\n",
    "model=Model(inputs=inp, outputs=outp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Model Train",
    "desc": "The code compiles the previously defined neural network model with the Adam optimizer, a learning rate of 0.001, and specifies 'categorical_crossentropy' as the loss function and 'accuracy' as a metric to evaluate during training.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9971028,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Model Train",
    "desc": "The code trains the compiled neural network model on the training data (`store_train`) and labels (`train_Y`), using a batch size of 64 and running for 50 epochs, while excluding the last 500 samples for validation or testing purposes.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99969625,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model.fit(store_train[0:-500,:,:], train_Y[0:-500,:], batch_size=64, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Model Train",
    "desc": "This snippet defines and compiles a sequential LSTM model for binary classification with an embedding layer initialized with pre-trained GloVe embeddings, followed by a dropout layer, an LSTM layer, a dense layer with a sigmoid activation, and uses the Adam optimizer.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9975012,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=3e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Model Train",
    "desc": "This snippet trains the LSTM model on the training data for 10 epochs with a batch size of 32, using the validation data to monitor performance, and sets the verbosity level to 2 for training output.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9996799,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Model Train",
    "desc": "This code snippet trains a RandomForestClassifier on the training dataset (`X_train` and `Y_train`) with specific hyperparameters and class weights, evaluates out-of-bag (OOB) score, and prints the OOB score and classification report of the model's predictions on the training set.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.9605358,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.metrics import classification_report\n",
    "forest = rfc(n_estimators = 128,max_depth = 8,min_samples_split = 15,\n",
    "             class_weight = {0:1,1:1.6},oob_score = True)\n",
    "forest.fit(X_train,Y_train)\n",
    "print(forest.oob_score_)\n",
    "Y_pred_train = forest.predict(X_train)\n",
    "print(classification_report(Y_pred_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Model Train",
    "desc": "This code snippet, if uncommented, would retrain the RandomForestClassifier on the reduced training feature set (`X_train_reduced`) using specific hyperparameters and class weights, then print the out-of-bag (OOB) score and the classification report for the model's predictions on the reduced training set.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.96954733,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#forest = rfc(n_estimators = 128,max_depth = 5,min_samples_split = 15,\n",
    "#             class_weight = {0:1,1:1.53},\n",
    "#             oob_score = True)\n",
    "#forest.fit(X_train_reduced,Y_train)\n",
    "#print(forest.oob_score_)\n",
    "#Y_pred_train = forest.predict(X_train_reduced)\n",
    "#print(classification_report(Y_pred_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 44,
    "class": "Model Train",
    "desc": "This code snippet, if uncommented, would train a Multinomial Naive Bayes classifier on the training dataset with `alpha = 0.1`, then print the classification report based on the model’s predictions on the training data.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.99421704,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#clf = MNB(alpha = 0.1)\n",
    "#clf.fit(X_train,Y_train)\n",
    "#Y_pred_train = clf.predict(X_train)\n",
    "#print(classification_report(Y_train,Y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 45,
    "class": "Model Train",
    "desc": "This code snippet trains an XGBoost classifier on the training dataset, evaluates its performance on the training set by printing a classification report, and measures the time taken for both training and prediction.",
    "notebook_id": 4,
    "predicted_subclass_probability": 0.49639666,
    "start_cell": false,
    "subclass": "init_hyperparams",
    "subclass_id": 59
   },
   "outputs": [],
   "source": [
    "#code taken from https://www.kaggle.com/lucidlenn/data-analysis-and-classification-using-xgboost\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "xgb = XGBClassifier(n_estimators=200,learning_rate = 0.2,max_depth = 8)\n",
    "training_start = time.perf_counter()\n",
    "xgb.fit(X_train, Y_train)\n",
    "training_end = time.perf_counter()\n",
    "prediction_start = time.perf_counter()\n",
    "pred_final = xgb.predict(test_data)\n",
    "pred_train = xgb.predict(X_train)\n",
    "print(classification_report(Y_train,pred_train))\n",
    "prediction_end = time.perf_counter()\n",
    "#acc_xgb = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "xgb_train_time = training_end-training_start\n",
    "xgb_prediction_time = prediction_end-prediction_start\n",
    "#print(\"XGBoost's prediction accuracy is: %3.2f\" % (acc_xgb))\n",
    "print(\"Time consumed for training: %4.3f\" % (xgb_train_time))\n",
    "print(\"Time consumed for prediction: %6.5f seconds\" % (xgb_prediction_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Model Train",
    "desc": "This code snippet defines a function to build a Keras model structure by adding dense layers on top of the pre-trained BERT model and compiling it with the RAdam optimizer and binary cross-entropy loss function.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.99608254,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Build model structure\n",
    "def get_model(neuron=256):\n",
    "    \n",
    "    inputs = bert_model.inputs[:2]\n",
    "    outputs = bert_model.get_layer('NSP-Dense').output\n",
    "    \n",
    "    outputs = keras.layers.Dense(neuron, activation='relu')(outputs)\n",
    "    outputs = keras.layers.Dense(units=1, activation='sigmoid')(outputs)\n",
    "\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    # Make the last 4 layers trainable\n",
    "    for layer in model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    \n",
    "    model.compile(RAdam(learning_rate =LR),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    # model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Model Train",
    "desc": "This code snippet initializes the model with the best configuration found, summarizes the model architecture, and trains it with the specified parameters while tracking its performance on a validation split.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9974727,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Choose the best config for the model\n",
    "model = get_model(neuron=32)\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train,y_train,\n",
    "          epochs=4,\n",
    "          batch_size=64, \n",
    "          verbose=1,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Train",
    "desc": "This code defines a custom neural network class, `BertForTwitter`, which incorporates a pre-trained BERT model followed by an additional linear layer for classification into 2 classes, initializing the weights of the new layer and defining the forward pass.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.67068714,
    "start_cell": true,
    "subclass": "set_options",
    "subclass_id": 23
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BertForTwitter(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BertForTwitter, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = model  # 学習済みのBERTモデル\n",
    "\n",
    "        # headにクラス予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元768、出力は2クラス\n",
    "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
    "\n",
    "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
    "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
    "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
    "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
    "        output = self.cls(vec_0)  # 全結合層\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Model Train",
    "desc": "This code creates an instance of the `BertForTwitter` model, sets it to training mode, and prints a confirmation message indicating that the network setup is complete.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.8265412,
    "start_cell": false,
    "subclass": "train_on_grid",
    "subclass_id": 6
   },
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "net = BertForTwitter()\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Model Train",
    "desc": "This code selectively enables gradient calculation only for the parameters in the last layer of the BERT encoder and the added classification layer, while freezing the rest of the model's parameters, facilitating focused fine-tuning.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.5731687,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
    "for param in net.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for param in net.cls.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Model Train",
    "desc": "This code sets up the Adam optimizer for fine-tuning the last BERT layer and the added classification layer with different learning rates, and initializes the cross-entropy loss function for evaluating the model's performance.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.9925128,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Model Train",
    "desc": "This code defines a function `train_model` that implements the training and validation loop for a specified number of epochs, tracks and prints losses and accuracies, and incorporates an early stopping mechanism if the validation accuracy does not improve after three consecutive epochs.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.99687886,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    max_acc = 0\n",
    "    Stop_flag = False\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BERTに入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 50 == 0):  # 10iterに1度、lossを表示\n",
    "                            acc = (torch.sum(preds == labels.data)\n",
    "                                   ).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
    "                                iteration, loss.item(),  acc))\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            # 検証フェーズの時に最高正解率より連続で3回正解率が下回ったら学習をやめる\n",
    "            if phase == \"val\":\n",
    "                if epoch_acc < max_acc:\n",
    "                    count += 1\n",
    "                    if count >= 3:\n",
    "                        Stop_flag = True\n",
    "                else:\n",
    "                    count = 0\n",
    "                    max_acc = epoch_acc\n",
    "                print(count)\n",
    "                \n",
    "        if Stop_flag:\n",
    "            break\n",
    "\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "This code initiates the training process for the pre-defined number of epochs (50) by calling the `train_model` function with the neural network, dataloaders, loss function, optimizer, and the specified number of epochs.",
    "notebook_id": 6,
    "predicted_subclass_probability": 0.99816954,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# 学習・検証を実行する。1epochに1分ほどかかる\n",
    "num_epochs = 50\n",
    "net_trained = train_model(net, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 49,
    "class": "Model Train",
    "desc": "This snippet initializes a Logistic Regression model with specified hyperparameters, trains it on the TF-IDF transformed training data, makes predictions on the validation data, and prints the accuracy score and classification report for the validation set.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9922645,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(max_iter=150,\n",
    "                            penalty='l2',\n",
    "                            solver='lbfgs',\n",
    "                            random_state=0)\n",
    "lr_clf.fit(X_train_onehot, y_train)\n",
    "lr_pred = lr_clf.predict(X_val_onehot)\n",
    "\n",
    "print('accuracy score: ',accuracy_score(lr_pred,y_val))\n",
    "print(classification_report(y_val, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 57,
    "class": "Model Train",
    "desc": "This snippet constructs a Sequential neural network model with an embedding layer initialized with pre-trained GloVe embeddings, followed by dropout, LSTM, and dense layers, and then compiles the model using the Adam optimizer and binary cross-entropy loss function, preparing it for training.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9975012,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=3e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 62,
    "class": "Model Train",
    "desc": "This commented-out snippet indicates the intention to train the neural network model on the training data for 10 epochs with a batch size of 4 and validate it on the validation data, while providing a recommendation for the number of epochs to use.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9590828,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Recomended 10-20 epochs\n",
    "#history=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 63,
    "class": "Model Train",
    "desc": "This commented-out snippet indicates the intention to make predictions on the training data using the trained neural network model and converts the predicted probabilities to integer class labels by rounding them.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.93073225,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#train_pred_GloVe = model.predict(train)\n",
    "#train_pred_GloVe_int = train_pred_GloVe.round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 67,
    "class": "Model Train",
    "desc": "This snippet defines a function to build a BERT-based neural network model, featuring input layers for word IDs, masks, and segment IDs, followed by a BERT layer, optional dropout, and a dense output layer, and compiles the model with Adam optimizer and binary cross-entropy loss.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.8722445,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    if Dropout_num == 0:\n",
    "        # Without Dropout\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    else:\n",
    "        # With Dropout(Dropout_num), Dropout_num > 0\n",
    "        x = Dropout(Dropout_num)(clf_output)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 79,
    "class": "Model Train",
    "desc": "This snippet builds a BERT-based neural network model using the previously defined `build_model` function with a maximum sequence length of 160, and then outputs a summary of the model architecture, preparing it for training and interpretation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.44024342,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model_BERT = build_model(bert_layer, max_len=160)\n",
    "model_BERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 82,
    "class": "Model Train",
    "desc": "This snippet trains the BERT-based model using the encoded training data and labels, with a specified validation split, number of epochs, and batch size, while saving the model at each epoch based on validation loss via the ModelCheckpoint callback.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9975091,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=False)\n",
    "\n",
    "train_history = model_BERT.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split = valid,\n",
    "    epochs = epochs_num, # recomended 3-5 epochs\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size = batch_size_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Model Train",
    "desc": "This code defines the configurations for training the sequence classifier, including learning rate, batch size, and training epochs, and then initiates the training process using the `SequenceClassifierTrainer`.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9962847,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "sc_train_configs = {\n",
    "        \"output_dir\": OUTPUT_DIR,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"mini_batch_size\": 32,\n",
    "        \"anneal_factor\": 0.5,\n",
    "        \"patience\": 2, # If the model does not improve after this this many steps the learning rate will decrease\n",
    "        \"max_epochs\": 10,\n",
    "        \"plot_weights\": False,\n",
    "        \"batch_growth_annealing\": False,\n",
    "}\n",
    "sc_trainer.train(**sc_train_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Model Train",
    "desc": "This code snippet builds a Sequential neural network model with two hidden layers and one output layer, compiles it with the Adam optimizer, and trains the model on the `train` and `Y_train` datasets for 200 epochs with a validation split of 20%.",
    "notebook_id": 10,
    "predicted_subclass_probability": 0.50303537,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
    "model.fit(train,Y_train,epochs=200,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Model Train",
    "desc": "This code snippet defines a Sequential neural network model for binary classification, including an embedding layer initialized with pre-trained GloVe embeddings, an LSTM layer with dropout, and a dense output layer, and then compiles the model with binary cross-entropy loss and the Adam optimizer.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.99750155,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "This code snippet trains the defined neural network model on the training data for 15 epochs with a batch size of 4, using the validation data to monitor performance, and outputs verbose training logs.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.999681,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Train",
    "desc": "This code initializes a Random Forest classifier and fits it to the training data using the token count matrix and corresponding labels.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9994931,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf_model = rf.fit(train_vectors,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Model Train",
    "desc": "This code initializes a Multinomial Naive Bayes classifier and fits it to the training data using the token count matrix and corresponding labels.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.98653954,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb_model = nb.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Model Train",
    "desc": "This code defines a Sequence-to-Sequence Attention model with embedding, LSTM, attention mechanism, and linear layers, along with methods for optimizing, adding a loss function, adjusting learning rate, and running an epoch on training data while evaluating it on validation data.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9780318,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, config, vocab_size, word_embeddings):\n",
    "        super(Seq2SeqAttention, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
    "        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "        \n",
    "        # Encoder RNN\n",
    "        self.lstm = nn.LSTM(input_size = self.config.embed_size,\n",
    "                            hidden_size = self.config.hidden_size,\n",
    "                            num_layers = self.config.hidden_layers,\n",
    "                            bidirectional = self.config.bidirectional)\n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(self.config.dropout_keep)\n",
    "        \n",
    "        # Fully-Connected Layer\n",
    "        self.fc = nn.Linear(\n",
    "            self.config.hidden_size * (1+self.config.bidirectional) * 2,\n",
    "            self.config.output_size\n",
    "        )\n",
    "        \n",
    "        # Sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "                \n",
    "    def apply_attention(self, rnn_output, final_hidden_state):\n",
    "        '''\n",
    "        Apply Attention on RNN output\n",
    "        \n",
    "        Input:\n",
    "            rnn_output (batch_size, seq_len, num_directions * hidden_size): tensor representing hidden state for every word in the sentence\n",
    "            final_hidden_state (batch_size, num_directions * hidden_size): final hidden state of the RNN\n",
    "            \n",
    "        Returns:\n",
    "            attention_output(batch_size, num_directions * hidden_size): attention output vector for the batch\n",
    "        '''\n",
    "        hidden_state = final_hidden_state.unsqueeze(2)\n",
    "        attention_scores = torch.bmm(rnn_output, hidden_state).squeeze(2)\n",
    "        soft_attention_weights = F.softmax(attention_scores, 1).unsqueeze(2) #shape = (batch_size, seq_len, 1)\n",
    "        attention_output = torch.bmm(rnn_output.permute(0,2,1), soft_attention_weights).squeeze(2)\n",
    "        return attention_output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape = (max_sen_len, batch_size)\n",
    "        embedded_sent = self.embeddings(x)\n",
    "        # embedded_sent.shape = (max_sen_len=20, batch_size=64,embed_size=300)\n",
    "\n",
    "        ##################################### Encoder #######################################\n",
    "        lstm_output, (h_n,c_n) = self.lstm(embedded_sent)\n",
    "        # lstm_output.shape = (seq_len, batch_size, num_directions * hidden_size)\n",
    "        \n",
    "        # Final hidden state of last layer (num_directions, batch_size, hidden_size)\n",
    "        batch_size = h_n.shape[1]\n",
    "        h_n_final_layer = h_n.view(self.config.hidden_layers,\n",
    "                                   self.config.bidirectional + 1,\n",
    "                                   batch_size,\n",
    "                                   self.config.hidden_size)[-1,:,:,:]\n",
    "        \n",
    "        ##################################### Attention #####################################\n",
    "        # Convert input to (batch_size, num_directions * hidden_size) for attention\n",
    "        final_hidden_state = torch.cat([h_n_final_layer[i,:,:] for i in range(h_n_final_layer.shape[0])], dim=1)\n",
    "        \n",
    "        attention_out = self.apply_attention(lstm_output.permute(1,0,2), final_hidden_state)\n",
    "        # Attention_out.shape = (batch_size, num_directions * hidden_size)\n",
    "        \n",
    "        #################################### Linear #########################################\n",
    "        concatenated_vector = torch.cat([final_hidden_state, attention_out], dim=1)\n",
    "        final_feature_map = self.dropout(concatenated_vector) # shape=(batch_size, num_directions * hidden_size)\n",
    "        final_out = self.fc(final_feature_map)\n",
    "        return self.sigmoid(final_out)\n",
    "    \n",
    "    def add_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def add_loss_op(self, loss_op):\n",
    "        self.loss_op = loss_op\n",
    "    \n",
    "    def reduce_lr(self):\n",
    "        print(\"Reducing LR\")\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] / 2\n",
    "                \n",
    "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
    "        train_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        # Reduce learning rate as number of epochs increase\n",
    "        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n",
    "            self.reduce_lr()\n",
    "            \n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            self.optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                x = batch.text.cuda()\n",
    "                y = (batch.label).type(torch.cuda.FloatTensor)\n",
    "            else:\n",
    "                x = batch.text\n",
    "                y = (batch.label).type(torch.FloatTensor)\n",
    "            y_pred = self.__call__(x).squeeze(1)\n",
    "            loss = self.loss_op(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "    \n",
    "        print(\"Iter: {}\".format(i+1))\n",
    "        avg_train_loss = np.mean(losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
    "        losses = []\n",
    "                \n",
    "        # Evalute Accuracy on validation set\n",
    "        val_accuracy = evaluate_model(self, val_iterator)\n",
    "        print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
    "#         self.train()\n",
    "                \n",
    "        return avg_train_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Model Train",
    "desc": "This code initializes the `Seq2SeqAttention` model with the specified optimizer (Adam), loss function (BCELoss), and other configurations, checks for CUDA availability, moves the model to GPU if available, and prints the model architecture and the number of trainable parameters.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.94854355,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Create Model with specified optimizer and loss function\n",
    "##############################################################\n",
    "model = Seq2SeqAttention(config, len(dataset.vocab), dataset.word_embeddings)\n",
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "NLLLoss = nn.BCELoss()\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(NLLLoss)\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Model Train",
    "desc": "This code trains the model for the specified number of epochs, tracks training losses and validation accuracies, saves the best model based on validation accuracy, and finally evaluates the best model on the training and validation datasets.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.23883364,
    "start_cell": false,
    "subclass": "load_pretrained",
    "subclass_id": 30
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_acc = 0\n",
    "    \n",
    "for i in range(config.max_epochs):\n",
    "    print (\"Epoch: {}\".format(i))\n",
    "    train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
    "#     print(val_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    #save the best model\n",
    "    if best_acc < val_accuracy:\n",
    "        best_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "\n",
    "# Evaluate best model\n",
    "PATH = \"../working/best_model.pt\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "train_acc = evaluate_model(model, dataset.train_iterator)\n",
    "val_acc = evaluate_model(model, dataset.val_iterator)\n",
    "\n",
    "print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "print ('Final Validation Accuracy: {:.4f}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Model Train",
    "desc": "This code snippet checks if a model with the specified display name exists in AutoML, trains and deploys the model if it does not exist, and prints timestamps at various stages of the process.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9967892,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "print(f'Getting model trained at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "\n",
    "if not amw.get_model_by_display_name():\n",
    "    print(f'Training model at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "    amw.train_model()\n",
    "\n",
    "print(f'Model trained. Ensuring model is deployed at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "amw.deploy_model()\n",
    "amw.model\n",
    "print(f'Model trained and deployed at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Model Train",
    "desc": "This code snippet accesses and prints the full path of the model within the AutoML system, likely for reference or verification purposes.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.5237418,
    "start_cell": false,
    "subclass": "find_best_params",
    "subclass_id": 2
   },
   "outputs": [],
   "source": [
    "amw.model_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "Trains the Bidirectional LSTM model on the training data for 15 epochs with a batch size of 64, using model checkpoints and learning rate reduction as callbacks, and validates the performance on a separate validation set.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.99968016,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = 15,\n",
    "    batch_size = 64,\n",
    "    validation_data = [X_test, y_test],\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Model Train",
    "desc": "The code defines and compiles a simple sequential neural network model with a single dense layer for binary classification, and prints the summary of the model architecture.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.990493,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers, metrics, optimizers, losses\n",
    "\n",
    "def setup_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "#     model.add(layers.Dense(16, activation='relu', input_shape=(vocab_size,)))\n",
    "#     model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', input_shape=(vocab_size,)))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = setup_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Model Train",
    "desc": "The code trains the neural network model using the one-hot encoded training data and corresponding labels for 20 epochs with a batch size of 512, and validates it on the validation set.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9996803,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_ohe, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_ohe, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Model Train",
    "desc": "The code initializes and summarizes the neural network model architecture by calling the `setup_model` function.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9821353,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model = setup_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Model Train",
    "desc": "The code trains the neural network model using the count-based training data and corresponding labels for 20 epochs with a batch size of 512, and validates it on the count-based validation set.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99967945,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Model Train",
    "desc": "The code initializes and summarizes the neural network model architecture by calling the `setup_model` function.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9821353,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model = setup_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Model Train",
    "desc": "The code trains the neural network model using the frequency-based training data and corresponding labels for 20 epochs with a batch size of 512, and validates it on the frequency-based validation set.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99967873,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_freq, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_freq, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "The code initializes and summarizes the neural network model architecture by calling the `setup_model` function.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9821353,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "model = setup_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Train",
    "desc": "The code trains the neural network model using the TF-IDF based training data and corresponding labels for 20 epochs with a batch size of 512, and validates it on the TF-IDF based validation set.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9996803,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_tfidf, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_tfidf, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 39,
    "class": "Model Train",
    "desc": "The code sets up a neural network model with an embedding layer initialized using GloVe embeddings, followed by a flattening layer, a dropout layer, and a dense layer for binary classification, and prints the summary of the model architecture.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9871293,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "\n",
    "n_latent_factors = 100\n",
    "model_glove = Sequential()\n",
    "model_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n",
    "                           input_length = max_len, trainable=True))\n",
    "model_glove.add(layers.Flatten())\n",
    "# model_glove.add(layers.Dense(16, activation='relu'))\n",
    "model_glove.add(layers.Dropout(0.5))\n",
    "# model_glove.add(layers.Dense(16, activation='relu'))\n",
    "model_glove.add(layers.Dense(1, activation='sigmoid'))\n",
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Model Train",
    "desc": "The code compiles the neural network model with specified loss function, optimizer, and metrics, and then trains it using the training sequences for 20 epochs with a batch size of 512, validating it on the validation set.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99457717,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "model_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\n",
    "              loss = losses.binary_crossentropy,\n",
    "              metrics = [metrics.binary_accuracy])\n",
    "\n",
    "history = model_glove.fit(X_train_seq,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val_seq, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Model Train",
    "desc": "The code re-processes the text data into padded sequences, initializes, summarizes, compiles, and trains the neural network model with GloVe embeddings on the training set for 20 epochs with a batch size of 512.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.80148226,
    "start_cell": false,
    "subclass": "categorify",
    "subclass_id": 20
   },
   "outputs": [],
   "source": [
    "max_len = 15\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\n",
    "y_train = np.array(train_data['target']).astype(int)\n",
    "\n",
    "print(f\"X_train shape: {X_train_seq.shape}\")\n",
    "print(f\"X_test shape: {X_test_seq.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\\n\")\n",
    "\n",
    "# Setting up the model\n",
    "\n",
    "n_latent_factors = 100\n",
    "model_glove = Sequential()\n",
    "model_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n",
    "                           input_length = max_len, trainable=True))\n",
    "model_glove.add(layers.Flatten())\n",
    "# model_glove.add(layers.Dense(16, activation='relu'))\n",
    "model_glove.add(layers.Dropout(0.5))\n",
    "# model_glove.add(layers.Dense(16, activation='relu'))\n",
    "model_glove.add(layers.Dense(1, activation='sigmoid'))\n",
    "print(f\"{model_glove.summary()}\\n\")\n",
    "\n",
    "\n",
    "model_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\n",
    "              loss = losses.binary_crossentropy,\n",
    "              metrics = [metrics.binary_accuracy])\n",
    "\n",
    "history = model_glove.fit(X_train_seq,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 43,
    "class": "Model Train",
    "desc": "The code tokenizes the original text data into count-based matrices for both training and testing sets, splits the training data into training and validation sets, initializes the neural network model, and trains it on the count representation of un-preprocessed text data for 20 epochs with a batch size of 512, validating on the count-based validation set.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.59386915,
    "start_cell": false,
    "subclass": "split",
    "subclass_id": 13
   },
   "outputs": [],
   "source": [
    "# Setting up the tokenizer\n",
    "vocab_size = 1000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\n",
    "tokenizer.fit_on_texts(list(train_data['text']) + list(test_data['text']))\n",
    "\n",
    "# Word count representation\n",
    "X_train_wc = tokenizer.texts_to_matrix(train_data['text'], mode = 'count')\n",
    "X_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\n",
    "y_train = np.array(train_data['target']).astype(int)\n",
    "\n",
    "print(f\"X_train shape: {X_train_wc.shape}\")\n",
    "print(f\"X_test shape: {X_test_wc.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Train Validation Split\n",
    "X_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(f\"X_train shape: {X_train_wc.shape}\")\n",
    "print(f\"X_val shape: {X_val_wc.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\\n\")\n",
    "\n",
    "# Setting up the model\n",
    "model = setup_model()\n",
    "\n",
    "# Fitting the model on un-preprocessed text\n",
    "history = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Model Train",
    "desc": "The code sets up and trains a Support Vector Classifier with the best hyperparameters found from grid search, evaluates it using 5-fold cross-validation on the training set, and prints the individual and average F1 scores.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9838514,
    "start_cell": true,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "best_params= {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
    "svc = SVC(**best_params)\n",
    "scores = cross_val_score(svc,X_train, y_train, cv=5, scoring='f1')\n",
    "print(scores)\n",
    "print(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Model Train",
    "desc": "The code trains the Support Vector Classifier on the entire training set and then predicts the class labels for some sample validation texts.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.6037878,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "val_texts = [\"A happy day!\", 'An earthquake happened!']\n",
    "val_data = tf_idf.transform(val_texts)\n",
    "svc.fit(X_train, y_train)\n",
    "print(svc.predict(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Model Train",
    "desc": "This code snippet defines and compiles a sequential neural network model with an embedding layer initialized with pre-trained GloVe vectors, followed by dropout, LSTM, and dense layers, using binary cross-entropy loss and the Adam optimizer.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99749494,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Model Train",
    "desc": "This code snippet trains the defined neural network model on the training data for 15 epochs with a batch size of 4 while also validating it on the validation set.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.999681,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 37,
    "class": "Model Train",
    "desc": "This code snippet defines a Sequential model for binary text classification, incorporating pre-trained GloVe embeddings, a SpatialDropout1D layer, an LSTM layer, and a Dense output layer with a sigmoid activation before compiling it with binary cross-entropy loss and the Adam optimizer.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99742764,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding = Embedding(num_words, 100, \n",
    "                      embeddings_initializer = Constant(embedding_matrix),\n",
    "                     input_length = MAX_LEN, trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "optimzer = Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Model Train",
    "desc": "This code snippet trains the model on the training data for 3 epochs with a batch size of 4 while validating on the validation set.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9996886,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=4, epochs=3, validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Model Train",
    "desc": "This code snippet trains the Logistic Regression classifier using the training data `X_train` and `y_train`.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9997003,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "#fit training data \n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Model Train",
    "desc": "The code snippet, currently commented out, is intended to build a sequential neural network model with embedding, convolutional, max pooling, and dense layers, then compile it with binary cross-entropy loss and the Adam optimizer, and finally print a summary of the model.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99597555,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into EMBEDDING_DIM dimensions\n",
    "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LEN))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# we add a Convolution1D, which will learn NUM_FILTERS filters\n",
    "model.add(Conv1D(NUM_FILTERS,\n",
    "                 KERNEL_SIZE,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(HIDDEN_DIMS))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Model Train",
    "desc": "The code defines a sequential neural network model with an embedding layer, bidirectional LSTM layers, fully connected dense layers with dropout for regularization, compiles it with binary cross-entropy loss, and the Adam optimizer to prepare it for training.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99717975,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM,  input_length=x_train.shape[1]))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(80, activation='elu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation='elu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Model Train",
    "desc": "The code snippet, with partially commented out sections, fits the neural network model on the training data using specified batch size and number of epochs, includes validation split for evaluation during training, and evaluates the model performance on the validation set, also preparing to make predictions.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99869686,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# fit a model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=1,\n",
    "          validation_split=0.15,\n",
    "          verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "#score, acc = model.evaluate(x_val, y_val, batch_size=BATCH_SIZE)\n",
    "#print('\\nAccuracy: ', acc*100)\n",
    "\n",
    "#pred = model.predict_classes(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Model Train",
    "desc": "This code initializes a Multinomial Naive Bayes classifier, trains it on the TF-IDF vectorized training data, and predicts the target labels for the test data.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.980895,
    "start_cell": true,
    "subclass": "predict_on_test",
    "subclass_id": 48
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vect, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Model Train",
    "desc": "This code initializes a Multinomial Naive Bayes classifier, trains it on the TF-IDF vectorized training data, and makes predictions on the TF-IDF vectorized test data.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.55512035,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vect, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 32,
    "class": "Model Train",
    "desc": "This code trains the Logistic Regression model on the transformed training data and the corresponding target labels.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9997023,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Model Train",
    "desc": "This code snippet creates and compiles a sequential TensorFlow model for binary classification, with embedding, global average pooling, and dense layers, and uses binary cross-entropy loss and the Adam optimizer.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99342585,
    "start_cell": true,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Reference https://www.kaggle.com/galvaowesley/nlp-tensorflow-predicting-sarcastic-sentences\n",
    "# Create a model\n",
    "model = tf.keras.Sequential([\n",
    "    # Embedding layer for NN                         \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),    \n",
    "    # Global Average pooling is similar to adding up vectors\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(14, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')                             \n",
    "])\n",
    "# Modelo compile for binary classification\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 42,
    "class": "Model Train",
    "desc": "This code snippet trains the model for a specified number of epochs, using the training data, and validates it using the testing data, with the training progress being outputted in verbose mode.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9996737,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epoch_num = 4\n",
    "history = model.fit(training_padded, \n",
    "                    training_labels, \n",
    "                    epochs = epoch_num, \n",
    "                    validation_data = (testing_padded, testing_labels), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Other",
    "desc": "This code snippet contains comments indicating intentions for plotting age-related visualizations, such as survival peaks by age and average survival rates by age, but it is not executed.",
    "notebook_id": 9,
    "predicted_subclass_probability": 0.9588765,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# .... continue with plot Age column\n",
    "\n",
    "# peaks for survived/not survived passengers by their age\n",
    "#facet = sns.FacetGrid(titanic_df, hue=\"Survived\",aspect=4)\n",
    "#facet.map(sns.kdeplot,'Age',shade= True)\n",
    "#facet.set(xlim=(0, titanic_df['Age'].max()))\n",
    "#facet.add_legend()\n",
    "\n",
    "# average survived passengers by age\n",
    "#fig, axis1 = plt.subplots(1,1,figsize=(18,4))\n",
    "#average_age = titanic_df[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\n",
    "#sns.barplot(x='Age', y='Survived', data=average_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Other",
    "desc": "The code snippet (if uncommented) initializes a RandomForestClassifier model with 100 trees, but it does not actually execute any operation.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9980448484420776,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Other",
    "desc": "The code snippet (if uncommented) initializes a Support Vector Classifier (SVC) model, but it does not actually execute any operation.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9971099495887756,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Other",
    "desc": "The code snippet (if uncommented) initializes a GradientBoostingClassifier model, but it does not actually execute any operation.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9975663423538208,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Other",
    "desc": "The code snippet (if uncommented) initializes a KNeighborsClassifier model with 3 neighbors, but it does not actually execute any operation.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9982737302780152,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Other",
    "desc": "The code snippet (if uncommented) initializes a Gaussian Naive Bayes (GaussianNB) model, but it does not actually execute any operation.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9971566200256348,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Other",
    "desc": "The code snippet initializes a LogisticRegression model, ready to be used for training, but it does not actually train or execute the model.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9948827028274536,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Other",
    "desc": "This code, if uncommented, would initialize a `RandomForestClassifier` model with 100 estimators.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9980448484420776,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 24,
    "class": "Other",
    "desc": "This code, if uncommented, would initialize a Support Vector Classifier (SVC) model.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9971099495887756,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Other",
    "desc": "This code, if uncommented, would initialize a `GradientBoostingClassifier` model.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9975663423538208,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Other",
    "desc": "This code, if uncommented, would initialize a `KNeighborsClassifier` model with 3 neighbors.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9982737302780152,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Other",
    "desc": "This code, if uncommented, would initialize a `GaussianNB` (Gaussian Naive Bayes) model.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9971566200256348,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Other",
    "desc": "This code snippet defines a function to calculate the Gini Impurity, which is a measure of the impurity or disorder in a dataset.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.6173425,
    "start_cell": true,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "# Define function to calculate Gini Impurity\n",
    "def get_gini_impurity(survived_count, total_count):\n",
    "    survival_prob = survived_count/total_count\n",
    "    not_survival_prob = (1 - survival_prob)\n",
    "    random_observation_survived_prob = survival_prob\n",
    "    random_observation_not_survived_prob = (1 - random_observation_survived_prob)\n",
    "    mislabelling_survided_prob = not_survival_prob * random_observation_survived_prob\n",
    "    mislabelling_not_survided_prob = survival_prob * random_observation_not_survived_prob\n",
    "    gini_impurity = mislabelling_survided_prob + mislabelling_not_survided_prob\n",
    "    return gini_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Other",
    "desc": "This code snippet calculates the Gini Impurity for the starting node of the dataset using the previously defined `get_gini_impurity` function with 342 survivors out of 891 total observations.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.49259475,
    "start_cell": false,
    "subclass": "choose_model_class",
    "subclass_id": 4
   },
   "outputs": [],
   "source": [
    "# Gini Impurity of starting node\n",
    "gini_impurity_starting_node = get_gini_impurity(342, 891)\n",
    "gini_impurity_starting_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Other",
    "desc": "This code snippet calculates the Gini Impurity for the node corresponding to male observations using the `get_gini_impurity` function with 109 survivors out of 577 male observations.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.43203324,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "# Gini Impurity decrease of node for 'male' observations\n",
    "gini_impurity_men = get_gini_impurity(109, 577)\n",
    "gini_impurity_men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Other",
    "desc": "This code snippet calculates the Gini Impurity for the node corresponding to female observations using the `get_gini_impurity` function with 233 survivors out of 314 female observations.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.34216994,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "# Gini Impurity decrease if node splited for 'female' observations\n",
    "gini_impurity_women = get_gini_impurity(233, 314)\n",
    "gini_impurity_women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Other",
    "desc": "This code snippet calculates the weighted Gini Impurity for nodes split by the \"Sex\" feature and determines the decrease in Gini Impurity when splitting the node by \"Sex\".",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.47323626,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Gini Impurity decrease if node splited by Sex\n",
    "men_weight = 577/891\n",
    "women_weight = 314/891\n",
    "weighted_gini_impurity_sex_split = (gini_impurity_men * men_weight) + (gini_impurity_women * women_weight)\n",
    "\n",
    "sex_gini_decrease = weighted_gini_impurity_sex_split - gini_impurity_starting_node\n",
    "sex_gini_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Other",
    "desc": "This code snippet calculates the Gini Impurity for the node corresponding to observations with the title \"Mr\" (Title == 1) using the `get_gini_impurity` function with 81 survivors out of 517 observations.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.612901,
    "start_cell": false,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "# Gini Impurity decrease of node for observations with Title == 1 == Mr\n",
    "gini_impurity_title_1 = get_gini_impurity(81, 517)\n",
    "gini_impurity_title_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Other",
    "desc": "This code snippet calculates the Gini Impurity for the node corresponding to observations with titles other than \"Mr\" (Title != 1) using the `get_gini_impurity` function with 261 survivors out of 374 observations.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.63723195,
    "start_cell": false,
    "subclass": "statistical_test",
    "subclass_id": 47
   },
   "outputs": [],
   "source": [
    "# Gini Impurity decrease if node splited for observations with Title != 1 != Mr\n",
    "gini_impurity_title_others = get_gini_impurity(261, 374)\n",
    "gini_impurity_title_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Other",
    "desc": "This code snippet calculates the weighted Gini Impurity for nodes split by the \"Title\" feature (specifically Title == 1 == Mr or otherwise) and determines the decrease in Gini Impurity when splitting the node by \"Title\".",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9896773,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Gini Impurity decrease if node splited for observations with Title == 1 == Mr\n",
    "title_1_weight = 517/891\n",
    "title_others_weight = 374/891\n",
    "weighted_gini_impurity_title_split = (gini_impurity_title_1 * title_1_weight) + (gini_impurity_title_others * title_others_weight)\n",
    "\n",
    "title_gini_decrease = weighted_gini_impurity_title_split - gini_impurity_starting_node\n",
    "title_gini_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Other",
    "desc": "The code demonstrates the use of `WordNetLemmatizer` from the NLTK library to lemmatize the word 'us' to its base form.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.5468907,
    "start_cell": true,
    "subclass": "show_table_attributes",
    "subclass_id": 40
   },
   "outputs": [],
   "source": [
    "WordNetLemmatizer().lemmatize('us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Other",
    "desc": "The code defines a function `distance` that calculates the Euclidean distance between two vectors.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9950052,
    "start_cell": false,
    "subclass": "compute_test_metric",
    "subclass_id": 49
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def distance(vec1,vec2):\n",
    "    sum=0\n",
    "    for i in range(len(vec1)):\n",
    "        sum+=(vec1[i]-vec2[i])**2\n",
    "    return math.sqrt(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Other",
    "desc": "The code uses the `spaCy` model to process the string \"father grandfather\" into a document object containing linguistic annotations.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.38565904,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"father grandfather\") # change your words here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 0,
    "class": "Other",
    "desc": "This snippet initializes several hyperparameters and configuration settings for a machine learning model, including random state, dropout number, learning rate, validation split, number of epochs, batch size, and certain target correction flags.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9965442,
    "start_cell": true,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "random_state_split = 7\n",
    "Dropout_num = 0\n",
    "learning_rate = 6e-6\n",
    "valid = 0.2\n",
    "epochs_num = 3\n",
    "batch_size_num = 16\n",
    "target_corrected = False\n",
    "target_big_corrected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Other",
    "desc": "This snippet defines a variable `example` with a sample string containing a tweet about a new competition, which may be used later for demonstration or testing purposes within the notebook.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.83129525,
    "start_cell": false,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Other",
    "desc": "This snippet defines a variable `example` with a sample HTML string containing tags and text, likely to be used later for demonstrating HTML content removal or text extraction.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.90362614,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "example = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 71,
    "class": "Other",
    "desc": "This snippet defines a dictionary of abbreviations and their expanded forms, which can be used later for text normalization and expanding informal abbreviations in the dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99585694,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 80,
    "class": "Other",
    "desc": "This snippet sets the baseline hyperparameters for the BERT model, including random state, dropout number, learning rate, validation split, number of epochs, batch size, and target correction flags, providing configuration settings for model training and evaluation.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.92335415,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Baseline Hyperparameters\n",
    "random_state_split = 7\n",
    "Dropout_num = 0\n",
    "learning_rate = 3e-5\n",
    "valid = 0.2\n",
    "epochs_num = 3\n",
    "batch_size_num = 16\n",
    "target_corrected = True \n",
    "target_big_corrected = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 81,
    "class": "Other",
    "desc": "This snippet sets customized hyperparameters for the BERT model, adjusting parameters such as dropout rate, learning rate, and validation split while keeping other configurations identical, to tailor the model training process.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.7728451,
    "start_cell": false,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Customized Hyperparameters\n",
    "random_state_split = 7\n",
    "Dropout_num = 0.3 \n",
    "learning_rate = 4e-5 \n",
    "validation_split = 0.3 \n",
    "epochs_num = 3 \n",
    "batch_size_num = 16 \n",
    "target_corrected = True \n",
    "target_big_corrected = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 90,
    "class": "Other",
    "desc": "This snippet contains comments explaining key evaluation metrics—recall, precision, accuracy, and F-measure—providing definitions and formulas to understand the metrics used for assessing the model's performance.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.5548555,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# recall : TP / (TP + FN), 즉 실측이 참인 값 중 정확하게 참이라고 분류한 비율\n",
    "# precision : TP / (TP + FP), 즉 예측이 참인 값중 정확하게 예측한 비율\n",
    "# accuracy : (TP + TN) / All, 즉 실측값대로 정확히 예측한 비율\n",
    "# F-measure : 2 * recall * precision / (recall + precision), recall 과 precision 의 조화평균 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Other",
    "desc": "This code snippet assigns a string, which could be an example tweet containing a URL, to the variable `example`.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.83129525,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Other",
    "desc": "This code snippet assigns a multi-line string, which appears to be an HTML example containing a header, paragraph, and hyperlink, to the variable `example`.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.90362614,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "example = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Other",
    "desc": "This code snippet, which appears to be commented out, suggests applying the spelling correction function to the 'text' column of the combined DataFrame to correct any misspellings in the text data.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9033952,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Other",
    "desc": "This code snippet defines a string variable `example` containing a sample text.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.7619429,
    "start_cell": true,
    "subclass": "train_model",
    "subclass_id": 7
   },
   "outputs": [],
   "source": [
    "example = \"New competition launched :https://www.kaggle.com/c/nlp-getting-started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Other",
    "desc": "This code snippet defines a string variable `example` containing an HTML formatted text block.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.90362614,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "example = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Other",
    "desc": "The code snippet, currently commented out, is intended to concatenate the text of real tweets (target = 0) and fake tweets (target = 1) into separate strings after converting them to lowercase.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9610638,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#real_tweets = ' '.join(train[train['target'] == 0]['text'].str.lower())\n",
    "#fake_tweets = ' '.join(train[train['target'] == 1]['text'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Other",
    "desc": "The code snippet, currently commented out, is intended to initialize a regular expression-based tokenizer from NLTK to split text into words using word characters only.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99831843,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 34,
    "class": "Other",
    "desc": "This code snippet initializes several variables related to text preprocessing and model input, including the vocabulary size, embedding dimension, maximum sequence length, padding/truncation types, out-of-vocabulary token, and the size of the training data split.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9323214,
    "start_cell": true,
    "subclass": "define_variables",
    "subclass_id": 77
   },
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "from math import ceil\n",
    "\n",
    "vocab_size = len(counter)\n",
    "embedding_dim = 16\n",
    "max_length = 30\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\" # Generic token to words out of vocabulary\n",
    "training_size = ceil(train_df.shape[0]*0.8) # 80% of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet visualizes the distribution of survived and non-survived passengers by age and plots the average survival rate by age using KDE plots and bar plots.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.634229,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# .... continue with plot Age column\n",
    "\n",
    "# peaks for survived/not survived passengers by their age\n",
    "facet = sns.FacetGrid(titanic_df, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, titanic_df['Age'].max()))\n",
    "facet.add_legend()\n",
    "\n",
    "# average survived passengers by age\n",
    "fig, axis1 = plt.subplots(1,1,figsize=(18,4))\n",
    "average_age = titanic_df[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\n",
    "sns.barplot(x='Age', y='Survived', data=average_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet sets up a grid of subplots to visualize distributions and counts of various features in the training DataFrame, including survival status, passenger class, gender, ports of embarkation, and age.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99860436,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Set up a grid of plots\n",
    "fig = plt.figure(figsize=fizsize_with_subplots) \n",
    "fig_dims = (3, 2)\n",
    "\n",
    "# Plot death and survival counts\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "df_train['Survived'].value_counts().plot(kind='bar', \n",
    "                                         title='Death and Survival Counts')\n",
    "\n",
    "# Plot Pclass counts\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "df_train['Pclass'].value_counts().plot(kind='bar', \n",
    "                                       title='Passenger Class Counts')\n",
    "\n",
    "# Plot Sex counts\n",
    "plt.subplot2grid(fig_dims, (1, 0))\n",
    "df_train['Sex'].value_counts().plot(kind='bar', \n",
    "                                    title='Gender Counts')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Plot Embarked counts\n",
    "plt.subplot2grid(fig_dims, (1, 1))\n",
    "df_train['Embarked'].value_counts().plot(kind='bar', \n",
    "                                         title='Ports of Embarkation Counts')\n",
    "\n",
    "# Plot the Age histogram\n",
    "plt.subplot2grid(fig_dims, (2, 0))\n",
    "df_train['Age'].hist()\n",
    "plt.title('Age Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet normalizes the cross-tabulation of passenger class against survival status to proportions, then plots it as a stacked bar chart to visualize the survival rate by passenger class.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9974917,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Normalize the cross tab to sum to 1:\n",
    "pclass_xt_pct = pclass_xt.div(pclass_xt.sum(1).astype(float), axis=0)\n",
    "\n",
    "pclass_xt_pct.plot(kind='bar', \n",
    "                   stacked=True, \n",
    "                   title='Survival Rate by Passenger Classes')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet creates a cross-tabulation of the numerical gender values against survival status, normalizes it to proportions, and plots it as a stacked bar chart to visualize the survival rate by gender.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9682996,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sex_val_xt = pd.crosstab(df_train['Sex_Val'], df_train['Survived'])\n",
    "sex_val_xt_pct = sex_val_xt.div(sex_val_xt.sum(1).astype(float), axis=0)\n",
    "sex_val_xt_pct.plot(kind='bar', stacked=True, title='Survival Rate by Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "This code snippet creates separate cross-tabulations for males and females to examine survival rates by passenger class, then normalizes and plots the data as stacked bar charts.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99723804,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Sex\n",
    "females_df = df_train[df_train['Sex'] == 'female']\n",
    "females_xt = pd.crosstab(females_df['Pclass'], df_train['Survived'])\n",
    "females_xt_pct = females_xt.div(females_xt.sum(1).astype(float), axis=0)\n",
    "females_xt_pct.plot(kind='bar', \n",
    "                    stacked=True, \n",
    "                    title='Female Survival Rate by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "\n",
    "# Plot survival rate by Pclass\n",
    "males_df = df_train[df_train['Sex'] == 'male']\n",
    "males_xt = pd.crosstab(males_df['Pclass'], df_train['Survived'])\n",
    "males_xt_pct = males_xt.div(males_xt.sum(1).astype(float), axis=0)\n",
    "males_xt_pct.plot(kind='bar', \n",
    "                  stacked=True, \n",
    "                  title='Male Survival Rate by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Visualization",
    "desc": "This code snippet plots a histogram of the numerical port of embarkation values to visualize the distribution of passengers by their embarkation points.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9972881,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "df_train['Embarked_Val'].hist(bins=len(embarked_locs), range=(0, 3))\n",
    "plt.title('Port of Embarkation Histogram')\n",
    "plt.xlabel('Port of Embarkation')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Visualization",
    "desc": "This code snippet creates a cross-tabulation of the numerical port of embarkation values against survival status, normalizes it to proportions, and plots it as a stacked bar chart to visualize the survival rate by port of embarkation.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99197793,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "embarked_val_xt = pd.crosstab(df_train['Embarked_Val'], df_train['Survived'])\n",
    "embarked_val_xt_pct = \\\n",
    "    embarked_val_xt.div(embarked_val_xt.sum(1).astype(float), axis=0)\n",
    "embarked_val_xt_pct.plot(kind='bar', stacked=True)\n",
    "plt.title('Survival Rate by Port of Embarkation')\n",
    "plt.xlabel('Port of Embarkation')\n",
    "plt.ylabel('Survival Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Visualization",
    "desc": "This code snippet sets up a grid of subplots to display bar charts for the distributions of 'Sex_Val' and 'Pclass' for each port of embarkation in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9984681,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Set up a grid of plots\n",
    "fig = plt.figure(figsize=fizsize_with_subplots) \n",
    "\n",
    "rows = 2\n",
    "cols = 3\n",
    "col_names = ('Sex_Val', 'Pclass')\n",
    "\n",
    "for portIdx in embarked_locs:\n",
    "    for colIdx in range(0, len(col_names)):\n",
    "        plt.subplot2grid((rows, cols), (colIdx, portIdx - 1))\n",
    "        df_train[df_train['Embarked_Val'] == portIdx][col_names[colIdx]] \\\n",
    "            .value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 26,
    "class": "Visualization",
    "desc": "This code snippet creates a grid with two subplots: a stacked histogram displaying the age distribution of survivors and non-survivors, and a scatter plot showing the relationship between survival status and the 'AgeFill' values.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99805164,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Set up a grid of plots\n",
    "fig, axes = plt.subplots(2, 1, figsize=fizsize_with_subplots)\n",
    "\n",
    "# Histogram of AgeFill segmented by Survived\n",
    "df1 = df_train[df_train['Survived'] == 0]['Age']\n",
    "df2 = df_train[df_train['Survived'] == 1]['Age']\n",
    "max_age = max(df_train['AgeFill'])\n",
    "axes[0].hist([df1, df2], \n",
    "             bins=max_age / bin_size, \n",
    "             range=(1, max_age), \n",
    "             stacked=True)\n",
    "axes[0].legend(('Died', 'Survived'), loc='best')\n",
    "axes[0].set_title('Survivors by Age Groups Histogram')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Scatter plot Survived and AgeFill\n",
    "axes[1].scatter(df_train['Survived'], df_train['AgeFill'])\n",
    "axes[1].set_title('Survivors by Age Plot')\n",
    "axes[1].set_xlabel('Survived')\n",
    "axes[1].set_ylabel('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 27,
    "class": "Visualization",
    "desc": "This code snippet generates kernel density plots of the 'AgeFill' values for each passenger class, overlaid on the same plot, to visualize the age distribution across different classes.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.76863956,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "for pclass in passenger_classes:\n",
    "    df_train.AgeFill[df_train.Pclass == pclass].plot(kind='kde')\n",
    "plt.title('Age Density Plot by Passenger Class')\n",
    "plt.xlabel('Age')\n",
    "plt.legend(('1st Class', '2nd Class', '3rd Class'), loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Visualization",
    "desc": "This code snippet sets up a grid with three subplots to display histograms of the 'AgeFill' values for survivors, female survivors, and first-class survivors respectively.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9986945,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Set up a grid of plots\n",
    "fig = plt.figure(figsize=fizsize_with_subplots) \n",
    "fig_dims = (3, 1)\n",
    "\n",
    "# Plot the AgeFill histogram for Survivors\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "survived_df = df_train[df_train['Survived'] == 1]\n",
    "survived_df['AgeFill'].hist(bins=max_age / bin_size, range=(1, max_age))\n",
    "\n",
    "# Plot the AgeFill histogram for Females\n",
    "plt.subplot2grid(fig_dims, (1, 0))\n",
    "females_df = df_train[(df_train['Sex_Val'] == 0) & (df_train['Survived'] == 1)]\n",
    "females_df['AgeFill'].hist(bins=max_age / bin_size, range=(1, max_age))\n",
    "\n",
    "# Plot the AgeFill histogram for first class passengers\n",
    "plt.subplot2grid(fig_dims, (2, 0))\n",
    "class1_df = df_train[(df_train['Pclass'] == 1) & (df_train['Survived'] == 1)]\n",
    "class1_df['AgeFill'].hist(bins=max_age / bin_size, range=(1, max_age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 30,
    "class": "Visualization",
    "desc": "This code snippet plots a histogram of the 'FamilySize' values to visualize the distribution of family sizes among the passengers in the training DataFrame.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.99601966,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "df_train['FamilySize'].hist()\n",
    "plt.title('Family Size Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 31,
    "class": "Visualization",
    "desc": "This code snippet creates a stacked histogram to compare the survival counts based on family size, distinguishing between those who died and those who survived.",
    "notebook_id": 2,
    "predicted_subclass_probability": 0.9982911,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Get the unique values of Embarked and its maximum\n",
    "family_sizes = sorted(df_train['FamilySize'].unique())\n",
    "family_size_max = max(family_sizes)\n",
    "\n",
    "df1 = df_train[df_train['Survived'] == 0]['FamilySize']\n",
    "df2 = df_train[df_train['Survived'] == 1]['FamilySize']\n",
    "plt.hist([df1, df2], \n",
    "         bins=family_size_max + 1, \n",
    "         range=(0, family_size_max), \n",
    "         stacked=True)\n",
    "plt.legend(('Died', 'Survived'), loc='best')\n",
    "plt.title('Survivors by Family Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "This code creates two count plots to visualize the relationship between 'Parch' (Number of Parents/Children Aboard) and 'SibSp' (Number of Siblings/Spouses Aboard) with the 'Survived' outcome.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9933176040649414,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# assess the 'Parch' attribute (Number of Parents/Children Aboard)\n",
    "sns.countplot(x='Parch', hue='Survived', data=df_full, ax=ax1)\n",
    "\n",
    "# assess the 'SibSp' releation (Number of Siblings/Spouses Aboard)\n",
    "sns.countplot(x='SibSp', hue='Survived', data=df_full, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Visualization",
    "desc": "This code creates a new feature 'Familiy_members' as the sum of 'SibSp', 'Parch', and 1, and then visualizes its relationship with the 'Survived' outcome using a count plot.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9845625758171082,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "df_full['Familiy_members'] = df_full.SibSp + df_full.Parch + 1\n",
    "\n",
    "# assess the new, derived parameter 'Familiy_members'\n",
    "sns.countplot(x='Familiy_members', hue='Survived', data=df_full);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Visualization",
    "desc": "This code creates a new categorical feature 'Familiy_size' based on the number of 'Familiy_members' and visualizes its relationship with the 'Survived' outcome using a count plot.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.8692624568939209,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "df_full['Familiy_size'] = df_full.loc[:, 'Familiy_members'].map({1:'S', 2:'M', 3:'M', 4:'M', 5:'L', 6:'L', 7:'L', 8:'L', 9:'L', 10:'L', 11:'L'})\n",
    "sns.countplot(x='Familiy_size', hue='Survived', data=df_full);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Visualization",
    "desc": "This code creates a new binary feature 'Mother' for female passengers with at least one parent/child aboard and aged 18 or older, and then visualizes its relationship with the 'Survived' outcome using a count plot.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.7714844346046448,
    "start_cell": false,
    "subclass": "feature_engineering",
    "subclass_id": 8
   },
   "outputs": [],
   "source": [
    "\n",
    "s_group = df_full.groupby('Surname').Surname.count() >= 2\n",
    "df_full.loc[(df_full.Surname.isin(s_group[s_group == True].index)) & (df_full.Sex == 'female') & (df_full.Parch > 0) & (df_full.Age >= 18),'Mother'] = 1\n",
    "df_full.loc[df_full.Mother.isnull(), 'Mother'] = 0\n",
    "\n",
    "sns.countplot(x='Mother', hue='Survived', data=df_full[df_full.Mother == 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Visualization",
    "desc": "This code creates two histograms to compare the age distributions of passengers who survived and those who did not, displayed side by side.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9971736669540404,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "df_full.loc[df_full.Survived == 1].groupby('Age').Age.count().hist(ax=ax1, bins=10, normed=1)\n",
    "df_full.loc[df_full.Survived == 0].groupby('Age').Age.count().hist(ax=ax2, bins=100, normed=1)\n",
    "ax1.set_title(\"Count of survivors\")\n",
    "ax1.set_xlabel(\"Age\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Count of victims\")\n",
    "ax2.set_xlabel(\"Age\")\n",
    "ax2.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Visualization",
    "desc": "This code assigns age groups to passengers based on their age and then visualizes the relationship between these age groups and the 'Survived' outcome using a count plot.",
    "notebook_id": 5,
    "predicted_subclass_probability": 0.9676710963249208,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "df_full.loc[df_full.Age <= 6, 'Age_group'] = 'B' # Baby\n",
    "df_full.loc[(df_full.Age > 6) & (df_full.Age <= 12), 'Age_group'] = 'C' # Children\n",
    "df_full.loc[(df_full.Age > 12) & (df_full.Age <= 17), 'Age_group'] = 'Y' # Youngster\n",
    "df_full.loc[(df_full.Age > 17) & (df_full.Age <= 26), 'Age_group'] = 'S' # Student\n",
    "df_full.loc[(df_full.Age > 26) & (df_full.Age <= 50), 'Age_group'] = 'A' # Adult\n",
    "df_full.loc[(df_full.Age > 50), 'Age_group'] = 'P' # Pensionier\n",
    "\n",
    "sns.countplot(x='Age_group', hue='Survived', data=df_full, order=['B','C','Y','S','A','P']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "The code snippet visualizes the count of missing values in the columns of the 'titanic' dataset with missing data using a horizontal bar plot.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99332523,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "pd.options.display.mpl_style = 'default'\n",
    "labels = []\n",
    "values = []\n",
    "for col in null_columns:\n",
    "    labels.append(col)\n",
    "    values.append(titanic[col].isnull().sum())\n",
    "ind = np.arange(len(labels))\n",
    "width=0.6\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "rects = ax.barh(ind, np.array(values), color='purple')\n",
    "ax.set_yticks(ind+((width)/2.))\n",
    "ax.set_yticklabels(labels, rotation='horizontal')\n",
    "ax.set_xlabel(\"Count of missing values\")\n",
    "ax.set_ylabel(\"Column Names\")\n",
    "ax.set_title(\"Variables with missing values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "The code snippet generates histograms for all numerical columns in the 'titanic' dataset with 10 bins and a specified figure size, while disabling the grid.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9816259,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "titanic.hist(bins=10,figsize=(9,7),grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "The code snippet creates a facet grid of histograms using the seaborn library to display the distribution of the 'Age' column, segmented by 'Sex' and 'Survived' status, with each plot colored purple.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99729246,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(titanic, col=\"Sex\", row=\"Survived\", margin_titles=True)\n",
    "g.map(plt.hist, \"Age\",color=\"purple\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "The code snippet generates a facet grid of scatter plots using seaborn to show the relationship between 'Fare' and 'Age', segmented by the 'Pclass' column and 'Survived' status, with different colors for survivors and non-survivors, and adds a legend.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99597377,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(titanic, hue=\"Survived\", col=\"Pclass\", margin_titles=True,\n",
    "                  palette={1:\"seagreen\", 0:\"gray\"})\n",
    "g=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "The code snippet creates a facet grid of scatter plots using seaborn to display the relationship between 'Fare' and 'Age', segmented by 'Sex' and 'Survived' status, with different markers for survivors and non-survivors, and adds a title and legend.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99738747,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(titanic, hue=\"Survived\", col=\"Sex\", margin_titles=True,\n",
    "                palette=\"Set1\",hue_kws=dict(marker=[\"^\", \"v\"]))\n",
    "g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('Survival by Gender , Age and Fare');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "The code snippet generates a bar plot to display the count of passengers from each boarding location ('Embarked' column) in the 'titanic' dataset, with semi-transparent bars and a title.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9982962,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "titanic.Embarked.value_counts().plot(kind='bar', alpha=0.55)\n",
    "plt.title(\"Passengers per boarding location\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Visualization",
    "desc": "The code snippet creates a factor plot using seaborn to visualize the survival rate by boarding location ('Embarked' column) in the 'titanic' dataset, with the bars colored red.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.90652424,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x = 'Embarked',y=\"Survived\", data = titanic,color=\"r\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Visualization",
    "desc": "The code snippet generates a series of bar plots using seaborn to show the survival rate by sex and passenger class ('Pclass') in the 'titanic' dataset, with adjusted labels, titles, y-axis limits, and a supertitle.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9973578,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "g = sns.factorplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\",\n",
    "                    data=titanic, saturation=.5,\n",
    "                    kind=\"bar\", ci=None, aspect=.6)\n",
    "(g.set_axis_labels(\"\", \"Survival Rate\")\n",
    "    .set_xticklabels([\"Men\", \"Women\"])\n",
    "    .set_titles(\"{col_name} {col_var}\")\n",
    "    .set(ylim=(0, 1))\n",
    "    .despine(left=True))  \n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('How many Men and Women Survived by Passenger Class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Visualization",
    "desc": "The code snippet creates a box plot and a strip plot using seaborn to visualize the distribution of 'Age' by survival status ('Survived') in the 'titanic' dataset, with a title added.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.7356959,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"Survived\", y=\"Age\", \n",
    "                data=titanic)\n",
    "ax = sns.stripplot(x=\"Survived\", y=\"Age\",\n",
    "                   data=titanic, jitter=True,\n",
    "                   edgecolor=\"gray\")\n",
    "sns.plt.title(\"Survival by Age\",fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Visualization",
    "desc": "The code snippet generates kernel density estimate (KDE) plots for the 'Age' distribution within each passenger class ('Pclass') in the 'titanic' dataset and adds labels, a title, and a legend to the plot.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9683331,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "titanic.Age[titanic.Pclass == 1].plot(kind='kde')    \n",
    "titanic.Age[titanic.Pclass == 2].plot(kind='kde')\n",
    "titanic.Age[titanic.Pclass == 3].plot(kind='kde')\n",
    " # plots an axis lable\n",
    "plt.xlabel(\"Age\")    \n",
    "plt.title(\"Age Distribution within classes\")\n",
    "# sets our legend for our graph.\n",
    "plt.legend(('1st Class', '2nd Class','3rd Class'),loc='best') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Visualization",
    "desc": "The code snippet calculates the correlation matrix for the 'titanic' dataset and visualizes it using a heatmap with annotations, specific color mapping, and a title to show the correlation between features.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99818474,
    "start_cell": false,
    "subclass": "heatmap",
    "subclass_id": 80
   },
   "outputs": [],
   "source": [
    "corr=titanic.corr()#[\"Survived\"]\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.heatmap(corr, vmax=.8, linewidths=0.01,\n",
    "            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\n",
    "plt.title('Correlation between features');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Visualization",
    "desc": "The code snippet creates a series of violin plots using seaborn to visualize the distribution of 'Age' by boarding location ('Embarked'), sex, and passenger class ('Pclass') for the non-null 'Embarked' entries in the 'titanic' dataset, with specific orientations, sizes, and color palettes.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9959103,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"Age\", y=\"Embarked\",\n",
    "                    hue=\"Sex\", row=\"Pclass\",\n",
    "                    data=titanic[titanic.Embarked.notnull()],\n",
    "                    orient=\"h\", size=2, aspect=3.5, \n",
    "                   palette={'male':\"purple\", 'female':\"blue\"},\n",
    "                    kind=\"violin\", split=True, cut=0, bw=.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 22,
    "class": "Visualization",
    "desc": "The code snippet creates a box plot using seaborn to visualize the distribution of 'Fare' by boarding location ('Embarked') and passenger class ('Pclass') in the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.98341227,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=titanic);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 28,
    "class": "Visualization",
    "desc": "The code snippet creates a series of count plots using seaborn to visualize the number of survivors segmented by the 'Deck' category in the 'titanic' dataset, specifically for non-null 'Deck' entries.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99777406,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(\"Survived\", col=\"Deck\", col_wrap=4,\n",
    "                    data=titanic[titanic.Deck.notnull()],\n",
    "                    kind=\"count\", size=2.5, aspect=.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 29,
    "class": "Visualization",
    "desc": "The code snippet converts the 'Deck' column to an object type, sorts the 'titanic' DataFrame by the 'Deck' column, and then creates a facet grid of box plots to show the distribution of 'Age' by 'Deck' within each passenger class ('Pclass').",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99713624,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "titanic = titanic.assign(Deck=titanic.Deck.astype(object)).sort(\"Deck\")\n",
    "g = sns.FacetGrid(titanic, col=\"Pclass\", sharex=False,\n",
    "                  gridspec_kws={\"width_ratios\": [5, 3, 3]})\n",
    "g.map(sns.boxplot, \"Deck\", \"Age\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Visualization",
    "desc": "The code snippet creates a factor plot using seaborn to visualize the survival rate by discrete family size categories ('FsizeD') in the 'titanic' dataset.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99782205,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"FsizeD\", y=\"Survived\", data=titanic);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 41,
    "class": "Visualization",
    "desc": "The code snippet creates a histogram to visualize the age distribution in the 'titanic' dataset, excluding NaN values, with customized plotting context and style.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9985392,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.distplot(titanic[\"Age\"].dropna(),\n",
    "                 bins=80,\n",
    "                 kde=False,\n",
    "                 color=\"red\")\n",
    "    sns.plt.title(\"Age Distribution\")\n",
    "    plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 44,
    "class": "Visualization",
    "desc": "The code snippet creates a histogram to visualize the age distribution in the 'titanic' dataset, excluding NaN values, with customized plotting context, style, color, and x-axis limits from 15 to 100.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9984162,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.distplot(titanic[\"Age\"].dropna(),\n",
    "                 bins=80,\n",
    "                 kde=False,\n",
    "                 color=\"tomato\")\n",
    "    sns.plt.title(\"Age Distribution\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlim((15,100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Visualization",
    "desc": "This code snippet generates a Seaborn violin plot to visualize the relationship between sex, age, and survival status in the training dataset.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.98996603,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Seaborn Violin Plot would be a nice way to look at this relationship\n",
    "sns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=train,\n",
    "               split=True, cut =0, inner=\"stick\", palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet creates a Seaborn swarm plot to visualize the relationship between passenger class, fare, and survival status in the training dataset.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9973636,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# swarm plot of embarked and fare\n",
    "sns.swarmplot(x=\"Pclass\", y=\"Fare\", hue=\"Survived\", data=train, palette=\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet creates and visualizes a heatmap displaying the survival rates based on the 'Child' status and 'Family_size' within the training dataset.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.99873966,
    "start_cell": false,
    "subclass": "heatmap",
    "subclass_id": 80
   },
   "outputs": [],
   "source": [
    "# Produce heatmap\n",
    "family = train.pivot_table(values=\"Survived\", index = [\"Child\"], columns = \"Family_size\")\n",
    "\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "htmp = sns.heatmap(family, annot=True, cmap=\"YlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet rolls up less common titles into a 'Vip' category in both the training and test datasets and then creates a Seaborn bar plot to show the survival chance based on the 'Title' category in the training dataset.",
    "notebook_id": 8,
    "predicted_subclass_probability": 0.9973942,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# roll-up titles\n",
    "train[\"Title\"] = [x if x in [\"Miss\", \"Mr\", \"Mrs\", \"Master\", \"Dr\", \"Rev\"] else \"Vip\" for x in train[\"Title\"] ]\n",
    "test[\"Title\"] = [x if x in [\"Miss\", \"Mr\", \"Mrs\", \"Master\", \"Dr\", \"Rev\"] else \"Vip\" for x in test[\"Title\"] ]\n",
    "\n",
    "# Seaborn Plot to show survival based on Title\n",
    "bar = sns.barplot(\"Title\", \"Survived\", data = train, palette=\"Greys\")\n",
    "bar.set_ylabel(\"Chance of Survival\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "The code snippet defines several functions for visualizing data distributions, categories, correlations, and feature importances using various plotting techniques.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "The code snippet generates a heatmap to visualize the correlation matrix of the variables in the Titanic dataset.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.9740528,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "The code snippet plots the distribution of passenger ages, separated by survival status and gender, using kernel density estimates (KDE).",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.99385697,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This placeholder code snippet defines an exercise to plot the distribution of passenger fares, separated by survival status, though it lacks implementation details.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.93868834,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "The code snippet generates a bar plot to visualize the survival rate of passengers based on their port of embarkation.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This placeholder code snippet defines an exercise to plot the survival rate of passengers based on their gender, though it lacks implementation details.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.63326055,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This placeholder code snippet defines an exercise to plot the survival rate of passengers based on their passenger class (Pclass), though it lacks implementation details.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.45175064,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This placeholder code snippet defines an exercise to plot the survival rate of passengers based on the number of siblings/spouses aboard (SibSp), though it lacks implementation details.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.5286518,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This placeholder code snippet defines an exercise to plot the survival rate of passengers based on the number of parents/children aboard (Parch), though it lacks implementation details.",
    "notebook_id": 11,
    "predicted_subclass_probability": 0.71496516,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines several functions for visualizing data distributions, categories, correlation maps, and variable importance in a dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9244308471679688,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Visualization",
    "desc": "The code snippet generates histograms for the 'Age' and 'Fare' columns in the Titanic dataset, showing their distributions and skewness.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.99654883146286,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "#titanic.describe( include = 'all' )\n",
    "plot_histograms( titanic , ['Age', 'Fare'] , 4 , 4  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "The code snippet (when uncommented) would plot a correlation heatmap for the variables in the Titanic dataset to visualize the relationships between them.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9970170259475708,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "The code snippet generates a KDE plot to visualize the distributions of the 'Age' variable for passengers who survived and those who did not in the Titanic dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9977664947509766,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "The code snippet generates a KDE plot to visualize the distributions of the 'Fare' variable (filtered to exclude fares above 200) for passengers who survived and those who did not in the Titanic dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9949631690979004,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic[ titanic.Fare < 200 ] , var = 'Fare' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "The code snippet generates a bar plot to visualize the survival rate by the 'Embarked' category in the Titanic dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9769639372825624,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "The code snippet generates a bar plot to visualize the survival rate by the 'Sex' category in the Titanic dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9957703948020936,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "The code snippet generates a bar plot to visualize the survival rate by the 'Pclass' (Passenger Class) category in the Titanic dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9969442486763,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "The code snippet generates a bar plot to visualize the survival rate by the 'Sibs' (number of siblings/spouses aboard) category in the Titanic dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.9976021647453308,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "plot_categories( titanic , cat = 'Sibs' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "The code snippet (if it included the function call) would generate a bar plot to visualize the survival rate by the 'Parch' (number of parents/children aboard) category in the Titanic dataset.",
    "notebook_id": 12,
    "predicted_subclass_probability": 0.4615671038627624,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code defines several functions for visualizing histograms, distributions, categories, correlation maps, and variable importance for a given dataset. ",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9244308471679688,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This commented code, if executed, would plot a correlation heatmap for the features in the Titanic dataset.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9970170259475708,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code plots the distribution of passenger ages, segmented by whether they survived or not.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9977664947509766,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code plots the distribution of passenger fares, segmented by whether they survived or not.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9958156943321228,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Fare' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code plots the survival rate of passengers based on their embarkation point.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9769639372825624,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code plots the survival rate of passengers based on their sex.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.9829771518707277,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code plots the survival rate of passengers based on their passenger class by calling the `plot_categories` function.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.3435868322849273,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code plots the survival rate of passengers based on their number of siblings or spouses aboard the Titanic by calling the `plot_categories` function.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.4484284222126007,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code plots the survival rate of passengers based on the number of parents or children they had aboard the Titanic by calling the `plot_categories` function.",
    "notebook_id": 13,
    "predicted_subclass_probability": 0.4615671038627624,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "The code snippet contains multiple visualization functions for plotting histograms, distributions, category relationships, correlation maps, and variable importance. ",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "The commented out code snippet is intended to plot a correlation heatmap of the variables in the Titanic dataset to visualize relationships between the features.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.997017,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "The code snippet plots the distribution of the 'Age' variable for passengers who survived and did not survive, using a Kernel Density Estimate (KDE) plot to visualize the distribution.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9977665,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "The code snippet plots the distribution of the 'Fare' variable for passengers who survived and did not survive, using a Kernel Density Estimate (KDE) plot to visualize the distribution.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.9976165,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "plot_distribution( titanic , var = 'Fare' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "The code snippet plots the survival rate by the 'Embarked' category, using bar plots to display the relationship between the embarkation point and survival.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "The code snippet plots the survival rate by the 'Pclass' category, using bar plots to display the relationship between passenger class and survival.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99696416,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "The code snippet plots the survival rate by the 'SibSp' (siblings/spouses aboard) category, using bar plots to display the relationship between the number of siblings/spouses and survival.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.99818593,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Excersise 2\n",
    "plot_categories( titanic , cat = 'SibSp' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "The commented out code snippet indicates an intention to plot the survival rate by the 'SibSp' (siblings/spouses aboard) category, likely using a bar plot.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.44842842,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "The commented out code snippet indicates an intention to plot the survival rate by the 'Parch' (parents/children aboard) category, likely using a bar plot.",
    "notebook_id": 14,
    "predicted_subclass_probability": 0.4615671,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines several functions for plotting histograms, distribution plots, category plots, correlation maps, and a variable importance plot, as well as a function to describe dataframe variables. ",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet generates and displays a correlation heatmap for the 'titanic' DataFrame to visualize the relationships between its numerical variables.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9740528,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet creates and shows a distribution plot of the 'Age' variable, segmented by the 'Survived' target variable and further divided by the 'Sex' variable.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99385697,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet creates and shows a distribution plot of the 'Fare' variable, segmented by the 'Survived' target variable.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.99561983,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution(titanic, var = 'Fare', target = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet creates and shows a bar plot of the survival rate categorized by the 'Embarked' variable.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet creates and shows a bar plot of the survival rate categorized by the 'Sex' variable.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.98297715,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet creates and shows a bar plot of the survival rate categorized by the 'Pclass' variable.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9763035,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet creates and shows a bar plot of the survival rate categorized by the 'SibSp' (siblings/spouses aboard) variable.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.9788583,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n",
    "plot_categories( titanic , cat = 'SibSp' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet creates and shows a bar plot of the survival rate categorized by the 'Parch' (parents/children aboard) variable.",
    "notebook_id": 15,
    "predicted_subclass_probability": 0.92094743,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n",
    "plot_categories( titanic , cat = 'Parch' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines multiple functions for visualizing data distributions, categorial relationships, correlation matrices, and variable importance via different types of plots.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9244308471679688,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet generates a heatmap to visualize the correlation matrix of the numerical features in the Titanic dataset.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9740527868270874,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet creates a KDE plot using Seaborn to show the distribution of passengers' ages, separated by survival status and gender.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.993856966495514,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This placeholder comment indicates the intention to plot the distributions of passengers' fare based on their survival status, similar to the previous age distribution plot.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.938688337802887,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet creates bar plots to visualize the survival rate of passengers based on their port of embarkation.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9769639372825624,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet creates bar plots to visualize the survival rate of passengers based on their gender.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9957703948020936,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet creates bar plots to visualize the survival rate of passengers based on their ticket class.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.9969442486763,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This placeholder comment indicates the intention to plot the survival rate of passengers based on the number of siblings or spouses they had aboard the Titanic.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.528651773929596,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This placeholder comment indicates the intention to plot the survival rate of passengers based on the number of parents or children they had aboard the Titanic.",
    "notebook_id": 16,
    "predicted_subclass_probability": 0.7149651646614075,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines multiple functions for creating various visualizations, including histograms, distributions, category plots, correlation maps, and variable importance plots.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9244308471679688,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet creates and displays a correlation heatmap for the features in the Titanic dataset to visualize the relationships between different variables.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9740527868270874,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet plots the distribution of the 'Age' variable for passengers who survived and did not survive, further separated by their sex.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.993856966495514,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet plots the distribution of the 'Fare' variable for passengers who survived and did not survive in the Titanic dataset.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.99569571018219,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Fare' , target = 'Survived'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers categorized by the 'Embarked' variable, showing the effect of the embarkation point on survival.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9769639372825624,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers categorized by 'Sex' to visualize the impact of gender on the likelihood of survival.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9829771518707277,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers categorized by 'Pclass' to show how the class of travel influenced survival chances.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9763035178184508,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers categorized by 'SibSp' (number of siblings/spouses aboard) to illustrate the relationship between family connections and survival chances.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9788582921028136,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n",
    "plot_categories( titanic , cat = 'SibSp' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers categorized by 'Parch' (number of parents/children aboard) to show the effect of having family onboard on survival rates.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.9209474325180054,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n",
    "plot_categories( titanic , cat = 'Parch' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Visualization",
    "desc": "This code snippet plots the importance of variables in the training set by fitting a DecisionTreeClassifier and displaying the feature importances in a horizontal bar chart.",
    "notebook_id": 17,
    "predicted_subclass_probability": 0.510510265827179,
    "start_cell": false,
    "subclass": "plot_predictions",
    "subclass_id": 56
   },
   "outputs": [],
   "source": [
    "plot_variable_importance(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines several functions used for plotting histograms, distributions, categorical variable relationships, correlation maps, and variable importance in a dataset. ",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet generates a heatmap to visualize the correlation matrix of the features in the Titanic dataset.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9740528,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet creates a distribution plot of passengers' age, segmented by survival status and sex, to analyze how age and gender correlate with survival in the Titanic dataset.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.99385697,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet generates a distribution plot of passengers' fare, segmented by survival status and sex, to explore the relationship between fare, gender, and survival in the Titanic dataset.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9960588,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Fare' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to show the survival rate of Titanic passengers based on their port of embarkation.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet generates a bar plot to illustrate the survival rate of Titanic passengers categorized by their sex.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.98297715,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the survival rate of Titanic passengers grouped by their passenger class (Pclass).",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9763035,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet generates a bar plot to show the survival rate of Titanic passengers based on the number of siblings or spouses (SibSp) they had on board.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.9788583,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n",
    "plot_categories( titanic , cat = 'SibSp' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to display the survival rate of Titanic passengers based on the number of parents or children (Parch) they had on board.",
    "notebook_id": 18,
    "predicted_subclass_probability": 0.92094743,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n",
    "plot_categories( titanic , cat = 'Parch' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines several functions for visualizing data distributions, category-wise plots, correlation heatmaps, and variable importance using various plotting libraries.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9244308471679688,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet generates a correlation heatmap of the Titanic dataset to visualize the relationships between different variables.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9740527868270874,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet plots the age distributions of passengers who survived or did not survive, separated by gender, to explore survival patterns based on age and sex.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.993856966495514,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet aims to generate plots for analyzing the fare distributions of passengers who survived or did not survive to explore any patterns related to fare.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9013327956199646,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet generates bar plots to examine the survival rate of passengers based on their embarkation point from the Titanic dataset.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9769639372825624,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet intends to create plots for analyzing the survival rate by sex in the Titanic dataset, exploring how gender affected survival chances.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.551699161529541,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet aims to create plots for analyzing the survival rate by passenger class (Pclass) in the Titanic dataset, investigating how class affected survival chances.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.3435868322849273,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet aims to generate plots to analyze the survival rate based on the number of siblings/spouses aboard (SibSp) in the Titanic dataset.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.4484284222126007,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet aims to generate plots to analyze the survival rate based on the number of parents/children aboard (Parch) in the Titanic dataset.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.4615671038627624,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines several functions for plotting histograms, distributions, categories, correlation maps, and variable importance, as well as a function to describe the dataset.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet generates a heatmap to visualize the correlation matrix of the variables in the 'titanic' dataset.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9740528,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet plots the distribution of the 'Age' variable against the 'Survived' target variable, further broken down by the 'Sex' variable.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99385697,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet plots the distribution of the 'Fare' variable against the 'Survived' target variable for passengers in the 'titanic' dataset.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9958157,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Fare' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers in the 'titanic' dataset categorized by their 'Embarked' location.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers in the 'titanic' dataset categorized by their 'Sex'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9861507,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "plot_categories( titanic, cat=\"Sex\", target = \"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers in the 'titanic' dataset categorized by their 'Pclass'.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9760642,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n",
    "plot_categories( titanic , cat=\"Pclass\", target=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers in the 'titanic' dataset categorized by their 'SibSp' (number of siblings or spouses aboard) variable.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.97842807,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n",
    "plot_categories( titanic , cat=\"SibSp\", target=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet plots the survival rate of passengers in the 'titanic' dataset categorized by their 'Parch' (number of parents or children aboard) variable.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9514561,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n",
    "plot_categories( titanic , cat=\"Parch\", target=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "The snippet defines multiple functions to plot histograms, distribution plots, categorical plots, correlation maps, variable importance, and provide detailed descriptions of a DataFrame's variables.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "The snippet calls a previously defined function to plot a correlation heatmap of the Titanic dataset, which visualizes the relationships between different numerical features.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9740528,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "The snippet calls a previously defined function to plot the age distribution of passengers who survived or did not survive, stratified by gender.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.99385697,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "The snippet sets up an area where the process of plotting the fare distribution of passengers, categorized by survival status, will be executed.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.9013328,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "The snippet calls a previously defined function to plot the survival rate of passengers categorized by their embarkation port.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "The snippet designates an area where a plot will be created to visualize the survival rate of passengers based on their gender.",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.55169916,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "The snippet designates an area where a plot will be created to visualize the survival rate of passengers based on their passenger class (Pclass).",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.34358683,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "The snippet designates an area where a plot will be created to visualize the survival rate of passengers based on the number of siblings or spouses aboard (SibSp).",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.44842842,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "The snippet designates an area where a plot will be created to visualize the survival rate of passengers based on the number of parents or children aboard (Parch).",
    "notebook_id": 21,
    "predicted_subclass_probability": 0.4615671,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Visualization",
    "desc": "This code snippet creates a heatmap to visualize the Pearson correlation coefficients among the features in the transformed training dataset.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9978072,
    "start_cell": true,
    "subclass": "heatmap",
    "subclass_id": 80
   },
   "outputs": [],
   "source": [
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet visualizes the age distribution of survived and not survived passengers using a KDE plot and shows the average survival rate by age using a bar plot.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.634229,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# .... continue with plot Age column\n",
    "\n",
    "# peaks for survived/not survived passengers by their age\n",
    "facet = sns.FacetGrid(titanic_df, hue=\"Survived\",aspect=4)\n",
    "facet.map(sns.kdeplot,'Age',shade= True)\n",
    "facet.set(xlim=(0, titanic_df['Age'].max()))\n",
    "facet.add_legend()\n",
    "\n",
    "# average survived passengers by age\n",
    "fig, axis1 = plt.subplots(1,1,figsize=(18,4))\n",
    "average_age = titanic_df[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\n",
    "sns.barplot(x='Age', y='Survived', data=average_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines several functions for creating visualizations, including histograms, distribution plots, category plots, a correlation heatmap, and variable importance plots. ",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9244308471679688,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet generates a correlation heatmap for the Titanic dataset to visualize the relationships between its numerical variables.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9740527868270874,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet creates a distribution plot to visualize the age distribution of passengers in the Titanic dataset, categorized by survival status and sex.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.993856966495514,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet creates a distribution plot to visualize the fare distribution of passengers in the Titanic dataset, categorized by their survival status.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9958156943321228,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Fare' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the survival rate of passengers in the Titanic dataset, categorized by their embarkation points.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9769639372825624,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the survival rate of passengers in the Titanic dataset, categorized by their sex.",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9829771518707277,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the survival rate of passengers in the Titanic dataset, categorized by their passenger class (Pclass).",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9763035178184508,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the survival rate of passengers in the Titanic dataset, categorized by the number of siblings or spouses aboard (SibSp).",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9788582921028136,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n",
    "plot_categories( titanic , cat = 'SibSp' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the survival rate of passengers in the Titanic dataset, categorized by the number of parents or children aboard (Parch).",
    "notebook_id": 24,
    "predicted_subclass_probability": 0.9209474325180054,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n",
    "plot_categories( titanic , cat = 'Parch' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code defines several plotting and descriptive functions for visualizing data distribution, category distributions, correlation maps, and variable importance, as well as describing data features. ",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code generates a correlation heatmap for the Titanic dataset to visualize the relationships between different numerical variables.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.9740528,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code creates a distribution plot of the 'Age' variable in the Titanic dataset, differentiated by survival status, and further segmented by gender.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.99385697,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code comment instructs to create a plot for the 'Fare' distributions in the Titanic dataset segmented by the survival status of passengers, but the actual plotting code is not provided.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.93868834,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code generates a bar plot to visualize the survival rate of passengers based on the 'Embarked' category within the Titanic dataset.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code comment instructs to create a plot for the survival rate based on the 'Sex' category in the Titanic dataset, but the actual plotting code is not provided.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.63326055,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code comment instructs to create a plot for the survival rate based on the 'Pclass' category in the Titanic dataset, but the actual plotting code is not provided.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.45175064,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code comment instructs to create a plot for the survival rate based on the 'SibSp' (number of siblings/spouses aboard) category in the Titanic dataset, but the actual plotting code is not provided.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.5286518,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code comment instructs to create a plot for the survival rate based on the 'Parch' (number of parents/children aboard) category in the Titanic dataset, but the actual plotting code is not provided.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.71496516,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "The code snippet defines several functions for generating various types of visualizations, including histograms, distribution plots, category plots, correlation maps, and variable importance plots.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9244308471679688,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "The code snippet generates and displays a heatmap to visualize the correlation matrix of the Titanic dataset, showing the relationships between different variables.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9740527868270874,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "The code snippet plots the age distribution of passengers who survived versus those who did not, segmented by sex, using kernel density estimates.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.993856966495514,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "The code snippet plots the fare distribution of passengers who survived versus those who did not using kernel density estimates.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.996241807937622,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic , cat = 'Fare' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "The code snippet plots the survival rate by the embarked location of the passengers using bar plots.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.9769639372825624,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "The code snippet plots the survival rate by various categorical variables—Sex, SibSp (number of siblings/spouses aboard), Parch (number of parents/children aboard), and Pclass (passenger class)—using bar plots.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.985077977180481,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived')\n",
    "plot_categories( titanic , cat = 'SibSp' , target = 'Survived')\n",
    "plot_categories( titanic , cat = 'Parch' , target = 'Survived')\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "The code snippet plots the distribution of passenger class (Pclass) by survival status using kernel density estimates.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.8015320301055908,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n",
    "plot_distribution(titanic, var =\"Pclass\", target=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "The code snippet plots the distribution of the number of siblings/spouses aboard (SibSp) by survival status using kernel density estimates.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.928896188735962,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n",
    "plot_distribution(titanic, var=\"SibSp\", target=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "The code snippet plots the distribution of the number of parents/children aboard (Parch) by survival status using kernel density estimates.",
    "notebook_id": 26,
    "predicted_subclass_probability": 0.8337555527687073,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n",
    "plot_distribution(titanic, var=\"Parch\", target=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code defines various plotting functions to visualize histograms, distributions, categorical data, correlation maps, and feature importance, along with a function to describe variables in the DataFrame. ",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code generates a heatmap to visualize the correlation between numeric features in the Titanic dataset.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9740528,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code plots the distribution of passengers' ages, segmented by survival status and sex, to explore the relationship between age, gender, and survival.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99385697,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code plots the distribution of passengers' fares, segmented by survival status and sex, to explore the relationship between fare, gender, and survival.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99687386,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution(titanic, var = \"Fare\", target = \"Survived\", row = \"Sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code plots the survival rate of passengers grouped by their embarked location to explore the relationship between the port of embarkation and survival.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code iterates over specified categorical variables to plot survival rates, thereby exploring the relationship between these categories (Sex, Pclass, SibSp, Parch) and survival.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9684948,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "for i in [\"Sex\", \"Pclass\", \"SibSp\",\"Parch\"]:\n",
    "    plot_categories(titanic, cat = i, target = \"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code would plot the survival rate of passengers grouped by their passenger class (Pclass) to explore the relationship between ticket class and survival.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.45175064,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "This code would plot the survival rate of passengers grouped by the number of siblings/spouses aboard (SibSp) to explore the relationship between family connections and survival.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.5286518,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Visualization",
    "desc": "This code would plot the survival rate of passengers grouped by the number of parents/children aboard (Parch) to explore the relationship between familial relationship and survival.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.71496516,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Visualization",
    "desc": "This code snippet creates a count plot to visualize the distribution of the 'Survived' variable in the training dataset.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9977641105651855,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Survived', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet creates a facet grid to show histograms of the 'Age' distribution for passengers who survived and did not survive.",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9964678287506104,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "g=sns.FacetGrid(train_df, col='Survived')\n",
    "g.map(plt.hist,'Age', bins =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet creates a facet grid to show histograms of the 'Age' distribution categorized by 'Survived' status and passenger class ('Pclass').",
    "notebook_id": 28,
    "predicted_subclass_probability": 0.9974844455718994,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train_df, col='Survived',row='Pclass')\n",
    "g.map(plt.hist,'Age',bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 1,
    "class": "Visualization",
    "desc": "This code snippet defines multiple functions to plot histograms, distributions, categorical data, correlation maps, variable descriptions, and variable importance for data visualization purposes.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.92443085,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_histograms( df , variables , n_rows , n_cols ):\n",
    "    fig = plt.figure( figsize = ( 16 , 12 ) )\n",
    "    for i, var_name in enumerate( variables ):\n",
    "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n",
    "        df[ var_name ].hist( bins=10 , ax=ax )\n",
    "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n",
    "        ax.set_xticklabels( [] , visible=False )\n",
    "        ax.set_yticklabels( [] , visible=False )\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories( df , cat , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , row = row , col = col )\n",
    "    facet.map( sns.barplot , cat , target )\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map( df ):\n",
    "    corr = titanic.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 }\n",
    "    )\n",
    "\n",
    "def describe_more( df ):\n",
    "    var = [] ; l = [] ; t = []\n",
    "    for x in df:\n",
    "        var.append( x )\n",
    "        l.append( len( pd.value_counts( df[ x ] ) ) )\n",
    "        t.append( df[ x ].dtypes )\n",
    "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n",
    "    levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels\n",
    "\n",
    "def plot_variable_importance( X , y ):\n",
    "    tree = DecisionTreeClassifier( random_state = 99 )\n",
    "    tree.fit( X , y )\n",
    "    plot_model_var_imp( tree , X , y )\n",
    "    \n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print (model.score( X , y ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet calls the `plot_correlation_map()` function to create and display a heatmap of the correlation matrix for the 'titanic' DataFrame.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9740528,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plot_correlation_map( titanic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet uses the `plot_distribution()` function to plot the age distribution of Titanic passengers, segmented by their survival status and sex.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99385697,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet uses the `plot_distribution()` function to plot the fare distribution of Titanic passengers, segmented by their survival status and sex.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9960588,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 1\n",
    "# Plot distributions of Fare of passangers who survived or did not survive\n",
    "plot_distribution( titanic , var = 'Fare' , target = 'Survived' , row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet uses the `plot_categories()` function to visualize the survival rates of Titanic passengers based on their embarked locations.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.97696394,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Plot survival rate by Embarked\n",
    "plot_categories( titanic , cat = 'Embarked' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet uses the `plot_categories()` function to visualize the survival rates of Titanic passengers based on their sex.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.98297715,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 2\n",
    "# Plot survival rate by Sex\n",
    "plot_categories( titanic , cat = 'Sex' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet uses the `plot_categories()` function to visualize the survival rates of Titanic passengers based on their passenger class (Pclass).",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9763035,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 3\n",
    "# Plot survival rate by Pclass\n",
    "plot_categories( titanic , cat = 'Pclass' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet uses the `plot_categories()` function to visualize the survival rates of Titanic passengers based on the number of siblings or spouses (SibSp) they had aboard.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9788583,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 4\n",
    "# Plot survival rate by SibSp\n",
    "plot_categories( titanic , cat = 'SibSp' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet uses the `plot_categories()` function to visualize the survival rates of Titanic passengers based on the number of parents or children (Parch) they had aboard.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.92094743,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Excersise 5\n",
    "# Plot survival rate by Parch\n",
    "plot_categories( titanic , cat = 'Parch' , target = 'Survived' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code snippet creates a count plot using Seaborn to visualize the distribution of the target variable in the training dataset.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.99344987,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.countplot(train_df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Visualization",
    "desc": "This code snippet generates plots to compare the training and validation loss and accuracy over the training epochs, providing insights into model performance and potential overfitting.",
    "notebook_id": 1,
    "predicted_subclass_probability": 0.99531174,
    "start_cell": false,
    "subclass": "learning_history",
    "subclass_id": 35
   },
   "outputs": [],
   "source": [
    "# Porovnanie treningových a validačných dát\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.subplot(221)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 2,
    "class": "Visualization",
    "desc": "This snippet visualizes the count of tweets classified as disaster vs. not disaster by replacing target values with descriptive labels and plotting a bar chart.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9971213,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "ax = tweet.replace({\"target\" : {1 : \"Disaster\", 0 : \"Not disaster\"}}).groupby(['target'])['target'].count().plot.bar(title = \"Train set count by disaster/not disaster\")\n",
    "_ = ax.set_xlabel('Disaster?')\n",
    "_ = ax.set_ylabel('Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Visualization",
    "desc": "This snippet creates a bar plot of the 40 most common non-stopword words in the tweet corpus, showing their respective counts. ",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9958823,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus()\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "x=[]\n",
    "y=[]\n",
    "for word,count in most[:40]:\n",
    "    if (word not in lst_stopwords) :\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This snippet visualizes the 10 most frequent bigrams from the tweet corpus by plotting them in a bar chart.",
    "notebook_id": 3,
    "predicted_subclass_probability": 0.9978225,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "top_tweet_bigrams=get_top_tweet_bigrams(corpus)[:10]\n",
    "x,y=map(list,zip(*top_tweet_bigrams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This snippet generates a bar plot to visualize and compare the number of examples for each class (Real and Not) in the train DataFrame.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.6196526,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# bar plot of the 3 classes\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "plt.bar(10,Real_len,3, label=\"Real\", color='blue')\n",
    "plt.bar(15,Not_len,3, label=\"Not\", color='red')\n",
    "plt.legend()\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Propertion of examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This snippet generates a histogram to visualize the distribution of text lengths for tweets labeled as 'Not' and 'Real', aiding in understanding the feature's characteristics across the different classes.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9931299,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
    "bins = 150\n",
    "plt.hist(tweet[tweet['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Not')\n",
    "plt.hist(tweet[tweet['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Real')\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('numbers')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(0,150)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This snippet generates side-by-side histograms to visualize and compare the distribution of character lengths in tweets labeled as 'Real' and 'Not', providing insights into text length characteristics across both classes.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.998451,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "tweet_len=tweet[tweet['target']==1]['text'].str.len()\n",
    "ax1.hist(tweet_len,color='blue')\n",
    "ax1.set_title('Real(disaster tweets)')\n",
    "tweet_len=tweet[tweet['target']==0]['text'].str.len()\n",
    "ax2.hist(tweet_len,color='red')\n",
    "ax2.set_title('Not(not disaster tweets)')\n",
    "ax2.xaxis.set_major_locator(ticker.MultipleLocator(20))\n",
    "fig.suptitle('Characters in tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "This snippet generates side-by-side histograms to visualize and compare the distribution of the number of words in tweets labeled as 'Real' and 'Not', helping to understand the word count characteristics across both classes.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9973506,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "tweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(tweet_len,color='blue')\n",
    "ax1.set_title('Real(disaster tweets)')\n",
    "tweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(tweet_len,color='red')\n",
    "ax2.set_title('Not(not disaster tweets)')\n",
    "fig.suptitle('Words in a tweet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Visualization",
    "desc": "This snippet generates side-by-side distribution plots to visualize and compare the average word lengths in tweets labeled as 'Real' and 'Not', offering insights into the average word length characteristics across both classes.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9909223,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "word=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\n",
    "ax1.set_title('Real(disaster tweets)')\n",
    "word=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\n",
    "ax2.set_title('Not(not disaster tweets)')\n",
    "fig.suptitle('Average word length in each tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Visualization",
    "desc": "This snippet generates a bar plot to visualize the frequencies of the top 10 most common stop words in tweets labeled as 'Not'.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9908353,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Visualization",
    "desc": "This snippet generates a bar plot to visualize the frequencies of the top 10 most common stop words in tweets labeled as 'Real', after constructing the relevant corpus and counting the occurrences.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9976382,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "corpus=create_corpus(1)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "    \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 20,
    "class": "Visualization",
    "desc": "This snippet generates a bar plot to visualize the frequency of punctuation marks in tweets labeled as 'Real' by constructing the corpus and counting occurrences of punctuation characters.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99747795,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "corpus=create_corpus(1)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "        \n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 21,
    "class": "Visualization",
    "desc": "This snippet generates a bar plot to visualize the frequency of punctuation marks in tweets labeled as 'Not' by constructing the corpus and counting occurrences of punctuation characters, with the bars colored green.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99550253,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "corpus=create_corpus(0)\n",
    "dic=defaultdict(int)\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "        \n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y,color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 23,
    "class": "Visualization",
    "desc": "This snippet generates a horizontal bar plot using Seaborn to visualize the 40 most common words (excluding stop words) and their frequencies in tweets labeled as 'Not'.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9969177,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 25,
    "class": "Visualization",
    "desc": "This snippet generates a horizontal bar plot using Seaborn to visualize the 10 most frequent bigrams in the combined text of the tweets DataFrame, showing their respective counts.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99627554,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "top_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\n",
    "x,y=map(list,zip(*top_tweet_bigrams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 38,
    "class": "Visualization",
    "desc": "This snippet defines a function to perform dimensionality reduction using TruncatedSVD and plot the 2D representation of transformed text data with a scatter plot colored by labels, then applies this function to visualize the training data's LSA components.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.74464726,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "        lsa = TruncatedSVD(n_components=2) # https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781783989485/1/ch01lvl1sec21/using-truncated-svd-to-reduce-dimensionality\n",
    "        lsa.fit(test_data)\n",
    "        lsa_scores = lsa.transform(test_data)\n",
    "        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "        color_column = [color_mapper[label] for label in test_labels]\n",
    "        colors = ['red','blue']\n",
    "        if plot:\n",
    "            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "            red_patch = mpatches.Patch(color='red', label='Not')\n",
    "            blue_patch = mpatches.Patch(color='blue', label='Real')\n",
    "            plt.legend(handles=[red_patch, blue_patch], prop={'size': 30})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_counts, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 40,
    "class": "Visualization",
    "desc": "This snippet generates a scatter plot to visualize the 2D representation of training data's TF-IDF components, after applying the plot_LSA function on the TF-IDF transformed train data.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9936701,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_tfidf, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 61,
    "class": "Visualization",
    "desc": "This snippet generates a scatter plot to visualize the 2D representation of the training data's LSA components using the `plot_LSA` function, showing how the target labels are distributed in the reduced feature space.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.99791366,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(train,tweet['target'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 85,
    "class": "Visualization",
    "desc": "This snippet creates a histogram plot using pandas to visualize the distribution of the predictions made by the BERT-based model on the test data.",
    "notebook_id": 7,
    "predicted_subclass_probability": 0.9984862,
    "start_cell": false,
    "subclass": "create_dataframe",
    "subclass_id": 12
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(test_pred_BERT, columns=['preds'])\n",
    "pred.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "Creates a bar chart of the top 30 unigrams in the text column of the dataset, excluding the specified stopwords, to visualize the most common single words.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.99681854,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "npt.bar_ngram(\n",
    "    title='uni-gram',\n",
    "    xaxis_label='word_count',\n",
    "    yaxis_label='word',\n",
    "    ngram=1,\n",
    "    top_n=30,\n",
    "    height=700,\n",
    "    stopwords=stopwords,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "Creates a bar chart of the top 30 bigrams in the text column of the dataset, excluding the specified stopwords, to visualize the most common pairs of consecutive words.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9969848,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "npt.bar_ngram(\n",
    "    title='bi-gram',\n",
    "    xaxis_label='word_count',\n",
    "    yaxis_label='word',\n",
    "    ngram=2,\n",
    "    top_n=30,\n",
    "    height=700,\n",
    "    stopwords=stopwords,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "Creates a bar chart of the top 30 trigrams in the text column of the dataset to visualize the most common sequences of three consecutive words.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.997412,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "npt.bar_ngram(\n",
    "    title='tri-gram',\n",
    "    xaxis_label='word_count',\n",
    "    yaxis_label='word',\n",
    "    ngram=3,\n",
    "    top_n=30,\n",
    "    height=700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "Visualizes the co-occurrence network of words in the text data, as built in the previous step, to show how often words appear together.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.97465795,
    "start_cell": false,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "npt.co_network(\n",
    "    title='Co-occurrence network',\n",
    "    width=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "Generates a sunburst chart to visualize the hierarchical structure and distribution of word frequencies in the text data, potentially with a colorscale for better insights.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.9868972,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "npt.sunburst(\n",
    "    title='sunburst chart',\n",
    "    colorscale=True,\n",
    "    width=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 36,
    "class": "Visualization",
    "desc": "Plots a histogram of the 'target' column in the submission DataFrame to visualize the distribution of the predicted classes.",
    "notebook_id": 19,
    "predicted_subclass_probability": 0.99571157,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sub['target'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "The code creates a count plot of the distribution of the target variable in the training dataset using Seaborn.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.99602413,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.countplot(train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 33,
    "class": "Visualization",
    "desc": "The code creates a histogram of the word count distribution in the preprocessed text column of the training dataset.",
    "notebook_id": 20,
    "predicted_subclass_probability": 0.9977956,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.hist(list(train_data['prep_text'].str.split().map(lambda x: len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the distribution of the target variable in the training dataset.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9983015,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "x=tweet.target.value_counts()\n",
    "sns.barplot(x.index,x)\n",
    "plt.gca().set_ylabel('samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet creates two histograms side by side to compare the character lengths of tweets classified as disasters and non-disasters.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99850535,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "tweet_len=tweet[tweet['target']==1]['text'].str.len()\n",
    "ax1.hist(tweet_len,color='red')\n",
    "ax1.set_title('disaster tweets')\n",
    "tweet_len=tweet[tweet['target']==0]['text'].str.len()\n",
    "ax2.hist(tweet_len,color='green')\n",
    "ax2.set_title('Not disaster tweets')\n",
    "fig.suptitle('Characters in tweets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet creates two histograms side by side to compare the word counts of tweets classified as disasters and non-disasters.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99790597,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "tweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(tweet_len,color='red')\n",
    "ax1.set_title('disaster tweets')\n",
    "tweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(tweet_len,color='green')\n",
    "ax2.set_title('Not disaster tweets')\n",
    "fig.suptitle('Words in a tweet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet creates two distribution plots side by side to compare the average word lengths in tweets classified as disasters and non-disasters.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.8642512,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "word=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\n",
    "ax1.set_title('disaster')\n",
    "word=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\n",
    "ax2.set_title('Not disaster')\n",
    "fig.suptitle('Average word length in each tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the frequency of the top ten most common stopwords in non-disaster tweets.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9976636,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "x,y=zip(*top)\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the frequency of the top ten most common stopwords in disaster tweets.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99733067,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "corpus=create_corpus(1)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "    \n",
    "\n",
    "\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the frequency of special punctuation characters in disaster tweets.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9969452,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "corpus=create_corpus(1)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "import string\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "        \n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the frequency of special punctuation characters in non-disaster tweets.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.99617517,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "corpus=create_corpus(0)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "import string\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "        \n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y,color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the frequency of the top 40 most common words in non-disaster tweets, excluding stopwords.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9969177,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the frequency of the top ten most common bigrams in the tweets dataset.",
    "notebook_id": 22,
    "predicted_subclass_probability": 0.9961832,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "top_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\n",
    "x,y=map(list,zip(*top_tweet_bigrams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 4,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the distribution of the target variable in the training dataset, adding a label to the y-axis.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9983051,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "x = tweet.target.value_counts()\n",
    "\n",
    "sns.barplot(x.index, x)\n",
    "plt.gca().set_ylabel('samples');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet creates two histograms side-by-side to compare the lengths of tweets classified as disasters and non-disasters, labeling each subplot accordingly.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99799573,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "tweet_len = tweet[tweet['target'] ==1]['text'].str.len()\n",
    "ax1.hist(tweet_len, color ='black')\n",
    "ax1.set_title('disaster tweets')\n",
    "\n",
    "tweet_len = tweet[tweet['target'] ==0]['text'].str.len()\n",
    "ax2.hist(tweet_len, color ='blue')\n",
    "ax2.set_title('characters in tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 6,
    "class": "Visualization",
    "desc": "This code snippet creates two histograms side-by-side to compare the word counts of tweets classified as disasters and non-disasters, labeling each subplot accordingly.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99718094,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "tweet_len = tweet[tweet['target']==1]['text'].str.split().map(lambda x : len(x))\n",
    "ax1.hist(tweet_len, color = 'red')\n",
    "ax1.set_title('diaster tweets')\n",
    "\n",
    "tweet_len = tweet[tweet['target'] == 0]['text'].str.split().map(lambda x : len(x))\n",
    "ax2.hist(tweet_len, color ='green')\n",
    "ax2.set_title('words in a tweet')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet creates two histograms side-by-side to compare the average word lengths of tweets classified as disasters and non-disasters, labeling each subplot and the figure title accordingly.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.96851635,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "word = tweet[tweet['target']==1]['text'].str.split().apply(lambda x: [len(i) for i in x])\n",
    "sns.histplot(word.map(lambda x: np.mean(x)),ax=ax1, color='red')\n",
    "ax1.set_title('disaster')\n",
    "\n",
    "word = tweet[tweet['target']== 0]['text'].str.split().apply(lambda x: [len(i) for i in x])\n",
    "sns.histplot(word.map(lambda x: np.mean(x)),ax=ax2, color='green')\n",
    "ax2.set_title('Not disaster')\n",
    "fig.suptitle('average word length in each tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 9,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the top 10 most frequent stopwords in the corpus of non-disaster tweets.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9977603,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(0)\n",
    "\n",
    "dic = defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word] += 1\n",
    "\n",
    "top = sorted(dic.items(), key=lambda x:x[1], reverse = True)[:10]\n",
    "\n",
    "x, y = zip(*top)\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the top 10 most frequent stopwords in the corpus of disaster tweets.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.9977896,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(1)\n",
    "\n",
    "dic = defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word] += 1\n",
    "\n",
    "top = sorted(dic.items(), key=lambda x:x[1], reverse = True)[:10]\n",
    "\n",
    "x, y = zip(*top)\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 11,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the frequency of punctuation marks in the corpus of disaster tweets.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99351406,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "### real diaster tweet, target == 1\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "corpus = create_corpus(1)\n",
    "\n",
    "dic = defaultdict(int)\n",
    "import string\n",
    "special = string.punctuation\n",
    "\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i] += 1\n",
    "\n",
    "x, y = zip(*dic.items())\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet creates a bar plot to visualize the frequency of punctuation marks in the corpus of non-disaster tweets.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99332184,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "### No diaster tweet, target == 0\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "corpus = create_corpus(0)\n",
    "\n",
    "dic = defaultdict(int)\n",
    "import string\n",
    "special = string.punctuation\n",
    "\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i] += 1\n",
    "\n",
    "x, y = zip(*dic.items())\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Visualization",
    "desc": "This code snippet creates a horizontal bar plot to visualize the counts of the 40 most common non-stopwords in the corpus.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.99633586,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=y, y=x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Visualization",
    "desc": "This code snippet creates a horizontal bar plot to visualize the top 10 most frequent bigrams in the tweet dataset.",
    "notebook_id": 23,
    "predicted_subclass_probability": 0.994639,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "top_tweet_bigrams = get_top_tweet_bigrams(tweet['text'])[:10]\n",
    "x, y = map(list, zip(*top_tweet_bigrams))\n",
    "sns.barplot(x=y, y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "The code snippet, currently commented out, is intended to generate and display a word cloud for real tweets using the `WordCloud` library to visually represent the most common words in real tweets.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.95494944,
    "start_cell": true,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(real_tweets)\n",
    "#plt.figure(figsize = (12, 8))\n",
    "#plt.imshow(wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.title(\"Real tweets Wordcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "class": "Visualization",
    "desc": "The code snippet, currently commented out, is intended to generate and display a word cloud for fake tweets using the `WordCloud` library to visually represent the most common words in fake tweets.",
    "notebook_id": 25,
    "predicted_subclass_probability": 0.90664035,
    "start_cell": false,
    "subclass": "commented",
    "subclass_id": 76
   },
   "outputs": [],
   "source": [
    "#wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(fake_tweets)\n",
    "#plt.figure(figsize = (12, 8))\n",
    "#plt.imshow(wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.title(\"Fake tweets Wordcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 8,
    "class": "Visualization",
    "desc": "This code creates a bar plot to compare the frequency of disaster tweets versus non-disaster tweets in the training data using seaborn and matplotlib.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.84215474,
    "start_cell": true,
    "subclass": "model_coefficients",
    "subclass_id": 79
   },
   "outputs": [],
   "source": [
    "sns.barplot(train['target'].value_counts().index,train['target'].value_counts()\n",
    "            ,palette='Spectral')\n",
    "plt.title('Comparing disaster tweets and non disaster tweets',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code creates a bar chart to display the top 10 most frequent keywords in the training data, setting the figure size, title, axis labels, and rotating the x-axis labels for better readability.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.9798607,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(9,6))\n",
    "# Bar chart showing amount of keywords values\n",
    "sns.barplot(y=train['keyword'].value_counts()[:10].index,\n",
    "            x=train['keyword'].value_counts()[:10])\n",
    "# Add title\n",
    "plt.title(' Top 10 Keyword ') \n",
    "# Add label for x axis\n",
    "plt.xlabel('COUNT')\n",
    "# Add label for y axis\n",
    "plt.ylabel('KEYWORD')\n",
    "# Rotate the label text for hotizontal axis\n",
    "plt.xticks(rotation=90) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "This code creates side-by-side bar charts to display the top 10 most frequent keywords for disaster tweets and non-disaster tweets, setting the figure size, titles, and colors for better interpretation.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99411774,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# create variables a,b (disaster , non-disaster)\n",
    "a = train[train.target==1].keyword.value_counts().head(10)\n",
    "b = train[train.target==0].keyword.value_counts().head(10)\n",
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(13,5))\n",
    "# Bar chart showing amount of disaster keywords values\n",
    "plt.subplot(121)\n",
    "sns.barplot(a, a.index, color='orange')\n",
    "# Add title\n",
    "plt.title('Top keywords for disaster tweets')\n",
    "# Bar chart showing amount of non-disaster keywords values\n",
    "plt.subplot(122)\n",
    "sns.barplot(b, b.index, color='pink')\n",
    "# Add title\n",
    "plt.title('Top keywords for non-disaster tweets')\n",
    "# display a graph \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 15,
    "class": "Visualization",
    "desc": "This code creates a bar chart to display the top 10 most frequent locations in the training data, setting the figure size, title, and ordering the locations by their counts.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.98178136,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(9,6))\n",
    "# Bar chart showing amount of location values and groups the top 10 location\n",
    "sns.countplot(y=train.location, order = train.location.value_counts().iloc[:10].index)\n",
    "# Add title\n",
    "plt.title('Top 10 locations')\n",
    "# display a graph \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Visualization",
    "desc": "This code creates side-by-side histograms to display the distribution of tweet lengths (in characters) for disaster and non-disaster tweets, setting the figure size, titles, and colors for better interpretation.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99827766,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "tweet_len=train[train['target']==1]['text'].str.len()\n",
    "ax1.hist(tweet_len,color='red')\n",
    "ax1.set_title('disaster tweets')\n",
    "\n",
    "tweet_len=train[train['target']==0]['text'].str.len()\n",
    "ax2.hist(tweet_len,color='blue')\n",
    "ax2.set_title('Not disaster tweets')\n",
    "fig.suptitle('Characters in tweets',fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Visualization",
    "desc": "This code creates side-by-side histograms to display the distribution of tweet lengths (in words) for disaster and non-disaster tweets, setting the figure size, titles, and colors for better interpretation.",
    "notebook_id": 27,
    "predicted_subclass_probability": 0.99817014,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "tweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(tweet_len,color='red')\n",
    "ax1.set_title('disaster tweets')\n",
    "tweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(tweet_len,color='blue')\n",
    "ax2.set_title('Not disaster tweets')\n",
    "fig.suptitle('Words in a tweets',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 5,
    "class": "Visualization",
    "desc": "This code snippet creates and titles a bar chart showing the count of disaster and non-disaster tweets, and labels the axes accordingly.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9952331,
    "start_cell": true,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Add title\n",
    "plt.title(\"Amount of tweets - Disaster(1) or not(0)\")\n",
    "# Bar chart showing amount of both target values\n",
    "sns.barplot(x.index, x)\n",
    "# Add label for vertical axis\n",
    "plt.ylabel(\"Count\")\n",
    "# Add label for hotizontal axis\n",
    "plt.xlabel(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 7,
    "class": "Visualization",
    "desc": "This code snippet creates and displays a pie chart to visually represent the proportion of disaster and non-disaster tweets, with specific formatting to highlight the disaster slice and a title for the chart.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9941082,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Reference: https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html\n",
    "# Pie chart\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(proportion, \n",
    "        explode = (0, 0.1), # only \"explode\" the 2nd slice\n",
    "        labels  = ['Not disaster', 'Disaster'], \n",
    "        autopct = '%1.1f%%',\n",
    "        shadow = True, \n",
    "        startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title(\"Percentual of tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "class": "Visualization",
    "desc": "This code snippet creates faceted histograms to compare the length distribution of tweets for disaster and non-disaster categories, adjusting the titles and formatting for clarity.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9981218,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Facet a plot by target column\n",
    "g = sns.FacetGrid(train_df, col = 'target', height = 5, hue = 'target')\n",
    "# Plot a histogram chart\n",
    "g.map(plt.hist, \"length\")\n",
    "# Adjust title position\n",
    "g.fig.subplots_adjust(top=0.8)\n",
    "# Add general title\n",
    "g.fig.suptitle('Text lenght by target', fontsize=16)\n",
    "# Set title to each chart\n",
    "axes = g.axes.flatten()\n",
    "axes[0].set_title(\"Not disaster\")\n",
    "axes[1].set_title(\"Disaster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "class": "Visualization",
    "desc": "This code snippet creates faceted distribution plots to compare the number of words in tweets for disaster and non-disaster categories, adjusting the titles and layout for better presentation.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9979069,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Facet a plot by target column\n",
    "g = sns.FacetGrid(train_df, col = 'target', height = 5, hue = 'target')\n",
    "# Plot a histogram chart\n",
    "g.map(sns.distplot, \"num_words\")\n",
    "# Adjust title position\n",
    "g.fig.subplots_adjust(top=0.8)\n",
    "# Add general title\n",
    "g.fig.suptitle('Distribution of number of words by target', fontsize=16)\n",
    "# Set title to each chart\n",
    "axes = g.axes.flatten()\n",
    "axes[0].set_title(\"Not disaster\")\n",
    "axes[1].set_title(\"Disaster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 13,
    "class": "Visualization",
    "desc": "This code snippet creates a boxplot to compare the distribution of the number of words in tweets across disaster and non-disaster categories, including titles and axis labels for clarity.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.7549678,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "# Add title\n",
    "plt.title(\"Boxplot - Comparing distribution of number of words by target\")\n",
    "# Boxplot\n",
    "sns.boxplot(x = \"target\", y = \"num_words\", hue=\"target\", data = train_df)\n",
    "# Add label for vertical axis\n",
    "plt.ylabel(\"Number of Words\")\n",
    "# Add label for hotizontal axis\n",
    "plt.xlabel(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 16,
    "class": "Visualization",
    "desc": "This code snippet creates a bar chart to display the top 20 most frequent keywords in the training dataset, with a title and labeled axes, and rotates the x-axis labels for better readability.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.9886214,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Groups the top 20 keywords\n",
    "x = train_df.keyword.value_counts()[:20]\n",
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(10,6))\n",
    "# Add title\n",
    "plt.title(\"20 hottest keyword in the text\")\n",
    "# Bar chart showing amount of both target values\n",
    "sns.barplot(x.index, x, color=\"c\")\n",
    "# Add label for vertical axis\n",
    "plt.ylabel(\"Count\")\n",
    "# Add label for hotizontal axis\n",
    "plt.xlabel(\"Keywords\")\n",
    "# Rotate the label text for hotizontal axis\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 17,
    "class": "Visualization",
    "desc": "This code snippet downloads an image to use as a mask for a word cloud, defines a function `create_word_cloud` to generate and display a word cloud from given text data, incorporating the mask image and additional stopwords.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.8659406,
    "start_cell": false,
    "subclass": "string_transform",
    "subclass_id": 78
   },
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud#6.-Train-Test-Split\n",
    "# Download a image to mask the wordcloud\n",
    "!wget --no-check-certificate \\\n",
    "  https://i.imgur.com/tyCaXHH.jpg \\\n",
    "    -O /tmp/Twitter_Logo.jpg\n",
    "\n",
    "STOPWORDS.add('NaN')  # remove NaN to the world Cloud\n",
    "STOPWORDS.add('https') # remove https to the world Cloud\n",
    "\n",
    "def create_word_cloud(text):\n",
    "    \n",
    "    comment_words = ' '\n",
    "    stopwords = set(STOPWORDS) \n",
    "    \n",
    "    for val in text: \n",
    "\n",
    "        # convert each val to string type \n",
    "        val = str(val)\n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "        # Converts each token to lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "\n",
    "        for words in tokens: \n",
    "            comment_words = comment_words + words + ' '\n",
    "            \n",
    "    maskArray = np.array(Image.open(\"/tmp/Twitter_Logo.jpg\"))\n",
    "    wordcloud = WordCloud(width = 5000, height = 4000,\n",
    "                    background_color ='white',\n",
    "                    mask = maskArray,      \n",
    "                    stopwords = stopwords,\n",
    "                    min_font_size = 10)\n",
    "    wordcloud.generate(comment_words)   \n",
    "    \n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (12, 12)) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 18,
    "class": "Visualization",
    "desc": "This code snippet generates and displays a word cloud using the keyword values from the training dataset.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.2477251,
    "start_cell": false,
    "subclass": "compute_train_metric",
    "subclass_id": 28
   },
   "outputs": [],
   "source": [
    "# Create a WordCloud from keyword values\n",
    "text = train_df.keyword.values\n",
    "create_word_cloud(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 19,
    "class": "Visualization",
    "desc": "This code snippet creates a bar chart to display the top 20 most frequent locations in the training dataset, with a title and labeled axes, and rotates the x-axis labels for better readability.",
    "notebook_id": 29,
    "predicted_subclass_probability": 0.99118876,
    "start_cell": false,
    "subclass": "distribution",
    "subclass_id": 33
   },
   "outputs": [],
   "source": [
    "# Groups the top 20 location\n",
    "x = train_df.location.value_counts()[:20]\n",
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(10,6))\n",
    "# Add title\n",
    "plt.title(\"Top 20 location\")\n",
    "# Bar chart showing amount of both target values\n",
    "sns.barplot(x.index, x, color = \"pink\")\n",
    "# Add label for vertical axis\n",
    "plt.ylabel(\"Count\")\n",
    "# Add label for hotizontal axis\n",
    "plt.xlabel(\"Location\")\n",
    "# Rotate the label text for hotizontal axis\n",
    "plt.xticks(rotation=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "visualization": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
