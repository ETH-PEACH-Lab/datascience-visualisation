{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "notebook_file = \"/home/ryounis/Documents/Zurich/PEACHLab/datascience-visualisation/data/datasets/Natural Language Processing with Disaster Tweets/classified_notebooks.json\"\n",
    "with open(notebook_file, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'notebooks': [{'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the read-only \"../input/\" directory\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \\n# You can also write temporary files to /kaggle/temp/, but they won\\'t be saved outside of the current session',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code snippet imports essential libraries for data manipulation and file handling, and lists all files under the input directory to understand the available data resources.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'list_files',\n",
       "       'subclass_id': 88,\n",
       "       'predicted_subclass_probability': 0.99921954}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\\nX_train = train.iloc[:, :4]\\ny_train = train.iloc[:, 4]\\nX_test = test\\nprint(X_train.shape, y_train.shape, X_test.shape)',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'The code snippet reads the training and testing data from CSV files, separates the features and target labels for the training data, and prints their shapes to verify the extraction process.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99967587}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'def lowercase_text(text):\\n    return text.lower()\\n\\nX_train.text=X_train.text.apply(lambda x: lowercase_text(x))\\nX_test.text=X_test.text.apply(lambda x: lowercase_text(x))\\nX_train.head()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function to convert text to lowercase and applies this transformation to the text columns in both the training and testing datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9221044}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"import re\\nimport string\\ndef remove_noise(text):\\n    text = re.sub('\\\\[.*?\\\\]', '', text)\\n    text = re.sub('https?://\\\\S+|www\\\\.\\\\S+', '', text)\\n    text = re.sub('<.*?>+', '', text)\\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\\n    text = re.sub('\\\\n', '', text)\\n    text = re.sub('\\\\w*\\\\d\\\\w*', '', text)\\n    text = re.sub('\\x89ûò', '', text)\\n    return text\\nX_train.text=X_train.text.apply(lambda x: remove_noise(x))\\nX_test.text=X_test.text.apply(lambda x: remove_noise(x))\\nX_train.head()\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code snippet defines a function to remove various types of noise from the text data and applies this function to clean the text columns in both the training and testing datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.7514645}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"# Tokenizing the training and the test set\\nimport nltk\\nfrom nltk.corpus import stopwords\\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\\\w+')\\nX_train['text'] = X_train['text'].apply(lambda x: tokenizer.tokenize(x))\\nX_test['text'] = X_test['text'].apply(lambda x: tokenizer.tokenize(x))\\nX_train['text'].head()\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code imports necessary components from the nltk library and tokenizes the text columns in both the training and testing datasets to split the text into individual words.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.92383534}},\n",
       "     {'cell_id': 5,\n",
       "      'code': \"# Removing stopwords belonging to english language\\ndef remove_stopwords(text):\\n    words = [w for w in text if w not in stopwords.words('english')]\\n    return words\\n\\nX_train['text'] = X_train['text'].apply(lambda x : remove_stopwords(x))\\nX_test['text'] = X_test['text'].apply(lambda x : remove_stopwords(x))\\nX_train.head()\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code snippet defines a function to remove English stopwords from the tokenized text data and applies this function to the text columns in both the training and testing datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.98372287}},\n",
       "     {'cell_id': 6,\n",
       "      'code': \"# After preprocessing, the text format\\ndef combine_text(list_of_text):\\n    '''Takes a list of text and combines them into one large chunk of text.'''\\n    combined_text = ' '.join(list_of_text)\\n    return combined_text\\n\\nX_train['text'] = X_train['text'].apply(lambda x : combine_text(x))\\nX_test['text'] = X_test['text'].apply(lambda x : combine_text(x))\\n# X_train['text']\\nX_train.head()\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code snippet defines a function to combine a list of tokenized words back into a continuous text and applies this function to recombine the tokenized text columns in both the training and testing datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.7641233}},\n",
       "     {'cell_id': 7,\n",
       "      'code': '# Stemming\\nfrom nltk.stem.snowball import SnowballStemmer\\nstemmer = SnowballStemmer(\"english\")\\n\\ndef stemming(text):\\n    text = [stemmer.stem(word) for word in text.split()]\\n    return \\' \\'.join(text)\\n\\n#X_train[\\'text\\'] = X_train[\\'text\\'].apply(lambda x : stemming(x))\\n#X_test[\\'text\\'] = X_test[\\'text\\'].apply(lambda x : stemming(x))\\n#X_train',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function to stem words in the text data using the Snowball Stemmer and includes commented-out lines to apply this stemming function to the text columns in both the training and testing datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.6512052}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'from wordcloud import WordCloud\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n%matplotlib inline\\ndef wordsCloud (dF):\\n    fig , ax1 = plt.subplots(1,figsize=(12,12))\\n    stopword_list = stopwords.words(\"english\")\\n    wordcloud=WordCloud(stopwords = stopword_list, background_color=\\'white\\',collocations = False , width=600,height=600).generate(\" \".join(dF))\\n    ax1.imshow(wordcloud)\\n    ax1.axis(\\'off\\')\\n    ax1.set_title(\"Frequent Words\",fontsize=24)    \\n    # print(stopword_list)\\n    return\\nwordsCloud(X_train.text)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet defines a function to generate and display a word cloud of frequent words in the training text data, excluding English stopwords, and then calls the function to visualize the word cloud.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.55752915}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'wordsCloud(X_test.text)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'The code snippet calls the previously defined `wordsCloud` function to generate and display a word cloud of frequent words in the test text data, excluding English stopwords.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.8625756}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'from sklearn.feature_extraction.text import CountVectorizer\\ncount_vectorizer=CountVectorizer() # analyzer=\\'word\\', stop_words = \"english\"\\ntrain_vec = count_vectorizer.fit_transform(X_train.text)\\ntest_vec = count_vectorizer.transform(X_test.text)\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nTfidf_vectorizer = TfidfVectorizer() # min_df=2, max_df=0.5, ngram_range=(1, 2)analyzer=\\'word\\', stop_words = \"english\"analyzer=\\'word\\', stop_words=\\'english\\'# , ngram_range=(1, 2), lowercase=True, max_features=150000\\ntrain_tfidf = Tfidf_vectorizer.fit_transform(X_train.text)\\ntest_tfidf = Tfidf_vectorizer.transform(X_test.text)\\n\\nprint(\"train_vec\" ,train_vec[7].todense())\\nprint(\"test_vec\", test_vec[7].todense())\\n\\nprint(\"train_tfidf\" ,train_tfidf[7].todense())\\nprint(\"test_tfidf\", test_vec[7].todense())',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet utilizes `CountVectorizer` and `TfidfVectorizer` from scikit-learn to convert the text data in the training and testing datasets into numerical representations and prints a sample from both vectorizations to verify the transformations.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.8695518}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'from sklearn.model_selection import KFold\\nkF = KFold(shuffle=True, random_state=241)      # разделение на 5 выборок\\n# MultinomialNB\\nfrom sklearn import model_selection\\nfrom sklearn.naive_bayes import MultinomialNB\\nclf = MultinomialNB() \\nscores = model_selection.cross_val_score(clf,train_vec,y_train,cv=kF,scoring=\\'f1\\')\\nprint(\"MultinomialNB score: \" ,scores.mean())\\n\\n# LogisticRegression\\nfrom sklearn.linear_model import LogisticRegression\\nclf_tfidf = LogisticRegression()\\nscores_tfidf = model_selection.cross_val_score(clf_tfidf,train_tfidf,y_train,\\n                                               cv=kF,scoring=\\'f1\\')\\nprint(\"LogisticRegretion score: \" ,scores_tfidf.mean())\\n\\n# SVC\\nfrom sklearn.svm import SVC   # реализация метода опорных векторов\\nclf_svc = SVC()#kernel=\\'linear\\', random_state=241\\nscores_svc = model_selection.cross_val_score(clf_svc,train_tfidf,y_train,\\n                                               cv=kF,scoring=\\'f1\\')\\nprint(\"SVC score: \" ,scores_svc.mean())\\n\\n# XGBoost\\nimport xgboost as xgb\\nclf_xgb_TFIDF = xgb.XGBClassifier()#max_depth=7, n_estimators=150, colsample_bytree=0.8, \\n                        #subsample=0.8, nthread=10, learning_rate=0.1\\nscores_xgb = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, y_train, cv=kF, scoring=\"f1\")\\nprint(\"XGBost score: \" ,scores_xgb.mean())\\n\\n',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes multiple machine learning classifiers, performs K-fold cross-validation on the count vectorized and TF-IDF vectorized training data, and calculates the mean F1 scores for each classifier to evaluate their performance.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.6108105}},\n",
       "     {'cell_id': 12,\n",
       "      'code': '# MultinomialNB\\nclf.fit(train_vec,y_train)\\ny_pred = clf.predict(test_vec)\\nscores = model_selection.cross_val_score(clf,test_vec,y_pred,cv=kF,scoring=\\'f1\\')\\nprint(\"MultinomialNB prediction score: \" ,scores.mean())\\n\\n# LogisticRegression\\nclf_tfidf.fit(train_tfidf, y_train)\\ny_pred_tfidf = clf_tfidf.predict(test_tfidf)\\nscores_tfidf = model_selection.cross_val_score(clf_tfidf,test_tfidf,y_pred_tfidf,cv=kF,\\n                                         scoring=\\'f1\\')\\nprint(\"LogisticRegretion prediction score: \" ,scores_tfidf.mean())\\n\\n# SVC\\nclf_svc.fit(train_tfidf, y_train)\\ny_pred_svc = clf_svc.predict(test_tfidf)\\nscores_svc = model_selection.cross_val_score(clf_svc,test_tfidf,y_pred_svc, cv=kF,\\n                                         scoring=\\'f1\\')  \\nprint(\"SVC prediction score: \" ,scores_svc.mean())\\n\\n# XGBoost\\nclf_xgb_TFIDF.fit(train_tfidf, y_train)\\ny_pred_xgb = clf_xgb_TFIDF.predict(test_tfidf)\\nscores_xgb = model_selection.cross_val_score(clf_xgb_TFIDF,test_tfidf,y_pred_xgb, cv=kF,\\n                                         scoring=\\'f1\\')  \\nprint(\"XGBoosting prediction score: \" ,scores_xgb.mean())',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet fits various trained classifiers on the entire training data, predicts labels for the test data, performs K-fold cross-validation on these predictions, and prints the mean F1 scores for each classifier to evaluate their predictive performance.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_model_class',\n",
       "       'subclass_id': 3,\n",
       "       'predicted_subclass_probability': 0.7733168}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'def submission(submission_file_path,model,test_vectors):\\n    sample_submission = pd.read_csv(submission_file_path)\\n    sample_submission[\"target\"] = model.predict(test_vectors)\\n    sample_submission.to_csv(\"submission.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet defines a function that reads a submission file, uses a given model to predict target values on the test vectors, and saves the results to a new CSV file named \"submission.csv\".',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9992994}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\\ntest_vectors=test_tfidf\\nclf = clf_xgb_TFIDF\\nsubmission(submission_file_path,clf,test_vectors)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet prepares the necessary parameters and calls the previously defined `submission` function to generate a submission file using the XGBoost classifier and the TF-IDF transformed test vectors.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.7684079}}],\n",
       "    'notebook_id': 0},\n",
       "   'notebook_id': 0},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the read-only \"../input/\" directory\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \\n# You can also write temporary files to /kaggle/temp/, but they won\\'t be saved outside of the current session',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports essential libraries like numpy and pandas, and then lists all files available in the Kaggle input directory for the current environment.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'list_files',\n",
       "       'subclass_id': 88,\n",
       "       'predicted_subclass_probability': 0.99921954}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'pip install cleantext',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet installs the `cleantext` package, which is likely required for text preprocessing or cleaning tasks later in the notebook.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.9904579}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'pip install ktrain',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet installs the `ktrain` package, which is a lightweight wrapper for TensorFlow Keras used for building, training, and deploying neural networks and other machine learning models.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.99148107}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'import pandas as pd\\nimport ktrain\\nfrom ktrain import text\\nimport cleantext\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports the pandas library for data manipulation, ktrain and its text module for machine learning model training and deployment, cleantext for text cleaning, and suppresses warning messages.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9993291}},\n",
       "     {'cell_id': 4,\n",
       "      'code': '# read training data\\ndf = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\\ndisplay(df.head())\\ndisplay(df.shape)',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the training data from a CSV file into a pandas DataFrame and displays the first few rows and the shape of the DataFrame.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99592197}},\n",
       "     {'cell_id': 5,\n",
       "      'code': '# clean training text\\nl=len(df)\\ndisplay(l)\\ncleanlist=[]\\ntextlength=[]\\nfor i in range(l):\\n    ct=cleantext.clean(df.iloc[i,3], clean_all= True)\\n    cleanlist.append(ct)\\n    lct=len(ct)\\n    textlength.append(lct)\\n    ',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet cleans the training text data in the DataFrame using the cleantext library, storing the cleaned text and their respective lengths in separate lists.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9911814}},\n",
       "     {'cell_id': 6,\n",
       "      'code': \"# combine clean text with training data\\ndf_clean=pd.DataFrame(cleanlist)\\ndf_clean.columns=['cleantext']\\nframes=[df,df_clean]\\nnewdf=pd.concat(frames, axis=1)\\ndisplay(newdf)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet combines the cleaned text data with the original training DataFrame, creating a new DataFrame that includes both the original and cleaned text data.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.9970824}},\n",
       "     {'cell_id': 7,\n",
       "      'code': '# distribution of clean text length\\ndisplay(pd.Series(textlength).describe())',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays descriptive statistics of the lengths of the cleaned text data, helping to understand the distribution of text lengths.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9994604}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"#https://github.com/amaiya/ktrain\\n# train model\\n(x_train, y_train), (x_test, y_test), preproc=text.texts_from_df(newdf, 'cleantext',label_columns=['target'],\\n                                                                 maxlen=127,max_features=100000,\\n                                                                 preprocess_mode='bert', val_pct=.1)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet prepares the data for training a BERT model by splitting it into training and testing sets and performing necessary preprocessing steps using the `ktrain` library.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.38557464}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"model=text.text_classifier('bert', (x_train, y_train), preproc=preproc)\\nlearner=ktrain.get_learner(model, train_data=(x_train, y_train),\\n                           val_data=(x_test, y_test),\\n                           batch_size=32)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes a BERT text classifier model using the preprocessed training data and sets up the learner object to handle the training process, including validation data and batch size configuration.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.87104684}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'learner.fit_onecycle(2e-5, 3)\\npredictor=ktrain.get_predictor(learner.model, preproc)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the BERT model for three epochs using the One Cycle learning rate policy and then creates a predictor object that can be used to make predictions with the trained model.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9968579}},\n",
       "     {'cell_id': 11,\n",
       "      'code': \"# example\\npredictor.predict(['calm','earthquake'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet uses the trained predictor to make predictions on example input texts, specifically \"calm\" and \"earthquake\", to demonstrate the model\\'s inference capabilities.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.9897764}},\n",
       "     {'cell_id': 12,\n",
       "      'code': '# apply model to test data\\ndf1 = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\\ndisplay(df1.head())\\ndisplay(df1.shape)',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the test data from a CSV file into a pandas DataFrame and displays the first few rows and the shape of the DataFrame for inspection.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99765056}},\n",
       "     {'cell_id': 13,\n",
       "      'code': '# example\\ndf1.iloc[0,3]',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet retrieves and displays the text data from the fourth column of the first row in the test DataFrame, providing an example of the test data content.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9183029}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'l=len(df1)\\ndisplay(l)\\npredlist=[]\\n#l=1\\nfor i in range(l):\\n    ct=cleantext.clean(df1.iloc[i,3], clean_all= True)\\n    new=predictor.predict(ct)\\n    predlist.append(new)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet iterates through the test data, cleans each text entry, predicts the corresponding output using the trained model, and stores the predictions in a list.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.99204236}},\n",
       "     {'cell_id': 15,\n",
       "      'code': \"df_pred=pd.DataFrame(predlist)\\ndf_pred.columns=['target']\\nframes=[df1,df_pred]\\ndf2=pd.concat(frames, axis=1)\\ndisplay(df2.head())\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet creates a DataFrame from the prediction list, labels the column as 'target', and concatenates it with the original test DataFrame, resulting in a DataFrame that includes both test data and their corresponding predictions.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'prepare_output',\n",
       "       'subclass_id': 55,\n",
       "       'predicted_subclass_probability': 0.9379232}},\n",
       "     {'cell_id': 16,\n",
       "      'code': \"df2.loc[df2['target']=='target','target']=1\\ndf2.loc[df2['target']=='not_target','target']=0\\ndisplay(df2['target'].mean())\\ndf2=df2[['id','target']]\\ndisplay(df2.shape)\\ndisplay(df2.head())\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet converts the prediction labels 'target' and 'not_target' to binary values 1 and 0 respectively, calculates the mean of the 'target' column, retains only the 'id' and 'target' columns in the final DataFrame, and displays its shape and the first few rows.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.8699204}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'df2.to_csv(\"submission.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet exports the final DataFrame containing the \\'id\\' and \\'target\\' columns to a CSV file named \"submission.csv\" without including the index in the output file.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9993235}}],\n",
       "    'notebook_id': 1},\n",
       "   'notebook_id': 1},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet downloads the \"tokenization.py\" file from the TensorFlow models repository.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_url',\n",
       "       'subclass_id': 42,\n",
       "       'predicted_subclass_probability': 0.8866123}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models, optimizers\\nfrom tensorflow.keras.callbacks import ModelCheckpoint',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports essential libraries and modules including NumPy, pandas, TensorFlow, and Keras components for building and training a machine learning model.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99937075}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads CSV files containing the training data, test data, and the sample submission format into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99973255}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'def bert_encode(texts, tokenizer, max_len):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function `bert_encode` that tokenizes text inputs, truncates or pads them to a specified maximum length, and converts them into input tokens, masks, and segment arrays suitable for input into a BERT model.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.77154285}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"train = train.fillna(' ')\\ntest = test.fillna(' ')\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet fills any missing values in the train and test DataFrames with empty spaces.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.77423024}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'import tensorflow_hub as hub\\nimport tokenization\\n\\n#%%time\\nmax_len = 60\\n\\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\\n#module_url = \\'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\\'\\nbert_layer = hub.KerasLayer(module_url, trainable=True)\\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\\n\\ntrain_input = bert_encode(train[\\'location\\']+\\' \\'+train[\\'keyword\\']+\\' \\'+train[\\'text\\'], tokenizer, max_len=max_len)\\ntest_input = bert_encode(test[\\'location\\']+\\' \\'+test[\\'keyword\\']+\\' \\'+test[\\'text\\'], tokenizer, max_len=max_len)\\ntrain_labels = train.target.values',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet sets up the BERT tokenizer and encodes the training and test text data into token, mask, and segment arrays, and extracts the target labels from the training dataset.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.733161}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'input_word_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\ninput_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\\nsegment_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\\n\\n_, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\nclf_output = sequence_output[:, 0, :]\\nout = layers.Dense(1, activation=\\'sigmoid\\')(clf_output)\\n\\nmodel = models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\\nmodel.compile(optimizers.Adam(lr=1e-5), loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\nmodel.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines a BERT-based neural network model using Keras, sets up the input layers for token IDs, masks, and segment IDs, extracts the CLS token output for classification, and compiles the model with a binary cross-entropy loss and accuracy metric.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.93497044}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\\n\\ntrain_history = model.fit(\\n    train_input, train_labels,\\n    validation_split=0.2,\\n    epochs=3,\\n    callbacks=[checkpoint],\\n    batch_size=16\\n)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the previously defined BERT-based model on the training data, using a checkpoint to save the best model based on validation loss, and tracks the training history over 3 epochs with a batch size of 16.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9945604}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"model.load_weights('model.h5')\\ntest_pred = model.predict(test_input)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet loads the best weights from the trained model and generates predictions on the test data.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.73428434}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"submission['target'] = test_pred.round().astype(int)\\nsubmission.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet rounds the test predictions to the nearest integer, assigns them to the 'target' column in the submission DataFrame, and then saves the DataFrame to a CSV file named 'submission.csv' without the index.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9991943}}],\n",
       "    'notebook_id': 2},\n",
       "   'notebook_id': 2},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import numpy as np\\nimport pandas as pd\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\\nfrom sklearn.metrics import make_scorer, f1_score',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports essential libraries and modules needed for data manipulation (NumPy, pandas), text feature extraction (CountVectorizer), model building (MultinomialNB), model selection (GridSearchCV, cross_val_score), and evaluation (make_scorer, f1_score).',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99930215}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code loads the training and testing datasets from CSV files into pandas DataFrame objects named `train` and `test`, respectively.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99975055}},\n",
       "     {'cell_id': 2,\n",
       "      'code': \"def filter_text(df):\\n    df['text']=df['text'].str.replace('http\\\\S+', '', case=False, regex=True)\\n    df['text']=df['text'].str.replace('@\\\\S+', '', regex=True)\\n    df['text']=df['text'].str.replace('&\\\\S+', '', regex=True)\\nfilter_text(train)\\nfilter_text(test)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code defines and applies a function to clean the text data in the 'text' column of the training and testing DataFrames by removing URLs, mentions, and special character sequences.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.99559855}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"sw=['the', 'a', 'an', 'in', 'on', 'with', 'by', 'for', 'at',\\n    'about', 'of', 'under', 'to', 'into', 'and', 'or', 'but',\\n    'nor', 'be']\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a list of common English stop words to be used for text preprocessing tasks, likely for the purpose of removing these words from text data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.99721485}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"v = CountVectorizer(stop_words=sw)\\ntrain_v = v.fit_transform(train['text'])\\ntest_v = v.transform(test['text'])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code initializes a CountVectorizer object with the predefined stop words and transforms the text data in the training and testing DataFrames into a matrix of token counts, storing the results in `train_v` and `test_v`.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.99726737}},\n",
       "     {'cell_id': 5,\n",
       "      'code': \"y = train['target']\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': \"This code extracts the 'target' column from the training DataFrame and stores it in a variable named `y` for use as the label data in the machine learning model.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.99906355}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'clf = MultinomialNB()\\nscorer = make_scorer(f1_score)\\nparam_grid = {\\'alpha\\': [0.01, 0.1, 1]}\\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scorer, cv=3)\\nmodel.fit(train_v, y)\\nprint(\"Best score: %f\" % model.best_score_)\\nprint(cross_val_score(model, train_v, y, cv=3, scoring=\\'f1\\'))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code sets up and trains a Multinomial Naive Bayes classifier using GridSearchCV to optimize the 'alpha' hyperparameter based on F1 score and performs a cross-validation to print the best score and F1 scores.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_on_grid',\n",
       "       'subclass_id': 6,\n",
       "       'predicted_subclass_probability': 0.9837585}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"y = model.predict(test_v)\\noutput = pd.DataFrame({'id':test.id, 'target':y})\\noutput.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code generates predictions on the test dataset using the trained model, creates a DataFrame with these predictions and their corresponding IDs, and exports the results to a CSV file named 'submission.csv'.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99914074}}],\n",
       "    'notebook_id': 3},\n",
       "   'notebook_id': 3},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': \"# setup\\n\\nfrom collections import Counter, defaultdict\\n\\nimport numpy as np \\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport os\\nimport re\\n\\nfrom sklearn.preprocessing import FunctionTransformer, MinMaxScaler\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split \\nfrom sklearn.decomposition import TruncatedSVD\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn import set_config\\n\\nimport optuna\\nimport nltk\\nfrom nltk.stem import WordNetLemmatizer\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\n\\nplt.style.use('ggplot')\\n%matplotlib inline\\n\\nset_config(display='diagram')\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports required libraries and modules, sets plot styles, and configures the sklearn display settings.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.99920374}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"# print files in input dir\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n    \",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet prints the file paths of all files in the specified directory.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'list_files',\n",
       "       'subclass_id': 88,\n",
       "       'predicted_subclass_probability': 0.9993166}},\n",
       "     {'cell_id': 2,\n",
       "      'code': \"train = pd.read_csv('../input/nlp-getting-started/train.csv')\\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\\n\\ntrain.head()\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the training and testing datasets from CSV files into pandas DataFrames and displays the first few rows of the training dataset.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9996737}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"print(f'Train dims {train.shape}', f'Test dims {test.shape}', sep = '\\\\n')\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the dimensions (number of rows and columns) of the training and testing datasets.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99101853}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"# plot prop of missing for each feature\\nsns.set_theme(style='white')\\nsns.barplot(x=train.columns, y=train.isnull().mean())\\nplt.show()\\n\\n# drop location and keyword\\ntrain.drop(columns=['id', 'keyword', 'location'], inplace=True)\\ntest.drop(columns=['id', 'keyword', 'location'], inplace=True)\\ntrain.drop_duplicates(inplace=True, ignore_index=True)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet visualizes the proportion of missing values for each feature using a bar plot and then drops specific columns and duplicates from the training and testing datasets.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.8657076}},\n",
       "     {'cell_id': 5,\n",
       "      'code': \"# plot target distribution\\nsns.countplot(x='target', data=train)\\nplt.title('Target distribution')\\nplt.show()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet plots the distribution of the target variable in the training dataset using a count plot.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99668735}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'wordnet_lemmatizer = WordNetLemmatizer()\\n\\ndef quick_clean(text):\\n    \"\"\"\\n    adapted from: https://www.kaggle.com/sophiejermy/sj-eda1\\n    \"\"\"\\n#     text = text + \\' \\'\\n    #remove links\\n    text = re.sub(r\\'(?:(?:https?|ftp):\\\\/\\\\/)?[\\\\w/\\\\-?=%.]+\\\\.[\\\\w/\\\\-&?=%.]+\\', \\'\\', text)\\n    #lower case\\n    text = text.lower()    \\n    #remove special characters\\n    text = re.sub(r\\'[\\\\W]+\\', \\' \\', text)\\n    #remove double spaces\\n    text = re.sub(r\\'\\\\s+\\', \\' \\', text)\\n    #tokenize\\n    text = word_tokenize(text)\\n    #remove stop words\\n    text = [word for word in text if not word in stopwords.words(\\'english\\')]    \\n    #lemmatize\\n    text= [wordnet_lemmatizer.lemmatize(word, pos=\\'v\\') for word in text]\\n    #rejoin text to string\\n    text = \\' \\'.join(text)\\n    return text\\n\\ndef quick_clean_vectorized(col):\\n    return pd.DataFrame(data=col.apply(lambda x: quick_clean(x)).tolist())\\n\\nquiklean_transformer = FunctionTransformer(quick_clean_vectorized) # to use in pipeline\\n    ',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines text preprocessing functions to clean and lemmatize text data, and creates a function transformer for use in a pipeline.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.75697106}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"def plot_top_n_words(target = 1, n=50):\\n    \\n    count_dict = defaultdict(int)\\n\\n    for tweet in train.query(f'target=={target}')['text']:\\n        for word in word_tokenize(tweet):\\n            count_dict[word] += 1\\n\\n    wc_df = pd.DataFrame(data=count_dict.items(), columns = ['word', 'count'])\\n    sns.barplot(x = 'count', y='word', data=wc_df.sort_values(by=['count'], ascending=False)[:n])\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet defines a function that plots the top N most frequent words in tweets with the specified target label using a bar plot.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.8610096}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'plot_top_n_words()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet calls the previously defined function to plot the top N most frequent words in tweets with the target label equal to 1.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'relationship',\n",
       "       'subclass_id': 81,\n",
       "       'predicted_subclass_probability': 0.58346504}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"x_train, x_test, y_train, y_test = train_test_split(train.loc[:,train.columns != 'target'], train.target, test_size=0.2)\\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the training dataset into training and testing subsets and prints their dimensions.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.99772364}},\n",
       "     {'cell_id': 10,\n",
       "      'code': \"tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', max_features = 300)\\n\\npreprocess = Pipeline(steps=[\\n                    ('clean',   ColumnTransformer([\\n                                    ('cl', quiklean_transformer, 'text')\\n                                    ],\\n                                        remainder='drop')),\\n                    ('TFIDF', ColumnTransformer([\\n                        ('tfidf', tfidf_vectorizer, 0)\\n                    ], \\n                            remainder='passthrough')),\\n                    ('dim_reduce', TruncatedSVD(n_components=250, random_state=42)),\\n                    ('scale', MinMaxScaler())\\n    \\n        ])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a preprocessing pipeline that includes text cleaning, TF-IDF vectorization, dimensionality reduction, and feature scaling.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9810301}},\n",
       "     {'cell_id': 11,\n",
       "      'code': \"def submission(model, test_df, fname = 'submission'):\\n    y_hat = model.predict(test_df)\\n    submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\\n    submission['target'] = y_hat\\n    submission.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet defines a function to generate and save a submission file by applying the model to the test data and storing the predictions in a CSV file.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9989374}},\n",
       "     {'cell_id': 12,\n",
       "      'code': '# Tune logistic regression\\ndef objective(trial):\\n    x, y = x_train, y_train\\n    C = trial.suggest_float(\\'C\\', 1e-6, 1e6, log=True)\\n    penalty = trial.suggest_categorical(\\'penalty\\', [\\'l1\\', \\'l2\\', \\'elasticnet\\'])\\n    l1_ratio = trial.suggest_float(\\'l1_ratio\\', 0, 1)\\n    if penalty != \\'elasticnet\\':\\n        l1_ratio = None\\n    \\n    clf = make_pipeline(preprocess, LogisticRegression(C=C,\\n                                                      penalty=penalty,\\n                                                      l1_ratio=l1_ratio,\\n                                                      solver=\\'saga\\',\\n                                                      max_iter=800))\\n    clf.fit(x,y)\\n    \\n    acc = accuracy_score(y_test, clf.predict(x_test))\\n    \\n    return acc\\n\\nclass EarlyStopping:\\n    \"\"\"stop tuning after value remains unchanged after 10 successive trials\"\"\"\\n    def __init__(self, max_rounds = 10):\\n        self.max_rounds = max_rounds\\n        self.current_rounds = 0\\n        \\n    def __call__(self, study, trial, tol = 1e-6):\\n        if abs(trial.value - study.best_value) <= tol:\\n            self.current_rounds += 1\\n        elif trial.value == study.best_value:\\n            self.current_rounds = 0\\n        if self.current_rounds >= self.max_rounds:\\n            study.stop()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code snippet defines an Optuna objective function for tuning a Logistic Regression model's hyperparameters and an EarlyStopping class to halt tuning after a specified number of trials without improvement.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_model_class',\n",
       "       'subclass_id': 3,\n",
       "       'predicted_subclass_probability': 0.3142369}},\n",
       "     {'cell_id': 13,\n",
       "      'code': \"# # create study and run trials\\nes = EarlyStopping()\\n\\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())  # using Tree-structured Parzen Estimator to sample\\nstudy.optimize(objective, n_trials=250, callbacks=[es])\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet creates an Optuna study, sets it to maximize the objective function using the Tree-structured Parzen Estimator sampler, and runs the optimization process with early stopping criteria.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.80090225}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'def to_class_label(probs, threshold):\\n    \"\"\"convert predicted probabilities to class labels\"\"\"\\n    return (probs >= threshold).astype(\\'int\\')\\n\\ndef get_optimal_threshold(fitted_model, x_test, y_test):\\n    \"\"\"Threshold tuning\"\"\"\\n    thresholds = np.arange(0, 1, 0.0005)\\n    y_hat = fitted_model.predict_proba(x_test)\\n    pos_clas_probs = y_hat[:, 1]\\n    acc_scores = [accuracy_score(y_test, to_class_label(pos_clas_probs, thres)) for thres in thresholds]\\n    idx = np.argmax(acc_scores)\\n    \\n    return thresholds[idx]\\n    ',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet defines functions to convert predicted probabilities to class labels based on a threshold and to find the optimal threshold that maximizes the accuracy score on the test set.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.54912084}},\n",
       "     {'cell_id': 15,\n",
       "      'code': \"# train LR on best parameters\\nlr = LogisticRegression(**study.best_params, solver='saga', max_iter=800)\\nlr = make_pipeline(preprocess, lr)\\nlr.fit(x_train, y_train)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains a Logistic Regression model using the best hyperparameters obtained from the Optuna study and a predefined preprocessing pipeline.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99761343}},\n",
       "     {'cell_id': 16,\n",
       "      'code': \"# get optimal threshold\\nopt_thres = get_optimal_threshold(lr, x_test, y_test)\\nprint(f'Optimal threshold for trained LR {get_optimal_threshold(lr, x_test, y_test):.4f}')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet calculates and prints the optimal threshold for converting predicted probabilities to class labels by maximizing the accuracy score on the test set.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.6867679}},\n",
       "     {'cell_id': 17,\n",
       "      'code': \"# submission\\ny_hat = lr.predict_proba(test)\\ny_hat = y_hat[:, 1]\\n\\npreds = to_class_label(y_hat, opt_thres)\\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\\nsubmission['target'] = preds\\n\\nsubmission.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet generates predictions on the test set using the trained model, applies the optimal threshold to convert probabilities to class labels, and saves the results to a CSV file for submission.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9982597}}],\n",
       "    'notebook_id': 4},\n",
       "   'notebook_id': 4},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import numpy as np \\nimport pandas as pd \\nimport re\\nimport string\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom collections import Counter\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports necessary libraries and modules required for data manipulation, natural language processing, model building, and evaluation in a machine learning task.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993357}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\\ntest = pd.read_csv(\\'../input/nlp-getting-started/test.csv\\')',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads training and testing datasets from CSV files into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9997564}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'train.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the training dataset to give an initial look at the data structure and content.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997507}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'train.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet outputs the dimensions (number of rows and columns) of the training dataset to give an overview of its size.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99950194}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'print((train.target == 1).sum()) # Disaster\\nprint((train.target == 0).sum()) # No Disaster',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet counts and prints the number of instances in the training dataset labeled as disasters and non-disasters, providing an understanding of the class distribution.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.99325174}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'string.punctuation',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet accesses and likely aims to utilize the list of punctuation characters in Python, which can be used for text preprocessing tasks such as removing punctuation from text data.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.73409814}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'def remove_URL(text):\\n    url = re.compile(r\"https?://\\\\S+|www\\\\.\\\\S+\")\\n    return url.sub(r\"\", text)\\n\\ndef remove_punct(text):\\n    translator = str.maketrans(\"\", \"\", string.punctuation)\\n    return text.translate(translator)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines two functions to preprocess text data: one to remove URLs and another to remove punctuation from the text.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.7244959}},\n",
       "     {'cell_id': 7,\n",
       "      'code': '#regex pattern to remove links\\npattern = re.compile(r\"https?://(\\\\S+|www)\\\\.\\\\S+\")\\n#for train\\nfor t in train.text:\\n    matches = pattern.findall(t)\\n    for match in matches:\\n        print(t)\\n        print(\\'After Transformation:\\')\\n        print(pattern.sub(r\"\", t))\\n    if len(matches) > 0:\\n        break',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet uses a regular expression pattern to identify and remove URLs from text data in the training dataset, demonstrating the transformation process on the first text that matches.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9533664}},\n",
       "     {'cell_id': 8,\n",
       "      'code': '#for test:\\nfor t in test.text:\\n    matches = pattern.findall(t)\\n    for match in matches:\\n        print(t)\\n        print(\\'After Transformation:\\')\\n        print(pattern.sub(r\"\", t))\\n    if len(matches) > 0:\\n        break',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet uses a regular expression pattern to identify and remove URLs from text data in the testing dataset, demonstrating the transformation process on the first text that matches.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9523689}},\n",
       "     {'cell_id': 9,\n",
       "      'code': '#preprocess data frames:\\n#train\\ntrain[\"text\"] = train.text.map(remove_URL) \\ntrain[\"text\"] = train.text.map(remove_punct)\\n#test\\ntest[\"text\"] = test.text.map(remove_URL) \\ntest[\"text\"] = test.text.map(remove_punct)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies the previously defined text preprocessing functions to remove URLs and punctuation from the 'text' column in both the training and testing datasets.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.90332294}},\n",
       "     {'cell_id': 10,\n",
       "      'code': '# remove stopwords\\nnltk.download(\\'stopwords\\')\\n\\nstop = set(stopwords.words(\"english\"))\\n\\ndef remove_stopwords(text):\\n    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\\n    return \" \".join(filtered_words)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet downloads the NLTK stopwords list and defines a function to remove stopwords from text data, further preprocessing the text by filtering out common but uninformative words.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9945669}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'stop',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet displays the set of stopwords used for filtering out common and uninformative words from the text data during preprocessing.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9975351}},\n",
       "     {'cell_id': 12,\n",
       "      'code': '#train\\ntrain[\"text\"] = train.text.map(remove_stopwords)\\n#test\\ntest[\"text\"] = test.text.map(remove_stopwords)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet removes stopwords from the 'text' column in both the training and testing datasets by applying the previously defined function.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.76800144}},\n",
       "     {'cell_id': 13,\n",
       "      'code': '#Check\\ntrain.text',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet displays the 'text' column of the training dataset, allowing verification of the preprocessing steps applied.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.53716487}},\n",
       "     {'cell_id': 14,\n",
       "      'code': '# Count unique words\\ndef counter_word(text_col):\\n    count = Counter()\\n    for text in text_col.values:\\n        for word in text.split():\\n            count[word] += 1\\n    return count\\n\\n\\ncounter = counter_word(train.text)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet defines a function to count the frequency of each unique word in a given text column and then applies this function to the 'text' column of the training dataset.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.94201726}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'len(counter)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet calculates and outputs the number of unique words in the training dataset, providing insight into the vocabulary size after preprocessing.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.98219407}},\n",
       "     {'cell_id': 16,\n",
       "      'code': '# counter',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet, when uncommented, will display the word frequency counter for the training dataset, providing detailed insights into word distribution.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.9903482}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'counter.most_common(5)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet retrieves and displays the five most common words along with their frequencies in the training dataset, highlighting the most frequent terms after preprocessing.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9768316}},\n",
       "     {'cell_id': 18,\n",
       "      'code': 'num_unique_words = len(counter)\\nnum_unique_words',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet stores and outputs the number of unique words in the training dataset, enabling further analysis and parameter setting for modeling.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_unique_values',\n",
       "       'subclass_id': 54,\n",
       "       'predicted_subclass_probability': 0.9817012}},\n",
       "     {'cell_id': 19,\n",
       "      'code': '# Split dataset into training and validation set\\nX = train.text\\ny = train.target\\ntrain_sentences, val_sentences , train_labels, val_labels = train_test_split(X, y, test_size=0.2)',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet splits the preprocessed training dataset into training and validation sets for both text data and labels, using an 80-20 split ratio.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.9967854}},\n",
       "     {'cell_id': 20,\n",
       "      'code': '#train/val\\ntrain_sentences = train_sentences.to_numpy()\\ntrain_labels = train_labels.to_numpy()\\nval_sentences = val_sentences.to_numpy()\\nval_labels = val_labels.to_numpy()',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet converts the training and validation text data and labels into NumPy arrays to facilitate subsequent processing and modeling.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.9735998}},\n",
       "     {'cell_id': 21,\n",
       "      'code': '#test\\ntest_sentences = test.text.to_numpy()',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet converts the preprocessed text data from the testing dataset into a NumPy array for further processing and model evaluation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.98378074}},\n",
       "     {'cell_id': 22,\n",
       "      'code': 'train_sentences.shape, val_sentences.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet outputs the dimensions of the training and validation text data arrays, providing insight into the data split sizes.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9996413}},\n",
       "     {'cell_id': 23,\n",
       "      'code': '# Tokenize\\n# vectorize a text corpus by turning each text into a sequence of integers\\n\\ntokenizer = Tokenizer(num_words=num_unique_words)\\ntokenizer.fit_on_texts(train_sentences) # fit only to training',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet initializes a Tokenizer with the number of unique words and fits it on the training sentences to convert text into sequences of integers.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.6493907}},\n",
       "     {'cell_id': 24,\n",
       "      'code': '# Now each word has unique index\\nword_index = tokenizer.word_index\\nword_index',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet retrieves and displays the word index mapping created by the tokenizer, where each unique word is assigned a unique integer index.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.66235614}},\n",
       "     {'cell_id': 25,\n",
       "      'code': '#apply on train, validation, and test sentences\\n\\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\\nval_sequences = tokenizer.texts_to_sequences(val_sentences)\\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet converts the text data in the training, validation, and testing sets into sequences of integers using the fitted tokenizer.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.5915242}},\n",
       "     {'cell_id': 26,\n",
       "      'code': '#Check\\nprint(train_sentences[10:15])\\nprint(train_sequences[10:15])',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints a small subset of the original training sentences and their corresponding sequences of integers to verify the tokenization process.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9992624}},\n",
       "     {'cell_id': 27,\n",
       "      'code': '# Pad the sequences to have the same length\\nmax_length = 15 #arbitrary number\\n\\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\") #post-> 0\\nval_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet pads the tokenized sequences in the training, validation, and testing datasets to a uniform length of 15, ensuring consistent input dimensions for the machine learning model.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.717097}},\n",
       "     {'cell_id': 28,\n",
       "      'code': '#Check\\ntrain_padded.shape, val_padded.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet checks and outputs the dimensions of the padded training and validation datasets to ensure they have been correctly transformed to the specified length.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9994609}},\n",
       "     {'cell_id': 29,\n",
       "      'code': '#Check\\ntrain_padded[10]',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet examines a specific padded sequence from the training dataset to verify the padding process.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9994702}},\n",
       "     {'cell_id': 30,\n",
       "      'code': '#Check\\nprint(train_sentences[10])\\nprint(train_sequences[10])\\nprint(train_padded[10])',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints a specific training sentence, its tokenized sequence, and the corresponding padded sequence to verify that the transformation processes have been correctly applied.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99758375}},\n",
       "     {'cell_id': 31,\n",
       "      'code': '# flip (key, value)\\nreverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet creates a reverse word index dictionary by flipping the keys and values of the original word index, which maps integer indices back to their corresponding words.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9888599}},\n",
       "     {'cell_id': 32,\n",
       "      'code': '#Check\\nreverse_word_index',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet, when executed, will display the reverse word index dictionary to verify the mappings of indices back to their corresponding words.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.98285496}},\n",
       "     {'cell_id': 33,\n",
       "      'code': '#decoding\\ndef decode(sequence):\\n    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function to decode a sequence of integers back into the original text using the reverse word index, facilitating the interpretation of tokenized sequences.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.95032895}},\n",
       "     {'cell_id': 34,\n",
       "      'code': 'decoded_text = decode(train_sequences[10])\\n#Check\\nprint(train_sequences[10])\\nprint(decoded_text)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet decodes a specific tokenized sequence back into its original text form and prints both the sequence and the decoded text to verify the decoding process.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.42135346}},\n",
       "     {'cell_id': 35,\n",
       "      'code': '# Create LSTM model\\n\\n# Embedding: Turns positive integers (indexes) into dense vectors of fixed size.\\n\\nmodel = keras.models.Sequential()\\nmodel.add(layers.Embedding(num_unique_words, 100, input_length=max_length))\\n\\nmodel.add(layers.LSTM(32, dropout=0.25))\\nmodel.add(layers.Dense(1, activation=\"sigmoid\"))\\n\\nmodel.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines a Sequential LSTM model architecture in Keras, including an Embedding layer to convert word indices to dense vectors, an LSTM layer for sequence processing, and a Dense output layer with a sigmoid activation function for binary classification, followed by a summary of the model.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.984528}},\n",
       "     {'cell_id': 36,\n",
       "      'code': 'loss = keras.losses.BinaryCrossentropy(from_logits=False)\\noptim = keras.optimizers.Adam(learning_rate=0.001)\\nmetrics = [\"accuracy\"]\\n\\nmodel.compile(loss=loss, optimizer=optim, metrics=metrics)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet compiles the LSTM model by specifying the binary cross-entropy loss function, Adam optimizer, and accuracy as the evaluation metric to prepare the model for training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9955751}},\n",
       "     {'cell_id': 37,\n",
       "      'code': 'model.fit(train_padded, train_labels, epochs=25, validation_data=(val_padded, val_labels), verbose=2)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the compiled LSTM model using the padded training data and labels over 25 epochs, while evaluating performance on validation data at the end of each epoch.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9996841}},\n",
       "     {'cell_id': 38,\n",
       "      'code': 'predictions = model.predict(test_padded)\\npredictions = [1 if p > 0.5 else 0 for p in predictions]',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet generates predictions on the padded test data using the trained model, and then converts these predictions into binary class labels based on a threshold of 0.5.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.99410737}},\n",
       "     {'cell_id': 39,\n",
       "      'code': \"submission = pd.DataFrame({'id':test['id'].values.tolist(),'target':predictions})\\nsubmission.to_csv('submission.csv',index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet creates a DataFrame for the submission combining test sample IDs with the model's predictions and exports it to a CSV file.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9992361}}],\n",
       "    'notebook_id': 5},\n",
       "   'notebook_id': 5},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the read-only \"../input/\" directory\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \\n# You can also write temporary files to /kaggle/temp/, but they won\\'t be saved outside of the current session',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports essential libraries such as NumPy and pandas, and lists the files available in the input directory to prepare the environment for further analysis.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'list_files',\n",
       "       'subclass_id': 88,\n",
       "       'predicted_subclass_probability': 0.99921954}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\\nsample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the train, test, and sample submission CSV files from the specified input directory using pandas.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99971575}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'train.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the training dataset to provide an overview of its structure and contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997507}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'test.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the test dataset to provide an overview of its structure and contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997483}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'print(train.apply(lambda col: col.unique()))\\nprint(train.apply(lambda col: col.nunique()))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the unique values and the number of unique values for each column in the training dataset to help understand its distribution and potential categorical variables.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_unique_values',\n",
       "       'subclass_id': 54,\n",
       "       'predicted_subclass_probability': 0.95746493}},\n",
       "     {'cell_id': 5,\n",
       "      'code': '!pip install spacy -q\\n!python -m spacy download en_core_web_sm -q',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet installs the spaCy library and downloads the English language model \"en_core_web_sm\" to prepare the environment for natural language processing tasks.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.993651}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'import matplotlib.pyplot as plt\\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\\nfrom sklearn.base import TransformerMixin\\nfrom sklearn.pipeline import Pipeline\\nimport string\\nfrom spacy.lang.en.stop_words import STOP_WORDS\\nfrom spacy.lang.en import English\\nimport spacy\\nimport en_core_web_sm\\nimport re',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports various libraries and modules necessary for data visualization, text feature extraction, and text preprocessing for the machine learning pipeline.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9992192}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"from sklearn.model_selection import train_test_split\\n\\nX = train['text'] + ' ' +  train['keyword'].astype(str) + ' ' +  train['location'].astype(str) # the features we want to analyze\\nylabels = train['target'] # the labels, or answers, we want to test against\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet combines text, keyword, and location columns into a single feature, and then splits the data into training and test sets with a 70-30 split.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.9936372}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'X_train[100:500]\\n#type(X_train[1])\\n#y_train[:100]',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet outputs a slice of the combined training data features from index 100 to 500 to review the data that will be used for training.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9915615}},\n",
       "     {'cell_id': 9,\n",
       "      'code': '\\npunctuations = string.punctuation \\nnlp = spacy.load(\\'en_core_web_sm\\') #, exclude=[\"tok2vec\", \"parser\", \"ner\", \"attribute_ruler\"]\\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\\nparser = English() # Load English tokenizer, tagger, parser, NER and word vectors\\n\\ndef spacy_tokenizer(sentence):\\n    mytokens = str(sentence)\\n    mytokens = nlp(mytokens)\\n    #mytokens = parser(sentence) \\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] \\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]    \\n    return mytokens      # return preprocessed list of tokens\\n\\nclass predictors(TransformerMixin):\\n    def transform(self, X, **transform_params):\\n        return [clean_text(text) for text in X]\\n\\n    def fit(self, X, y=None, **fit_params):\\n        return self\\n\\n    def get_params(self, deep=True):\\n        return {}\\n\\ndef clean_text(text):\\n    text =  text.strip().lower()\\n    #text = re.sub(r\\'[^A-Za-z0-9 ]+\\', \\'\\', text)\\n    return text #.split()\\n\\nbow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1), stop_words = None)\\ntfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer,  stop_words = None) #token_pattern=\\'(?u)\\\\b\\\\w\\\\w+\\\\b\\', stop_words = \\'english\\'\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import RandomForestClassifier\\nclassifier = LogisticRegression()\\n# classifier = RandomForestClassifier()\\n\\npipe = Pipeline([(\"cleaner\", predictors()),\\n                 (\\'vectorizer\\', tfidf_vector),\\n                 (\\'classifier\\', classifier)])\\n\\n#clean_text(X_train[1773])\\n#spacy_tokenizer(X_train[1773])\\n#mytokens = parser(X_train[1773])\\n\\n# mytokens = str(X_train[1773])\\n# #mytokens = re.sub(r\\'[^A-Za-z0-9 ]+\\', \\'\\', mytokens)\\n# #mytokens = parser(mytokens)\\n# mytokens = nlp(mytokens)\\n# mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\\n# print(mytokens)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines the tokenizer and text preprocessing pipeline, sets up text vectorization using CountVectorizer and TfidfVectorizer, and constructs a machine learning pipeline integrating text preprocessing and a logistic regression classifier for training.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.5153541}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'pipe.fit(X_train, y_train)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet fits the machine learning pipeline to the training data, effectively training the model.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99970585}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'from sklearn import metrics\\n# Predicting with a test dataset\\npredicted = pipe.predict(X_test)\\n\\n# Model Accuracy\\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))\\nprint(\"Precision:\",metrics.precision_score(y_test, predicted))\\nprint(\"Recall:\",metrics.recall_score(y_test, predicted))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet predicts the target values for the test dataset using the trained pipeline and then calculates and prints the accuracy, precision, and recall scores to evaluate the model's performance.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.758966}},\n",
       "     {'cell_id': 12,\n",
       "      'code': 'predicted_df = pd.DataFrame(predicted)\\npredicted_df.value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet converts the predicted labels into a DataFrame and then counts the occurrences of each unique predicted label to understand the distribution of the predictions.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.99671495}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'predicted_df.plot.hist()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet plots a histogram of the predicted label counts to visually inspect the distribution of the predictions.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9975446}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'predicted_df.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the DataFrame containing the predicted labels to provide an overview of its contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997552}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'test.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the test dataset to provide an overview of its structure and contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997483}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'sample_submission.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the sample submission dataset to provide an overview of its structure and contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99975234}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'predicted',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet outputs the array of predicted labels resulting from the trained pipeline applied to the test dataset to inspect the predictions.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99976486}},\n",
       "     {'cell_id': 18,\n",
       "      'code': 'test',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the entire test dataset to review its contents, features, and structure comprehensively.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997199}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'my_submission_preds = pipe.predict(test[\\'text\\']+ \\' \\' +  test[\\'keyword\\'].astype(str) + \\' \\' +  test[\\'location\\'].astype(str))\\n\\nmy_submission = pd.DataFrame({\"id\":test[\\'id\\'], \\'target\\':my_submission_preds})',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet generates predictions for the test dataset using the trained pipeline and stores them in a new DataFrame along with the respective IDs from the test dataset.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.7191785}},\n",
       "     {'cell_id': 20,\n",
       "      'code': 'my_submission.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the `my_submission` DataFrame to review the structure and content of the submission, including the IDs and corresponding predicted target values.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997489}},\n",
       "     {'cell_id': 21,\n",
       "      'code': 'len(my_submission)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet outputs the length of the `my_submission` DataFrame to confirm the number of rows (predictions) generated for the test dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99885213}},\n",
       "     {'cell_id': 22,\n",
       "      'code': \"my_submission.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet saves the `my_submission` DataFrame to a CSV file named 'submission.csv' without including the index to prepare for submission.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99912554}}],\n",
       "    'notebook_id': 6},\n",
       "   'notebook_id': 6},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': \"! pip install tf-models-official==2.4.0 -q\\n! pip install tensorflow-gpu==2.4.1 -q\\n! pip install tensorflow-text==2.4.1 -q\\n! python -m spacy download en_core_web_sm -q\\n! pip install dataprep | grep -v 'already satisfied'\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': \"The code installs specific versions of TensorFlow, TensorFlow models, TensorFlow Text, spaCy's English language model, and the dataprep library.\",\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.9604686}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"import pandas as pd\\nimport numpy as np\\nnp.set_printoptions(precision=4)\\n\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n# Visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom dataprep.eda import plot, plot_diff, plot_correlation, create_report\\nfrom dataprep.clean import clean_text\\n\\n# Preprocessing and Modelling\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nimport spacy\\nimport tensorflow_text as text\\nimport tensorflow_hub as hub\\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, concatenate \\nfrom tensorflow.keras import Model, regularizers \\nfrom tensorflow.keras.metrics import BinaryAccuracy\\nfrom tensorflow.keras.losses import BinaryCrossentropy\\nfrom official.nlp.optimization import create_optimizer # AdamW optimizer\\n# Warning\\nimport warnings\\nwarnings.filterwarnings('ignore')\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code imports necessary libraries and modules for data manipulation, visualization, preprocessing, modeling, and utility functions while also setting a few configurations.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.98988277}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'tf.__version__',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code retrieves and prints the version of the installed TensorFlow library.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9983907}},\n",
       "     {'cell_id': 3,\n",
       "      'code': '# Random seeds\\nimport random\\nimport numpy as np\\nimport tensorflow as tf\\nrandom.seed(319)\\nnp.random.seed(319)\\ntf.random.set_seed(319)',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code sets random seeds for the random, NumPy, and TensorFlow libraries to ensure reproducibility of results.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9086065}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"train_full = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\\ntest_full = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\\n\\nprint('Training Set Shape = {}'.format(train_full.shape))\\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()/2**20))\\n\\nprint('Test Set Shape = {}'.format(test_full.shape))\\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()/2**20))\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'The code reads training and testing datasets from CSV files into Pandas DataFrames and prints their shapes and memory usage.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9972459}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'plot(train_full)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"The code generates and displays exploratory data analysis visualizations for the training dataset using the 'plot' function from the dataprep library.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99482274}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'create_report(train_full)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"The code generates a comprehensive report for the training dataset using the 'create_report' function from the dataprep library for exploratory data analysis purposes.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.5819901}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"plot(train_full, 'text')\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"The code generates and displays visualizations specifically for the 'text' column in the training dataset using the 'plot' function from the dataprep library.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99791676}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'train_full.text',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': \"The code extracts and displays the 'text' column from the training dataset DataFrame.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9863797}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'plot(train_full, \"text\", \"target\")',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"The code generates and displays visualizations for the relationship between the 'text' and 'target' columns in the training dataset using the 'plot' function from the dataprep library.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.988972}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'df1 = train_full.text[train_full.target == 0]\\ndf2 = train_full.text[train_full.target == 1]\\nplot_diff([df1, df2])',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"The code extracts the 'text' column for the two groups defined by the 'target' column in the training dataset and then generates a comparative visualization using the 'plot_diff' function from the dataprep library.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9446237}},\n",
       "     {'cell_id': 11,\n",
       "      'code': '# Read commited-dataset\\ndf_train = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/train_prepared.csv\")\\ndf_test = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/test_prepared.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'The code reads prepared training and testing datasets from CSV files into Pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99974114}},\n",
       "     {'cell_id': 12,\n",
       "      'code': \"# Only apply 'keyword' columns in full data, because other features cleaned in df_train/test\\ntrain_full = clean_text(train_full,'keyword')\\ntest_full = clean_text(test_full, 'keyword')\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"The code cleans the 'keyword' column in both the training and test datasets using the 'clean_text' function from the dataprep library, as the other features have already been cleaned in the prepared data.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.9898755}},\n",
       "     {'cell_id': 13,\n",
       "      'code': \"# Adding cleaned data into df_train/test\\ndf_train['keyword'] = train_full['keyword']\\ndf_test['keyword'] = test_full['keyword']\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"The code adds the cleaned 'keyword' column from the original datasets to the prepared training and testing DataFrames.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.99662316}},\n",
       "     {'cell_id': 14,\n",
       "      'code': \"# Load Spacy Library\\nnlp_spacy = spacy.load('en_core_web_sm')\\n# Load the sentence encoder\\nsentence_enc = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code initializes the spaCy English language model and loads the Universal Sentence Encoder from TensorFlow Hub.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.99203765}},\n",
       "     {'cell_id': 15,\n",
       "      'code': \"def extract_keywords(text):\\n    potential_keywords = []\\n    TOP_KEYWORD = -1\\n    # Create a list for keyword parts of speech\\n    pos_tag = ['ADJ', 'NOUN', 'PROPN']\\n    doc = nlp_spacy(text)\\n    \\n    for i in doc:\\n        if i.pos_ in pos_tag:\\n            potential_keywords.append(i.text)\\n\\n    document_embed = sentence_enc([text])\\n    potential_embed = sentence_enc(potential_keywords)    \\n    \\n    vector_distances = cosine_similarity(document_embed, potential_embed)\\n    keyword = [potential_keywords[i] for i in vector_distances.argsort()[0][TOP_KEYWORD:]]\\n\\n    return keyword\\n\\ndef keyword_filler(keyword, text):\\n    if pd.isnull(keyword):\\n        try:\\n            keyword = extract_keywords(text)[0]\\n        except:\\n            keyword = '' \\n        \\n    return keyword\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code defines two functions: `extract_keywords`, which identifies and extracts keywords from a given text using spaCy for POS tagging and the Universal Sentence Encoder for embeddings; and `keyword_filler`, which fills missing keywords by extracting them from the corresponding text.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.41046342}},\n",
       "     {'cell_id': 16,\n",
       "      'code': \"df_train.keyword = pd.DataFrame(list(map(keyword_filler, df_train.keyword, df_train.text))).astype(str)\\ndf_test.keyword = pd.DataFrame(list(map(keyword_filler, df_test.keyword, df_test.text))).astype(str)\\n\\nprint('Null Training Keywords => ', df_train['keyword'].isnull().any())\\nprint('Null Test Keywords => ', df_test['keyword'].isnull().any())\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"The code fills in missing keywords for both the training and test DataFrames by applying the `keyword_filler` function, converts the results to string type, and then checks for any remaining null values in the 'keyword' columns.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.95129913}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'df_train',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'The code displays the training DataFrame.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9994585}},\n",
       "     {'cell_id': 18,\n",
       "      'code': \"keyword_non_disaster = df_train.keyword[df_train.target==0].value_counts().reset_index()\\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\\nplt.title('Non-Disaster Keyword Frequency (0)')\\nplt.xlabel('Frequency')\\nplt.ylabel('Top 10 Keywords')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'The code generates a bar plot showing the frequencies of the top 10 keywords associated with non-disaster tweets (target=0) in the training dataset using seaborn and matplotlib.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.88048565}},\n",
       "     {'cell_id': 19,\n",
       "      'code': \"keyword_disaster = df_train.keyword[df_train.target==1].value_counts().reset_index()\\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\\nplt.title('Non-Disaster Keyword Frequency (0)')\\nplt.xlabel('Frequency')\\nplt.ylabel('Top 10 Keywords')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'The code generates a bar plot showing the frequencies of the top 10 keywords associated with non-disaster tweets (target=0) in the training dataset, but incorrectly labels the plot instead of showing disaster tweets (target=1).',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.8303329}},\n",
       "     {'cell_id': 20,\n",
       "      'code': \"# Spilt data\\nX_train, X_val, y_train, y_val = train_test_split(df_train[['text','keyword']],\\n                                                    df_train.target, \\n                                                    test_size=0.2, \\n                                                    random_state=42)\\nX_train.shape, X_val.shape\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"The code splits the prepared training dataset into training and validation sets, segregating 'text' and 'keyword' features from the target, and then displays the shapes of the resulting subsets.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.998221}},\n",
       "     {'cell_id': 21,\n",
       "      'code': \"train_ds = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\\nval_ds = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(df_test[['text','keyword']]))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code converts the training, validation, and test data subsets into TensorFlow datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.8992567}},\n",
       "     {'cell_id': 22,\n",
       "      'code': 'AUTOTUNE = tf.data.experimental.AUTOTUNE\\n\\nBUFFER_SIZE = 1000\\nBATCH_SIZE = 32\\nRANDOM_SEED = 319\\n\\ndef configure_dataset(dataset, shuffle=False, test=False):\\n    if shuffle:\\n        dataset = dataset.cache()\\\\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\\\\n                        .prefetch(AUTOTUNE)\\n    elif test:\\n        dataset = dataset.cache()\\\\\\n                        .batch(BATCH_SIZE, drop_remainder=False)\\\\\\n                        .prefetch(AUTOTUNE)\\n    else:\\n        dataset = dataset.cache()\\\\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\\\\n                        .prefetch(AUTOTUNE)\\n    return dataset',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code defines a function named `configure_dataset` that configures TensorFlow datasets by optionally shuffling, batching, and prefetching them for optimized performance during training, validation, and testing phases.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.49705026}},\n",
       "     {'cell_id': 23,\n",
       "      'code': \"a3 = configure_dataset(train_ds, shuffle=True)\\ndict3 = []\\nfor elem in a3:\\n    dict3.append(elem[0]['text'][0])\\ndict3[:10]\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code configures the training dataset with shuffling, batching, and prefetching, then iterates through it to extract and display the first text elements from the batches, demonstrating the data preparation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.61363506}},\n",
       "     {'cell_id': 24,\n",
       "      'code': '# Configure the datasets\\ntrain_ds = configure_dataset(train_ds, shuffle=True)\\nval_ds = configure_dataset(val_ds)\\ntest_ds = configure_dataset(test_ds, test=True)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code configures the training, validation, and test datasets with appropriate settings for shuffling, batching, and prefetching by utilizing the `configure_dataset` function.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.36260855}},\n",
       "     {'cell_id': 25,\n",
       "      'code': '# Free memory\\ndel X_train, X_val, y_train, y_val, df_train, df_test, train_full, test_full',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"The code frees up memory by deleting the unused variables and datasets after they've been transformed into TensorFlow datasets.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.4480442}},\n",
       "     {'cell_id': 26,\n",
       "      'code': '# Bidirectional Encoder Representations from Transformers (BERT).\\nbert_encoder_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\\n# Text preprocessing for BERT.\\nbert_preprocessor_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\\n# Token based text embedding trained on English Google News 200B corpus.\\nkeyword_embedding_path = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code sets the paths for the BERT encoder, BERT preprocessor, and a token-based text embedding model trained on the English Google News corpus, referencing TensorFlow Hub modules.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9857056}},\n",
       "     {'cell_id': 27,\n",
       "      'code': 'bert_encoder = hub.KerasLayer(bert_encoder_path, trainable=True, name=\"BERT_Encoder\")\\nbert_preprocessor = hub.KerasLayer(bert_preprocessor_path, name=\"BERT_Preprocessor\")\\nnnlm_embed = hub.KerasLayer(keyword_embedding_path, name=\"NNLM_Embedding\")',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code initializes TensorFlow Hub Keras layers for the BERT encoder, BERT preprocessor, and NNLM embedding models, making them ready for use in model construction.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.506471}},\n",
       "     {'cell_id': 28,\n",
       "      'code': 'kernel_initializer = tf.keras.initializers.GlorotNormal(seed=319)\\n# Model function\\ndef create_model():\\n    # Keyword Branch\\n    text_input = Input(shape=(), dtype=tf.string, name=\"text\")\\n    encoder_inputs = bert_preprocessor(text_input)\\n    encoder_outputs = bert_encoder(encoder_inputs)\\n    # Pooled output\\n    pooled_output = encoder_outputs[\"pooled_output\"]\\n    bert_branch = Dropout(0.1,\\n                          seed=319,\\n                          name=\"BERT_Dropout\")(pooled_output)\\n    # Construct keyword layers\\n    keyword_input = Input(shape=(), dtype=tf.string, name=\\'keyword\\')\\n    keyword_embed = nnlm_embed(keyword_input)\\n    keyword_flat = Flatten(name=\"Keyword_Flatten\")(keyword_embed)\\n    keyword_dense1 = Dense(128, \\n                          activation=\\'relu\\',\\n                          kernel_initializer=kernel_initializer,\\n                          kernel_regularizer=regularizers.l2(1e-4),\\n                          name=\"Keyword_Dense1\"\\n                         )(keyword_flat)\\n    keyword_branch1 = Dropout(0.5,\\n                             seed=319,\\n                             name=\\'Keyword_dropout1\\'\\n                            )(keyword_dense1)\\n    keyword_dense2 = Dense(128, \\n                          activation=\\'relu\\',\\n                          kernel_initializer=kernel_initializer,\\n                          kernel_regularizer=regularizers.l2(1e-4),\\n                          name=\"Keyword_Dense2\"\\n                         )(keyword_branch1)\\n    keyword_branch2 = Dropout(0.5,\\n                             seed=319,\\n                             name=\\'Keyword_dropout2\\'\\n                            )(keyword_dense2)\\n    keyword_dense3 = Dense(128, \\n                          activation=\\'relu\\',\\n                          kernel_initializer=kernel_initializer,\\n                          kernel_regularizer=regularizers.l2(1e-4),\\n                          name=\"Keyword_Dense3\"\\n                         )(keyword_branch2)\\n    keyword_branch3 = Dropout(0.5,\\n                             seed=319,\\n                             name=\\'Keyword_dropout3\\'\\n                            )(keyword_dense3)\\n    \\n    # Merge the layers and classify\\n    merge = concatenate([bert_branch, keyword_branch3], name=\"Concatenate\")\\n    dense = Dense(128, \\n                  activation=\\'relu\\',\\n                  kernel_initializer=kernel_initializer,\\n                  kernel_regularizer=regularizers.l2(1e-4), \\n                  name=\"Merged_Dense\")(merge)\\n    dropout = Dropout(0.5,\\n                      seed=319,\\n                      name=\"Merged_Dropout\"\\n                     )(dense)\\n    clf = Dense(1,\\n                activation=\"sigmoid\", \\n                kernel_initializer=kernel_initializer,\\n                name=\"Classifier\"\\n               )(dropout)\\n    return Model([text_input, keyword_input], \\n                 clf, \\n                 name=\"BERT_Classifier\")',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code defines a function to create a BERT-based classification model that processes both text and keywords through separate branches, merges their outputs, and applies dense layers with dropout for the final classification.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.99885345}},\n",
       "     {'cell_id': 29,\n",
       "      'code': 'bert_classifier = create_model()\\nbert_classifier.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code creates an instance of the BERT-based classification model using the `create_model` function and displays its summary, which includes the model architecture and parameter details.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.99306774}},\n",
       "     {'cell_id': 30,\n",
       "      'code': 'keras.utils.plot_model(bert_classifier, \\n                      show_shapes=False)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"The code generates and displays a visual representation of the BERT-based classification model architecture using Keras' `plot_model` function.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.9785347}},\n",
       "     {'cell_id': 31,\n",
       "      'code': \"EPOCHS = 3\\nLEARNING_RATE = 5e-5\\n\\nSTEPS_PER_EPOCH = int(train_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\\nVAL_STEPS = int(val_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\\n# Calculate the train and warmup steps for the optimizer\\nTRAIN_STEPS = STEPS_PER_EPOCH * EPOCHS\\nWARMUP_STEPS = int(TRAIN_STEPS * 0.1)\\n\\nadamw_optimizer = create_optimizer(\\n    init_lr=LEARNING_RATE,\\n    num_train_steps=TRAIN_STEPS,\\n    num_warmup_steps=WARMUP_STEPS,\\n    optimizer_type='adamw'\\n)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code sets training parameters like epochs and learning rate, calculates steps per epoch, validation steps, total train steps, warmup steps, and initializes the AdamW optimizer configured with these settings for training the model.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'init_hyperparams',\n",
       "       'subclass_id': 59,\n",
       "       'predicted_subclass_probability': 0.6065405}},\n",
       "     {'cell_id': 32,\n",
       "      'code': 'STEPS_PER_EPOCH, VAL_STEPS, TRAIN_STEPS, WARMUP_STEPS',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code retrieves and prints the values for steps per epoch, validation steps, total training steps, and warmup steps, which were previously calculated for configuring the optimizer.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.94944763}},\n",
       "     {'cell_id': 33,\n",
       "      'code': 'bert_classifier.compile(loss=BinaryCrossentropy(from_logits=True),\\n                   optimizer=adamw_optimizer, \\n                   metrics=[BinaryAccuracy(name=\"accuracy\")]\\n                  )\\nhistory = bert_classifier.fit(train_ds, \\n                         epochs=EPOCHS,\\n                         steps_per_epoch=STEPS_PER_EPOCH,\\n                         validation_data=val_ds,\\n                         validation_steps=VAL_STEPS\\n                        )',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code compiles the BERT-based classifier model with a binary cross-entropy loss function and AdamW optimizer, then trains the model using the configured training and validation datasets across the specified number of epochs.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.97614974}},\n",
       "     {'cell_id': 34,\n",
       "      'code': \"def submission(model, test):\\n    sample_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\\n    predictions =  model.predict(test)\\n    y_preds = [ int(i) for i in np.rint(predictions)]\\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\\n    sub.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"The code defines a function to generate a submission file by making predictions on the test dataset using the trained model, converting predictions to integer values, and saving them in a CSV file named 'submission.csv' with the appropriate format.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9988925}},\n",
       "     {'cell_id': 35,\n",
       "      'code': 'submission(bert_classifier, test_ds)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"The code calls the `submission` function to generate and save the predictions for the test dataset to a 'submission.csv' file using the trained BERT classifier model.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.61737144}}],\n",
       "    'notebook_id': 7},\n",
       "   'notebook_id': 7},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import numpy as np \\nimport pandas as pd \\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom wordcloud import WordCloud\\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\\n\\n#sklearn \\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils.class_weight import compute_sample_weight\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\nfrom sklearn.feature_extraction.text import TfidfTransformer\\nfrom sklearn.metrics import accuracy_score, confusion_matrix\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import SGDClassifier\\n\\n# nlp preprocessing lib\\nimport gensim\\nfrom gensim.utils import simple_preprocess\\nfrom gensim.parsing.preprocessing import STOPWORDS\\nimport string \\npunctation = string.punctuation',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports various libraries and modules required for data analysis, visualization, and machine learning tasks, including NLP preprocessing.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99901414}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the training and testing data from CSV files into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99975425}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'train_df.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the training DataFrame to inspect the structure and content of the dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997545}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"train_df = train_df.drop(['id', 'keyword', 'location'], axis = 1)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet removes the 'id', 'keyword', and 'location' columns from the training DataFrame.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.9992505}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'train_df.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the dimensions of the training DataFrame to understand how many rows and columns are present after dropping specified columns.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9995821}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'train_df.columns',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet lists the column names of the training DataFrame to provide an overview of the available features after the column drop operation.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_columns',\n",
       "       'subclass_id': 71,\n",
       "       'predicted_subclass_probability': 0.9984144}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'train_df.info()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet provides a summary of the training DataFrame, including the data types of each column and the number of non-null entries, to understand the dataset's structure and data completeness.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9993624}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'train_df.describe()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet generates descriptive statistics of the numeric columns in the training DataFrame to summarize the central tendency, dispersion, and shape of the dataset's distribution.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9994492}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'train_df[train_df[\"target\"] == 1][\"text\"].values[0]',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet retrieves the text of the first row in the training DataFrame where the 'target' column is equal to 1, providing a sample of the data associated with a specific label.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.95915043}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'train_df[train_df[\"target\"] == 1][\"text\"].values[1]',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet retrieves the text of the second row in the training DataFrame where the 'target' column is equal to 1, providing another example of the data associated with a specific label.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.5788779}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'print(\"Number of duplicates in data : {}\".format(len(train_df[train_df.duplicated()])))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet calculates and prints the number of duplicate rows in the training DataFrame to identify the presence of redundant entries in the dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.8543922}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'print(\"Duplicated rows before remove them : \")\\ntrain_df[train_df.duplicated(keep=False)].sort_values(by=\"text\").head(8)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet prints and displays the first 8 duplicated rows (sorted by 'text') in the training DataFrame to examine the redundant entries before their removal.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.859677}},\n",
       "     {'cell_id': 12,\n",
       "      'code': '#remove duplicated rows\\ntrain_df.drop_duplicates(inplace=True)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet removes duplicated rows from the training DataFrame to clean the dataset by eliminating redundant entries.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'remove_duplicates',\n",
       "       'subclass_id': 19,\n",
       "       'predicted_subclass_probability': 0.8869491}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'print(\"Number of duplicates in data : {}\".format(len(train_df[train_df.duplicated()])))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet recalculates and prints the number of duplicate rows in the training DataFrame after removing duplicates to confirm the elimination of redundant entries.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.8543922}},\n",
       "     {'cell_id': 14,\n",
       "      'code': \"train_df['target'].value_counts()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet counts the occurrences of each unique value in the 'target' column of the training DataFrame to understand the distribution of the target variable.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9995184}},\n",
       "     {'cell_id': 15,\n",
       "      'code': '# count plot \"Histogram\" of Frequencies of Subjects for true news\\nplt.figure(figsize=(10,6))\\nplt.title(\"Frequencies of tweets for Disaster\")\\nsns.countplot(x = \\'target\\', data = train_df)\\nplt.xlabel(\\'Disaster Type\\')',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays a count plot to visualize the frequency distribution of disaster-related tweets in the training DataFrame using Seaborn and Matplotlib.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9293306}},\n",
       "     {'cell_id': 16,\n",
       "      'code': \"Real_Disaster_df = train_df[train_df['target'] == 1]\\nReal_Disaster_df.head()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet filters the training DataFrame to include only rows where the target indicates a real disaster and displays the first few rows of this filtered DataFrame to inspect its content.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.5334526}},\n",
       "     {'cell_id': 17,\n",
       "      'code': \"Not_Real_Disaster_df = train_df[train_df['target'] == 0]\\nNot_Real_Disaster_df.head()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet filters the training DataFrame to include only rows where the target indicates a non-disaster and displays the first few rows of this filtered DataFrame to inspect its content.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'filter',\n",
       "       'subclass_id': 14,\n",
       "       'predicted_subclass_probability': 0.5007087}},\n",
       "     {'cell_id': 18,\n",
       "      'code': \"Real_Disaster_text = ' '.join(Real_Disaster_df.text.tolist())\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet concatenates all text entries from the filtered DataFrame of real disaster tweets into a single string for further text analysis.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9610504}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'wordcloud_true = WordCloud().generate(Real_Disaster_text)\\nplt.figure(figsize=(10,10))\\nplt.imshow(wordcloud_true)\\nplt.axis(\\'off\\')\\nplt.title(\"Word Cloud of Real Disaster news\")\\nplt.tight_layout(pad=0)\\nplt.show()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates and displays a word cloud visualization of the most frequent words in real disaster tweets to highlight common terms in disaster-related content.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.96466434}},\n",
       "     {'cell_id': 20,\n",
       "      'code': \"Not_Real_Disaster_text = ' '.join(Not_Real_Disaster_df.text.tolist())\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet concatenates all text entries from the filtered DataFrame of non-disaster tweets into a single string for further text analysis.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9576982}},\n",
       "     {'cell_id': 21,\n",
       "      'code': 'wordcloud_true = WordCloud().generate(Not_Real_Disaster_text)\\nplt.figure(figsize=(10,10))\\nplt.imshow(wordcloud_true)\\nplt.axis(\\'off\\')\\nplt.title(\"Word Cloud of Not RealDisaster twittes\")\\nplt.tight_layout(pad=0)\\nplt.show()\\n',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates and displays a word cloud visualization of the most frequent words in non-disaster tweets to highlight common terms in content not related to disasters.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9845956}},\n",
       "     {'cell_id': 22,\n",
       "      'code': '# take text and preprocess \\'remove stopwords [a, the, and, thus, ... etc] and punctations[,%$ ..etc] and len of text less than 3\\' \\ndef clean_text(text):\\n    \"\"\"\\n        text: a string \\n        return: cleaned string\\n    \"\"\"\\n    result = []\\n    for token in simple_preprocess(text):\\n        if token not in STOPWORDS and token not in punctation and  len(token) >= 3 :\\n            token = token.lower() \\n            result.append(token)    \\n    return \" \".join(result)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function to preprocess text data by removing stopwords, punctuation, and short words for cleaning and standardizing the text entries.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9118299}},\n",
       "     {'cell_id': 23,\n",
       "      'code': \"train_df['text'] = train_df['text'].map(clean_text)\\ntrain_df.head()\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies the `clean_text` function to the 'text' column of the training DataFrame and displays the first few rows to show the cleaned text entries.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.99127215}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'from sklearn.utils import shuffle\\ntrain_df_shuffled = shuffle(train_df)\\ntrain_df_shuffled.head()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet shuffles the training DataFrame to randomize the order of the rows and displays the first few rows of the shuffled DataFrame for verification.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'normalization',\n",
       "       'subclass_id': 18,\n",
       "       'predicted_subclass_probability': 0.7203666}},\n",
       "     {'cell_id': 25,\n",
       "      'code': \"X = train_df_shuffled['text']\\ny = train_df_shuffled['target']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42, stratify = y)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the shuffled DataFrame into training and test sets for the features and target variable, ensuring that the proportion of classes is maintained using stratified sampling.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.995934}},\n",
       "     {'cell_id': 26,\n",
       "      'code': 'X_test',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet likely displays the test set features to inspect the data that will be used for evaluating the model's performance.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99974364}},\n",
       "     {'cell_id': 27,\n",
       "      'code': \"from sklearn.model_selection import cross_val_score\\nnb_classifier = Pipeline([('vect', CountVectorizer()),\\n               ('tfidf', TfidfTransformer()),\\n               ('clf', MultinomialNB()),])\\n\\nnb_classifier.fit(X_train, y_train)\\n\\ny_pred = nb_classifier.predict(X_test)\\nprint('accuracy {}'.format(accuracy_score(y_pred, y_test)))\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet creates a machine learning pipeline using a CountVectorizer, TfidfTransformer, and Multinomial Naive Bayes classifier, fits it to the training data, makes predictions on the test set, and prints the accuracy score.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.7887582}},\n",
       "     {'cell_id': 28,\n",
       "      'code': \"sgd = Pipeline([('vect', CountVectorizer()),\\n                ('tfidf', TfidfTransformer()),\\n                ('clf', SGDClassifier(loss='epsilon_insensitive', penalty='l2',alpha=1e-3, random_state=42, max_iter=1000, tol=None)),])\\n\\n\\nsgd.fit(X_train, y_train)\\ny_pred = sgd.predict(X_test)\\nprint('accuracy {}'.format(accuracy_score(y_pred, y_test)))\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet creates a machine learning pipeline using a CountVectorizer, TfidfTransformer, and a Stochastic Gradient Descent (SGD) classifier, fits it to the training data, makes predictions on the test set, and prints the accuracy score.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.95042425}},\n",
       "     {'cell_id': 29,\n",
       "      'code': \"test_df = test_df.drop(['id', 'keyword', 'location'], axis = 1)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet removes the 'id', 'keyword', and 'location' columns from the testing DataFrame to retain only the relevant features for prediction.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.99925584}},\n",
       "     {'cell_id': 30,\n",
       "      'code': \"test_df['text'] = test_df['text'].map(clean_text)\\ntest_df.head()\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies the `clean_text` function to the 'text' column of the testing DataFrame and displays the first few rows to show the cleaned text entries.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.99245125}},\n",
       "     {'cell_id': 31,\n",
       "      'code': \"y_pred = nb_classifier.predict(test_df['text'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet uses the trained Naive Bayes classifier to make predictions on the cleaned text data from the testing DataFrame.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.994578}},\n",
       "     {'cell_id': 32,\n",
       "      'code': 'sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the sample submission CSV file into a pandas DataFrame, which likely contains the format for submitting predictions.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99969256}},\n",
       "     {'cell_id': 33,\n",
       "      'code': 'sample_submission[\"target\"] = y_pred',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet assigns the predictions made by the Naive Bayes classifier to the \"target\" column of the sample submission DataFrame.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'prepare_output',\n",
       "       'subclass_id': 55,\n",
       "       'predicted_subclass_probability': 0.7656245}},\n",
       "     {'cell_id': 34,\n",
       "      'code': 'sample_submission.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the sample submission DataFrame to verify the inclusion and correctness of the predicted target values.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99975234}},\n",
       "     {'cell_id': 35,\n",
       "      'code': 'sample_submission.to_csv(\"submission.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet saves the sample submission DataFrame to a CSV file named \"submission.csv\" without including the DataFrame index to prepare for submission.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.999154}}],\n",
       "    'notebook_id': 8},\n",
       "   'notebook_id': 8},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import numpy as np\\nimport pandas as pd\\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import make_pipeline',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports necessary libraries for data manipulation, machine learning model development, and preprocessing.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.999337}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the training and testing datasets from CSV files into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99975425}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'count_vectorizer = feature_extraction.text.CountVectorizer()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet initializes a CountVectorizer object for converting a collection of text documents to a matrix of token counts.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.59570295}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\\n\\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet transforms the text data from the training set into a matrix of token counts and applies the same transformation to the test set.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.965019}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'clf = LinearSVC()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes a Linear Support Vector Classifier (LinearSVC) model.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_params',\n",
       "       'subclass_id': 2,\n",
       "       'predicted_subclass_probability': 0.2675037}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'clf.fit(train_vectors, train_df[\"target\"])',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the Linear Support Vector Classifier model using the token count matrix from the training data and the corresponding target labels.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.999706}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\\nscores',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates the Linear Support Vector Classifier model using 3-fold cross-validation and calculates the F1 score for each fold.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.967577}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'clf.fit(train_vectors, train_df[\"target\"])\\nsample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\\nsample_submission[\"target\"] = clf.predict(test_vectors)\\nsample_submission.head()\\nsample_submission.to_csv(\"submission.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet retrains the Linear Support Vector Classifier model, predicts the target labels for the test set, updates the sample submission DataFrame with these predictions, and saves the results to a CSV file.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9986327}}],\n",
       "    'notebook_id': 9},\n",
       "   'notebook_id': 9},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': \"import pandas as pd\\nimport platform\\nfrom sklearn.model_selection import train_test_split\\nimport nltk\\nimport string\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem import SnowballStemmer\\nfrom nltk.corpus import words\\n#nltk.download('punkt')\\nfrom sklearn.pipeline import Pipeline\\n#from sklearn.linear_model import LogisticRegression\\nimport sklearn.linear_model as lm\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics import precision_score, recall_score, precision_recall_curve, f1_score\\nfrom sklearn import metrics\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.metrics import plot_precision_recall_curve\\nimport numpy as np\\nfrom sklearn.model_selection import GridSearchCV\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports necessary libraries and modules for data manipulation, NLP processing, machine learning model building, evaluation, and plotting.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993006}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"pd.set_option('display.max_colwidth', None)\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code sets a pandas option to display the full contents of columns without truncation, facilitating easier inspection of data.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.99864024}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'df = pd.read_csv(\"../input/nlp-getting-started/train.csv\", sep=\",\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code reads a CSV file containing the training data into a pandas DataFrame.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9997379}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'df.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code outputs the dimensions (number of rows and columns) of the DataFrame.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9995491}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'df.head(20)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the first 20 rows of the DataFrame to give an initial glimpse of the data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997683}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'df.head(20)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the first 20 rows of the DataFrame to give an initial glimpse of the data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997683}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'df.tail(20)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the last 20 rows of the DataFrame to provide insights into the end portion of the data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99977094}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'df[\"target\"].value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code counts the occurrences of each unique value in the \"target\" column, providing information about the distribution of the target variable.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9994948}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'count_class_0, count_class_1 = df[\"target\"].value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code assigns the count of each unique value in the \"target\" column to the variables `count_class_0` and `count_class_1`, separating the counts for different classes.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.99944013}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'print(count_class_0, count_class_1)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code prints the counts of the two unique values in the \"target\" column to show the distribution of the classes.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.988794}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'class_ratio = count_class_0 / count_class_1',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code calculates the ratio of the count of class 0 to the count of class 1 in the \"target\" column, providing a measure of class imbalance.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9978563}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'print(\"{0:.3f}\".format(class_ratio))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code prints the class ratio with three decimal places of precision, giving a clearer view of the class distribution imbalance.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.98255676}},\n",
       "     {'cell_id': 12,\n",
       "      'code': 'df.head(20)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the first 20 rows of the DataFrame to give an initial glimpse of the data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997683}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'df.loc[df[\"target\"] == 1].head(10)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the first 10 rows of the DataFrame where the \"target\" column equals 1, giving insight into samples of the positive class.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99977666}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'df.loc[df[\"target\"] == 0].head(10)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the first 10 rows of the DataFrame where the \"target\" column equals 0, giving insight into samples of the negative class.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99977714}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'for c in df[df[\"target\"] == 1][\"text\"].head(10):\\n    print(c)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code prints the text content of the first 10 rows where the \"target\" column equals 1, helping to understand the characteristics of the positive class.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.999645}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'for c in df[df[\"target\"] == 0][\"text\"].head(10):\\n    print(c)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code prints the text content of the first 10 rows where the \"target\" column equals 0, helping to understand the characteristics of the negative class.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99963677}},\n",
       "     {'cell_id': 17,\n",
       "      'code': '# Since classes are imbalanced, we need to resample the dataframe\\n# First divide by class\\ndf_class_0 = df[df[\"target\"] == 0]\\ndf_class_1 = df[df[\"target\"] == 1]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code divides the DataFrame into two separate DataFrames based on the class label in the \"target\" column to handle class imbalance for resampling.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'filter',\n",
       "       'subclass_id': 14,\n",
       "       'predicted_subclass_probability': 0.45292664}},\n",
       "     {'cell_id': 18,\n",
       "      'code': 'df_class_0',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code outputs the DataFrame containing only the rows where the \"target\" column equals 0, isolating the negative class instances.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9996333}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'df_class_1',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code outputs the DataFrame containing only the rows where the \"target\" column equals 1, isolating the positive class instances.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99961746}},\n",
       "     {'cell_id': 20,\n",
       "      'code': '# Second resample - try both under- and over-sampling\\ndf_class_0_under = df_class_0.sample(count_class_1) # undersampling by loosing objects\\ndf_under = pd.concat([df_class_0_under, df_class_1], axis=0)\\n\\ndf_class_1_over = df_class_1.sample(count_class_0, replace=True) # oversampling by duplicaitng objects\\ndf_over = pd.concat([df_class_0, df_class_1_over], axis=0)\\n\\n#df = df_under\\n#df = df_over\\n\\n# Looks like oversampling works better since we use more objects - more training cases',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code performs both undersampling of the majority class and oversampling of the minority class to create two resampled DataFrames for handling class imbalance and concatenates the resampled data with the present majority and minority classes.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'concatenate',\n",
       "       'subclass_id': 11,\n",
       "       'predicted_subclass_probability': 0.7727316}},\n",
       "     {'cell_id': 21,\n",
       "      'code': 'df[\"target\"].value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code counts the occurrences of each unique value in the \"target\" column of the current DataFrame, providing information about the distribution of the target variable.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9994948}},\n",
       "     {'cell_id': 22,\n",
       "      'code': 'train_df, test_df = train_test_split(df, train_size=0.9)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code splits the DataFrame into training and testing sets with a 90/10 ratio, preparing the data for the machine learning model training and evaluation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.9968256}},\n",
       "     {'cell_id': 23,\n",
       "      'code': 'test_df.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code outputs the dimensions (number of rows and columns) of the test DataFrame, providing insight into the size of the test dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99960893}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'train_df[\"target\"].value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code counts the occurrences of each unique value in the \"target\" column of the training DataFrame, giving an overview of the distribution of the target variable in the training set.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.999521}},\n",
       "     {'cell_id': 25,\n",
       "      'code': 'test_df[\"target\"].value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code counts the occurrences of each unique value in the \"target\" column of the testing DataFrame, providing insights into the class distribution within the test set.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9995276}},\n",
       "     {'cell_id': 26,\n",
       "      'code': '#eng_words = words.words(\"en\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This commented-out code line appears intended to load a list of English words from the NLTK corpus, likely for use in text preprocessing or feature extraction.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.9975278}},\n",
       "     {'cell_id': 27,\n",
       "      'code': '#print(\"wort\" in eng_words)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This commented-out code line prints whether the word \"wort\" is in the list of English words (`eng_words`), which is useful for verifying the presence of specific words in the corpus for text preprocessing.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.9939898}},\n",
       "     {'cell_id': 28,\n",
       "      'code': 'snowball = SnowballStemmer(language=\"english\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code initializes a SnowballStemmer for the English language, setting up a stemming mechanism for reducing words to their root forms during text preprocessing.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9888525}},\n",
       "     {'cell_id': 29,\n",
       "      'code': 'def tokenize_sentence(sentence: str, remove_stop_words: bool = True):\\n    \\'\\'\\'Tokenize sentences with nltk dropping non-english words and punctuation and optionally stop words\\'\\'\\'\\n    tokens = word_tokenize(sentence, language=\"english\")\\n    #tokens = [i for i in tokens if i in eng_words and i not in string.punctuation]\\n    tokens = [i for i in tokens if i not in string.punctuation]\\n    if remove_stop_words:\\n        tokens = [i for i in tokens if i not in stopwords.words(\"english\")]\\n    tokens = [snowball.stem(i) for i in tokens]\\n    return tokens',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code defines a function that tokenizes a given sentence by removing punctuation, optionally stop words, and performing stemming on the tokens, preparing the text for further NLP processing.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.99213856}},\n",
       "     {'cell_id': 30,\n",
       "      'code': 'tokenize_sentence(\"the sentence and asdf fy krkr\", False)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code calls the `tokenize_sentence` function with a sample sentence and the option to keep stopwords, demonstrating how the function tokenizes the sentence while only applying stemming and punctuation removal.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9920244}},\n",
       "     {'cell_id': 31,\n",
       "      'code': 'vectorizer_params = {\\n    #\"max_features\": 500,\\n    #\"max_features\": None,\\n    #\"tokenizer\": lambda x: tokenize_sentence(x, remove_stop_words=False),\\n    #\"tokenizer\": None,\\n    #\"ngram_range\": (1, 100),\\n    #\"min_df\": 0,\\n    #\"max_df\": 100,\\n    #\"use_idf\": False,\\n    #\"decode_error\": \"replace\",\\n    #\"sublinear_tf\": True,\\n    #\"analyzer\": \"char\"\\n}',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code defines a dictionary of parameters intended for a TfidfVectorizer, though all parameters are currently commented out, likely for potential future customization of the vectorizer.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'init_hyperparams',\n",
       "       'subclass_id': 59,\n",
       "       'predicted_subclass_probability': 0.91740084}},\n",
       "     {'cell_id': 32,\n",
       "      'code': 'vectorizer = TfidfVectorizer(**vectorizer_params)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code initializes a TfidfVectorizer using the parameters provided in the `vectorizer_params` dictionary, setting up for text feature extraction.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9972772}},\n",
       "     {'cell_id': 33,\n",
       "      'code': 'vectorizer',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code outputs the TfidfVectorizer instance, which is configured for transforming text data into TF-IDF feature vectors.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9994728}},\n",
       "     {'cell_id': 34,\n",
       "      'code': 'features = vectorizer.fit_transform(train_df[\"text\"])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code applies the TfidfVectorizer to the training text data to fit the model and transform the text into a sparse matrix of TF-IDF features.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.99820554}},\n",
       "     {'cell_id': 35,\n",
       "      'code': 'print(features.shape)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code outputs the dimensions of the feature matrix obtained from the TfidfVectorizer, providing information on the number of samples and features.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9995484}},\n",
       "     {'cell_id': 36,\n",
       "      'code': 'feature_names = vectorizer.get_feature_names()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code retrieves the list of feature names generated by the TfidfVectorizer, which correspond to terms in the text corpus used for model training.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.54732066}},\n",
       "     {'cell_id': 37,\n",
       "      'code': 'print(\"Feature names (unique tokens): {0}.\\\\nFeature count: {1}\".format(feature_names, len(feature_names)))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code prints out the unique tokens (feature names) extracted by the TfidfVectorizer and the total count of these features, providing insights into the vocabulary size used for text representation.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_unique_values',\n",
       "       'subclass_id': 54,\n",
       "       'predicted_subclass_probability': 0.9350609}},\n",
       "     {'cell_id': 38,\n",
       "      'code': \"print('fire' in feature_names)\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code checks if the word 'fire' is present in the list of feature names created by the TfidfVectorizer, helping to confirm whether the term is included in the feature set.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9059723}},\n",
       "     {'cell_id': 39,\n",
       "      'code': 'X_train = train_df[\"text\"]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code assigns the \"text\" column of the training DataFrame to the variable `X_train`, isolating the input features for model training.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.9986425}},\n",
       "     {'cell_id': 40,\n",
       "      'code': 'X_train',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code outputs the \"text\" data from the training DataFrame, displaying the input features used for training the model.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99974626}},\n",
       "     {'cell_id': 41,\n",
       "      'code': 'y_train = train_df[\"target\"]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code assigns the target labels from the training DataFrame to the variable `y_train`, isolating the output labels for model training.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.9991716}},\n",
       "     {'cell_id': 42,\n",
       "      'code': 'y_train',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code outputs the target labels from the training DataFrame, displaying the class labels used for training the model.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99974114}},\n",
       "     {'cell_id': 43,\n",
       "      'code': 'lr_model_params = {\\n    #\"class_weight\": \"balanced\",\\n    #\"class_weight\": None,\\n    #\"class_weight\": {1: 1, 0: 1/class_ratio},\\n    #\"random_state\": 0,\\n    #\"Cs\": 5,\\n    #\"penalty\": \"none\",\\n    #\"penalty\": \"elasticnet\",\\n    \"solver\": \"liblinear\",\\n    #\"l1_ratio\": 0.5,\\n    #\"max_iter\": 10000,\\n    #\"cv\": 10\\n}',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code defines a dictionary of parameters intended for configuring a logistic regression model, with various parameter options being commented out for potential future adjustments.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'init_hyperparams',\n",
       "       'subclass_id': 59,\n",
       "       'predicted_subclass_probability': 0.98202676}},\n",
       "     {'cell_id': 44,\n",
       "      'code': 'model = lm.LogisticRegressionCV(**lr_model_params)\\n#features = features[:,-2000:]\\nmodel.fit(features, y_train)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code initializes a logistic regression model with cross-validation using the specified parameters and then fits the model to the training features and labels.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_on_grid',\n",
       "       'subclass_id': 6,\n",
       "       'predicted_subclass_probability': 0.86612976}},\n",
       "     {'cell_id': 45,\n",
       "      'code': 'model.n_features_in_',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code outputs the number of features that were used to fit the logistic regression model, providing insight into the dimensionality of the training data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_params',\n",
       "       'subclass_id': 2,\n",
       "       'predicted_subclass_probability': 0.2740468}},\n",
       "     {'cell_id': 46,\n",
       "      'code': 'text_n = 10\\nfeatures[text_n]',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code retrieves the TF-IDF feature vector corresponding to the 10th text sample in the training data, providing a view of the features for a specific instance.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9924711}},\n",
       "     {'cell_id': 47,\n",
       "      'code': 'test_model_y = model.predict(features[text_n])',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code uses the trained logistic regression model to predict the target label of the 10th text sample in the training data, evaluating the model's prediction for that instance.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.99349266}},\n",
       "     {'cell_id': 48,\n",
       "      'code': 'test_model_y[0]',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code outputs the predicted target label for the 10th text sample, allowing verification of the model's prediction for a specific instance.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9995427}},\n",
       "     {'cell_id': 49,\n",
       "      'code': 'train_df[\"text\"].iloc[text_n]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code retrieves and outputs the original text of the 10th sample in the training DataFrame, providing the actual input that was used to generate the prediction.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'filter',\n",
       "       'subclass_id': 14,\n",
       "       'predicted_subclass_probability': 0.9789242}},\n",
       "     {'cell_id': 50,\n",
       "      'code': 'model_pipeline = Pipeline([\\n    (\"vectorizer\", vectorizer),\\n    (\"model\", model)\\n]\\n)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code sets up a pipeline that first applies the TfidfVectorizer to transform the text data into feature vectors and then uses the logistic regression model for prediction, streamlining the preprocessing and prediction process.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.92996585}},\n",
       "     {'cell_id': 51,\n",
       "      'code': 'model_pipeline.fit(X_train, y_train)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code fits the pipeline, which includes both the TfidfVectorizer and the logistic regression model, to the training text and target data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9997104}},\n",
       "     {'cell_id': 52,\n",
       "      'code': 'model_pipeline.classes_',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code outputs the classes seen by the logistic regression model within the pipeline, providing information about the target labels the model was trained on.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_params',\n",
       "       'subclass_id': 2,\n",
       "       'predicted_subclass_probability': 0.4742122}},\n",
       "     {'cell_id': 53,\n",
       "      'code': 'len(model.coef_[0])',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code outputs the number of coefficients (features) in the logistic regression model, providing insight into the dimensionality of the feature space learned by the model.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9872142}},\n",
       "     {'cell_id': 54,\n",
       "      'code': 'model.C_',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code outputs the inverse of regularization strength for each class in the logistic regression model, indicating the regularization parameters tuned during model training.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.84522194}},\n",
       "     {'cell_id': 55,\n",
       "      'code': 'model_pipeline.named_steps',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code outputs a dictionary of named steps in the pipeline, showing the components (vectorizer and model) and their respective configurations within the pipeline.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.3446452}},\n",
       "     {'cell_id': 56,\n",
       "      'code': '#model_pipeline.predict([\"Attention: bush fire reported!\"])',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This commented-out code line appears intended to use the pipeline to predict the target label for a given text input, demonstrating a real-world application of the trained model.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.9979772}},\n",
       "     {'cell_id': 57,\n",
       "      'code': '#model_pipeline.predict([\"Kids were playing in the park.\"])',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This commented-out code line appears intended to use the pipeline to predict the target label for a benign, non-disaster-related text input, demonstrating the model's ability to distinguish between different types of messages.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.99806374}},\n",
       "     {'cell_id': 58,\n",
       "      'code': '#model_pipeline.get_params()',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This commented-out code line appears intended to output the configuration parameters of the entire pipeline, useful for reviewing and debugging the pipeline settings and their components.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.9974099}},\n",
       "     {'cell_id': 59,\n",
       "      'code': '#y_test = y_train\\ny_test = test_df[\"target\"]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code assigns the target labels from the test DataFrame to the variable `y_test`, preparing the ground truth labels for model evaluation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.99827564}},\n",
       "     {'cell_id': 60,\n",
       "      'code': '#y_pred = model_pipeline.predict(X_train)\\ny_pred = model_pipeline.predict(test_df[\"text\"])',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code uses the pipeline to predict the target labels for the test dataset's text and assigns the predictions to `y_pred`, setting up for evaluation of the model's performance on unseen data.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.9517882}},\n",
       "     {'cell_id': 61,\n",
       "      'code': '#print(precision_score(y_true=y_test, y_pred=y_pred))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This commented-out code line appears intended to print the precision score of the model's predictions on the test dataset, providing a measure of how many of the predicted positive instances are actually positive.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9712182}},\n",
       "     {'cell_id': 62,\n",
       "      'code': '#print(recall_score(y_true=y_test, y_pred=y_pred))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This commented-out code line appears intended to print the recall score of the model's predictions on the test dataset, providing a measure of the model's ability to identify all actual positive instances.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9629949}},\n",
       "     {'cell_id': 63,\n",
       "      'code': '#print(f1_score(y_true=y_test, y_pred=y_pred))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This commented-out code line appears intended to print the F1 score of the model's predictions on the test dataset, offering a balanced measure of precision and recall.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.91732484}},\n",
       "     {'cell_id': 64,\n",
       "      'code': 'print(metrics.classification_report(y_test, y_pred, labels = [1, 0], digits=5))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code prints a detailed classification report including precision, recall, and F1-score for each class in the test dataset, summarized with five decimal places for accuracy.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.8476077}},\n",
       "     {'cell_id': 65,\n",
       "      'code': 'f1_1 = metrics.classification_report(y_test, y_pred, output_dict=True)[\"1\"][\"f1-score\"]\\nf1_0 = metrics.classification_report(y_test, y_pred, output_dict=True)[\"0\"][\"f1-score\"]\\nprint(\"Mean f1 score: {0:.5f}\".format((f1_1 + f1_0)/2))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code calculates the F1-scores for both classes from the classification report, computes their mean, and prints the average F1-score with five decimal places of precision.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9809368}},\n",
       "     {'cell_id': 66,\n",
       "      'code': 'plot_precision_recall_curve(estimator=model_pipeline, X=test_df[\"text\"], y=y_test)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code plots the precision-recall curve for the pipeline's predictions on the test dataset, visualizing the trade-off between precision and recall at different threshold levels.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.6427428}},\n",
       "     {'cell_id': 67,\n",
       "      'code': 'model_pipeline.score(test_df[\"text\"], y_test)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code computes and outputs the accuracy score of the pipeline on the test dataset, indicating the proportion of correct predictions made by the model.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.99815553}},\n",
       "     {'cell_id': 68,\n",
       "      'code': 'pred_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\", sep=\",\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code reads a CSV file containing the prediction data into a pandas DataFrame, preparing it for further processing and prediction.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.999752}},\n",
       "     {'cell_id': 69,\n",
       "      'code': 'pred_df.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code outputs the dimensions (number of rows and columns) of the prediction DataFrame, providing insights into the size of the data to be used for prediction.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99961746}},\n",
       "     {'cell_id': 70,\n",
       "      'code': 'pred_df.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the first five rows of the prediction DataFrame to give an initial glimpse of the data to be used for making predictions.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997497}},\n",
       "     {'cell_id': 71,\n",
       "      'code': 'pred_df[\"target\"] = model_pipeline.predict(pred_df[\"text\"])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code uses the pipeline to predict target labels for the text in the prediction DataFrame and assigns these predictions to a new \"target\" column.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.9946057}},\n",
       "     {'cell_id': 72,\n",
       "      'code': 'pred_df.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code outputs the dimensions (number of rows and columns) of the prediction DataFrame, including the newly added \"target\" column with the predicted labels.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99961746}},\n",
       "     {'cell_id': 73,\n",
       "      'code': 'pred_df.head(20)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the first 20 rows of the prediction DataFrame, including the newly added \"target\" column with predicted labels.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997557}},\n",
       "     {'cell_id': 74,\n",
       "      'code': 'pred_df.tail(20)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the last 20 rows of the prediction DataFrame, including the newly added \"target\" column with predicted labels.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997602}},\n",
       "     {'cell_id': 75,\n",
       "      'code': 'pred_df.drop(columns=[\"keyword\", \"location\", \"text\"], inplace=True)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code removes the \"keyword\", \"location\", and \"text\" columns from the prediction DataFrame, retaining only relevant columns for the final output.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.99920505}},\n",
       "     {'cell_id': 76,\n",
       "      'code': 'pred_df.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code displays the first five rows of the modified prediction DataFrame, showing the cleaned-up structure with only the necessary columns retained.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997497}},\n",
       "     {'cell_id': 77,\n",
       "      'code': 'pred_df.to_csv(\"/kaggle/working/nlp_disaster_tweets_tfidf_lr_submission.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code exports the prediction DataFrame to a CSV file named \"nlp_disaster_tweets_tfidf_lr_submission.csv\" without including the index, preparing it for submission.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99896014}}],\n",
       "    'notebook_id': 10},\n",
       "   'notebook_id': 10},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nfrom tensorflow.keras.layers import Dense, Input\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import ModelCheckpoint\\nimport tensorflow_hub as hub\\n\\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\\nimport tokenization',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code imports necessary libraries including NumPy, pandas, TensorFlow, TensorFlow Hub, and a tokenization script for a machine learning task.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_url',\n",
       "       'subclass_id': 42,\n",
       "       'predicted_subclass_probability': 0.39663005}},\n",
       "     {'cell_id': 1,\n",
       "      'code': '#setting a seed for reproducability\\nSEED = 2718\\ndef seed_everything(seed):\\n    np.random.seed(seed)\\n    tf.random.set_seed(seed) \\n    \\nseed_everything(SEED) ',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code sets a seed for reproducibility by defining and calling a function that sets seeds for NumPy and TensorFlow operations.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.98266476}},\n",
       "     {'cell_id': 2,\n",
       "      'code': '#reading input data with pandas\\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\\n\\n#visualizing some of the tweets\\nfor i, val in enumerate(train.iloc[:2][\"text\"].to_list()):\\n    print(\"Tweet {}: {}\".format(i+1, val))',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'The code reads train, test, and sample submission CSV files into pandas DataFrames and prints the first two tweets from the training data.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99962354}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'def bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code defines a function to tokenize and encode a list of texts into BERT-compatible input formats, including tokens, masks, and segment embeddings, ensuring they meet a specified maximum length.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.80152774}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'def build_model(bert_layer, max_len=512):\\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\\n\\n    #could be pooled_output, sequence_output yet sequence output provides for each input token (in context)\\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\n    clf_output = sequence_output[:, 0, :]\\n    out = Dense(1, activation=\\'sigmoid\\')(clf_output)\\n    \\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\\n    \\n    #specifying optimizer\\n    model.compile(Adam(learning_rate=1e-5), loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n    \\n    return model',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code defines a function to build and compile a BERT-based model for binary classification, taking token IDs, input masks, and segment IDs as inputs, and producing a single sigmoid-activated output for classification.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.96481687}},\n",
       "     {'cell_id': 5,\n",
       "      'code': '#load uncased bert model\\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\\nbert_layer = hub.KerasLayer(module_url, trainable=True)',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code loads a pre-trained BERT model from TensorFlow Hub and makes it trainable for subsequent fine-tuning.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.8462454}},\n",
       "     {'cell_id': 6,\n",
       "      'code': '#vocab file from pre-trained BERT for tokenization\\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\n\\n#returns true/false depending on if we selected cased/uncased bert layer\\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\n\\n#Create the tokenizer\\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\\n\\n#tokenizing the training and testing data\\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\\ntrain_labels = train.target.values',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code initializes a tokenizer using the vocabulary file and case-sensitivity settings from the pre-trained BERT layer, and then tokenizes both the training and testing data, preparing them for model input.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.95965594}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'model = build_model(bert_layer, max_len=160)\\nmodel.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code builds the BERT-based model using the defined function and prints a summary of the model architecture.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9372223}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"checkpoint = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\\n\\ntrain_history = model.fit(\\n    train_input, train_labels,\\n    validation_split=0.1,\\n    epochs=3,\\n    callbacks=[checkpoint],\\n    batch_size=16\\n)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code trains the BERT-based model on the training data with a validation split, using a checkpoint callback to save the best model based on validation accuracy, and stores the training history.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9952106}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"test_pred = model.predict(test_input)\\n\\nsubmission['target'] = test_pred.round().astype(int)\\nsubmission.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"The code predicts labels for the test data using the trained model, assigns these predictions to the 'target' field of the submission DataFrame, and exports the result to a CSV file.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9992293}}],\n",
       "    'notebook_id': 11},\n",
       "   'notebook_id': 11},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '!pip install -q transformers ekphrasis keras-tuner',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code installs the `transformers`, `ekphrasis`, and `keras-tuner` libraries using pip.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.99613416}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'import numpy as np\\nimport pandas as pd\\nimport urllib\\nimport statistics\\nimport math\\nimport pprint\\nimport sklearn\\nfrom sklearn.linear_model import LogisticRegression\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\nfrom tensorflow.keras.layers import (\\n    Input,\\n    Dense,\\n    Embedding,\\n    Flatten,\\n    Dropout,\\n    GlobalMaxPooling1D,\\n    GRU,\\n    concatenate,\\n)\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nfrom transformers import (\\n    DistilBertTokenizerFast,\\n    TFDistilBertModel,\\n    DistilBertConfig,\\n)\\n\\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\\nfrom ekphrasis.classes.tokenizer import Tokenizer\\nfrom ekphrasis.dicts.emoticons import emoticons\\nfrom ekphrasis.dicts.noslang.slangdict import slangdict\\n\\nimport kerastuner',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports various essential libraries and modules, including NumPy, Pandas, Sklearn, TensorFlow, Keras, Transformers, and Ekphrasis, which are used for data processing, model creation, and natural language processing tasks.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993593}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'def print_metrics(model, x_train, y_train, x_val, y_val):\\n    train_acc = dict(model.evaluate(x_train, y_train, verbose=0, return_dict=True))[\\n        \"accuracy\"\\n    ]\\n    val_acc = dict(model.evaluate(x_val, y_val, verbose=0, return_dict=True))[\\n        \"accuracy\"\\n    ]\\n\\n    val_preds = model.predict(x_val)\\n    val_preds_bool = val_preds >= 0.5\\n\\n    print(\"\")\\n    print(f\"Training Accuracy:   {train_acc:.2%}\")\\n    print(f\"Validation Accuracy: {val_acc:.2%}\")\\n    print(\"\")\\n    print(f\"Validation f1 score: {sklearn.metrics.f1_score(val_preds_bool, y_val):.2%}\")',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This function evaluates a model's performance by calculating training and validation accuracy as well as the validation F1 score, then prints these metrics.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.5876385}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"# Using DistilBERT:\\nmodel_class, tokenizer_class, pretrained_weights = (TFDistilBertModel, DistilBertTokenizerFast, 'distilbert-base-uncased')\\n\\npretrained_bert_tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\\n\\ndef get_pretrained_bert_model(config=pretrained_weights):\\n    if not config:\\n        config = DistilBertConfig(num_labels=2)\\n\\n    return model_class.from_pretrained(pretrained_weights, config=config)\\n\\n\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This snippet sets up and initializes a pre-trained DistilBERT model and tokenizer for further use in a machine learning pipeline.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.9911644}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code reads the training and testing datasets from CSV files into Pandas DataFrames using the `read_csv` function.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99975425}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'print(train_df.info())\\n\\nprint(\"\")\\nprint(\"train rows:\", len(train_df.index))\\nprint(\"test rows:\", len(test_df.index))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code prints information about the training DataFrame, including data types and non-null counts, and then prints the number of rows in both the training and testing DataFrames.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9827071}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'print(\"label counts:\")\\ntrain_df.target.value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code prints the counts of each unique value in the 'target' column of the training DataFrame, providing insight into the distribution of class labels.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.99948514}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'print(\"train precentage of nulls:\")\\nprint(round(train_df.isnull().sum() / train_df.count() * 100, 2))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code calculates and prints the percentage of null values in each column of the training DataFrame.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.9960549}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'print(\"test precentage of nulls:\")\\nprint(round(test_df.isnull().sum() / test_df.count() * 100, 2))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code calculates and prints the percentage of null values in each column of the testing DataFrame.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.99783856}},\n",
       "     {'cell_id': 9,\n",
       "      'code': '# check that we don\\'t have any keywords appearing in one set and not the other\\ntrain_keywords = set(train_df[\"keyword\"].dropna())\\ntest_keywords = set(test_df[\"keyword\"].dropna())\\n\\nall_keywords = train_keywords.union(test_keywords)\\nunique_test_keywords = all_keywords - train_keywords\\nunique_train_keywords = all_keywords - test_keywords\\n\\nprint(f\"unique_test_keywords: {unique_test_keywords}\")\\nprint(f\"unique_train_keywords: {unique_train_keywords}\")',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet identifies keywords that are unique to either the training or testing datasets and prints these unique keywords, ensuring no keywords appear in only one dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_unique_values',\n",
       "       'subclass_id': 57,\n",
       "       'predicted_subclass_probability': 0.966651}},\n",
       "     {'cell_id': 10,\n",
       "      'code': '# We\\'ll use these weights later on to make up for the slightly imbalanced dataset\\nclasses = np.unique(train_df[\"target\"])\\nclass_weights = sklearn.utils.class_weight.compute_class_weight(\\n    \"balanced\", classes=classes, y=train_df[\"target\"]\\n)\\n\\nclass_weights = {clazz : weight for clazz, weight in zip(classes, class_weights)}',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code calculates the class weights for the training dataset, which are used later to handle class imbalance during model training by assigning higher weights to minority classes.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.3043228}},\n",
       "     {'cell_id': 11,\n",
       "      'code': '# Commented out the graceful handling of duplicated because the Kaggle kernel version of statistics.mode()\\n# won\\'t handle multimodal results\\n\\n# Duplicates aren\\'t consistently labeled, so we keep one example of the most frequently occuring label\\n# train_df[\"duplicated\"] = train_df.duplicated(subset=\"text\")\\n# duplicated_tweets = train_df.loc[lambda df: df[\"duplicated\"] == True, :]\\n# aggregated_duplicates = duplicated_tweets.groupby(\"text\", as_index=False).aggregate(\\n#     statistics.mode\\n# )\\n\\n# train_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\\n# train_df = train_df.append(aggregated_duplicates, ignore_index=True)\\n\\ntrain_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\\nprint(\"train rows:\", len(train_df.index))\\nprint(\"test rows:\", len(test_df.index))',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet removes duplicate rows based on the 'text' column from the training DataFrame and prints the resulting number of rows in both the training and testing DataFrames.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.7884562}},\n",
       "     {'cell_id': 12,\n",
       "      'code': 'class TweetPreProcessor:\\n    \"\"\"\\n    This class does some cleaning and normalization prior to BPE tokenization\\n    \"\"\"\\n\\n    def __init__(self):\\n\\n        self.text_processor = TextPreProcessor(\\n            # terms that will be normalized\\n            normalize=[\\n                \"url\",\\n                \"email\",\\n                \"phone\",\\n                \"user\",\\n                \"time\",\\n                \"date\",\\n            ],\\n            # terms that will be annotated\\n            annotate={\"repeated\", \"elongated\"},\\n            # corpus from which the word statistics are going to be used\\n            # for word segmentation\\n            segmenter=\"twitter\",\\n            # corpus from which the word statistics are going to be used\\n            # for spell correction\\n            spell_correction=True,\\n            corrector=\"twitter\",\\n            unpack_hashtags=False,  # perform word segmentation on hashtags\\n            unpack_contractions=False,  # Unpack contractions (can\\'t -> can not)\\n            spell_correct_elong=True,  # spell correction for elongated words\\n            fix_bad_unicode=True,\\n            tokenizer=Tokenizer(lowercase=True).tokenize,\\n            # list of dictionaries, for replacing tokens extracted from the text,\\n            # with other expressions. You can pass more than one dictionaries.\\n            dicts=[emoticons, slangdict],\\n        )\\n\\n    def preprocess_tweet(self, tweet):\\n        return \" \".join(self.text_processor.pre_process_doc(tweet))\\n    \\n    # this will return the tokenized text     \\n    def __call__(self, tweet):\\n        return self.text_processor.pre_process_doc(tweet)\\n    \\ntweet_preprocessor = TweetPreProcessor()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code defines a `TweetPreProcessor` class for cleaning and normalizing tweets using the Ekphrasis library, including normalization, annotation, segmentation, spell correction, and tokenization, and then instantiates an object of this class.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.95362186}},\n",
       "     {'cell_id': 13,\n",
       "      'code': '# Have a look at how the TweetProcessor is doing\\nfor tweet in train_df[100:120][\"text\"]:\\n    print(\"original:  \", tweet)\\n    print(\"processed: \", tweet_preprocessor.preprocess_tweet(tweet))\\n    print(\"\")',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet prints the original and processed versions of a sample of tweets from the training DataFrame to inspect the TweetPreProcessor's cleaning and normalization results.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.7224992}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'train_df[\"text\"] = train_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)\\ntest_df[\"text\"] = test_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code applies the `TweetPreProcessor` to clean and normalize the text in both the training and testing DataFrames by updating their 'text' columns.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.72112745}},\n",
       "     {'cell_id': 15,\n",
       "      'code': '# Fill NA\\ntrain_df[\"keyword\"].fillna(\"\", inplace=True)\\ntest_df[\"keyword\"].fillna(\"\", inplace=True)\\n\\n# remove %20 from keywords\\ntrain_df[\"keyword\"] = train_df[\"keyword\"].apply(urllib.parse.unquote)\\ntest_df[\"keyword\"] = test_df[\"keyword\"].apply(urllib.parse.unquote)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code fills missing values in the 'keyword' column with empty strings and removes URL-encoded characters (e.g., `%20`) from the 'keyword' column in both the training and testing DataFrames.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.98655176}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\\n    train_df[[\"text\", \"keyword\"]], train_df[\"target\"], test_size=0.3, random_state=42, stratify=train_df[\"target\"]\\n)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code splits the training DataFrame into training and validation sets based on the 'text' and 'keyword' columns and the 'target' column, ensuring a stratified split with a 70-30 ratio.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.9980861}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'def tokenize_encode(tweets, max_length=None):\\n    return pretrained_bert_tokenizer(\\n        tweets,\\n        add_special_tokens=True,\\n        truncation=True,\\n        padding=\"max_length\",\\n        max_length=max_length,\\n        return_tensors=\"tf\",\\n    )\\n\\n\\n# need to be explicit about the lengths (instead of just specifying padding=True in the tokenizer)\\n# otherwise train tweets end up being 71 and validation tweets end up as 70, which causes problems/warnings\\nmax_length_tweet = 72\\nmax_length_keyword = 8\\n\\ntrain_tweets_encoded = tokenize_encode(x_train[\"text\"].to_list(), max_length_tweet) \\nvalidation_tweets_encoded = tokenize_encode(x_val[\"text\"].to_list(), max_length_tweet) \\n\\ntrain_keywords_encoded = tokenize_encode(x_train[\"keyword\"].to_list(), max_length_keyword) \\nvalidation_keywords_encoded = tokenize_encode(x_val[\"keyword\"].to_list(), max_length_keyword) \\n\\ntrain_inputs_encoded = dict(train_tweets_encoded)\\ntrain_inputs_encoded[\"keywords\"] = train_keywords_encoded[\"input_ids\"]\\n\\nvalidation_inputs_encoded = dict(validation_tweets_encoded)\\nvalidation_inputs_encoded[\"keywords\"] = validation_keywords_encoded[\"input_ids\"]\\n',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code tokenizes and encodes the text and keyword data from the training and validation sets using the pre-trained BERT tokenizer, specifying maximum lengths and padding to ensure consistent input sizes.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.8425683}},\n",
       "     {'cell_id': 18,\n",
       "      'code': 'train_dataset = tf.data.Dataset.from_tensor_slices(\\n    (dict(train_tweets_encoded), y_train)\\n)\\n\\nval_dataset = tf.data.Dataset.from_tensor_slices(\\n    (dict(validation_tweets_encoded), y_val)\\n)\\n\\ntrain_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\\n    (train_inputs_encoded, y_train)\\n)\\n\\nval_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\\n    (validation_inputs_encoded, y_val)\\n)\\n',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code creates TensorFlow datasets from the encoded training and validation data, including datasets for both single-input (tweets only) and multi-input (tweets and keywords) configurations.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.5969537}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\\n    tokenizer=tweet_preprocessor, min_df=1, ngram_range=(1, 1), norm=\"l2\"\\n)\\n\\ntrain_vectors = tfidf_vectorizer.fit_transform(raw_documents=x_train[\"text\"]).toarray()\\nvalidation_vectors = tfidf_vectorizer.transform(x_val[\"text\"]).toarray()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code initializes a TF-IDF vectorizer with the TweetPreProcessor as the tokenizer, then fits and transforms the text data from the training set and transforms the text data from the validation set into TF-IDF vectors.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9926306}},\n",
       "     {'cell_id': 20,\n",
       "      'code': '# I obtained the value of C by experimenting with LogisticRegressionCV but I\\'m leaving it out for brevity\\nlogisticRegressionClf = LogisticRegression(n_jobs=-1, C=2.78)\\nlogisticRegressionClf.fit(train_vectors, y_train)\\n\\ndef print_metrics_sk(clf, x_train, y_train, x_val, y_val):\\n    print(f\"Train Accuracy:         {clf.score(x_train, y_train):.2%}\")\\n    print(f\"Validation Accuracy:    {clf.score(x_val, y_val):.2%}\")\\n    print(\"\")\\n    print(f\"f1 score:               {sklearn.metrics.f1_score(y_val, clf.predict(x_val)):.2%}\")\\n\\nprint_metrics_sk(logisticRegressionClf, train_vectors, y_train, validation_vectors, y_val)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code initializes, trains a logistic regression model using the TF-IDF vectors of the training data, and then evaluates its training and validation accuracy as well as the validation F1 score, which are printed.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9330293}},\n",
       "     {'cell_id': 21,\n",
       "      'code': 'feature_extractor = get_pretrained_bert_model()\\n\\n# Run a forward pass on the tokenized inputs\\n# model_outputs = feature_extractor(\\n#     train_tweets_encoded[\"input_ids\"], train_tweets_encoded[\"attention_mask\"]\\n# )\\nmodel_outputs = feature_extractor.predict(\\n    train_dataset.batch(32)\\n)\\n# BERT\\'s sentence representation can be retrieved from a hidden vector at index 0 in the sequence, \\n# (where the special token CLS was prepended by the tokenizer)\\ntrain_sentence_vectors = model_outputs.last_hidden_state[:, 0, :]\\n\\n# The rest of the sequence contains the embeddings \\n# (modified by successive layers of self-attention) for each token\\ntrain_word_vectors = model_outputs.last_hidden_state[:, 1:, :]\\n\\n# And the same again for the validation set\\n# model_outputs = feature_extractor(\\n#     validation_tweets_encoded[\"input_ids\"], validation_tweets_encoded[\"attention_mask\"]\\n# )\\nmodel_outputs = feature_extractor.predict(\\n    val_dataset.batch(32)\\n)\\nvalidation_sentence_vectors = model_outputs.last_hidden_state[:, 0, :]\\nvalidation_word_vectors = model_outputs.last_hidden_state[:, 1:, :]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code uses a pre-trained BERT model to generate sentence and word vectors for the training and validation datasets by running a forward pass on the tokenized inputs and extracting the relevant hidden states.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_model_class',\n",
       "       'subclass_id': 3,\n",
       "       'predicted_subclass_probability': 0.2968278}},\n",
       "     {'cell_id': 22,\n",
       "      'code': 'logisticRegressionClf = LogisticRegression(n_jobs=-1, class_weight=class_weights)\\nlogisticRegressionClf.fit(train_sentence_vectors, y_train)\\n\\nprint_metrics_sk(\\n    logisticRegressionClf,\\n    train_sentence_vectors,\\n    y_train,\\n    validation_sentence_vectors,\\n    y_val,\\n)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code initializes and trains a logistic regression model using the BERT-derived sentence vectors of the training data while accounting for class weights, and then evaluates and prints metrics on both the training and validation sets.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99934644}},\n",
       "     {'cell_id': 23,\n",
       "      'code': 'def create_gru_model() -> keras.Model:\\n\\n    model = keras.Sequential()\\n    model.add(keras.layers.InputLayer(input_shape=train_word_vectors.shape[1:]))\\n    model.add(GRU(32, return_sequences=True))\\n    model.add(GlobalMaxPooling1D())\\n    model.add(Dense(1, activation=\"sigmoid\"))\\n\\n    model.compile(\\n        optimizer=keras.optimizers.Adam(),\\n        loss=\"binary_crossentropy\",\\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\\n    )\\n    return model\\n\\nmodel = create_gru_model()\\n\\nhistory = model.fit(\\n    train_word_vectors,\\n    y_train,\\n    validation_data=(validation_word_vectors, y_val),\\n    class_weight=class_weights,\\n    epochs=20,\\n    verbose=0,\\n    callbacks=[\\n        EarlyStopping(\\n            monitor=\"val_accuracy\",\\n            min_delta=0.001,\\n            patience=5,\\n            restore_best_weights=True,\\n        )\\n    ],\\n)\\n\\nprint_metrics(model, train_word_vectors, y_train, validation_word_vectors, y_val)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This snippet defines, compiles, and trains a GRU-based neural network model on the BERT-derived word vectors from the training data, incorporating class weights and early stopping based on validation accuracy, and then evaluates and prints the model's performance.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_on_grid',\n",
       "       'subclass_id': 6,\n",
       "       'predicted_subclass_probability': 0.509667}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'def create_multi_input_model() -> keras.Model:\\n\\n    keyword_ids = keras.Input((8,), name=\"keywords\")\\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\\n    keyword_features = Flatten()(keyword_features)\\n    keyword_features = Dense(1)(keyword_features)\\n\\n    tweet_classification_vectors = keras.Input((train_sentence_vectors.shape[1],), name=\"tweets\")\\n    tweet_features = Dense(1, activation=\\'relu\\')(tweet_classification_vectors)    \\n\\n    combined_features = concatenate([keyword_features, tweet_features])\\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\\n\\n    model = keras.Model(inputs = [keyword_ids, tweet_classification_vectors], outputs=combined_prediction)\\n\\n    model.compile(\\n        optimizer=keras.optimizers.Adam(),\\n        loss=\"binary_crossentropy\",\\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\\n    )\\n    return model\\n\\n\\nmodel = create_multi_input_model()\\n\\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_sentence_vectors}\\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_sentence_vectors}\\n\\nhistory = model.fit(\\n    train_inputs,\\n    y_train,\\n    validation_data=(validation_inputs, y_val),\\n    class_weight=class_weights,\\n    epochs=20,\\n    verbose=0,\\n    callbacks=[\\n        EarlyStopping(\\n            monitor=\"val_accuracy\",\\n            min_delta=0.001,\\n            patience=5,\\n            restore_best_weights=True,\\n        )\\n    ],\\n)\\n\\n\\nprint_metrics(model, train_inputs, y_train, validation_inputs, y_val)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This snippet defines, compiles, and trains a multi-input neural network model that combines keyword embeddings with BERT-derived sentence vectors from tweets, applies early stopping based on validation accuracy, and then evaluates and prints the model's performance.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9820961}},\n",
       "     {'cell_id': 25,\n",
       "      'code': 'def create_multi_input_rnn_model() -> keras.Model:\\n\\n    keyword_ids = keras.Input((8,), name=\"keywords\")\\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\\n    keyword_features = Flatten()(keyword_features)\\n    keyword_features = Dense(1)(keyword_features)\\n\\n    tweet_token_embeddings = Input(train_word_vectors.shape[1:], name=\"tweets\")\\n    tweet_features = GRU(32, return_sequences=True)(tweet_token_embeddings)\\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\\n    tweet_features = Dense(1, activation=\\'relu\\')(tweet_features)    \\n\\n    combined_features = concatenate([keyword_features, tweet_features])\\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\\n\\n    model = keras.Model(inputs = [keyword_ids, tweet_token_embeddings], outputs=combined_prediction)\\n\\n    model.compile(\\n        optimizer=keras.optimizers.Adam(),\\n        loss=\"binary_crossentropy\",\\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\\n    )\\n    return model\\n\\n\\nmodel = create_multi_input_rnn_model()\\n\\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_word_vectors}\\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_word_vectors}\\n\\nhistory = model.fit(\\n    train_inputs,\\n    y_train,\\n    validation_data=(validation_inputs, y_val),\\n    class_weight=class_weights,\\n    epochs=20,\\n    verbose=0,\\n    callbacks=[\\n        EarlyStopping(\\n            monitor=\"val_accuracy\",\\n            min_delta=0.001,\\n            patience=5,\\n            restore_best_weights=True,\\n        )\\n    ],\\n)\\n\\nprint_metrics(model, train_inputs, y_train, validation_inputs, y_val)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code defines, compiles, and trains a multi-input RNN model that processes both keyword embeddings and BERT-derived word vectors from tweets, applying early stopping based on validation accuracy, and then evaluates and prints the model's performance.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.8343507}},\n",
       "     {'cell_id': 26,\n",
       "      'code': 'def create_candidate_model_with_fx(hp: kerastuner.HyperParameters) -> keras.Model:\\n\\n    keyword_ids = keras.Input((8,), name=\"keywords\")\\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\\n    keyword_features = Flatten()(keyword_features)\\n    keyword_features = Dense(hp.Choice(\"keyword_units\", values=[1, 8, 16, 32], default=1))(keyword_features)\\n\\n    tweet_token_embeddings = Input(train_word_vectors.shape[1:], name=\"tweets\")\\n    \\n    tweet_features = GRU(hp.Choice(\"GRU_units\", values=[8, 16, 32, 64, 128], default=32), return_sequences=True)(tweet_token_embeddings)\\n    tweet_features = Dropout(hp.Float(\"GRU_dropout\", min_value=0.0, max_value=0.5, step=0.1))(tweet_features)\\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\\n    \\n    for i in range(hp.Int(\"num_layers\", min_value=0, max_value=3, step=1)):\\n        tweet_features = Dense(hp.Choice(\"layer_\" + str(i) + \"_units\", values=[2, 8, 16, 32, 64, 128, 256]), activation=\"relu\")(tweet_features)\\n        tweet_features = Dropout(hp.Float(\"layer_\" + str(i) + \"_dropout\", min_value=0.0, max_value=0.5, step=0.1))(tweet_features)\\n    \\n    combined_features = concatenate([keyword_features, tweet_features])\\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\\n\\n    model = keras.Model(inputs = [keyword_ids, tweet_token_embeddings], outputs=combined_prediction)\\n\\n    model.compile(\\n        optimizer=keras.optimizers.Adam(),\\n        loss=\"binary_crossentropy\",\\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\\n    )\\n    return model\\n\\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_word_vectors}\\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_word_vectors}\\n',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code defines a function to create a candidate RNN model using Keras Tuner for hyperparameter optimization, incorporating features from both keyword embeddings and tweet word vectors, while setting up the model's training and validation inputs.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.37123346}},\n",
       "     {'cell_id': 27,\n",
       "      'code': '# Hyperband Tuning\\nMAX_EPOCHS = 10\\nFACTOR = 3\\nITERATIONS = 3\\n\\nprint(f\"Number of models in each bracket: {math.ceil(1 + math.log(MAX_EPOCHS, FACTOR))}\")\\nprint(f\"Number of epochs over all trials: {round(ITERATIONS * (MAX_EPOCHS * (math.log(MAX_EPOCHS, FACTOR) ** 2)))}\")',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code calculates and prints the number of models in each bracket and the total number of epochs over all trials for Hyperband tuning based on specified parameters (`MAX_EPOCHS`, `FACTOR`, `ITERATIONS`).',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.5920417}},\n",
       "     {'cell_id': 28,\n",
       "      'code': 'tuner = kerastuner.Hyperband(\\n    create_candidate_model_with_fx,\\n    max_epochs=MAX_EPOCHS,\\n    hyperband_iterations=ITERATIONS, \\n    factor=FACTOR, \\n    objective=\"val_accuracy\",\\n    directory=\"hyperparam-search\",\\n    project_name=\"architecture-hyperband\",\\n)\\n\\ntuner.search(\\n    train_inputs,\\n    y_train,\\n    validation_data=(validation_inputs, y_val),\\n    class_weight=class_weights,\\n    epochs=10,\\n    verbose=1,\\n    callbacks=[\\n        EarlyStopping(\\n            monitor=\"val_accuracy\",\\n            min_delta=0.001,\\n            patience=3,\\n            restore_best_weights=True,\\n        )\\n    ],\\n)\\n',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code initializes a Hyperband tuner to search for the best hyperparameters for the candidate RNN model, and then performs the search by training multiple model configurations on the training data while using early stopping based on validation accuracy.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_on_grid',\n",
       "       'subclass_id': 6,\n",
       "       'predicted_subclass_probability': 0.7966217}},\n",
       "     {'cell_id': 29,\n",
       "      'code': '# tuner.results_summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code (currently commented out) would generate and display a summary of the results from the Hyperband tuning process, including the best hyperparameters and performance metrics.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.9971831}},\n",
       "     {'cell_id': 30,\n",
       "      'code': 'best_model = tuner.get_best_models()[0]\\n# best_model.summary()\\nprint(\"\")\\nbest_arch_hp = tuner.get_best_hyperparameters()[0]\\npprint.pprint(best_arch_hp.values, indent=4)\\nprint(\"\")\\n\\nprint_metrics(best_model, train_inputs, y_train, validation_inputs, y_val)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code retrieves and prints the best hyperparameters from the Hyperband search, evaluates the corresponding best model on the training and validation data using predefined metrics, and optionally prints the model summary.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.43187767}},\n",
       "     {'cell_id': 31,\n",
       "      'code': '# To create a baseline for the simplest possible fine-tuned BERT\\ndef create_bert_simple_for_ft():\\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\\n\\n    pretrained_bert_model = get_pretrained_bert_model()\\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\\n\\n    prediction = Dense(1, activation=\"sigmoid\")(bert_outputs.last_hidden_state[:, 0, :])\\n    return keras.Model(inputs=[input_ids, attention_mask], outputs=prediction)\\n\\nmodel = create_bert_simple_for_ft()\\n\\nmodel.compile(\\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\\n    loss=\"binary_crossentropy\",\\n    metrics=[\"accuracy\"],\\n)\\n\\nmodel.fit(\\n    train_dataset.batch(32),\\n    validation_data=val_dataset.batch(32),\\n    class_weight=class_weights,\\n    epochs=20,\\n    callbacks=[\\n        EarlyStopping(\\n            monitor=\"val_accuracy\",\\n            min_delta=0.001,\\n            patience=5,\\n            restore_best_weights=True,\\n        )\\n    ],\\n)\\n\\nprint_metrics(\\n    model, dict(train_tweets_encoded), y_train, dict(validation_tweets_encoded), y_val\\n)\\n',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code defines, compiles, and trains a simple fine-tuned BERT model on the tokenized tweet data, applying early stopping based on validation accuracy, and then evaluates and prints the model's performance.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.98817086}},\n",
       "     {'cell_id': 32,\n",
       "      'code': 'def create_bert_rnn_for_ft():\\n    \\n    pretrained_bert_model = get_pretrained_bert_model()\\n    \\n    keyword_ids = keras.Input((8,), name=\"keywords\")\\n    keyword_features = Embedding(input_dim=pretrained_bert_model.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\\n    keyword_features = Flatten()(keyword_features)\\n    keyword_features = Dense(1)(keyword_features)\\n\\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\\n\\n    bert_token_embeddings = bert_outputs.last_hidden_state[:, 1:, :]\\n    tweet_features = GRU(32, return_sequences=True)(bert_token_embeddings)\\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\\n\\n    combined_features = concatenate([keyword_features, tweet_features])\\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\\n\\n    model = keras.Model(inputs = [keyword_ids, input_ids, attention_mask], outputs=combined_prediction)\\n\\n    model.compile(\\n        optimizer=keras.optimizers.Adam(learning_rate=5e-5),\\n        loss=\"binary_crossentropy\",\\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\\n    )\\n    return model\\n\\nmodel = create_bert_rnn_for_ft()\\n\\nmodel.fit(\\n    train_multi_input_dataset.batch(32),\\n    validation_data=val_multi_input_dataset.batch(32),\\n    epochs=20,\\n    class_weight=class_weights,\\n    callbacks=[\\n        EarlyStopping(\\n            monitor=\"val_accuracy\",\\n            min_delta=0.001,\\n            patience=3,\\n            restore_best_weights=True,\\n        )\\n    ],\\n)\\n\\nprint_metrics(\\n    model, train_inputs_encoded, y_train, validation_inputs_encoded, y_val\\n)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code defines, compiles, and trains a BERT and RNN combined model that processes both keyword embeddings and BERT-derived features from tweets, with early stopping based on validation accuracy, and then evaluates and prints the model's performance.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.7985944}},\n",
       "     {'cell_id': 33,\n",
       "      'code': 'def create_model_candidate() -> keras.Model:\\n    pretrained_bert_model = get_pretrained_bert_model()\\n\\n    keyword_ids = keras.Input((8,), name=\"keywords\")\\n    keyword_features = Embedding(input_dim=pretrained_bert_model.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\\n    keyword_features = Flatten()(keyword_features)\\n    keyword_features = Dense(best_arch_hp.get(\"keyword_units\"))(keyword_features)\\n\\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\\n    bert_token_embeddings = bert_outputs.last_hidden_state[:, 1:, :]\\n    tweet_features = GRU(best_arch_hp.get(\"GRU_units\"), return_sequences=True)(bert_token_embeddings)\\n    tweet_features = Dropout(best_arch_hp.get(\"GRU_dropout\"))(tweet_features)\\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\\n    \\n    for i in range(best_arch_hp.get(\"num_layers\")):\\n        tweet_features = Dense(best_arch_hp.get(\"layer_\" + str(i) + \"_units\"), activation=\"relu\")(tweet_features)\\n        tweet_features = Dropout(best_arch_hp.get(\"layer_\" + str(i) + \"_dropout\"))(tweet_features)\\n    \\n    combined_features = concatenate([keyword_features, tweet_features])\\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\\n\\n    model = keras.Model(inputs = [keyword_ids, input_ids, attention_mask], outputs=combined_prediction)\\n\\n    model.compile(\\n        optimizer=keras.optimizers.Adam(learning_rate=5e-5),\\n        loss=\"binary_crossentropy\",\\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\\n    )\\n    return model\\n',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code defines a function to create a model candidate using the best hyperparameters obtained from the hyperparameter tuning results, integrating both keyword embeddings and BERT-derived features for tweet classification.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.90136576}},\n",
       "     {'cell_id': 34,\n",
       "      'code': 'model = create_model_candidate()\\n\\nhistory = model.fit(\\n    train_multi_input_dataset.batch(32),\\n    validation_data=val_multi_input_dataset.batch(32),\\n    epochs=6,\\n    class_weight=class_weights,\\n    callbacks=[\\n        keras.callbacks.EarlyStopping(\\n            monitor=\"val_accuracy\", restore_best_weights=True\\n        )\\n    ],\\n)\\n\\nbest_epoch = len(history.history[\"val_accuracy\"]) - 1\\n\\nprint_metrics(\\n    model, train_inputs_encoded, y_train, validation_inputs_encoded, y_val\\n)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code trains the model candidate created using the best hyperparameters on the multi-input dataset, applies early stopping based on validation accuracy, determines the best epoch and evaluates and prints the model's performance.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99578863}},\n",
       "     {'cell_id': 35,\n",
       "      'code': 'test_tweets_encoded = tokenize_encode(test_df[\"text\"].to_list(), max_length_tweet)\\ntest_inputs_encoded = dict(test_tweets_encoded)\\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)\\n\\ntest_keywords_encoded = tokenize_encode(test_df[\"keyword\"].to_list(), max_length_keyword)\\ntest_inputs_encoded[\"keywords\"] = test_keywords_encoded[\"input_ids\"]\\ntest_multi_input_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code tokenizes and encodes the text and keyword data from the testing dataset using the same BERT tokenizer, creating both single-input and multi-input TensorFlow datasets for the test data.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.78393567}},\n",
       "     {'cell_id': 36,\n",
       "      'code': 'full_train_dataset = train_multi_input_dataset.concatenate(val_multi_input_dataset)\\nmodel = create_model_candidate()\\n\\nmodel.fit(\\n    full_train_dataset.batch(32),\\n    epochs=best_epoch,\\n    class_weight=class_weights,\\n)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code concatenates the training and validation datasets into a full training dataset, reinitializes the model candidate, and trains it on the full dataset for the number of epochs determined as optimal in the previous training phase.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9959798}},\n",
       "     {'cell_id': 37,\n",
       "      'code': 'preds = np.squeeze(model.predict(test_multi_input_dataset.batch(32)))\\npreds = (preds >= 0.5).astype(int)\\npd.DataFrame({\"id\": test_df.id, \"target\": preds}).to_csv(\"submission.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code generates predictions on the test dataset using the trained model, converts these predictions to binary class labels, and saves the results in a CSV file named \"submission.csv\" for submission.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9992853}}],\n",
       "    'notebook_id': 12},\n",
       "   'notebook_id': 12},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import numpy as np\\nimport pandas as pd\\nfrom fastai.text.all import *\\nimport re',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': \"This code imports the necessary libraries, including numpy, pandas, fastai's text module, and the re module for regular expressions.\",\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99934}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'dir_path = \"/kaggle/input/nlp-getting-started/\"\\ntrain_df = pd.read_csv(dir_path + \"train.csv\")\\ntest_df = pd.read_csv(dir_path + \"test.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code sets the directory path for the dataset and loads the training and testing data from CSV files into Pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99974483}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'train_df',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code outputs the contents of the training DataFrame to display its data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99972683}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'train_df = train_df.drop(columns=[\"id\", \"keyword\", \"location\"])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code removes the columns \"id\", \"keyword\", and \"location\" from the training DataFrame.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.99919885}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'train_df[\"target\"].value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code counts the occurrences of each unique value in the \"target\" column of the training DataFrame.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.999521}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'def remove_URL(text):\\n    url = re.compile(r\\'https?://\\\\S+|www\\\\.\\\\S+\\')\\n    return url.sub(r\\'\\',text)\\n\\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_URL)\\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_URL)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code defines a function to remove URLs from text using regular expressions and applies this function to the \"text\" column of both the training and testing DataFrames.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9983814}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'def remove_html(text):\\n    html=re.compile(r\\'<.*?>\\')\\n    return html.sub(r\\'\\',text)\\n\\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_html)\\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_html)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code defines a function to remove HTML tags from text using regular expressions and applies this function to the \"text\" column of both the training and testing DataFrames.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.995577}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'def remove_emoji(text):\\n    emoji_pattern = re.compile(\"[\"\\n                           u\"\\\\U0001F600-\\\\U0001F64F\"  # emoticons\\n                           u\"\\\\U0001F300-\\\\U0001F5FF\"  # symbols & pictographs\\n                           u\"\\\\U0001F680-\\\\U0001F6FF\"  # transport & map symbols\\n                           u\"\\\\U0001F1E0-\\\\U0001F1FF\"  # flags (iOS)\\n                           u\"\\\\U00002702-\\\\U000027B0\"\\n                           u\"\\\\U000024C2-\\\\U0001F251\"\\n                           \"]+\", flags=re.UNICODE)\\n    return emoji_pattern.sub(r\\'\\', text)\\n\\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_emoji)\\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_emoji)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code defines a function to remove emojis from text using a regular expression pattern and applies this function to the \"text\" column of both the training and testing DataFrames.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.62529504}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'train_df',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code outputs the contents of the training DataFrame to display its data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99972683}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'train_df[\"text\"].apply(lambda x:len(x.split())).plot(kind=\"hist\");',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code creates and displays a histogram that shows the distribution of word counts in the \"text\" column of the training DataFrame.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.6361653}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'from transformers import AutoTokenizer, AutoModelForSequenceClassification',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports the AutoTokenizer and AutoModelForSequenceClassification classes from the transformers library.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99929786}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code initializes a tokenizer by loading a pre-trained model configuration from the \"roberta-large\" model in the transformers library.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.9938804}},\n",
       "     {'cell_id': 12,\n",
       "      'code': 'train_tensor = tokenizer(list(train_df[\"text\"]), padding=\"max_length\",\\n                        truncation=True, max_length=30,\\n                        return_tensors=\"pt\")[\"input_ids\"]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code tokenizes the text in the \"text\" column of the training DataFrame using the pre-trained RoBERTa tokenizer, and converts the tokenized text into PyTorch tensors with padding and truncation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.96896535}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'class TweetDataset:\\n    def __init__(self, tensors, targ, ids):\\n        self.text = tensors[ids, :]\\n        self.targ = targ[ids].reset_index(drop=True)\\n    \\n    def __len__(self):\\n        return len(self.text)\\n    \\n    def __getitem__(self, idx):\\n        \\n        t = self.text[idx]\\n        y = self.targ[idx]\\n        \\n        return t, tensor(y)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code defines a custom dataset class called TweetDataset, which takes tokenized text tensors, target values, and selected indices to create a dataset suitable for use with PyTorch DataLoaders.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.69012034}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'train_ids, valid_ids = RandomSplitter()(train_df)\\n\\n\\ntarget = train_df[\"target\"]\\n\\ntrain_ds = TweetDataset(train_tensor, target, train_ids)\\nvalid_ds = TweetDataset(train_tensor, target, valid_ids)\\n\\ntrain_dl = DataLoader(train_ds, bs=64)\\nvalid_dl = DataLoader(valid_ds, bs=512)\\ndls = DataLoaders(train_dl, valid_dl).to(\"cuda\")',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code splits the training DataFrame into training and validation sets, initializes TweetDataset instances for both sets, creates DataLoaders for them, and combines these DataLoaders into a DataLoaders object for PyTorch, with an intention to use GPU (CUDA).',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.50315964}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'bert = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2).train().to(\"cuda\")\\n\\nclass BertClassifier(Module):\\n    def __init__(self, bert):\\n        self.bert = bert\\n    def forward(self, x):\\n        return self.bert(x).logits\\n\\nmodel = BertClassifier(bert)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code loads a pre-trained \"roberta-large\" model for sequence classification with two labels, initializes it in training mode on a CUDA device, defines a custom classifier class using this model, and then instantiates an object of this classifier class.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.86304575}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'learn = Learner(dls, model, metrics=[accuracy, F1Score()]).to_fp16()\\nlearn.lr_find()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code initializes a fastai Learner object with the provided DataLoaders, custom model, and specified metrics (accuracy and F1 score), sets training to mixed precision (FP16), and then performs a learning rate finder procedure to identify an optimal learning rate.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9948531}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'learn.fit_one_cycle(3, lr_max=1e-5)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code trains the model for three epochs using the One-Cycle learning rate policy, with a maximum learning rate set to \\\\(1 \\\\times 10^{-5}\\\\).',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9996973}},\n",
       "     {'cell_id': 18,\n",
       "      'code': 'from sklearn.metrics import f1_score\\n\\npreds, targs = learn.get_preds()\\n\\nmin_threshold = None\\nmax_f1 = -float(\"inf\")\\nthresholds = np.linspace(0.3, 0.7, 50)\\nfor threshold in thresholds:\\n    f1 = f1_score(targs, F.softmax(preds, dim=1)[:, 1]>threshold)\\n    if f1 > max_f1:\\n        min_threshold = threshold\\n        min_f1 = f1\\n    print(f\"threshold:{threshold:.4f} - f1:{f1:.4f}\")',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code evaluates the model by computing prediction probabilities, then iterates through a range of thresholds to find the optimal threshold that maximizes the F1 score between the true targets and predicted values.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_params',\n",
       "       'subclass_id': 2,\n",
       "       'predicted_subclass_probability': 0.6100683}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'test_tensor = tokenizer(list(test_df[\"text\"]),\\n                        padding=\"max_length\",\\n                        truncation=True,\\n                        max_length=30,\\n                        return_tensors=\"pt\")[\"input_ids\"]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code tokenizes the text in the test DataFrame using the pre-trained RoBERTa tokenizer, and converts the tokenized text into PyTorch tensors with padding and truncation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9433598}},\n",
       "     {'cell_id': 20,\n",
       "      'code': 'class TestDS:\\n    def __init__(self, tensors):\\n        self.tensors = tensors\\n    \\n    def __len__(self):\\n        return len(self.tensors)\\n    \\n    def __getitem__(self, idx):\\n        t = self.tensors[idx]\\n        return t, tensor(0)\\n\\ntest_dl = DataLoader(TestDS(test_tensor), bs=128)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code defines a custom dataset class called TestDS for the test dataset, which initializes with tokenized text tensors and implements methods to get the length and individual items, and then creates a DataLoader for this test dataset.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.9415805}},\n",
       "     {'cell_id': 21,\n",
       "      'code': 'test_preds = learn.get_preds(dl=test_dl)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code obtains predictions from the model on the test DataLoader using the learn object.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.9943605}},\n",
       "     {'cell_id': 22,\n",
       "      'code': 'sub = pd.read_csv(dir_path + \"sample_submission.csv\")\\nprediction = (F.softmax(test_preds[0], dim=1)[:, 1]>min_threshold).int()\\nsub = pd.read_csv(dir_path + \"sample_submission.csv\")\\nsub[\"target\"] = prediction\\nsub.to_csv(\"submission.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code loads the sample submission file, generates predictions for the test dataset based on the previously determined optimal threshold, assigns these predictions to the \"target\" column of the sample submission file, and saves the modified DataFrame to a new CSV file named \"submission.csv\".',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.974422}}],\n",
       "    'notebook_id': 13},\n",
       "   'notebook_id': 13},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports the NumPy library for numerical operations and the pandas library for data manipulation and CSV file processing.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993907}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"train=pd.read_csv('../input/nlp-getting-started/train.csv')\\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code reads the training and test datasets from CSV files into pandas DataFrames named `train` and `test`.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99974996}},\n",
       "     {'cell_id': 2,\n",
       "      'code': \"import nltk\\nnltk.download('punkt')\\nnltk.download('stopwords')\\nimport re\\n!pip install contractions\\nimport contractions\\nfrom nltk.stem import SnowballStemmer\\nfrom nltk.stem import WordNetLemmatizer\\nnltk.download('wordnet')\\n!pip install pyspellchecker\\nfrom spellchecker import SpellChecker\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports various NLP libraries and downloads necessary resources for text preprocessing, including `nltk`, `contractions`, and `pyspellchecker` libraries, and installs their required datasets and modules.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.799391}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"stop_words=nltk.corpus.stopwords.words('english')\\ni=0\\n#sc=SpellChecker()\\n#data=pd.concat([train,test])\\nwnl=WordNetLemmatizer()\\nstemmer=SnowballStemmer('english')\\nfor doc in train.text:\\n    doc=re.sub(r'https?://\\\\S+|www\\\\.\\\\S+','',doc)\\n    doc=re.sub(r'<.*?>','',doc)\\n    doc=re.sub(r'[^a-zA-Z\\\\s]','',doc,re.I|re.A)\\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\\n    doc=contractions.fix(doc)\\n    tokens=nltk.word_tokenize(doc)\\n    filtered=[token for token in tokens if token not in stop_words]\\n    doc=' '.join(filtered)\\n    train.text[i]=doc\\n    i+=1\\ni=0\\nfor doc in test.text:\\n    doc=re.sub(r'https?://\\\\S+|www\\\\.\\\\S+','',doc)\\n    doc=re.sub(r'<.*?>','',doc)\\n    doc=re.sub(r'[^a-zA-Z\\\\s]','',doc,re.I|re.A)\\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\\n    doc=contractions.fix(doc)\\n    tokens=nltk.word_tokenize(doc)\\n    filtered=[token for token in tokens if token not in stop_words]\\n    doc=' '.join(filtered)\\n    test.text[i]=doc\\n    i+=1\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code preprocesses the text data in the `train` and `test` DataFrames by performing URL and HTML tag removal, non-alphabetic character filtering, contraction expansion, lemmatization, tokenization, and stop word removal. ',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.54864067}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'import tensorflow_hub as hub\\n!pip install tensorflow_text\\nimport tensorflow as tf\\nimport tensorflow_text as text',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports TensorFlow Hub, TensorFlow, and installs and imports the TensorFlow Text extension for handling text data in TensorFlow models.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.93685156}},\n",
       "     {'cell_id': 5,\n",
       "      'code': \" # choose any one of the below models and try them out\\n\\nbert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\\n\\nmap_name_to_handle = {\\n    'bert_en_uncased_L-12_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\\n    'bert_en_cased_L-12_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\\n    'bert_multi_cased_L-12_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\\n    'albert_en_base':\\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\\n    'electra_small':\\n        'https://tfhub.dev/google/electra_small/2',\\n    'electra_base':\\n        'https://tfhub.dev/google/electra_base/2',\\n    'experts_pubmed':\\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\\n    'experts_wiki_books':\\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\\n    'talking-heads_base':\\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\\n}\\n\\nmap_model_to_preprocess = {\\n    'bert_en_uncased_L-12_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'bert_en_cased_L-12_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'bert_multi_cased_L-12_H-768_A-12':\\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\\n    'albert_en_base':\\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\\n    'electra_small':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'electra_base':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'experts_pubmed':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'experts_wiki_books':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n    'talking-heads_base':\\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\\n}\\n\\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\\n\\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code maps and selects a specific BERT model and its corresponding preprocessing model from TensorFlow Hub based on predefined dictionaries, then prints out the selected models' URLs.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9735415}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\\ntext_test = [\\'this is such an amazing movie!\\']\\ntext_preprocessed = bert_preprocess_model(text_test)\\n\\nprint(f\\'Keys       : {list(text_preprocessed.keys())}\\')\\nprint(f\\'Shape      : {text_preprocessed[\"input_word_ids\"].shape}\\')\\nprint(f\\'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}\\')\\nprint(f\\'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}\\')\\nprint(f\\'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}\\')',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code applies the selected BERT preprocessing model to a sample text, displaying the resulting preprocessed inputs including word IDs, input mask, and type IDs, along with their shapes and partial content.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.3991709}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'bert_model = hub.KerasLayer(tfhub_handle_encoder)\\nbert_results = bert_model(text_preprocessed)\\n\\nprint(f\\'Loaded BERT: {tfhub_handle_encoder}\\')\\nprint(f\\'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}\\')\\nprint(f\\'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}\\')\\nprint(f\\'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}\\')\\nprint(f\\'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}\\')',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code initializes and runs the selected BERT model on the preprocessed text input, then prints out shapes and sample values of the BERT model's pooled and sequence outputs.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.5216088}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"def build_classifier_model():\\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\\n    encoder_inputs = preprocessing_layer(text_input)\\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\\n    outputs = encoder(encoder_inputs)\\n    net = outputs['pooled_output']\\n    net = tf.keras.layers.Dropout(0.1)(net)\\n    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\\n    return tf.keras.Model(text_input, net)\\n\\nclassifier_model = build_classifier_model()\\n\\nclassifier_model.compile(optimizer=tf.keras.optimizers.Adam(3e-5), # can also try 1e-5,5e-5,2e-5 whichever performs best\\n                         loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\n                         metrics=tf.metrics.BinaryAccuracy())\\ncheckpoint=tf.keras.callbacks.ModelCheckpoint('model.h5',monitor='val_loss',save_best_only=True)\\nes=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\\nhistory = classifier_model.fit(train.text,train.target,validation_split=0.2,epochs=10,callbacks=[checkpoint,es],batch_size=8)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code defines, compiles, and trains a BERT-based text classification model using the training data, with callbacks for model checkpointing and early stopping.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.5413505}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"classifier_model.load_weights('./model.h5')\\npred=classifier_model.predict(test.text)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code loads the best weights into the classifier model and then uses it to make predictions on the test data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.54062176}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'pd.DataFrame(np.where(pred>0.5,1,0)).value_counts()',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code converts the model's predictions into binary classification results (0 or 1) based on a threshold of 0.5 and then counts the occurrences of each class value.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9979062}},\n",
       "     {'cell_id': 11,\n",
       "      'code': \"pd.DataFrame({\\n    'id':test.id,\\n    'target':np.where(pred>0.5,1,0)[:,0]\\n}).to_csv('submission.csv',index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code creates a DataFrame with the test dataset IDs and their associated predicted binary target values, then exports it to a CSV file named 'submission.csv'.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9992298}}],\n",
       "    'notebook_id': 14},\n",
       "   'notebook_id': 14},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport seaborn as sns # data visualization\\n\\n# Input data files are available in the read-only \"../input/\" directory\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \\n# You can also write temporary files to /kaggle/temp/, but they won\\'t be saved outside of the current session',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports necessary libraries for data processing, linear algebra, and visualization, and also prints out the paths of the files available in the specified input directory.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'list_files',\n",
       "       'subclass_id': 88,\n",
       "       'predicted_subclass_probability': 0.99922085}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\\ntrain_data.head(5)',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet loads the training data from a CSV file into a pandas DataFrame and displays the first five rows of the dataset.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9996433}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'test_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\\ntest_data.head(5)',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet loads the test data from a CSV file into a pandas DataFrame and displays the first five rows of the dataset.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9996711}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'train_data.info()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet provides concise information about the training dataset, including the number of entries, column names, data types, and the count of non-null values for each column.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.99936634}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'test_data.info()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet provides concise information about the test dataset, including the number of entries, column names, data types, and the count of non-null values for each column.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9993579}},\n",
       "     {'cell_id': 5,\n",
       "      'code': \"sns.countplot(train_data['target'])\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a count plot to visualize the distribution of the target variable in the training dataset.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99602413}},\n",
       "     {'cell_id': 6,\n",
       "      'code': '!pip install BeautifulSoup4',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet installs the BeautifulSoup4 library, which is commonly used for web scraping and HTML parsing.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.9954203}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'from bs4 import BeautifulSoup # Text Cleaning\\nimport re, string # Regular Expressions, String\\nfrom nltk.corpus import stopwords # stopwords\\nfrom nltk.stem.porter import PorterStemmer # for word stemming\\nfrom nltk.stem import WordNetLemmatizer # for word lemmatization\\nimport unicodedata\\nimport html\\n\\n# set of stopwords to be removed from text\\nstop = set(stopwords.words(\\'english\\'))\\n\\n# update stopwords to have punctuation too\\nstop.update(list(string.punctuation))\\n\\ndef clean_tweets(text):\\n    \\n    # Remove unwanted html characters\\n    re1 = re.compile(r\\'  +\\')\\n    x1 = text.lower().replace(\\'#39;\\', \"\\'\").replace(\\'amp;\\', \\'&\\').replace(\\'#146;\\', \"\\'\").replace(\\n    \\'nbsp;\\', \\' \\').replace(\\'#36;\\', \\'$\\').replace(\\'\\\\\\\\n\\', \"\\\\n\").replace(\\'quot;\\', \"\\'\").replace(\\n    \\'<br />\\', \"\\\\n\").replace(\\'\\\\\\\\\"\\', \\'\"\\').replace(\\'<unk>\\', \\'u_n\\').replace(\\' @.@ \\', \\'.\\').replace(\\n    \\' @-@ \\', \\'-\\').replace(\\'\\\\\\\\\\', \\' \\\\\\\\ \\')\\n    text = re1.sub(\\' \\', html.unescape(x1))\\n    \\n    # remove non-ascii characters\\n    text = unicodedata.normalize(\\'NFKD\\', text).encode(\\'ascii\\', \\'ignore\\').decode(\\'utf-8\\', \\'ignore\\')\\n    \\n    # strip html\\n    soup = BeautifulSoup(text, \\'html.parser\\')\\n    text = soup.get_text()\\n    \\n    # remove between square brackets\\n    text = re.sub(\\'\\\\[[^]]*\\\\]\\', \\'\\', text)\\n    \\n    # remove URLs\\n    text = re.sub(r\\'http\\\\S+\\', \\'\\', text)\\n    \\n    # remove twitter tags\\n    text = text.replace(\"@\", \"\")\\n    \\n    # remove hashtags\\n    text = text.replace(\"#\", \"\")\\n    \\n    # remove all non-alphabetic characters\\n    text = re.sub(r\\'[^a-zA-Z ]\\', \\'\\', text)\\n    \\n    # remove stopwords from text\\n    final_text = []\\n    for word in text.split():\\n        if word.strip().lower() not in stop:\\n            final_text.append(word.strip().lower())\\n    \\n    text = \" \".join(final_text)\\n    \\n    # lemmatize words\\n    lemmatizer = WordNetLemmatizer()    \\n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\\n    text = \" \".join([lemmatizer.lemmatize(word, pos = \\'v\\') for word in text.split()])\\n    \\n    # replace all numbers with \"num\"\\n    text = re.sub(\"\\\\d\", \"num\", text)\\n    \\n    return text.lower()\\n\\ntrain_data[\\'prep_text\\'] = train_data[\\'text\\'].apply(clean_tweets)\\ntrain_data[\\'prep_text\\'].head(5)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet defines and applies a text preprocessing function to clean and preprocess the text data in the training set, involving steps such as HTML character removal, non-ASCII character filtering, stopword removal, and lemmatization, and then stores the processed text in a new column 'prep_text'.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.27900088}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"test_data['text'] = test_data['text'].apply(clean_tweets)\\ntest_data['text'].head(5)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies the previously defined text preprocessing function to clean and preprocess the text data in the test set, updating the 'text' column with the cleaned text.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9986481}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"from keras.preprocessing.text import Tokenizer # Text tokenization\\n\\n# Setting up the tokenizer\\nvocab_size = 1000\\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet sets up a tokenizer for converting the cleaned text data into sequences of tokens, with a vocabulary size limit, and fits it on the combined text data from both the training and test sets.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9994017}},\n",
       "     {'cell_id': 10,\n",
       "      'code': '# Representing texts as one hot encoded sequence\\n\\nX_train_ohe = tokenizer.texts_to_matrix(train_data[\\'prep_text\\'], mode = \\'binary\\')\\nX_test_ohe = tokenizer.texts_to_matrix(test_data[\\'text\\'], mode = \\'binary\\')\\ny_train = np.array(train_data[\\'target\\']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_ohe.shape}\")\\nprint(f\"X_test shape: {X_test_ohe.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet converts the preprocessed text data in the training and test sets into one-hot encoded sequences using the fitted tokenizer and transforms the target labels into a numpy array, then prints the shapes of the resulting datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.5156552}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'from sklearn.model_selection import train_test_split\\nX_train_ohe, X_val_ohe, y_train, y_val = train_test_split(X_train_ohe, y_train, random_state = 42, test_size = 0.2)\\n\\nprint(f\"X_train shape: {X_train_ohe.shape}\")\\nprint(f\"X_val shape: {X_val_ohe.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the one-hot encoded training data into training and validation sets using an 80-20 split and a specified random seed, then prints the shapes of the resulting datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.961794}},\n",
       "     {'cell_id': 12,\n",
       "      'code': \"from keras.models import Sequential\\nfrom keras import layers, metrics, optimizers, losses\\n\\ndef setup_model():\\n    \\n    model = Sequential()\\n#     model.add(layers.Dense(16, activation='relu', input_shape=(vocab_size,)))\\n#     model.add(layers.Dense(16, activation='relu'))\\n    model.add(layers.Dense(1, activation='sigmoid', input_shape=(vocab_size,)))\\n    \\n    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\\n              loss=losses.binary_crossentropy,\\n              metrics=[metrics.binary_accuracy])\\n    \\n    return model\\n\\nmodel = setup_model()\\nmodel.summary()\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines a function to set up a neural network model using Keras, which includes adding layers, compiling the model with an optimizer and loss function, and then builds and summarizes the model.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.990493}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'history = model.fit(X_train_ohe, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_ohe, y_val))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the neural network model on the training data for a specified number of epochs and batch size, also evaluating it on the validation data during training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9996803}},\n",
       "     {'cell_id': 14,\n",
       "      'code': '_, accuracy = model.evaluate(X_val_ohe, y_val)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates the trained neural network model on the validation set and calculates the accuracy.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.99220455}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'import matplotlib.pyplot as plt\\n\\ndef plot_history(history): \\n\\n    history_dict = history.history\\n    history_dict.keys()\\n\\n\\n    acc = history.history[\\'binary_accuracy\\']\\n    val_acc = history.history[\\'val_binary_accuracy\\']\\n    loss = history.history[\\'loss\\']\\n    val_loss = history.history[\\'val_loss\\']\\n\\n    epochs = range(1, len(acc) + 1)\\n\\n    # \"bo\" is for \"blue dot\"\\n    plt.plot(epochs, loss, \\'bo\\', label=\\'Training loss\\')\\n    # b is for \"solid blue line\"\\n    plt.plot(epochs, val_loss, \\'b\\', label=\\'Validation loss\\')\\n    plt.title(\\'Training and validation loss\\')\\n    plt.xlabel(\\'Epochs\\')\\n    plt.ylabel(\\'Loss\\')\\n    plt.legend()\\n\\n    plt.show()\\n    \\nplot_history(history)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet defines and executes a function to plot the training and validation loss over epochs, displaying the performance of the model during training.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.9967168}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'X_train_wc = tokenizer.texts_to_matrix(train_data[\\'prep_text\\'], mode = \\'count\\')\\nX_test_wc = tokenizer.texts_to_matrix(test_data[\\'text\\'], mode = \\'count\\')\\ny_train = np.array(train_data[\\'target\\']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_wc.shape}\")\\nprint(f\"X_test shape: {X_test_wc.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\n',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet converts the preprocessed text data in the training and test sets into count-based encoded sequences using the fitted tokenizer and transforms the target labels into a numpy array, then prints the shapes of the resulting datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.61748403}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'X_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, random_state = 42, test_size = 0.2)\\n\\nprint(f\"X_train shape: {X_train_wc.shape}\")\\nprint(f\"X_val shape: {X_val_wc.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the count-based encoded training data into training and validation sets using an 80-20 split and a specified random seed, then prints the shapes of the resulting datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.80229384}},\n",
       "     {'cell_id': 18,\n",
       "      'code': 'model = setup_model()\\nmodel.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code snippet initializes a new neural network model by calling the previously defined `setup_model` function, and then displays the model's architecture summary.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9821353}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'history = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the neural network model on the count-based encoded training data for a specified number of epochs and batch size, while also evaluating it on the validation data during training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99967945}},\n",
       "     {'cell_id': 20,\n",
       "      'code': '_, accuracy = model.evaluate(X_val_wc, y_val)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates the trained neural network model on the count-based encoded validation set and calculates the accuracy.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9896338}},\n",
       "     {'cell_id': 21,\n",
       "      'code': 'plot_history(history)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet calls the previously defined `plot_history` function to visualize the training and validation loss over epochs for the most recent model training history.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9654706}},\n",
       "     {'cell_id': 22,\n",
       "      'code': 'X_train_freq = tokenizer.texts_to_matrix(train_data[\\'prep_text\\'], mode = \\'freq\\')\\nX_test_freq = tokenizer.texts_to_matrix(test_data[\\'text\\'], mode = \\'freq\\')\\ny_train = np.array(train_data[\\'target\\']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_freq.shape}\")\\nprint(f\"X_test shape: {X_test_freq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet converts the preprocessed text data in the training and test sets into frequency-based encoded sequences using the fitted tokenizer and transforms the target labels into a numpy array, then prints the shapes of the resulting datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.74439836}},\n",
       "     {'cell_id': 23,\n",
       "      'code': 'X_train_freq, X_val_freq, y_train, y_val = train_test_split(X_train_freq, y_train, test_size = 0.2, random_state = 42)\\nprint(f\"X_train shape: {X_train_freq.shape}\")\\nprint(f\"X_val shape: {X_val_freq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the frequency-based encoded training data into training and validation sets using an 80-20 split and a specified random seed, then prints the shapes of the resulting datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.84670115}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'model = setup_model()\\nmodel.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code snippet initializes a new neural network model by calling the previously defined `setup_model` function, and then displays the model's architecture summary.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9821353}},\n",
       "     {'cell_id': 25,\n",
       "      'code': 'history = model.fit(X_train_freq, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_freq, y_val))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the neural network model on the frequency-based encoded training data for a specified number of epochs and batch size, while also evaluating it on the validation data during training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99967873}},\n",
       "     {'cell_id': 26,\n",
       "      'code': 'plot_history(history)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet calls the previously defined `plot_history` function to visualize the training and validation loss over epochs for the most recent model training history.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9654706}},\n",
       "     {'cell_id': 27,\n",
       "      'code': 'train_data.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first five rows of the training dataset to provide a quick view of its contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997532}},\n",
       "     {'cell_id': 28,\n",
       "      'code': 'from sklearn.feature_extraction.text import TfidfVectorizer # Term Frequency - Inverse Document Frequency\\n\\nvectorizer = TfidfVectorizer(max_features = vocab_size)\\nvectorizer.fit(list(train_data[\\'prep_text\\']) + list(test_data[\\'text\\']))\\n\\n# Fitting on training and testing data\\nX_train_tfidf = vectorizer.transform(list(train_data[\\'prep_text\\'])).toarray() \\nX_test_tfidf = vectorizer.transform(list(test_data[\\'text\\'])).toarray()\\n\\ny_train = np.array(train_data[\\'target\\']).astype(int)\\n\\nprint(f\"X_train shape {X_train_tfidf.shape}\")\\nprint(f\"X_test shape {X_test_tfidf.shape}\")\\nprint(f\"y_train shape {y_train.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet uses the TF-IDF vectorizer to transform the preprocessed text data in the training and test sets into TF-IDF weighted term-document matrices, and prints the shapes of the resulting datasets along with their target labels.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.57227826}},\n",
       "     {'cell_id': 29,\n",
       "      'code': 'X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size = 0.2, random_state = 42)\\nprint(f\"X_train shape: {X_train_tfidf.shape}\")\\nprint(f\"X_val shape: {X_val_tfidf.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the TF-IDF encoded training data into training and validation sets using an 80-20 split and a specified random seed, then prints the shapes of the resulting datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.70971876}},\n",
       "     {'cell_id': 30,\n",
       "      'code': 'model = setup_model()\\nmodel.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code snippet initializes a new neural network model by calling the previously defined `setup_model` function, and then displays the model's architecture summary.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9821353}},\n",
       "     {'cell_id': 31,\n",
       "      'code': 'history = model.fit(X_train_tfidf, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_tfidf, y_val))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the neural network model on the TF-IDF encoded training data for a specified number of epochs and batch size, while also evaluating it on the validation data during training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9996803}},\n",
       "     {'cell_id': 32,\n",
       "      'code': 'plot_history(history)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet calls the previously defined `plot_history` function to visualize the training and validation loss over epochs for the most recent model training history.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9654706}},\n",
       "     {'cell_id': 33,\n",
       "      'code': \"plt.hist(list(train_data['prep_text'].str.split().map(lambda x: len(x))))\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a histogram to visualize the distribution of the lengths (in words) of the preprocessed text entries in the training dataset.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9977956}},\n",
       "     {'cell_id': 34,\n",
       "      'code': \"# Loading the embedding dictionary from file\\n\\nembedding_dict={}\\nwith open('../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt','r') as f:\\n    for line in f:\\n        values=line.split()\\n        word = values[0]\\n        vectors=np.asarray(values[1:],'float32')\\n        embedding_dict[word]=vectors\\nf.close()\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet loads pre-trained word embeddings from a GloVe file into a dictionary where the keys are words and the values are their corresponding vector representations.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.23091643}},\n",
       "     {'cell_id': 35,\n",
       "      'code': '# Sequences creation, truncation and padding\\n\\nfrom keras.preprocessing.sequence import pad_sequences\\n\\n# Setting up the tokenizer\\nvocab_size = 10000\\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = \\'UNK\\')\\ntokenizer.fit_on_texts(list(train_data[\\'prep_text\\']) + list(test_data[\\'text\\']))\\n\\nmax_len = 15\\nX_train_seq = tokenizer.texts_to_sequences(train_data[\\'prep_text\\'])\\nX_test_seq = tokenizer.texts_to_sequences(test_data[\\'text\\'])\\n\\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = \\'post\\', padding = \\'post\\')\\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = \\'post\\', padding = \\'post\\')\\ny_train = np.array(train_data[\\'target\\']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_seq.shape}\")\\nprint(f\"X_test shape: {X_test_seq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet sets up a tokenizer with a vocabulary size of 10,000, converts the preprocessed text data in the training and test sets into sequences, and pads or truncates them to a maximum length of 15, then prints the shapes of the resulting datasets and their target labels.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.7396719}},\n",
       "     {'cell_id': 36,\n",
       "      'code': 'X_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\\nprint(f\"X_train shape: {X_train_seq.shape}\")\\nprint(f\"X_val shape: {X_val_seq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the sequence-based encoded training data into training and validation sets using an 80-20 split and a specified random seed, then prints the shapes of the resulting datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.9086002}},\n",
       "     {'cell_id': 37,\n",
       "      'code': 'num_words = len(tokenizer.word_index)\\nprint(f\"Number of unique words: {num_words}\")',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet prints the number of unique words present in the tokenizer's word index, giving insight into the vocabulary size derived from the text data.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_unique_values',\n",
       "       'subclass_id': 54,\n",
       "       'predicted_subclass_probability': 0.9865096}},\n",
       "     {'cell_id': 38,\n",
       "      'code': '# Applying GloVE representations on our corpus\\n\\nembedding_matrix=np.zeros((num_words,100))\\n\\nfor word,i in tokenizer.word_index.items():\\n    if i < num_words:\\n        emb_vec = embedding_dict.get(word)\\n        if emb_vec is not None:\\n            embedding_matrix[i] = emb_vec    ',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet creates an embedding matrix by mapping the words in the tokenizer's word index to their corresponding GloVe vector representations, initializing the embeddings for words not found in the GloVe dictionary to zero vectors.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.91961074}},\n",
       "     {'cell_id': 39,\n",
       "      'code': \"# Setting up the model\\n\\nn_latent_factors = 100\\nmodel_glove = Sequential()\\nmodel_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \\n                           input_length = max_len, trainable=True))\\nmodel_glove.add(layers.Flatten())\\n# model_glove.add(layers.Dense(16, activation='relu'))\\nmodel_glove.add(layers.Dropout(0.5))\\n# model_glove.add(layers.Dense(16, activation='relu'))\\nmodel_glove.add(layers.Dense(1, activation='sigmoid'))\\nmodel_glove.summary()\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code snippet sets up a neural network model that includes an embedding layer initialized with pre-trained GloVe embeddings, followed by a flattening layer, a dropout layer, and a dense output layer, and then displays the model's architecture summary.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9871293}},\n",
       "     {'cell_id': 40,\n",
       "      'code': 'model_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\\n              loss = losses.binary_crossentropy,\\n              metrics = [metrics.binary_accuracy])\\n\\nhistory = model_glove.fit(X_train_seq,\\n                    y_train,\\n                    epochs=20,\\n                    batch_size=512,\\n                    validation_data=(X_val_seq, y_val))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet compiles the neural network model with specified optimizer, loss function, and evaluation metrics, then trains the model on the sequence-based encoded training data for a specified number of epochs and batch size, while also evaluating it on the validation data during training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99457717}},\n",
       "     {'cell_id': 41,\n",
       "      'code': 'plot_history(history)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet calls the previously defined `plot_history` function to visualize the training and validation loss over epochs for the most recent model training history.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9654706}},\n",
       "     {'cell_id': 42,\n",
       "      'code': 'max_len = 15\\nX_train_seq = tokenizer.texts_to_sequences(train_data[\\'prep_text\\'])\\nX_test_seq = tokenizer.texts_to_sequences(test_data[\\'text\\'])\\n\\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = \\'post\\', padding = \\'post\\')\\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = \\'post\\', padding = \\'post\\')\\ny_train = np.array(train_data[\\'target\\']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_seq.shape}\")\\nprint(f\"X_test shape: {X_test_seq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\\\\n\")\\n\\n# Setting up the model\\n\\nn_latent_factors = 100\\nmodel_glove = Sequential()\\nmodel_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \\n                           input_length = max_len, trainable=True))\\nmodel_glove.add(layers.Flatten())\\n# model_glove.add(layers.Dense(16, activation=\\'relu\\'))\\nmodel_glove.add(layers.Dropout(0.5))\\n# model_glove.add(layers.Dense(16, activation=\\'relu\\'))\\nmodel_glove.add(layers.Dense(1, activation=\\'sigmoid\\'))\\nprint(f\"{model_glove.summary()}\\\\n\")\\n\\n\\nmodel_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\\n              loss = losses.binary_crossentropy,\\n              metrics = [metrics.binary_accuracy])\\n\\nhistory = model_glove.fit(X_train_seq,\\n                    y_train,\\n                    epochs=20,\\n                    batch_size=512)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet processes and pads the sequence data, sets up and compiles a neural network model incorporating GloVe embeddings, and then trains the model on the full training dataset for a specified number of epochs and batch size.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.80148226}},\n",
       "     {'cell_id': 43,\n",
       "      'code': '# Setting up the tokenizer\\nvocab_size = 1000\\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = \\'UNK\\')\\ntokenizer.fit_on_texts(list(train_data[\\'text\\']) + list(test_data[\\'text\\']))\\n\\n# Word count representation\\nX_train_wc = tokenizer.texts_to_matrix(train_data[\\'text\\'], mode = \\'count\\')\\nX_test_wc = tokenizer.texts_to_matrix(test_data[\\'text\\'], mode = \\'count\\')\\ny_train = np.array(train_data[\\'target\\']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_wc.shape}\")\\nprint(f\"X_test shape: {X_test_wc.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\n\\n# Train Validation Split\\nX_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, test_size = 0.2, random_state = 42)\\n\\nprint(f\"X_train shape: {X_train_wc.shape}\")\\nprint(f\"X_val shape: {X_val_wc.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\\\\n\")\\n\\n# Setting up the model\\nmodel = setup_model()\\n\\n# Fitting the model on un-preprocessed text\\nhistory = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet sets up a tokenizer to convert raw text data into word-count encoded matrices, splits the data into training and validation sets, initializes and compiles a neural network model, and trains the model on the un-preprocessed raw text data for a specified number of epochs and batch size.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.59386915}},\n",
       "     {'cell_id': 44,\n",
       "      'code': 'submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\\ntest_pred = model_glove.predict(X_test_seq)\\ntest_pred_int = test_pred.round().astype(\\'int\\')\\nsubmission[\\'target\\'] = test_pred_int\\nsubmission.to_csv(\\'submission.csv\\', index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet predicts the target variable for the test set using the trained GloVe-based model, rounds the predictions to binary integers, assigns them to the 'target' column of a submission DataFrame, and saves the updated DataFrame to a CSV file.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99928826}}],\n",
       "    'notebook_id': 15},\n",
       "   'notebook_id': 15},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '# importing libraries\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom IPython.display import display\\nfrom collections import Counter, defaultdict\\n%matplotlib inline',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports essential libraries for data manipulation, visualization, and displays, and configures inline plotting for Jupyter notebooks.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9993229}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"# display 100 columns\\npd.set_option('display.max_columns', 100)\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet sets the pandas option to display a maximum of 100 columns when printing DataFrames, enhancing visibility for broader datasets in the notebook.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9973706}},\n",
       "     {'cell_id': 2,\n",
       "      'code': '# loading the dataset\\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet loads training and testing datasets from CSV files into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99973744}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'train_df.head(5)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet displays the first five rows of the training DataFrame to provide an initial overview of the dataset's structure and contents.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997615}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'train_df.info()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet provides a concise summary of the training DataFrame, including the data types, non-null counts, and memory usage.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9993624}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'test_df.info()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet provides a concise summary of the testing DataFrame, including the data types, non-null counts, and memory usage.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9993699}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'print(f\"Rows | Columns\\\\n{train_df.shape}\")\\nprint(test_df.shape)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the number of rows and columns in both the training and testing DataFrames, giving an overview of the dataset dimensions.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99951255}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"table = plt.table(cellText=[[train_df.location.isnull().sum()], [test_df.location.isnull().sum()]],\\n         rowLabels=['Train Data', 'Test Data'],\\n         colLabels=['Number of Missing values'],\\n         loc='top')\\nplt.box(on=None)\\nplt.axis('off')\\n# plt.subplots_adjust(top = 1, bottom = 0.1, right = 1, left = 0, \\n#             hspace = 0, wspace = 0)\\n#plt.margins(0,0)\\ntable.set_fontsize(14);\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet creates a table visualization displaying the number of missing values in the 'location' column for both the training and testing DataFrames.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.86134624}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"# checking for missing values\\nfig, (ax1, ax2) = plt.subplots(2,1, figsize=(8,5))\\n\\nsns.heatmap(train_df.isnull(),yticklabels=False,cbar=False,cmap='flare', ax = ax1)\\nax1.tick_params(axis='x', labelsize=13, rotation = 45)\\nax1.set_title('train data')\\n\\nsns.heatmap(test_df.isnull(),yticklabels=False,cbar=False,cmap='flare', ax = ax2)\\nax2.tick_params(axis='x', labelsize=13, rotation = 45)\\nax2.set_title('test data')\\nfig.tight_layout();\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates two heatmaps to visualize the distribution of missing values in the training and testing DataFrames, making it easier to identify which parts of the datasets have missing data.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'heatmap',\n",
       "       'subclass_id': 80,\n",
       "       'predicted_subclass_probability': 0.99526525}},\n",
       "     {'cell_id': 9,\n",
       "      'code': '# we notice many missing values for the location column',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This comment notes the observation of many missing values in the 'location' column from the previous visualizations, emphasizing a key insight about the dataset.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.2938043}},\n",
       "     {'cell_id': 10,\n",
       "      'code': \"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\\ntrain_df.at[train_df['id'].isin(ids_with_target_error),'target'] = 0\\ntrain_df[train_df['id'].isin(ids_with_target_error)]\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet corrects the 'target' values to 0 for specific rows in the training DataFrame that have identified error IDs, then displays these corrected rows.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9975581}},\n",
       "     {'cell_id': 11,\n",
       "      'code': \"# checking for duplicates\\nprint(train_df.duplicated(subset=['text']).sum())\\nprint(test_df.duplicated(subset=['text']).sum())\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet checks and prints the number of duplicate rows based on the 'text' column in both the training and testing DataFrames, helping to identify redundancy in the datasets.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.87618726}},\n",
       "     {'cell_id': 12,\n",
       "      'code': \"print(train_df.duplicated(subset=['text','target']).sum())\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet checks and prints the number of duplicate rows based on both 'text' and 'target' columns in the training DataFrame, providing a more detailed look at duplicate entries within the dataset.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.7811897}},\n",
       "     {'cell_id': 13,\n",
       "      'code': \"set1 = set(train_df[train_df.duplicated(subset=['text'])]['id'].values)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet creates a set of 'id' values from rows in the training DataFrame that have duplicate 'text' entries, preparing for potential cleanup or further analysis steps.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.8107515}},\n",
       "     {'cell_id': 14,\n",
       "      'code': \"set2 = (train_df[train_df.duplicated(subset=['text','target'])]['id'].values)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet creates an array of 'id' values from rows in the training DataFrame that have duplicates in both 'text' and 'target' columns, identifying redundant entries for further processing.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.8105881}},\n",
       "     {'cell_id': 15,\n",
       "      'code': \"tweet_ids_possible_wrong = set1.difference(set2)\\n# those are the tweets which have duplicated text but different label\\nprint(train_df[train_df['id'].isin(tweet_ids_possible_wrong)].text.values)\\nprint(train_df[train_df['id'].isin(tweet_ids_possible_wrong)].target.values)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet identifies tweets with duplicate texts but different labels, then prints their text values and corresponding target values, highlighting possible mislabeled entries for further investigation or correction.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.4053326}},\n",
       "     {'cell_id': 16,\n",
       "      'code': \"# setting the argument keep=False, drops all the duplicates\\ntrain_df.drop_duplicates(subset=['text'], keep=False, inplace=True)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet removes all duplicate rows based on the 'text' column in the training DataFrame, ensuring only unique rows are retained by using the `keep=False` argument.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'remove_duplicates',\n",
       "       'subclass_id': 19,\n",
       "       'predicted_subclass_probability': 0.89899284}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'train_df.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet outputs the current shape (number of rows and columns) of the training DataFrame after removing duplicate rows, providing a quick check on the dataset's new dimensions.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9995821}},\n",
       "     {'cell_id': 18,\n",
       "      'code': \"fig = plt.figure(figsize=(20,5))\\ntrain_df.keyword.value_counts().head(25).plot(kind = 'bar')\\nplt.tick_params(axis='x', labelsize=11, rotation = 45)\\nplt.title('Top keywords');\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a bar plot of the 25 most frequent keywords in the training DataFrame, providing a visual representation of the distribution of top keywords.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99566513}},\n",
       "     {'cell_id': 19,\n",
       "      'code': \"fig = plt.figure(figsize=(20,5))\\ntrain_df.location.value_counts().head(25).plot(kind = 'bar')\\nplt.tick_params(axis='x', labelsize=9, rotation = 45)\\nplt.title('Top Locations');\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates a bar plot displaying the 25 most common locations in the training DataFrame, visualizing the distribution of top locations.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99453616}},\n",
       "     {'cell_id': 20,\n",
       "      'code': 'fig = plt.figure(figsize=(7,5))\\nsns.countplot(data = train_df, x = \"target\")\\nplt.title(\\'Non-actual vs actual distater tweets counts\\');',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a count plot to visualize the distribution of non-disaster and disaster tweets in the training DataFrame, highlighting class imbalances in the target variable.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9882289}},\n",
       "     {'cell_id': 21,\n",
       "      'code': \"fig = plt.figure(figsize=(7,5))\\n(train_df.groupby(['target']).count() / len(train_df['target']))['id'].plot(kind='bar', width = 0.85, color = ['tomato', 'steelblue'])\\nplt.title('Non-actual vs actual distater tweets percentage');\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a bar plot to visualize the percentage of non-disaster and disaster tweets in the training DataFrame, providing insights into the relative distribution of the target variable.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9961461}},\n",
       "     {'cell_id': 22,\n",
       "      'code': \"# Somes actual tweets (non-disaster)\\nprint(train_df.text[train_df['target'] == 0][:10].values)\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the text of the first ten tweets classified as non-disaster in the training DataFrame, offering a sample of typical non-disaster tweet content.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'filter',\n",
       "       'subclass_id': 14,\n",
       "       'predicted_subclass_probability': 0.8921785}},\n",
       "     {'cell_id': 23,\n",
       "      'code': \"print(train_df.text[train_df['target'] == 1][:10].values)\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the text of the first ten tweets classified as disaster in the training DataFrame, providing examples of typical disaster tweet content.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'filter',\n",
       "       'subclass_id': 14,\n",
       "       'predicted_subclass_probability': 0.5046063}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'from wordcloud import WordCloud',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports the WordCloud class from the wordcloud library, which is used for generating word clouds from text data for visualization.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9992773}},\n",
       "     {'cell_id': 25,\n",
       "      'code': 'wordcloud_non_dis = WordCloud( background_color=\\'white\\',\\n                        width=600,\\n                        height=400).generate(\" \".join(train_df.text[train_df[\\'target\\'] == 0]))\\nwordcloud_dis = WordCloud( background_color=\\'white\\',\\n                        width=600,\\n                        height=400).generate(\" \".join(train_df.text[train_df[\\'target\\'] == 1]))',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates word clouds for non-disaster and disaster tweets by combining the text of each class and creating visual representations of the most frequent words.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.68369985}},\n",
       "     {'cell_id': 26,\n",
       "      'code': 'fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,14))\\n\\nax1.imshow(wordcloud_non_dis, interpolation=\\'bilinear\\')\\nax1.set_title(\\'Non disaster\\')\\nax1.axis(\"off\")\\nax2.imshow(wordcloud_dis, interpolation=\\'bilinear\\')\\nax2.set_title(\\'Disaster\\')\\nax2.axis(\"off\");',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a side-by-side comparison of word clouds for non-disaster and disaster tweets, allowing visualization of the most frequent words in each class.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99756926}},\n",
       "     {'cell_id': 27,\n",
       "      'code': \"train_df['location'] = train_df['location'].str.lower()\\ntrain_df['location'] = train_df['location'].str.strip()\\ntest_df['location'] = train_df['location'].str.lower()\\ntest_df['location'] = train_df['location'].str.strip()\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet standardizes the 'location' column in both training and testing DataFrames by converting text to lowercase and stripping any leading or trailing whitespace, ensuring consistency in the data.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.7562164}},\n",
       "     {'cell_id': 28,\n",
       "      'code': \"loc_dict = {'united states':'usa',\\n                            'us':'usa',\\n                            'united kingdom':'uk',\\n                              'nyc':'new york',\\n                             'london, uk': 'london',\\n                              'london, england':'london',\\n                            'new york, ny':'new york',\\n                            'everywhere':'worldwide'}\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a dictionary to map various location names or abbreviations to standardized location names, aiding in data normalization.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.99767166}},\n",
       "     {'cell_id': 29,\n",
       "      'code': \"train_df['location'].replace(loc_dict, inplace=True)\\ntest_df['location'].replace(loc_dict, inplace=True)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet replaces the values in the 'location' column of both the training and testing DataFrames according to the standardization mappings defined in `loc_dict`, ensuring uniform location data.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.92937183}},\n",
       "     {'cell_id': 30,\n",
       "      'code': \"fig, axes = plt.subplots(1,2,figsize=(25,6))\\nax1 = train_df[train_df['target']==0]['location'].value_counts().head(25).plot(kind = 'bar', color = 'b', alpha = 0.5, ax=axes[0])\\nax1.tick_params(axis='x', labelsize=9, rotation = 45)\\nax1.set_title('Non-Disaster')\\nax2 = train_df[train_df['target']==1]['location'].value_counts().head(25).plot(kind = 'bar', color = 'r', alpha = 0.5, ax=axes[1])\\nax2.tick_params(axis='x', labelsize=9, rotation = 45)\\nax2.set_title('Disaster');\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates bar plots to visualize the 25 most common locations for non-disaster and disaster tweets in the training DataFrame, highlighting location distribution differences between the two classes.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99818254}},\n",
       "     {'cell_id': 31,\n",
       "      'code': \"fig, axes = plt.subplots(1,2,figsize=(25,6))\\nax1 = train_df[train_df['target']==0]['keyword'].value_counts().head(25).plot(kind = 'bar', color = 'b', alpha = 0.5, ax=axes[0])\\nax1.tick_params(axis='x', labelsize=9, rotation = 45)\\nax1.set_title('Top Keywords (non disaster)')\\nax2 = train_df[train_df['target']==1]['keyword'].value_counts().head(25).plot(kind = 'bar', color = 'r', alpha = 0.5, ax=axes[1])\\nax2.tick_params(axis='x', labelsize=9, rotation = 45)\\nax2.set_title('Top Keywords (disaster)');\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates bar plots to visualize the 25 most frequent keywords associated with non-disaster and disaster tweets in the training DataFrame, providing insights into keyword distribution for each class.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.981807}},\n",
       "     {'cell_id': 32,\n",
       "      'code': 'import string\\nimport emoji\\nimport nltk\\nimport re',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports additional libraries and modules for text processing, including string handling, emoji support, natural language processing (NLP), and regular expressions (regex).',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99923146}},\n",
       "     {'cell_id': 33,\n",
       "      'code': 'abbreviations = {\\n    \"$\" : \" dollar \",\\n    \"€\" : \" euro \",\\n    \"4ao\" : \"for adults only\",\\n    \"a.m\" : \"before midday\",\\n    \"a3\" : \"anytime anywhere anyplace\",\\n    \"aamof\" : \"as a matter of fact\",\\n    \"acct\" : \"account\",\\n    \"adih\" : \"another day in hell\",\\n    \"afaic\" : \"as far as i am concerned\",\\n    \"afaict\" : \"as far as i can tell\",\\n    \"afaik\" : \"as far as i know\",\\n    \"afair\" : \"as far as i remember\",\\n    \"afk\" : \"away from keyboard\",\\n    \"app\" : \"application\",\\n    \"approx\" : \"approximately\",\\n    \"apps\" : \"applications\",\\n    \"asap\" : \"as soon as possible\",\\n    \"asl\" : \"age, sex, location\",\\n    \"atk\" : \"at the keyboard\",\\n    \"ave.\" : \"avenue\",\\n    \"aymm\" : \"are you my mother\",\\n    \"ayor\" : \"at your own risk\", \\n    \"b&b\" : \"bed and breakfast\",\\n    \"b+b\" : \"bed and breakfast\",\\n    \"b.c\" : \"before christ\",\\n    \"b2b\" : \"business to business\",\\n    \"b2c\" : \"business to customer\",\\n    \"b4\" : \"before\",\\n    \"b4n\" : \"bye for now\",\\n    \"b@u\" : \"back at you\",\\n    \"bae\" : \"before anyone else\",\\n    \"bak\" : \"back at keyboard\",\\n    \"bbbg\" : \"bye bye be good\",\\n    \"bbc\" : \"british broadcasting corporation\",\\n    \"bbias\" : \"be back in a second\",\\n    \"bbl\" : \"be back later\",\\n    \"bbs\" : \"be back soon\",\\n    \"be4\" : \"before\",\\n    \"bfn\" : \"bye for now\",\\n    \"blvd\" : \"boulevard\",\\n    \"bout\" : \"about\",\\n    \"brb\" : \"be right back\",\\n    \"bros\" : \"brothers\",\\n    \"brt\" : \"be right there\",\\n    \"bsaaw\" : \"big smile and a wink\",\\n    \"btw\" : \"by the way\",\\n    \"bwl\" : \"bursting with laughter\",\\n    \"c/o\" : \"care of\",\\n    \"cet\" : \"central european time\",\\n    \"cf\" : \"compare\",\\n    \"cia\" : \"central intelligence agency\",\\n    \"csl\" : \"can not stop laughing\",\\n    \"cu\" : \"see you\",\\n    \"cul8r\" : \"see you later\",\\n    \"cv\" : \"curriculum vitae\",\\n    \"cwot\" : \"complete waste of time\",\\n    \"cya\" : \"see you\",\\n    \"cyt\" : \"see you tomorrow\",\\n    \"dae\" : \"does anyone else\",\\n    \"dbmib\" : \"do not bother me i am busy\",\\n    \"diy\" : \"do it yourself\",\\n    \"dm\" : \"direct message\",\\n    \"dwh\" : \"during work hours\",\\n    \"e123\" : \"easy as one two three\",\\n    \"eet\" : \"eastern european time\",\\n    \"eg\" : \"example\",\\n    \"embm\" : \"early morning business meeting\",\\n    \"encl\" : \"enclosed\",\\n    \"encl.\" : \"enclosed\",\\n    \"etc\" : \"and so on\",\\n    \"faq\" : \"frequently asked questions\",\\n    \"fawc\" : \"for anyone who cares\",\\n    \"fb\" : \"facebook\",\\n    \"fc\" : \"fingers crossed\",\\n    \"fig\" : \"figure\",\\n    \"fimh\" : \"forever in my heart\", \\n    \"ft.\" : \"feet\",\\n    \"ft\" : \"featuring\",\\n    \"ftl\" : \"for the loss\",\\n    \"ftw\" : \"for the win\",\\n    \"fwiw\" : \"for what it is worth\",\\n    \"fyi\" : \"for your information\",\\n    \"g9\" : \"genius\",\\n    \"gahoy\" : \"get a hold of yourself\",\\n    \"gal\" : \"get a life\",\\n    \"gcse\" : \"general certificate of secondary education\",\\n    \"gfn\" : \"gone for now\",\\n    \"gg\" : \"good game\",\\n    \"gl\" : \"good luck\",\\n    \"glhf\" : \"good luck have fun\",\\n    \"gmt\" : \"greenwich mean time\",\\n    \"gmta\" : \"great minds think alike\",\\n    \"gn\" : \"good night\",\\n    \"g.o.a.t\" : \"greatest of all time\",\\n    \"goat\" : \"greatest of all time\",\\n    \"goi\" : \"get over it\",\\n    \"gps\" : \"global positioning system\",\\n    \"gr8\" : \"great\",\\n    \"gratz\" : \"congratulations\",\\n    \"gyal\" : \"girl\",\\n    \"h&c\" : \"hot and cold\",\\n    \"hp\" : \"horsepower\",\\n    \"hr\" : \"hour\",\\n    \"hrh\" : \"his royal highness\",\\n    \"ht\" : \"height\",\\n    \"ibrb\" : \"i will be right back\",\\n    \"ic\" : \"i see\",\\n    \"icq\" : \"i seek you\",\\n    \"icymi\" : \"in case you missed it\",\\n    \"idc\" : \"i do not care\",\\n    \"idgadf\" : \"i do not give a damn fuck\",\\n    \"idgaf\" : \"i do not give a fuck\",\\n    \"idk\" : \"i do not know\",\\n    \"ie\" : \"that is\",\\n    \"i.e\" : \"that is\",\\n    \"ifyp\" : \"i feel your pain\",\\n    \"IG\" : \"instagram\",\\n    \"iirc\" : \"if i remember correctly\",\\n    \"ilu\" : \"i love you\",\\n    \"ily\" : \"i love you\",\\n    \"imho\" : \"in my humble opinion\",\\n    \"imo\" : \"in my opinion\",\\n    \"imu\" : \"i miss you\",\\n    \"iow\" : \"in other words\",\\n    \"irl\" : \"in real life\",\\n    \"j4f\" : \"just for fun\",\\n    \"jic\" : \"just in case\",\\n    \"jk\" : \"just kidding\",\\n    \"jsyk\" : \"just so you know\",\\n    \"l8r\" : \"later\",\\n    \"lb\" : \"pound\",\\n    \"lbs\" : \"pounds\",\\n    \"ldr\" : \"long distance relationship\",\\n    \"lmao\" : \"laugh my ass off\",\\n    \"lmfao\" : \"laugh my fucking ass off\",\\n    \"lol\" : \"laughing out loud\",\\n    \"ltd\" : \"limited\",\\n    \"ltns\" : \"long time no see\",\\n    \"m8\" : \"mate\",\\n    \"mf\" : \"motherfucker\",\\n    \"mfs\" : \"motherfuckers\",\\n    \"mfw\" : \"my face when\",\\n    \"mofo\" : \"motherfucker\",\\n    \"mph\" : \"miles per hour\",\\n    \"mr\" : \"mister\",\\n    \"mrw\" : \"my reaction when\",\\n    \"ms\" : \"miss\",\\n    \"mte\" : \"my thoughts exactly\",\\n    \"nagi\" : \"not a good idea\",\\n    \"nbc\" : \"national broadcasting company\",\\n    \"nbd\" : \"not big deal\",\\n    \"nfs\" : \"not for sale\",\\n    \"ngl\" : \"not going to lie\",\\n    \"nhs\" : \"national health service\",\\n    \"nrn\" : \"no reply necessary\",\\n    \"nsfl\" : \"not safe for life\",\\n    \"nsfw\" : \"not safe for work\",\\n    \"nth\" : \"nice to have\",\\n    \"nvr\" : \"never\",\\n    \"nyc\" : \"new york city\",\\n    \"oc\" : \"original content\",\\n    \"og\" : \"original\",\\n    \"ohp\" : \"overhead projector\",\\n    \"oic\" : \"oh i see\",\\n    \"omdb\" : \"over my dead body\",\\n    \"omg\" : \"oh my god\",\\n    \"omw\" : \"on my way\",\\n    \"p.a\" : \"per annum\",\\n    \"p.m\" : \"after midday\",\\n    \"pm\" : \"prime minister\",\\n    \"poc\" : \"people of color\",\\n    \"pov\" : \"point of view\",\\n    \"pp\" : \"pages\",\\n    \"ppl\" : \"people\",\\n    \"prw\" : \"parents are watching\",\\n    \"ps\" : \"postscript\",\\n    \"pt\" : \"point\",\\n    \"ptb\" : \"please text back\",\\n    \"pto\" : \"please turn over\",\\n    \"qpsa\" : \"what happens\", #\"que pasa\",\\n    \"ratchet\" : \"rude\",\\n    \"rbtl\" : \"read between the lines\",\\n    \"rlrt\" : \"real life retweet\", \\n    \"rofl\" : \"rolling on the floor laughing\",\\n    \"roflol\" : \"rolling on the floor laughing out loud\",\\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\\n    \"rt\" : \"retweet\",\\n    \"ruok\" : \"are you ok\",\\n    \"sfw\" : \"safe for work\",\\n    \"sk8\" : \"skate\",\\n    \"smh\" : \"shake my head\",\\n    \"sq\" : \"square\",\\n    \"srsly\" : \"seriously\", \\n    \"ssdd\" : \"same stuff different day\",\\n    \"tbh\" : \"to be honest\",\\n    \"tbs\" : \"tablespooful\",\\n    \"tbsp\" : \"tablespooful\",\\n    \"tfw\" : \"that feeling when\",\\n    \"thks\" : \"thank you\",\\n    \"tho\" : \"though\",\\n    \"thx\" : \"thank you\",\\n    \"tia\" : \"thanks in advance\",\\n    \"til\" : \"today i learned\",\\n    \"tl;dr\" : \"too long i did not read\",\\n    \"tldr\" : \"too long i did not read\",\\n    \"tmb\" : \"tweet me back\",\\n    \"tntl\" : \"trying not to laugh\",\\n    \"ttyl\" : \"talk to you later\",\\n    \"u\" : \"you\",\\n    \"u2\" : \"you too\",\\n    \"u4e\" : \"yours for ever\",\\n    \"utc\" : \"coordinated universal time\",\\n    \"w/\" : \"with\",\\n    \"w/o\" : \"without\",\\n    \"w8\" : \"wait\",\\n    \"wassup\" : \"what is up\",\\n    \"wb\" : \"welcome back\",\\n    \"wtf\" : \"what the fuck\",\\n    \"wtg\" : \"way to go\",\\n    \"wtpa\" : \"where the party at\",\\n    \"wuf\" : \"where are you from\",\\n    \"wuzup\" : \"what is up\",\\n    \"wywh\" : \"wish you were here\",\\n    \"yd\" : \"yard\",\\n    \"ygtr\" : \"you got that right\",\\n    \"ynk\" : \"you never know\",\\n    \"zzz\" : \"sleeping bored and tired\"\\n}',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a dictionary of common abbreviations and their full forms, aimed at standardizing and expanding shorthand text during data preprocessing.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9984091}},\n",
       "     {'cell_id': 34,\n",
       "      'code': \"def convert_abb(x):\\n    word_list = x.split()\\n    r_string = []\\n    for word in word_list:\\n        r_string.append(abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word)\\n    return ' '.join(r_string)\\n\\ntest = 'afk hello world!'\\nconvert_abb(test)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function `convert_abb` that replaces abbreviations in a given text with their full forms based on the `abbreviations` dictionary and tests it with a sample string.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.8039966}},\n",
       "     {'cell_id': 35,\n",
       "      'code': \"train_df['text'] = train_df.text.apply(convert_abb)\\ntest_df['text'] = test_df.text.apply(convert_abb)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies the `convert_abb` function to the 'text' column in both the training and testing DataFrames, replacing abbreviations with their full forms in all tweet texts.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9974021}},\n",
       "     {'cell_id': 36,\n",
       "      'code': \"train_df['clean_text'] = train_df.text.apply(lambda x: re.sub('https?://\\\\S+|www\\\\.\\\\S+', '', x))\\ntest_df['clean_text'] = test_df.text.apply(lambda x: re.sub('https?://\\\\S+|www\\\\.\\\\S+', '', x))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet creates a new column 'clean_text' in both the training and testing DataFrames by removing URLs from the tweet texts using regular expressions.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9974329}},\n",
       "     {'cell_id': 37,\n",
       "      'code': \"train_df['clean_text'] = train_df.clean_text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet updates the 'clean_text' column in both the training and testing DataFrames by removing any non-ASCII characters, ensuring the text is ASCII-encoded.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.9229938}},\n",
       "     {'cell_id': 38,\n",
       "      'code': \"non_alpha = string.punctuation + '0123456789'\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a string of non-alphabetic characters, including punctuation and digits, to be used for further data cleaning or preprocessing steps.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9975495}},\n",
       "     {'cell_id': 39,\n",
       "      'code': \"train_df['clean_text'] = train_df.clean_text.apply(lambda x: x.translate(str.maketrans('','',non_alpha)))\\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: x.translate(str.maketrans('','',non_alpha)))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet updates the 'clean_text' column in both the training and testing DataFrames by removing all punctuation and digits, leaving only alphabetic characters in the text.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.5500296}},\n",
       "     {'cell_id': 40,\n",
       "      'code': \"train_df['token_text'] = train_df.clean_text.str.lower()\\ntrain_df['token_text'] = train_df.token_text.apply(lambda x: nltk.word_tokenize(x))\\ntest_df['token_text'] = test_df.clean_text.str.lower()\\ntest_df['token_text'] = test_df.token_text.apply(lambda x: nltk.word_tokenize(x))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet tokenizes the lowercased clean text in both the training and testing DataFrames, splitting the text into individual words and storing them in a new column 'token_text'.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.3968303}},\n",
       "     {'cell_id': 41,\n",
       "      'code': 'stopwords = nltk.corpus.stopwords.words(\"english\")\\naddingStopWords = [\\'im\\',\\'get\\',\\'dont\\',\\'got\\',\\'amp\\']\\nstopwords.extend(addingStopWords)\\ntrain_df[\\'token_text\\'] = train_df[\\'token_text\\'].apply(lambda x: [word for word in x if word not in stopwords])\\ntest_df[\\'token_text\\'] = test_df[\\'token_text\\'].apply(lambda x: [word for word in x if word not in stopwords])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet extends the list of English stopwords with additional custom stopwords, then updates the 'token_text' column in both the training and testing DataFrames by removing all stopwords from the tokenized text.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9972505}},\n",
       "     {'cell_id': 42,\n",
       "      'code': \"# since we have a dataset containing text from social media, there might be many spelling mistakes and words which cannot be found in the word lemmatizer corpus, so they would be remained untouched.\\n# To this end we would use the Porter Stemming which just removes affixes of the words\\nfrom nltk.stem.porter import PorterStemmer\\nps = PorterStemmer()\\ntrain_df['token_text'] = train_df['token_text'].apply(lambda x: [ps.stem(word) for word in x ])\\ntest_df['token_text'] = test_df['token_text'].apply(lambda x: [ps.stem(word) for word in x ])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies the Porter Stemming algorithm to the 'token_text' column in both the training and testing DataFrames, reducing words to their stem forms by removing affixes.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.988744}},\n",
       "     {'cell_id': 43,\n",
       "      'code': 'train_df.reset_index(inplace=True, drop=True)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet resets the index of the training DataFrame, dropping the old index and making the dataframe indexing consistent after transformations.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.9991253}},\n",
       "     {'cell_id': 44,\n",
       "      'code': 'from sklearn.feature_extraction.text import TfidfVectorizer',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports the `TfidfVectorizer` class from the `sklearn.feature_extraction.text` module, which is used to transform text data into TF-IDF feature vectors for model training and evaluation.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99927324}},\n",
       "     {'cell_id': 45,\n",
       "      'code': 'def dummy(doc):\\n    return doc\\ntfidf_vect = TfidfVectorizer(analyzer=dummy)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a dummy function to be used in the `TfidfVectorizer` and initializes the vectorizer with this function as the analyzer, permitting direct token input without further processing.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9941847}},\n",
       "     {'cell_id': 46,\n",
       "      'code': \"tfidf_fit = tfidf_vect.fit(train_df['token_text'])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet fits the TF-IDF vectorizer on the tokenized text from the training DataFrame, learning the vocabulary and IDF (Inverse Document Frequency) values from the training data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9996877}},\n",
       "     {'cell_id': 47,\n",
       "      'code': \"matrix_train = tfidf_fit.transform(train_df['token_text'])\\nmatrix_test = tfidf_fit.transform(test_df['token_text'])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet transforms the tokenized text in both the training and testing DataFrames into TF-IDF feature matrices using the fitted vectorizer.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9759149}},\n",
       "     {'cell_id': 48,\n",
       "      'code': 'print(matrix_train.shape)\\nprint(matrix_test.shape)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the shapes of the TF-IDF feature matrices for the training and testing datasets, providing a dimensional overview of the transformed data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9989506}},\n",
       "     {'cell_id': 49,\n",
       "      'code': 'counts_df_train = pd.DataFrame(matrix_train.toarray())\\ncounts_df_test = pd.DataFrame(matrix_test.toarray())',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet converts the TF-IDF feature matrices for the training and testing datasets into pandas DataFrames, facilitating easier manipulation and analysis of the vectorized data.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.9986713}},\n",
       "     {'cell_id': 50,\n",
       "      'code': \"train_df['length'] = train_df.text.apply(lambda x: len(x) - x.count(' '))\\ntest_df['length'] = test_df.text.apply(lambda x: len(x) - x.count(' '))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet adds a new column 'length' to both the training and testing DataFrames, calculating the length of each tweet by counting the number of characters excluding spaces.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9990262}},\n",
       "     {'cell_id': 51,\n",
       "      'code': \"train_df['punct_perc'] = train_df.text.apply(lambda x: sum([1 for char in x if char in non_alpha])/(len(x) - x.count(' '))*100)\\ntest_df['punct_perc'] = test_df.text.apply(lambda x: sum([1 for char in x if char in non_alpha])/(len(x) - x.count(' '))*100)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet adds a new column 'punct_perc' to both the training and testing DataFrames, calculating the percentage of characters in each tweet that are punctuation marks or digits.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9990037}},\n",
       "     {'cell_id': 52,\n",
       "      'code': \"train_df['word_count'] = train_df.token_text.apply(len)\\ntest_df['word_count'] = train_df.token_text.apply(len)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet adds a new column 'word_count' to both the training and testing DataFrames by counting the number of words in the 'token_text' column for each tweet.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9987459}},\n",
       "     {'cell_id': 53,\n",
       "      'code': \"fig, axes = plt.subplots(1,2,figsize=(17,5))\\nsns.histplot(data = train_df, \\n             x= 'length',  \\n             hue = 'target',\\n             element='step',\\n             stat='probability',\\n            bins=40,\\n            ax=axes[0])\\nsns.boxplot(data = train_df, x = 'target', y = 'length',ax=axes[1]);\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates a histogram and a box plot to visualize the distribution and statistical properties of tweet lengths, categorized by the target variable (disaster vs. non-disaster), in the training DataFrame.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9957883}},\n",
       "     {'cell_id': 54,\n",
       "      'code': \"train_df['length_int'] =pd.cut(train_df.length, 14, include_lowest=True)\\n                               #bins=[0, 15, 30, 40, 50,60, 80, 100, 120, 140, 180]\\ntest_df['length_int'] =pd.cut(test_df.length, 14, include_lowest=True)\\n                              #bins=[0, 15, 30, 40, 50,60, 80, 100, 120, 140, 180]\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet creates a new column 'length_int' in both the training and testing DataFrames by binning the 'length' values into 14 intervals, facilitating categorical analysis of tweet lengths.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'relationship',\n",
       "       'subclass_id': 81,\n",
       "       'predicted_subclass_probability': 0.30801016}},\n",
       "     {'cell_id': 55,\n",
       "      'code': \"fig = plt.figure(figsize=(7,5))\\ntrain_df[train_df['target']==0]['length_int'].value_counts(sort=False).plot(kind='bar', alpha = 0.5, color='blue', label = 'No')\\ntrain_df[train_df['target']==1]['length_int'].value_counts(sort=False).plot(kind='bar', alpha = 0.5, color='orange', label = 'Yes')\\nplt.legend(title='Actual Disaster', fontsize=11, title_fontsize=12);\\n                             \",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet generates a bar plot to compare the distribution of tweet lengths, binned into 14 intervals, between non-disaster ('No') and disaster ('Yes') tweets in the training DataFrame.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9890703}},\n",
       "     {'cell_id': 56,\n",
       "      'code': \"fig, axes = plt.subplots(1,2,figsize=(17,5))\\nsns.histplot(data = train_df, \\n             x= 'punct_perc',  \\n             hue = 'target',\\n             element='step',\\n             stat='probability',\\n            bins=40,\\n            ax=axes[0])\\nsns.boxplot(data = train_df, x = 'target', y = 'punct_perc',ax=axes[1]);\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates a histogram and a box plot to visualize the distribution and statistical properties of punctuation percentage in tweets, categorized by the target variable (disaster vs. non-disaster), in the training DataFrame.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99661547}},\n",
       "     {'cell_id': 57,\n",
       "      'code': \"fig, axes = plt.subplots(1,2,figsize=(17,5))\\nsns.histplot(data = train_df, \\n             x= 'word_count',  \\n             hue = 'target',\\n             element='step',\\n             stat='probability',\\n            bins=40,\\n            ax=axes[0])\\nsns.boxplot(data = train_df, x = 'target', y = 'word_count',ax=axes[1]);\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet generates a histogram and a box plot to visualize the distribution and statistical properties of word counts in tweets, categorized by the target variable (disaster vs. non-disaster), in the training DataFrame.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9946801}},\n",
       "     {'cell_id': 58,\n",
       "      'code': \"## data transformation\\nplt.hist(train_df[train_df.target==1]['length']**2.3, bins = 40, color = 'blue', alpha=0.5)\\nplt.hist(train_df[train_df.target==0]['length']**2.3, bins = 40, color = 'red', alpha=0.5);\\n## by transforming the distribution into a bimodal we can notice that the data are more separated\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies a power transformation (raising to the power of 2.3) to the 'length' of tweets and then generates overlaid histograms to visualize the transformed distributions for disaster and non-disaster tweets, indicating improved class separation.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9983096}},\n",
       "     {'cell_id': 59,\n",
       "      'code': \"train_df['length'] = train_df['length']**2.3\\ntest_df['length'] = train_df['length']**2.3\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies a power transformation (raising to the power of 2.3) to the 'length' column in both the training and testing DataFrames, modifying the data to potentially improve class separation.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9994217}},\n",
       "     {'cell_id': 60,\n",
       "      'code': \"plt.hist(train_df['punct_perc']**(1/3), bins = 40);\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies a cube root transformation to the 'punct_perc' column in the training DataFrame and visualizes the transformed distribution through a histogram.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9975884}},\n",
       "     {'cell_id': 61,\n",
       "      'code': \"train_df['punct_perc'] = train_df['punct_perc']**(1/3)\\ntest_df['punct_perc'] = train_df['punct_perc']**(1/3)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies a cube root transformation to the 'punct_perc' column in both the training and testing DataFrames, modifying the data distribution for potentially improved analysis or modeling.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.99934}},\n",
       "     {'cell_id': 62,\n",
       "      'code': '!pip install text2emotion',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet installs the `text2emotion` library, which is used for extracting and analyzing the emotions present in text data.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.99531555}},\n",
       "     {'cell_id': 63,\n",
       "      'code': 'import text2emotion as te',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports the `text2emotion` library, which provides tools for detecting emotions in text.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993248}},\n",
       "     {'cell_id': 64,\n",
       "      'code': \"# assign an emotion to each tweet\\ntrain_df['emotion'] = train_df.text.apply(lambda x: te.get_emotion(x))\\ntest_df['emotion'] = test_df.text.apply(lambda x: te.get_emotion(x))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet assigns an emotion to each tweet in both the training and testing DataFrames by applying the `text2emotion` library's `get_emotion` function to the 'text' column, adding the results to a new 'emotion' column.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.8783199}},\n",
       "     {'cell_id': 65,\n",
       "      'code': \"# exploding the dictionary into 4 different columns, based on the dictionary keys\\ntrain_df = pd.concat([train_df, pd.DataFrame(train_df['emotion'].tolist())], axis =1)\\ntest_df = pd.concat([test_df, pd.DataFrame(test_df['emotion'].tolist())], axis =1)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet splits the 'emotion' dictionary column into four separate columns based on the dictionary keys by using the `pd.DataFrame` function and concatenates these new columns with the original DataFrames, thus enriching the feature set.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'concatenate',\n",
       "       'subclass_id': 11,\n",
       "       'predicted_subclass_probability': 0.99380034}},\n",
       "     {'cell_id': 66,\n",
       "      'code': \"total_emotions = train_df[['Happy', 'Angry', 'Surprise', 'Sad', 'Fear','target']].groupby('target').sum()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet sums the emotion columns ('Happy', 'Angry', 'Surprise', 'Sad', 'Fear') grouped by the target variable (disaster vs. non-disaster tweets), providing aggregated emotion counts for each class.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'groupby',\n",
       "       'subclass_id': 60,\n",
       "       'predicted_subclass_probability': 0.99574137}},\n",
       "     {'cell_id': 67,\n",
       "      'code': \"mean_emotions = train_df[['Happy', 'Angry', 'Surprise', 'Sad', 'Fear','target']].groupby('target').mean()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet calculates the mean values of the emotion columns ('Happy', 'Angry', 'Surprise', 'Sad', 'Fear') grouped by the target variable, providing the average emotion scores for disaster and non-disaster tweets.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'groupby',\n",
       "       'subclass_id': 60,\n",
       "       'predicted_subclass_probability': 0.99561524}},\n",
       "     {'cell_id': 68,\n",
       "      'code': \"fig, axes = plt.subplots(1,2,figsize=(14,5))\\nax1 = total_emotions.plot(kind='bar', ax = axes[0])\\nax1.set_title('Total values of emotion scores per target class')\\nax2 = mean_emotions.plot(kind='bar', ax = axes[1])\\nax2.set_title('Mean Scores of emotions per target class');\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates bar plots to visualize the total values and mean scores of emotion metrics for non-disaster and disaster tweets in the training DataFrame, comparing emotional content between the two target classes.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'time_series',\n",
       "       'subclass_id': 75,\n",
       "       'predicted_subclass_probability': 0.8920648}},\n",
       "     {'cell_id': 69,\n",
       "      'code': \"fig, axes = plt.subplots(2,2,figsize=(20,10))\\nsns.histplot(data = train_df, \\n             x= 'Happy',  \\n             hue = 'target',\\n             element='step',\\n             stat='probability',\\n            bins=40,\\n            ax=axes[0,0])\\nsns.boxplot(data = train_df, x = 'target', y = 'Happy',ax=axes[0,1])\\nsns.histplot(data = train_df, \\n             x= 'Fear',  \\n             hue = 'target',\\n             element='step',\\n             stat='probability',\\n            bins=40,\\n            ax=axes[1,0])\\nsns.boxplot(data = train_df, x = 'target', y = 'Fear',ax=axes[1,1])\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet generates histograms and box plots to visualize the distribution and statistical properties of 'Happy' and 'Fear' emotion scores, categorized by the target variable (disaster vs. non-disaster), in the training DataFrame.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99813986}},\n",
       "     {'cell_id': 70,\n",
       "      'code': \"# From the graphs and tables above we notice that, in cases of actual disasters the tweets have greater fear score\\n# while for the non-disaster the mean score of 'Happy' is higher.\\n# however those variables appear not to have a great importance. They might be dropped from the model.\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This comment summarizes the insights gained from the previous visualizations and tables, noting that 'Fear' scores are higher for disaster tweets and 'Happy' scores for non-disaster tweets, but overall suggesting these variables might be less important for the model and could be dropped.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.57685375}},\n",
       "     {'cell_id': 71,\n",
       "      'code': '!pip install twython\\nfrom nltk.sentiment import SentimentIntensityAnalyzer\\nsia = SentimentIntensityAnalyzer()',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet installs the `twython` library and imports the `SentimentIntensityAnalyzer` from `nltk.sentiment`, initializing it for use in sentiment analysis of text data.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.8762451}},\n",
       "     {'cell_id': 72,\n",
       "      'code': \"train_df['sentiment'] = train_df.text.astype(str).apply(lambda x: sia.polarity_scores(x))\\ntest_df['sentiment'] = test_df.text.astype(str).apply(lambda x: sia.polarity_scores(x))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet calculates sentiment scores for each tweet in both the training and testing DataFrames using the `SentimentIntensityAnalyzer` and stores the results in a new 'sentiment' column.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.9847083}},\n",
       "     {'cell_id': 73,\n",
       "      'code': \"train_df = pd.concat([train_df, pd.DataFrame(train_df['sentiment'].tolist())], axis =1)\\ntest_df = pd.concat([test_df, pd.DataFrame(test_df['sentiment'].tolist())], axis =1)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet splits the 'sentiment' dictionary column into separate columns based on the dictionary keys by using the `pd.DataFrame` function and concatenates these new columns with the original DataFrames, expanding the feature set with sentiment scores.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'concatenate',\n",
       "       'subclass_id': 11,\n",
       "       'predicted_subclass_probability': 0.98916924}},\n",
       "     {'cell_id': 74,\n",
       "      'code': \"mean_sentiment = train_df[['neg', 'neu', 'pos', 'compound','target']].groupby('target').mean()\\ntotal_sentiment = train_df[['neg', 'neu', 'pos', 'compound','target']].groupby('target').sum()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet calculates the mean and total values of the sentiment columns ('neg', 'neu', 'pos', 'compound') grouped by the target variable, providing average and aggregated sentiment scores for disaster and non-disaster tweets.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'groupby',\n",
       "       'subclass_id': 60,\n",
       "       'predicted_subclass_probability': 0.9969091}},\n",
       "     {'cell_id': 75,\n",
       "      'code': \"fig, axes = plt.subplots(1,2,figsize=(14,5))\\nax1 = total_sentiment.plot(kind='bar', ax = axes[0])\\nax1.set_title('Total values of emotion scores per target class')\\nax2 = mean_sentiment.plot(kind='bar', ax = axes[1])\\nax2.set_title('Mean Scores of emotions per target class');\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates bar plots to visualize the total values and mean scores of sentiment metrics for non-disaster and disaster tweets in the training DataFrame, comparing the sentiment distribution between the two target classes.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'time_series',\n",
       "       'subclass_id': 75,\n",
       "       'predicted_subclass_probability': 0.86384803}},\n",
       "     {'cell_id': 76,\n",
       "      'code': \"fig, axes = plt.subplots(1,2,figsize=(17,5))\\nsns.histplot(data = train_df, \\n             x= 'compound',  \\n             hue = 'target',\\n             element='step',\\n             stat='probability',\\n            bins=40,\\n            ax=axes[0])\\nsns.boxplot(data = train_df, x = 'target', y = 'compound',ax=axes[1]);\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet generates a histogram and a box plot to visualize the distribution and statistical properties of the 'compound' sentiment score, categorized by the target variable (disaster vs. non-disaster), in the training DataFrame.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9953399}},\n",
       "     {'cell_id': 77,\n",
       "      'code': \"fig, axes = plt.subplots(2,2,figsize=(20,10))\\nsns.histplot(data = train_df, \\n             x= 'neg',  \\n             hue = 'target',\\n             element='step',\\n             stat='probability',\\n            bins=40,\\n            ax=axes[0,0])\\nsns.boxplot(data = train_df, x = 'target', y = 'neg',ax=axes[0,1])\\nsns.histplot(data = train_df, \\n             x= 'pos',  \\n             hue = 'target',\\n             element='step',\\n             stat='probability',\\n            bins=40,\\n            ax=axes[1,0])\\nsns.boxplot(data = train_df, x = 'target', y = 'pos',ax=axes[1,1])\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet generates histograms and box plots to visualize the distribution and statistical properties of 'neg' and 'pos' sentiment scores, categorized by the target variable (disaster vs. non-disaster), in the training DataFrame.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99770266}},\n",
       "     {'cell_id': 78,\n",
       "      'code': \"#full_train_df = pd.concat([train_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int'], axis=1), counts_df_train], axis=1)\\nfull_train_df = pd.concat([train_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int','Happy', 'Angry', 'Surprise', 'Sad', 'Fear'], axis=1), counts_df_train], axis=1)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet creates a new DataFrame `full_train_df` by concatenating the training DataFrame (with specified columns dropped) and the TF-IDF feature DataFrame, resulting in a more focused feature set for model training.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.64140093}},\n",
       "     {'cell_id': 79,\n",
       "      'code': \"#full_test_df = pd.concat([test_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int'], axis=1), counts_df_test], axis=1)\\nfull_test_df = pd.concat([test_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int','Happy', 'Angry', 'Surprise', 'Sad', 'Fear'], axis=1), counts_df_test], axis=1)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet creates a new DataFrame `full_test_df` by concatenating the testing DataFrame (with specified columns dropped) and the TF-IDF feature DataFrame, resulting in a final feature set for model evaluation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.699594}},\n",
       "     {'cell_id': 80,\n",
       "      'code': '# deleting unnecessary dataframes to save memory\\ndel train_df\\ndel test_df\\ndel counts_df_train\\ndel counts_df_test',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet deletes intermediate DataFrames that are no longer needed (`train_df`, `test_df`, `counts_df_train`, and `counts_df_test`) to free up memory.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.9989153}},\n",
       "     {'cell_id': 81,\n",
       "      'code': 'from sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import GridSearchCV',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports the `RandomForestClassifier` from `sklearn.ensemble` and `GridSearchCV` from `sklearn.model_selection`, which are used for building and tuning a machine learning model through hyperparameter optimization.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99927956}},\n",
       "     {'cell_id': 82,\n",
       "      'code': \"forest =  RandomForestClassifier()\\nparam = {'n_estimators':[200, 500],\\n        'max_depth':[200, 300]}\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes a `RandomForestClassifier` and defines a parameter grid for hyperparameter tuning, specifying the number of estimators and maximum depth values to be evaluated during training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.99327236}},\n",
       "     {'cell_id': 83,\n",
       "      'code': 'gs = GridSearchCV(forest, param, cv=3)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes a `GridSearchCV` object to perform hyperparameter tuning on the `RandomForestClassifier` using the specified parameter grid and 3-fold cross-validation.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_on_grid',\n",
       "       'subclass_id': 6,\n",
       "       'predicted_subclass_probability': 0.40436822}},\n",
       "     {'cell_id': 84,\n",
       "      'code': \"gs_fit = gs.fit(full_train_df.drop(['target','id'], axis=1), full_train_df['target'])\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet fits the `GridSearchCV` object to the training data, performing hyperparameter tuning using the specified parameter grid to find the best model configuration.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9996861}},\n",
       "     {'cell_id': 85,\n",
       "      'code': \"pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet creates a DataFrame from the cross-validation results of the grid search, sorting the entries by the mean test score in descending order to identify the best-performing model configurations.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'sort_values',\n",
       "       'subclass_id': 9,\n",
       "       'predicted_subclass_probability': 0.9971615}},\n",
       "     {'cell_id': 86,\n",
       "      'code': 'rf =  RandomForestClassifier(n_estimators=200, max_depth=200)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes a `RandomForestClassifier` with the chosen hyperparameters of 200 estimators and a maximum depth of 200, based on the best-performing model configuration from the grid search.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9980399}},\n",
       "     {'cell_id': 87,\n",
       "      'code': \"rf.fit(full_train_df.drop(['target','id'], axis=1), full_train_df['target'])\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the `RandomForestClassifier` on the training dataset, using the selected features and target variable, to build the final model.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99969363}},\n",
       "     {'cell_id': 88,\n",
       "      'code': \"predictions_rf = rf.predict(full_test_df.drop(['id'], axis=1))\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet generates predictions using the trained `RandomForestClassifier` on the testing dataset, excluding the 'id' column from the feature set.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.9943375}},\n",
       "     {'cell_id': 89,\n",
       "      'code': 'output = pd.DataFrame({\\'id\\': full_test_df.id, \\'target\\': predictions_rf})\\noutput.to_csv(\\'my_submission.csv\\', index=False)\\nprint(\"Your submission was successfully saved!\")',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet creates a DataFrame with the 'id' and predicted 'target' values, then saves it to a CSV file named 'my_submission.csv' and prints a confirmation message.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9990031}}],\n",
       "    'notebook_id': 16},\n",
       "   'notebook_id': 16},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': \"import numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n        if filename == 'train.csv':\\n            dataset_train = pd.read_csv(os.path.join(dirname, filename))\\n        elif filename == 'test.csv':\\n            dataset_test = pd.read_csv(os.path.join(dirname, filename))\\n        else:\\n            dataset_sample = pd.read_csv(os.path.join(dirname, filename))\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code imports necessary libraries and reads CSV files into pandas DataFrames from a given directory.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.999468}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'dataset_test_original = dataset_test',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code creates a copy of the `dataset_test` DataFrame and assigns it to `dataset_test_original`.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.40974942}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'dataset_test_original',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code outputs the `dataset_test_original` DataFrame to display its content.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99960965}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"import re\\nimport nltk\\nnltk.download('stopwords')\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem.porter import PorterStemmer\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports the necessary libraries for text processing, downloads the stopwords dataset, and loads the PorterStemmer for stemming words.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.9607691}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'index_train = dataset_train.index\\nindex_test = dataset_test.index\\ntrain_len = index_train\\ntest_len = index_test',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code extracts the index ranges of the training and testing datasets and assigns them to respective variables.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.99675506}},\n",
       "     {'cell_id': 5,\n",
       "      'code': \"dataset_train = dataset_train[['text','target']]\\nprint(dataset_train)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code selects only the 'text' and 'target' columns from the training dataset and prints the resulting DataFrame.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.3228975}},\n",
       "     {'cell_id': 6,\n",
       "      'code': \"corpus = []\\nfor i in range(0, len(train_len)):\\n  review = re.sub('[^a-zA-Z]', ' ', dataset_train['text'][i])\\n  review = review.lower()\\n  review = review.split()\\n  ps = PorterStemmer()\\n  all_stopwords = stopwords.words('english')\\n  all_stopwords.remove('not')\\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\\n  review = ' '.join(review)\\n  corpus.append(review)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code processes the text data in the training dataset by cleaning, lowercasing, removing stopwords (except 'not'), and stemming, and then appends the cleaned text to a new list called `corpus`.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.91630965}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'from sklearn.feature_extraction.text import CountVectorizer\\ncv = CountVectorizer(max_features = 1500)\\nX = cv.fit_transform(corpus).toarray()\\ny = dataset_train.iloc[:, -1].values',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code vectorizes the processed text data using CountVectorizer to create a feature matrix `X` with a maximum of 1500 features, and extracts the target variable into `y`.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.99771535}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'from sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code splits the feature matrix `X` and target variable `y` into training and testing sets, using 20% of the data for testing and ensuring reproducibility with a fixed random state.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.99815947}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'from sklearn.naive_bayes import GaussianNB\\nclassifier = GaussianNB()\\nclassifier.fit(X_train, y_train)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code creates a Gaussian Naive Bayes classifier and fits it to the training data `X_train` and `y_train`.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9989604}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'y_pred = classifier.predict(X_test)\\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code uses the trained Gaussian Naive Bayes classifier to predict the test set labels and prints the predicted and actual labels side by side for comparison.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.99374145}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'from sklearn.metrics import confusion_matrix, accuracy_score\\ncm = confusion_matrix(y_test, y_pred)\\nprint(cm)\\naccuracy_score(y_test, y_pred)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code computes and prints the confusion matrix and the accuracy score for the model's predictions on the test set.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.996639}},\n",
       "     {'cell_id': 12,\n",
       "      'code': \"dataset_test = dataset_test[['text']]\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code selects only the 'text' column from the test dataset.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.8538315}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'print(len(test_len))',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code prints the number of rows in the test dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99938715}},\n",
       "     {'cell_id': 14,\n",
       "      'code': \"testcorpus = []\\nfor i in range(0, len(test_len)):\\n  review = re.sub('[^a-zA-Z]', ' ', dataset_test['text'][i])\\n  review = review.lower()\\n  review = review.split()\\n  ps = PorterStemmer()\\n  all_stopwords = stopwords.words('english')\\n  all_stopwords.remove('not')\\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\\n  review = ' '.join(review)\\n  testcorpus.append(review)\\n\\nxtest = cv.transform(testcorpus).toarray()\\ntest_dataframe_prediction = classifier.predict(xtest)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code processes the text data in the test dataset similarly to the training data, creates a feature matrix using the same CountVectorizer, and then predicts the test dataset labels using the trained classifier.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.39147574}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'print(test_dataframe_prediction)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code prints the predicted labels for the processed test dataset.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.9981881}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'print(dataset_test.head())',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code prints the first few rows of the test dataset for inspection.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99974805}},\n",
       "     {'cell_id': 17,\n",
       "      'code': \"my_submission = pd.DataFrame({'Id': dataset_test_original.id, 'target': test_dataframe_prediction})\\nmy_submission.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code creates a DataFrame with prediction results and corresponding IDs, and then saves it to a CSV file named 'submission.csv' without including the index.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9993311}}],\n",
       "    'notebook_id': 17},\n",
       "   'notebook_id': 17},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import pandas as pd\\nimport numpy as np\\nfrom sklearn.metrics import f1_score',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': \"This code snippet imports necessary libraries, including Pandas and NumPy for data manipulation and sklearn's f1_score for evaluation metric calculation.\",\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99931085}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"train=pd.read_csv('../input/nlp-getting-started/train.csv')\\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads training and testing data from CSV files into Pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99974996}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'train.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the training DataFrame to give an initial look at the dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997507}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'test.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the testing DataFrame to examine its structure and contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997483}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'print(train.shape)\\nprint(test.shape)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the dimensions (number of rows and columns) of the training and testing DataFrames to understand the size of the datasets.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.99933213}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'print(train.info())\\nprint(test.info())',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints a summary of the training and testing DataFrames, including the data types and non-null counts of each column, to assess the structure and completeness of the datasets.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9994165}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'train.target.value_counts()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the distribution of the target variable in the training DataFrame to understand the class balance.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.99950993}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"import nltk\\nnltk.download('punkt')\\nnltk.download('stopwords')\\nimport re\\n!pip install contractions\\nimport contractions\\nfrom nltk.stem import SnowballStemmer\\nfrom nltk.stem import WordNetLemmatizer\\nnltk.download('wordnet')\\n!pip install pyspellchecker\\nfrom spellchecker import SpellChecker\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet installs and imports additional NLP-related libraries and resources, such as NLTK, contractions handling, stemming, lemmatization, and spell checking tools, which are necessary for preprocessing text data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.799391}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"stop_words=nltk.corpus.stopwords.words('english')\\ni=0\\n#sc=SpellChecker()\\n#data=pd.concat([train,test])\\nwnl=WordNetLemmatizer()\\nstemmer=SnowballStemmer('english')\\nfor doc in train.text:\\n    doc=re.sub(r'https?://\\\\S+|www\\\\.\\\\S+','',doc)\\n    doc=re.sub(r'<.*?>','',doc)\\n    doc=re.sub(r'[^a-zA-Z\\\\s]','',doc,re.I|re.A)\\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\\n    doc=contractions.fix(doc)\\n    tokens=nltk.word_tokenize(doc)\\n    filtered=[token for token in tokens if token not in stop_words]\\n    doc=' '.join(filtered)\\n    train.text[i]=doc\\n    i+=1\\ni=0\\nfor doc in test.text:\\n    doc=re.sub(r'https?://\\\\S+|www\\\\.\\\\S+','',doc)\\n    doc=re.sub(r'<.*?>','',doc)\\n    doc=re.sub(r'[^a-zA-Z\\\\s]','',doc,re.I|re.A)\\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\\n    doc=contractions.fix(doc)\\n    tokens=nltk.word_tokenize(doc)\\n    filtered=[token for token in tokens if token not in stop_words]\\n    doc=' '.join(filtered)\\n    test.text[i]=doc\\n    i+=1\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet preprocesses the 'text' column in both the training and testing DataFrames by removing URLs, HTML tags, non-alphabetic characters, expanding contractions, tokenizing, lemmatizing, and filtering out stopwords.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.54864067}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'train.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the processed training DataFrame to observe the effects of the preprocessing steps applied to the text data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997507}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'test.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the processed testing DataFrame to inspect the changes made by the preprocessing procedures.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997483}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'from sklearn.feature_extraction.text import CountVectorizer\\ncv=CountVectorizer(ngram_range=(1,1)) \\n\\n#    ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, \\n#    and (2, 2) means only bigrams.\\n\\ncv_matrix=cv.fit_transform(train.text).toarray()\\ntrain_df=pd.DataFrame(cv_matrix,columns=cv.get_feature_names())\\ntest_df=pd.DataFrame(cv.transform(test.text).toarray(),columns=cv.get_feature_names())\\ntrain_df.head()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet converts the preprocessed text data in the training and testing DataFrames into a matrix of token counts (unigrams) using CountVectorizer, and then creates new DataFrames from these matrices.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.17395471}},\n",
       "     {'cell_id': 12,\n",
       "      'code': 'from sklearn.feature_extraction.text import TfidfVectorizer\\ntfidf=TfidfVectorizer(ngram_range=(1,1),use_idf=True)\\nmat=tfidf.fit_transform(train.text).toarray()\\ntrain_df=pd.DataFrame(mat,columns=tfidf.get_feature_names())\\ntest_df=pd.DataFrame(tfidf.transform(test.text).toarray(),columns=tfidf.get_feature_names())\\ntrain_df.head()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet transforms the preprocessed text data in the training and testing DataFrames into TF-IDF feature matrices (using unigrams) with TfidfVectorizer and creates new DataFrames from these matrices.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9525827}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'from sklearn.linear_model import LogisticRegression\\nmodel=LogisticRegression()\\nmodel.fit(train_df,train.target)\\nprint(f1_score(model.predict(train_df),train.target))\\npred=model.predict(test_df)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains a Logistic Regression model using the transformed training data, evaluates it with the F1 score on the training set, and then makes predictions on the transformed testing data.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.5177371}},\n",
       "     {'cell_id': 14,\n",
       "      'code': \"pd.DataFrame({\\n    'id':test.id,\\n    'target':pred\\n}).to_csv('submission.csv',index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet creates a new DataFrame with the test IDs and the model's predictions, and then exports it as a CSV file named 'submission.csv'.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9992506}}],\n",
       "    'notebook_id': 18},\n",
       "   'notebook_id': 18},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom nltk.corpus import stopwords\\nfrom nltk.util import ngrams\\nfrom nltk.stem import WordNetLemmatizer\\nimport re\\nfrom textblob import TextBlob\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.model_selection import GridSearchCV\\nimport tensorflow as tf\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.utils import to_categorical\\nfrom gensim.models import Word2Vec\\nfrom gensim.models.keyedvectors import KeyedVectors\\nimport time\\nfrom keras.layers import Dense, Input, Flatten, Dropout\\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\\nfrom keras.models import Sequential\\nfrom keras import losses\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.models import Model',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports various libraries and modules necessary for data manipulation, visualization, natural language processing, machine learning, and deep learning tasks.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993228}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\\nsubmit_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the training and test datasets from CSV files into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9997477}},\n",
       "     {'cell_id': 2,\n",
       "      'code': \"train_data[train_data['text'].isna()]\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet identifies and displays rows in the training dataset where the 'text' column contains missing values.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.88515306}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'train_data.info()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet provides a concise summary of the training dataset, including the number of non-null entries and data types for each column.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.99936634}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"train_data.groupby('target').count()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet groups the training dataset by the 'target' column and counts the number of entries in each group.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'groupby',\n",
       "       'subclass_id': 60,\n",
       "       'predicted_subclass_probability': 0.9970409}},\n",
       "     {'cell_id': 5,\n",
       "      'code': '%matplotlib inline',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet sets up the Matplotlib environment to display plots inline within the Jupyter notebook.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9991172}},\n",
       "     {'cell_id': 6,\n",
       "      'code': \"piedata = train_data['target']\\nplt.figure(figsize=(6,6))\\npiedata.value_counts().plot(kind = 'pie',autopct = '%.2f%%')\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet creates a pie chart to visualize the distribution of the 'target' variable in the training data, displaying the counts as percentages.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9974884}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"num_words_0 = train_data[train_data['target']==0]['text'].apply(lambda x: len(x.split()))\\nnum_words_1 = train_data[train_data['target']==1]['text'].apply(lambda x: len(x.split()))\\nplt.figure(figsize=(12,6))\\nsns.kdeplot(num_words_0, shade=True, color = 'b').set_title('Kernel distribution of number of words')\\nsns.kdeplot(num_words_1, shade=True, color = 'r')\\nplt.legend(labels=['0_no disaster', '1_disaster'])\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet calculates the number of words in the 'text' column for each class in the 'target' variable and then creates a kernel density plot to visualize the distribution of these word counts for each class.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.97927135}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"len_word_0 = train_data[train_data['target']==0]['text'].str.split().map(lambda x: [len(i) for i in x])\\nave_len_0 = len_word_0.map(lambda x: np.mean(x))\\nlen_word_1 = train_data[train_data['target']==1]['text'].str.split().map(lambda x: [len(i) for i in x])\\nave_len_1 = len_word_1.map(lambda x: np.mean(x))\\nplt.figure(figsize=(12,6))\\nsns.kdeplot(ave_len_0, shade=True, color='b').set_title('Kernel distribution of average words lenth')\\nsns.kdeplot(ave_len_1, shade=True, color='r')\\nplt.legend(labels=['0_no disaster', '1_disaster'])\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet calculates the average word length in the 'text' column for each class in the 'target' variable and then creates a kernel density plot to visualize the distribution of these average word lengths for each class.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.98665464}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'data = pd.concat([train_data, submit_data])\\ndata.shape',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet concatenates the training and test datasets into a single DataFrame and prints the shape of the resulting DataFrame.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'concatenate',\n",
       "       'subclass_id': 11,\n",
       "       'predicted_subclass_probability': 0.9939737}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'data[\\'text\\'] = data[\\'text\\'].apply(lambda x: re.sub(re.compile(r\\'https?\\\\S+\\'), \\'\\', x))\\ndata[\\'text\\'] = data[\\'text\\'].apply(lambda x: re.sub(re.compile(r\\'[\\\\//:,.!?@&\\\\-\\\\\\'\\\\`\\\\\"\\\\_\\\\n\\\\#]\\'), \\' \\', x))\\ndata[\\'text\\'] = data[\\'text\\'].apply(lambda x: re.sub(re.compile(r\\'<.*?>\\'), \\'\\', x))\\ndata[\\'text\\'] = data[\\'text\\'].apply(lambda x: re.sub(re.compile(\"[\"\\n                           u\"\\\\U0001F600-\\\\U0001F64F\"  \\n                           u\"\\\\U0001F300-\\\\U0001F5FF\"  \\n                           u\"\\\\U0001F680-\\\\U0001F6FF\"  \\n                           u\"\\\\U0001F1E0-\\\\U0001F1FF\"  \\n                           u\"\\\\U00002702-\\\\U000027B0\"\\n                           u\"\\\\U000024C2-\\\\U0001F251\"\\n                           \"]+\", flags=re.UNICODE), \\'\\', x))\\ndata[\\'text\\'] = data[\\'text\\'].apply(lambda x: re.sub(re.compile(r\\'\\\\d\\'), \\'\\', x))\\ndata[\\'text\\'] = data[\\'text\\'].apply(lambda x: re.sub(re.compile(r\\'[^\\\\w]\\'), \\' \\', x))\\ndata[\\'text\\'] = data[\\'text\\'].str.lower()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet performs text preprocessing on the 'text' column by removing URLs, special characters, HTML tags, emojis, digits, and converting all text to lowercase.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9965412}},\n",
       "     {'cell_id': 11,\n",
       "      'code': \"'''\\ntext_series = data.loc[:,'text']\\nfor i in range(len(text_series)):\\n    content = text_series.iloc[i]\\n    textblob = TextBlob(content)\\n    text_series.iloc[i] = textblob.correct()\\n'''\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This commented-out code snippet intends to correct spelling mistakes in the 'text' column using the TextBlob library, although the actual operation is not executed due to being commented out.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.9023682}},\n",
       "     {'cell_id': 12,\n",
       "      'code': \"clean_train = data[0:train_data.shape[0]]\\nclean_submit = data[train_data.shape[0]:-1]\\n\\nX_train, X_test, y_train, y_test = train_test_split(clean_train['text'], clean_train['target'],\\n                                                   test_size = 0.2, random_state = 4)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet splits the concatenated DataFrame back into cleaned training and test datasets, then further splits the training data into training and validation sets by extracting the 'text' and 'target' columns.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.9978915}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'def tfidf(words):\\n    tfidf_vectorizer = TfidfVectorizer()\\n    data_feature = tfidf_vectorizer.fit_transform(words)\\n    return data_feature, tfidf_vectorizer\\n\\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train.tolist())\\nX_test_tfidf = tfidf_vectorizer.transform(X_test.tolist())',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function to convert text into TF-IDF features, then applies the function to the training and test sets to create and transform these features.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9976406}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'X_train_tfidf.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the shape of the TF-IDF transformed training dataset to provide insight into the number of samples and features.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9994598}},\n",
       "     {'cell_id': 15,\n",
       "      'code': \"lr_tfidf = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', n_jobs = -1)\\nlr_tfidf.fit(X_train_tfidf, y_train)\\ny_predicted_lr = lr_tfidf.predict(X_test_tfidf)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains a Logistic Regression model using the TF-IDF transformed training dataset and generates predictions on the TF-IDF transformed test dataset.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.6878745}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'def score_metrics(y_test, y_predicted):\\n    accuracy = accuracy_score(y_test, y_predicted)\\n    precision = precision_score(y_test, y_predicted)\\n    recall = recall_score(y_test, y_predicted)\\n    print(\"accuracy = %0.3f, precision = %0.3f, recall = %0.3f\" % (accuracy, precision, recall))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet defines a function to calculate and print the accuracy, precision, and recall metrics for model evaluation.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9981325}},\n",
       "     {'cell_id': 17,\n",
       "      'code': 'score_metrics(y_test, y_predicted_lr)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet calls the `score_metrics` function to evaluate the Logistic Regression model's performance on the test dataset by printing accuracy, precision, and recall.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9981065}},\n",
       "     {'cell_id': 18,\n",
       "      'code': \"def plot_confusion_matrix(y_test, y_predicted, title='Confusion Matrix'):\\n    cm = confusion_matrix(y_test, y_predicted)\\n    plt.figure(figsize=(8,6))\\n    sns.heatmap(cm,annot=True, fmt='.20g')\\n    plt.title(title)\\n    plt.ylabel('True label')\\n    plt.xlabel('Predicted label')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet defines a function to plot a confusion matrix using Seaborn's heatmap for visual evaluation of the model's prediction performance.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'heatmap',\n",
       "       'subclass_id': 80,\n",
       "       'predicted_subclass_probability': 0.7525936}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'plot_confusion_matrix(y_test, y_predicted_lr)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet calls the `plot_confusion_matrix` function to visualize the confusion matrix of the Logistic Regression model's predictions on the test dataset.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'plot_predictions',\n",
       "       'subclass_id': 56,\n",
       "       'predicted_subclass_probability': 0.76843596}},\n",
       "     {'cell_id': 20,\n",
       "      'code': \"# fail to sort and plot the top 10 most important features in disaster and non-disaster text\\n'''\\nindex_to_word = [(v,k) for k,v in tfidf_vectorizer.vocabulary_.items()]\\nsorted(index_to_word, key=lambda x: x[0], reverse=True)\\n'''\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This commented-out code snippet aims to sort and display the top 10 most important features in disaster and non-disaster text based on their TF-IDF scores, but the actual sorting and plotting are not executed because the code is commented out.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.8911329}},\n",
       "     {'cell_id': 21,\n",
       "      'code': \"pipeline = Pipeline([\\n    ('clf', DecisionTreeClassifier(splitter='random', class_weight='balanced'))\\n])\\nparameters = {\\n    'clf__max_depth':(150,160,165),\\n    'clf__min_samples_split':(18,20,23),\\n    'clf__min_samples_leaf':(5,6,7)\\n}\\n\\ndf_tfidf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=-1, scoring='f1')\\ndf_tfidf.fit(X_train_tfidf, y_train)\\n\\nprint(df_tfidf.best_estimator_.get_params())\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet sets up a pipeline with a Decision Tree Classifier and uses GridSearchCV to perform hyperparameter tuning on the TF-IDF transformed training data, then prints the best model parameters.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_on_grid',\n",
       "       'subclass_id': 6,\n",
       "       'predicted_subclass_probability': 0.9916215}},\n",
       "     {'cell_id': 22,\n",
       "      'code': 'y_predicted_dt = df_tfidf.predict(X_test_tfidf)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet makes predictions on the TF-IDF transformed test dataset using the best Decision Tree model obtained from the GridSearchCV tuning.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.99451977}},\n",
       "     {'cell_id': 23,\n",
       "      'code': 'score_metrics(y_test, y_predicted_dt)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet calls the `score_metrics` function to evaluate the Decision Tree model's performance on the test dataset by printing accuracy, precision, and recall.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9980464}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'plot_confusion_matrix(y_test, y_predicted_dt)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet calls the `plot_confusion_matrix` function to visualize the confusion matrix of the Decision Tree model's predictions on the test dataset.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'plot_predictions',\n",
       "       'subclass_id': 56,\n",
       "       'predicted_subclass_probability': 0.7584198}},\n",
       "     {'cell_id': 25,\n",
       "      'code': '!pip install gensim -i http://pypi.douban.com/simple --trusted-host pypi.douban.com',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet installs the `gensim` library from a specified PyPI mirror for natural language processing tasks.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.9962657}},\n",
       "     {'cell_id': 26,\n",
       "      'code': 'import requests\\nurl = \\'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\\'\\nfilename = url.split(\\'/\\')[-1]\\nr = requests.get(url)\\nwith open(filename, \"wb\") as file:\\n        file.write(r.content)\\n        \\n!ls',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet downloads the pre-trained Google News Word2Vec model file and lists the contents of the current directory.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'list_files',\n",
       "       'subclass_id': 88,\n",
       "       'predicted_subclass_probability': 0.48958522}},\n",
       "     {'cell_id': 27,\n",
       "      'code': \"stop_words = stopwords.words('english')\\nfor word in ['us','no','yet']:\\n    stop_words.append(word)\\n\\ndata_list = []\\ntext_series = data['text']\\nfor i in range(len(text_series)):\\n    content = text_series.iloc[i]\\n    cutwords = [word for word in content.split(' ') if word not in  stop_words if len(word) != 0]\\n    data_list.append(cutwords)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet processes each text entry in the dataset by removing the stopwords and converts the cleaned text into a list of words for further analysis.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9944119}},\n",
       "     {'cell_id': 28,\n",
       "      'code': 'for i in range(len(data_list)):\\n    content = data_list[i]\\n    if len(content) <1:\\n        print(i)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet checks for and prints the indices of any entries in the `data_list` that are empty after stopword removal and text preprocessing.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.8840107}},\n",
       "     {'cell_id': 29,\n",
       "      'code': 'data_list[7626]',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet retrieves and displays the processed text content at index 7626 in the `data_list`.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9993057}},\n",
       "     {'cell_id': 30,\n",
       "      'code': \"'''\\nstarttime = time.time()\\nword2vec_model = Word2Vec(data_list, size=300, iter=10, min_count=10)\\nusedtime = time.time() - starttime\\nprint('It took %.2fseconds to train word2vec' %usedtime)\\n'''\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This commented-out code snippet intends to train a Word2Vec model on the processed text data, measuring and printing the time taken for the training process, but the actual operation is not executed due to being commented out.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.9858438}},\n",
       "     {'cell_id': 31,\n",
       "      'code': \"import gensim\\nword2vec_path='./GoogleNews-vectors-negative300.bin.gz'\\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet loads the pre-trained Google News Word2Vec model in binary format using Gensim.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.99531686}},\n",
       "     {'cell_id': 32,\n",
       "      'code': \"word2vec_model.wv['earthquake'].shape\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet retrieves and prints the shape of the Word2Vec embedding vector for the word 'earthquake' to verify its dimensionality.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9989441}},\n",
       "     {'cell_id': 33,\n",
       "      'code': 'def get_textVector(data_list, word2vec, textsVectors_list):\\n    for i in range(len(data_list)):\\n        words_perText = data_list[i]\\n        if len(words_perText) < 1:\\n            words_vector = [np.zeros(300)]\\n        else:\\n            words_vector = [word2vec.wv[k]  if k in word2vec_model else  np.zeros(300) for k in words_perText]\\n        text_vector = np.array(words_vector).mean(axis=0)\\n        textsVectors_list.append(text_vector)\\n    return textsVectors_list',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function to generate text vectors by averaging the Word2Vec embeddings of words in each text entry, appending the resulting vectors to a provided list.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.966133}},\n",
       "     {'cell_id': 34,\n",
       "      'code': 'textsVectors_list = []\\nget_textVector(data_list, word2vec_model, textsVectors_list)\\nX = np.array(textsVectors_list)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet applies the `get_textVector` function to the processed text data and converts the resulting list of text vectors into a NumPy array for further analysis.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.91989833}},\n",
       "     {'cell_id': 35,\n",
       "      'code': 'pd.isnull(X).any()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet checks if there are any null values in the NumPy array `X` containing the text vectors.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.9978543}},\n",
       "     {'cell_id': 36,\n",
       "      'code': \"word2vec_X = X[0:train_data.shape[0]]\\ny = data['target'][0:train_data.shape[0]]\\nword2vec_submit = X[train_data.shape[0]:-1]\\n\\nX_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(word2vec_X, y,\\n                                                   test_size = 0.2, random_state = 4)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the word2vec-encoded data into training and test sets and then further splits the training data into training and validation sets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.9981791}},\n",
       "     {'cell_id': 37,\n",
       "      'code': 'print(X_train_word2vec.shape, y_train_word2vec.shape)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the shapes of the training data and labels for the word2vec-encoded dataset to confirm their dimensions.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9980696}},\n",
       "     {'cell_id': 38,\n",
       "      'code': \"word2vec_lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', n_jobs = -1)\\nword2vec_lr.fit(X_train_word2vec, y_train_word2vec)\\ny_predicted_word2vec_lr = word2vec_lr.predict(X_test_word2vec)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains a Logistic Regression model using the word2vec-encoded training dataset and generates predictions on the word2vec-encoded test dataset.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.57594216}},\n",
       "     {'cell_id': 39,\n",
       "      'code': 'score_metrics(y_test_word2vec, y_predicted_word2vec_lr)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet calls the `score_metrics` function to evaluate the logistic regression model trained on word2vec-encoded data by printing accuracy, precision, and recall.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9982146}},\n",
       "     {'cell_id': 40,\n",
       "      'code': 'plot_confusion_matrix(y_test_word2vec, y_predicted_word2vec_lr)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet calls the `plot_confusion_matrix` function to visualize the confusion matrix for the Logistic Regression model's predictions on the word2vec-encoded test dataset.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'plot_predictions',\n",
       "       'subclass_id': 56,\n",
       "       'predicted_subclass_probability': 0.84631115}},\n",
       "     {'cell_id': 41,\n",
       "      'code': 'compare_list = []\\nfor (i,j) in zip(y_test_word2vec, y_predicted_word2vec_lr):\\n    k = i - j\\n    compare_list.append(k)\\n\\nwrong_num = [i for i,j in enumerate(compare_list) if j != 0]\\ntext_series[0:train_data.shape[0]][wrong_num]',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet identifies and lists the indices where the logistic regression model made incorrect predictions on the word2vec-encoded test dataset, and displays the corresponding text entries.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.96445125}},\n",
       "     {'cell_id': 42,\n",
       "      'code': 'lenlen = []\\nfor i in range(len(data_list)):\\n    content = data_list[i]\\n    perlen = len(content)\\n    lenlen.append(perlen)\\nprint(max(lenlen))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet calculates and prints the maximum length of text entries (in terms of word count) in the preprocessed `data_list`.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.7533818}},\n",
       "     {'cell_id': 43,\n",
       "      'code': 'max_sequence_length = 26\\nembedding_dim = 300',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet sets two constants: `max_sequence_length` to 26 and `embedding_dim` to 300, likely for use in modeling or text preprocessing steps.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9975586}},\n",
       "     {'cell_id': 44,\n",
       "      'code': \"tokenizer = Tokenizer()\\ntokenizer.fit_on_texts(data_list)\\nsequences = tokenizer.texts_to_sequences(data_list)\\nword_index = tokenizer.word_index\\ncnn_data = pad_sequences(sequences, maxlen = max_sequence_length)\\ncnn_label = to_categorical(np.asarray(train_data['target']))\\nprint('len of word_index:', len(word_index))\\nprint('shape of data tensor:', cnn_data.shape)\\nprint('shape of label tensoe:', cnn_label.shape)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet tokenizes the text data, converts it into sequences of integers, pads these sequences to a fixed maximum length, and converts the training targets to categorical format while printing word index size, data tensor shape, and label tensor shape.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.6572918}},\n",
       "     {'cell_id': 45,\n",
       "      'code': 'trainCNN_data = cnn_data[0:train_data.shape[0]]\\nX_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(trainCNN_data, cnn_label,\\n                                                   test_size = 0.2, random_state = 4)\\nX_cnn, X_val_cnn, y_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train_cnn,\\n                                                   test_size = 0.2, random_state = 4)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the CNN-preprocessed data into training and test sets, and then further splits the training data into training and validation sets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.99836665}},\n",
       "     {'cell_id': 46,\n",
       "      'code': \"CNNmodel = Sequential()\\nCNNmodel.add(Embedding(len(word_index)+1, embedding_dim, input_length = max_sequence_length))\\nCNNmodel.add(Conv1D(filters=250, kernel_size=3, strides=1, padding='valid', activation = 'relu'))\\nCNNmodel.add(MaxPooling1D(pool_size=3))\\nCNNmodel.add(Flatten())\\nCNNmodel.add(Dense(embedding_dim, activation='relu'))\\nCNNmodel.add(Dropout(0.8))\\nCNNmodel.add(Dense(cnn_label.shape[1], activation='sigmoid'))\\n\\nCNNmodel.summary()\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet constructs and summarizes a sequential Convolutional Neural Network (CNN) model for text classification, including embedding, convolution, pooling, and dense layers, with dropout for regularization.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9960479}},\n",
       "     {'cell_id': 47,\n",
       "      'code': \"CNNmodel.compile(optimizer='adam', loss=losses.binary_crossentropy, metrics=['accuracy'])\\nhistory = CNNmodel.fit(X_cnn, y_cnn, epochs=3, validation_data=(X_val_cnn, y_val_cnn))\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet compiles the CNN model with the Adam optimizer and binary cross-entropy loss function, then trains the model for three epochs using the training and validation datasets, capturing the training history.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.925682}},\n",
       "     {'cell_id': 48,\n",
       "      'code': \"plt.plot(history.history['accuracy'], label='accuracy')\\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.ylim([0.5,1])\\nplt.legend()\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet plots the training and validation accuracy over each epoch to visualize the CNN model's performance during training.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.9901933}},\n",
       "     {'cell_id': 49,\n",
       "      'code': \"test_loss, test_acc = CNNmodel.evaluate(X_test_cnn, y_test_cnn, verbose=2)\\nprint('test loss:',test_loss)\\nprint('test acc:',test_acc)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet evaluates the CNN model on the test dataset, printing the test loss and accuracy to assess the model's performance.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9945109}},\n",
       "     {'cell_id': 50,\n",
       "      'code': 'embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\\nfor word, i in word_index.items(): \\n    if word in word2vec_model:\\n        embedding_matrix[i] = np.asarray(word2vec_model.wv[word])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet creates an embedding matrix by mapping each word in the tokenizer's word index to its corresponding Word2Vec embedding vector, initializing the matrix with zeros for out-of-vocabulary words.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.739272}},\n",
       "     {'cell_id': 51,\n",
       "      'code': 'embedding_layer = Embedding(len(word_index)+1,\\n                           embedding_dim,\\n                           weights = [embedding_matrix],\\n                           input_length = max_sequence_length,\\n                           trainable = False)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet initializes an embedding layer using the pre-trained embedding matrix, setting the layer's weights to be non-trainable and specifying the input length and embedding dimension.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.99493694}},\n",
       "     {'cell_id': 52,\n",
       "      'code': \"model = Sequential()\\nmodel.add(embedding_layer)\\nmodel.add(Conv1D(filters=150, kernel_size=3, strides=1, padding='valid', activation = 'relu'))\\nmodel.add(MaxPooling1D(pool_size=3))\\nmodel.add(Flatten())\\nmodel.add(Dense(embedding_dim, activation='relu'))\\nmodel.add(Dropout(0.8))\\nmodel.add(Dense(cnn_label.shape[1], activation='sigmoid'))\\n\\nmodel.summary()\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet constructs and summarizes a sequential Convolutional Neural Network (CNN) model incorporating the pre-trained embedding layer, followed by convolution, pooling, flattening, dense, and dropout layers, with a sigmoid activation for the output.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.99737453}},\n",
       "     {'cell_id': 53,\n",
       "      'code': \"model.compile(optimizer='adam', loss=losses.binary_crossentropy, metrics=['accuracy'])\\nhistory = model.fit(X_cnn, y_cnn, epochs=10, validation_data=(X_val_cnn, y_val_cnn))\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet compiles the CNN model with the Adam optimizer and binary cross-entropy loss function, then trains the model for ten epochs using the training and validation datasets, capturing the training history.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9875873}},\n",
       "     {'cell_id': 54,\n",
       "      'code': \"plt.plot(history.history['accuracy'], label='accuracy')\\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.ylim([0.5,1])\\nplt.legend()\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet plots the training and validation accuracy over each epoch to visualize the CNN model's performance during training.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.9901933}},\n",
       "     {'cell_id': 55,\n",
       "      'code': \"test_loss, test_acc = model.evaluate(X_test_cnn, y_test_cnn, verbose=2)\\nprint('test loss:',test_loss)\\nprint('test acc:',test_acc)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet evaluates the CNN model on the test dataset, printing the test loss and accuracy to assess the model's final performance.\",\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9897344}},\n",
       "     {'cell_id': 56,\n",
       "      'code': 'tf.__version__',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet outputs the version of the TensorFlow library being used in the current environment.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9983907}},\n",
       "     {'cell_id': 57,\n",
       "      'code': 'import tensorflow_hub as hub\\nhub.__version__',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports the TensorFlow Hub library and outputs its version to check the installed version in the current environment.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99918145}},\n",
       "     {'cell_id': 58,\n",
       "      'code': '!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet downloads the `tokenization.py` script from the TensorFlow models repository, which is typically used for tokenizing text for BERT models.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_url',\n",
       "       'subclass_id': 42,\n",
       "       'predicted_subclass_probability': 0.8866123}},\n",
       "     {'cell_id': 59,\n",
       "      'code': 'import tensorflow as tf\\nfrom tensorflow.keras.callbacks import ModelCheckpoint\\nimport tensorflow_hub as hub\\n\\nimport tokenization',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports necessary libraries and modules, including TensorFlow, TensorFlow Hub, and the tokenization script, for further pre-processing or model-building tasks.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99932754}},\n",
       "     {'cell_id': 60,\n",
       "      'code': 'def bert_encode(texts, bert_layer, max_len=128):\\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\\n    \\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n        text = text[:max_len - 2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        input_ids = tokens + [0]* pad_len\\n        all_tokens.append(input_ids)\\n\\n        masks = [1]*len(input_sequence) + [0]* pad_len\\n        all_masks.append(masks)\\n        \\n        segments = [0]* max_len\\n        all_segments.append(segments)\\n        \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n\\n    \\ndef build_model(bert_layer, max_len = 128, lr = 1e-5):\\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_word_ids\")\\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_mask\")\\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"segment_ids\")\\n        \\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\n    dense_out = Dense(1,activation=\"relu\")(pooled_output)\\n    drop_out = tf.keras.layers.Dropout(0.8)(dense_out)\\n    out = Dense(1,activation=\"sigmoid\")(pooled_output)\\n    \\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\\n    adam = tf.keras.optimizers.Adam(lr)\\n    model.compile(optimizer=adam, loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n        \\n    return model\\n\\n\\ndef plot_curve(history):\\n    plt.plot(history.history[\\'accuracy\\'], label=\\'accuracy\\')\\n    plt.plot(history.history[\\'val_accuracy\\'], label=\\'val_accuracy\\')\\n    plt.xlabel(\\'Epoch\\')\\n    plt.ylabel(\\'Accuracy\\')\\n    plt.ylim([0.5,1])\\n    plt.legend()\\n    plt.show()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines three functions: `bert_encode` for tokenizing text for BERT, `build_model` for constructing and compiling a BERT-based model, and `plot_curve` for plotting training and validation accuracy over epochs.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9129451}},\n",
       "     {'cell_id': 61,\n",
       "      'code': '%%time\\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\\nbert_layer = hub.KerasLayer(module_url, trainable=True)',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet loads a BERT model from TensorFlow Hub, making it trainable, and measures the time taken for this operation.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.856542}},\n",
       "     {'cell_id': 62,\n",
       "      'code': '# read and encode train data\\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\n\\ntrain_input = bert_encode(train.text.values, bert_layer, max_len=128)\\ntrain_labels = np.array(train.target)',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the training data from a CSV file, encodes the text data using the `bert_encode` function, and converts the target labels into a NumPy array.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99927765}},\n",
       "     {'cell_id': 63,\n",
       "      'code': \"# train model\\nmodel = build_model(bert_layer, max_len=128, lr = 1e-5)\\nmodel.summary()\\n\\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\\n\\ntrain_history = model.fit(\\n    train_input, train_labels,\\n    validation_split=0.2,\\n    epochs=3,\\n    callbacks=[checkpoint],\\n    batch_size=16\\n)\\n\\nplot_curve(train_history)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet constructs the BERT-based model using the pre-loaded BERT layer, sets up a model checkpoint, trains the model on the encoded training data for three epochs, and then plots the training and validation accuracy curves.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.61692107}},\n",
       "     {'cell_id': 64,\n",
       "      'code': '# predict\\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\\n\\ntest_input = bert_encode(test.text.values, bert_layer, max_len=128)\\nmodel.load_weights(\\'model.h5\\')\\ntest_pred = model.predict(test_input)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet reads the test data from a CSV file, encodes the text data using the `bert_encode` function, loads the best model weights from training, and generates predictions on the encoded test data.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.93680686}},\n",
       "     {'cell_id': 65,\n",
       "      'code': '# submit\\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\\nsubmission[\\'target\\'] = np.round(test_pred).astype(\\'int\\')\\nsubmission.to_csv(\\'submission.csv\\', index=False)\\nsubmission.groupby(\\'target\\').count()',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet prepares the submission file by filling in the 'target' column with rounded predictions, saves it as 'submission.csv', and displays the count of entries for each target class.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99921453}}],\n",
       "    'notebook_id': 19},\n",
       "   'notebook_id': 19},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': \"# LOADING THE TRAIN DATA\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))       \\ndata = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\\ndata.sample(10)\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code imports necessary libraries and loads the training dataset from a CSV file to print out 10 random samples.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9061924}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"print ('Train data shape:', data.shape)\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code prints the shape of the training dataset to analyze its dimensions.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9942768}},\n",
       "     {'cell_id': 2,\n",
       "      'code': \"# CHECK FOR DUPLICATE SAMPLES WITH CONFLICTING LABELS\\n\\ntext = data.text\\nduplicates = data[text.isin(text[text.duplicated()])].sort_values(by='text')\\n\\n# If the mean target value is different from 0 or 1 - we have duplicate samples with conflicting value\\nconflicting_check = pd.DataFrame(duplicates.groupby(['text']).target.mean())\\nconflicting_check.sample(10)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code identifies duplicate text samples with conflicting labels by grouping and checking the mean target values for duplicates in the dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.8007328}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"# DROP DUPLICATE SAMPLES WITH CONFLICTING LABELS\\n\\nconflicting = conflicting_check.loc[(conflicting_check.target != 1) & (conflicting_check.target != 0)].index\\ndata = data.drop(data[text.isin(conflicting)].index)\\nprint ('Conflicting samples count:', conflicting.shape[0])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code drops duplicate samples with conflicting labels from the dataset and prints the count of conflicting samples removed.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'filter',\n",
       "       'subclass_id': 14,\n",
       "       'predicted_subclass_probability': 0.43520904}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"# CONNECT KAGGLE GPU FOR SPEED UP\\n\\nimport tensorflow as tf\\n# Get the GPU device name.\\ndevice_name = tf.test.gpu_device_name()\\nif device_name == '/device:GPU:0':\\n    print('Found GPU at: {}'.format(device_name))\\nelse:\\n    raise SystemError('GPU device not found')\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code attempts to identify and connect to a GPU device on Kaggle for computational speed-up using TensorFlow, and raises an error if no GPU is found.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9054986}},\n",
       "     {'cell_id': 5,\n",
       "      'code': '# SPECIFY THE GPU AS THE TORCH DEVICE\\n\\nimport torch\\nif torch.cuda.is_available():    \\n    # Tell PyTorch to use the GPU.    \\n    device = torch.device(\"cuda\")\\n    print(\\'There are %d GPU(s) available.\\' % torch.cuda.device_count())\\n    print(\\'We will use the GPU:\\', torch.cuda.get_device_name(0))\\nelse:\\n    print(\\'No GPU available, using the CPU instead.\\')\\n    device = torch.device(\"cpu\")',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code checks for the availability of a GPU using PyTorch, sets it as the device for computation if available, and prints the GPU details; otherwise, it defaults to the CPU.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9812268}},\n",
       "     {'cell_id': 6,\n",
       "      'code': '# INSTALL THE TRANSFORMERS PACKAGE TO GET A PYTORCH INTERFACE FOR BERT\\n!pip install transformers',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code installs the `transformers` package, which provides a PyTorch interface for utilizing BERT and other transformer models.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.989985}},\n",
       "     {'cell_id': 7,\n",
       "      'code': '# GET THE LISTS OF TWEETS AND THEIR LABELS\\n\\nsentences = data.text.values\\nlabels =data.target.values',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': \"This code extracts the 'text' and 'target' columns from the dataset into separate lists for tweets and their corresponding labels.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9971167}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"# LOAD THE BERT TOKENIZER\\n\\nfrom transformers import BertTokenizer\\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': \"This code imports the BertTokenizer from the transformers package and initializes it with the pre-trained 'bert-base-uncased' model.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.9928005}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"# LOOK HOW THE TOKENIZER WORK\\n\\n# Print the original sentence.\\nprint(' Original: ', sentences[0])\\n\\n# Print the sentence split into tokens.\\nprint('Tokenized: ', tokenizer.tokenize(sentences[0]))\\n\\n# Print the sentence mapped to token ids.\\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code demonstrates how the BERT tokenizer processes a sample sentence by displaying the original text, its tokenized form, and the corresponding token IDs.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.74620485}},\n",
       "     {'cell_id': 10,\n",
       "      'code': \"# GET MAX LENGTH OF THE TWEETS\\n\\nmax_len = 0\\n# For every sentence...\\nfor sent in sentences:\\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\\n    # Update the maximum sentence length.\\n    max_len = max(max_len, len(input_ids))\\n\\nprint('Max tweet length: ', max_len)\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code calculates and prints the maximum length of the tokenized tweets by iterating through each sentence and updating the maximum length encountered.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.858368}},\n",
       "     {'cell_id': 11,\n",
       "      'code': \"# TOKENIZE ALL THE SENTENCES AND MAP THE TOKENS TO THEIR WORD IDs\\n\\ninput_ids = []\\nattention_masks = []\\n\\n# For every sentence...\\nfor sent in sentences:\\n    # `encode_plus` will:\\n    #   (1) Tokenize the sentence.\\n    #   (2) Prepend the `[CLS]` token to the start.\\n    #   (3) Append the `[SEP]` token to the end.\\n    #   (4) Map tokens to their IDs.\\n    #   (5) Pad or truncate the sentence to `max_length`\\n    #   (6) Create attention masks for [PAD] tokens.\\n    encoded_dict = tokenizer.encode_plus(\\n                        sent,                      # Sentence to encode.\\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\\n                        max_length = 64,           # Pad & truncate all sentences.\\n                        pad_to_max_length = True,\\n                        return_attention_mask = True,   # Construct attn. masks.\\n                        return_tensors = 'pt',     # Return pytorch tensors.\\n                   )\\n    \\n    # Add the encoded sentence to the list.    \\n    input_ids.append(encoded_dict['input_ids'])\\n    \\n    # And its attention mask (simply differentiates padding from non-padding).\\n    attention_masks.append(encoded_dict['attention_mask'])\\n\\n# Convert the lists into tensors.\\ninput_ids = torch.cat(input_ids, dim=0)\\nattention_masks = torch.cat(attention_masks, dim=0)\\nlabels = torch.tensor(labels)\\n\\n# Print sentence 0, now as a list of IDs.\\nprint('Original: ', sentences[0])\\nprint('Token IDs:', input_ids[0])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code tokenizes all sentences, maps them to their respective word IDs, pads/truncates to a maximum length, and creates attention masks, converting the resulting lists into PyTorch tensors.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.99864405}},\n",
       "     {'cell_id': 12,\n",
       "      'code': \"# SPLIT TRAIN DATA INTO TRAIN AND TEST SET\\n# I used small test set (SPLIT=0,999) in order to train the model on the majority of the data, after all parameters were tuned\\n# Use 0,9 or lower to train the model and look at the perfomance/ tune parameters\\n\\nSPLIT = 0.999\\n\\nfrom torch.utils.data import TensorDataset, random_split\\n\\n# Combine the training inputs into a TensorDataset.\\ndataset = TensorDataset(input_ids, attention_masks, labels)\\n\\n# Create a 90-10 train-validation split.\\n\\n# Calculate the number of samples to include in each set.\\ntrain_size = int(SPLIT * len(dataset))\\nval_size = len(dataset) - train_size\\n\\n# Divide the dataset by randomly selecting samples.\\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\\n\\nprint('{:>5,} training samples'.format(train_size))\\nprint('{:>5,} validation samples'.format(val_size))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code splits the dataset into training and validation sets using a specified ratio, converting the dataset into a TensorDataset and using random selection for splitting.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.9402513}},\n",
       "     {'cell_id': 13,\n",
       "      'code': \"# CREATE DATA ITERATOR TO SAVE MEMORY\\n\\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\\n\\n# The DataLoader needs to know our batch size for training, so we specify it \\n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \\n# size of 16 or 32.\\nbatch_size = 32\\n\\n# Create the DataLoaders for our training and validation sets.\\n# We'll take training samples in random order. \\ntrain_dataloader = DataLoader(\\n            train_dataset,  # The training samples.\\n            sampler = RandomSampler(train_dataset), # Select batches randomly\\n            batch_size = batch_size # Trains with this batch size.\\n        )\\n\\n# For validation the order doesn't matter, so we'll just read them sequentially.\\nvalidation_dataloader = DataLoader(\\n            val_dataset, # The validation samples.\\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\\n            batch_size = batch_size # Evaluate with this batch size.\\n        )\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code creates data loaders for training and validation datasets using a specified batch size to efficiently manage memory during training and evaluation.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.98473305}},\n",
       "     {'cell_id': 14,\n",
       "      'code': '# GET BERT MODEL FOR CLASSIFICATION\\n\\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\\n\\n# Load BertForSequenceClassification, the pretrained BERT model with a single \\n# linear classification layer on top. \\nmodel = BertForSequenceClassification.from_pretrained(\\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\\n    num_labels = 2, # The number of output labels--2 for binary classification.\\n                    # You can increase this for multi-class tasks.   \\n    output_attentions = False, # Whether the model returns attentions weights.\\n    output_hidden_states = False, # Whether the model returns all hidden-states.\\n)\\n\\n# Tell pytorch to run this model on the GPU.\\nmodel.cuda()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code initializes a pre-trained BERT model specifically configured for sequence classification with two output labels and sets it to run on the GPU.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.567976}},\n",
       "     {'cell_id': 15,\n",
       "      'code': '# PRINT NAMES AND DIMENSIONS FOR THE MODEL LAYERS\\n\\n# Get all of the model\\'s parameters as a list of tuples.\\nparams = list(model.named_parameters())\\n\\nprint(\\'The BERT model has {:} different named parameters.\\\\n\\'.format(len(params)))\\n\\nprint(\\'==== Embedding Layer ====\\\\n\\')\\n\\nfor p in params[0:5]:\\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\\n\\nprint(\\'\\\\n==== First Transformer ====\\\\n\\')\\n\\nfor p in params[5:21]:\\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\\n\\nprint(\\'\\\\n==== Output Layer ====\\\\n\\')\\n\\nfor p in params[-4:]:\\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code prints the names and dimensions of the layers and parameters within the BERT model for insight into its structure, including embedding, transformer, and output layers.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.24847664}},\n",
       "     {'cell_id': 16,\n",
       "      'code': '# SET UP THE OPTIMIZER\\n\\n# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \\n# I believe the \\'W\\' stands for \\'Weight Decay fix\"\\noptimizer = AdamW(model.parameters(),\\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\\n                )',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code sets up the AdamW optimizer with specified learning rate and epsilon for tuning the BERT model's parameters.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9948212}},\n",
       "     {'cell_id': 17,\n",
       "      'code': \"# SET UP MODEL HYPERPARAMETERS\\n\\nfrom transformers import get_linear_schedule_with_warmup\\n\\n# Number of training epochs. The BERT authors recommend between 2 and 4. \\n# We chose to run for 4, but we'll see later that this may be over-fitting the\\n# training data.\\nepochs = 2\\n\\n# Total number of training steps is [number of batches] x [number of epochs]. \\n# (Note that this is not the same as the number of training samples).\\ntotal_steps = len(train_dataloader) * epochs\\n\\n# Create the learning rate scheduler.\\nscheduler = get_linear_schedule_with_warmup(optimizer, \\n                                            num_warmup_steps = 0, # Default value in run_glue.py\\n                                            num_training_steps = total_steps)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code sets up model hyperparameters, including the number of training epochs, calculates the total training steps, and creates a learning rate scheduler to adjust the learning rate during training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'init_hyperparams',\n",
       "       'subclass_id': 59,\n",
       "       'predicted_subclass_probability': 0.6898819}},\n",
       "     {'cell_id': 18,\n",
       "      'code': '# HELPER FUNCTION TO CALCULATE ACCURACY\\n\\nimport numpy as np\\n\\n# Function to calculate the accuracy of our predictions vs labels\\ndef flat_accuracy(preds, labels):\\n    pred_flat = np.argmax(preds, axis=1).flatten()\\n    labels_flat = labels.flatten()\\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code defines a helper function to calculate the accuracy of model predictions by comparing the flattened predicted labels with the actual labels.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.984026}},\n",
       "     {'cell_id': 19,\n",
       "      'code': \"# HELPER FUNCTION FOR TIME FORMAT\\n\\nimport time\\nimport datetime\\n\\ndef format_time(elapsed):\\n    '''\\n    Takes a time in seconds and returns a string hh:mm:ss\\n    '''\\n    # Round to the nearest second.\\n    elapsed_rounded = int(round((elapsed)))\\n    \\n    # Format as hh:mm:ss\\n    return str(datetime.timedelta(seconds=elapsed_rounded))\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code defines a helper function to format elapsed time into a string in the format hh:mm:ss using the `datetime` module.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.5951723}},\n",
       "     {'cell_id': 20,\n",
       "      'code': '# TRAINING SCRIPT\\n\\nimport random\\nimport numpy as np\\n\\n# This training code is based on the `run_glue.py` script here:\\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\\n\\n# Set the seed value all over the place to make this reproducible.\\nseed_val = 42\\n\\nrandom.seed(seed_val)\\nnp.random.seed(seed_val)\\ntorch.manual_seed(seed_val)\\ntorch.cuda.manual_seed_all(seed_val)\\n\\n# We\\'ll store a number of quantities such as training and validation loss, \\n# validation accuracy, and timings.\\ntraining_stats = []\\n\\n# Measure the total training time for the whole run.\\ntotal_t0 = time.time()\\n\\n# For each epoch...\\nfor epoch_i in range(0, epochs):\\n    \\n    # ========================================\\n    #               Training\\n    # ========================================\\n    \\n    # Perform one full pass over the training set.\\n\\n    print(\"\")\\n    print(\\'======== Epoch {:} / {:} ========\\'.format(epoch_i + 1, epochs))\\n    print(\\'Training...\\')\\n\\n    # Measure how long the training epoch takes.\\n    t0 = time.time()\\n\\n    # Reset the total loss for this epoch.\\n    total_train_loss = 0\\n\\n    # Put the model into training mode. Don\\'t be mislead--the call to \\n    # `train` just changes the *mode*, it doesn\\'t *perform* the training.\\n    # `dropout` and `batchnorm` layers behave differently during training\\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\\n    model.train()\\n\\n    # For each batch of training data...\\n    for step, batch in enumerate(train_dataloader):\\n\\n        # Progress update every 40 batches.\\n        if step % 40 == 0 and not step == 0:\\n            # Calculate elapsed time in minutes.\\n            elapsed = format_time(time.time() - t0)\\n            \\n            # Report progress.\\n            print(\\'  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.\\'.format(step, len(train_dataloader), elapsed))\\n\\n        # Unpack this training batch from our dataloader. \\n        #\\n        # As we unpack the batch, we\\'ll also copy each tensor to the GPU using the \\n        # `to` method.\\n        #\\n        # `batch` contains three pytorch tensors:\\n        #   [0]: input ids \\n        #   [1]: attention masks\\n        #   [2]: labels \\n        b_input_ids = batch[0].to(device)\\n        b_input_mask = batch[1].to(device)\\n        b_labels = batch[2].to(device)\\n\\n        # Always clear any previously calculated gradients before performing a\\n        # backward pass. PyTorch doesn\\'t do this automatically because \\n        # accumulating the gradients is \"convenient while training RNNs\". \\n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\\n        model.zero_grad()        \\n\\n        # Perform a forward pass (evaluate the model on this training batch).\\n        # The documentation for this `model` function is here: \\n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\\n        # It returns different numbers of parameters depending on what arguments\\n        # arge given and what flags are set. For our useage here, it returns\\n        # the loss (because we provided labels) and the \"logits\"--the model\\n        # outputs prior to activation.\\n        outputs = model(b_input_ids, \\n                             token_type_ids=None, \\n                             attention_mask=b_input_mask, \\n                             labels=b_labels)\\n        \\n        loss = outputs[0]\\n        logits = outputs[1]\\n\\n        # Accumulate the training loss over all of the batches so that we can\\n        # calculate the average loss at the end. `loss` is a Tensor containing a\\n        # single value; the `.item()` function just returns the Python value \\n        # from the tensor.\\n        total_train_loss += loss.item()\\n\\n        # Perform a backward pass to calculate the gradients.\\n        loss.backward()\\n\\n        # Clip the norm of the gradients to 1.0.\\n        # This is to help prevent the \"exploding gradients\" problem.\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n\\n        # Update parameters and take a step using the computed gradient.\\n        # The optimizer dictates the \"update rule\"--how the parameters are\\n        # modified based on their gradients, the learning rate, etc.\\n        optimizer.step()\\n\\n        # Update the learning rate.\\n        scheduler.step()\\n\\n    # Calculate the average loss over all of the batches.\\n    avg_train_loss = total_train_loss / len(train_dataloader)            \\n    \\n    # Measure how long this epoch took.\\n    training_time = format_time(time.time() - t0)\\n\\n    print(\"\")\\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\\n    print(\"  Training epcoh took: {:}\".format(training_time))\\n        \\n    # ========================================\\n    #               Validation\\n    # ========================================\\n    # After the completion of each training epoch, measure our performance on\\n    # our validation set.\\n\\n    print(\"\")\\n    print(\"Running Validation...\")\\n\\n    t0 = time.time()\\n\\n    # Put the model in evaluation mode--the dropout layers behave differently\\n    # during evaluation.\\n    model.eval()\\n\\n    # Tracking variables \\n    total_eval_accuracy = 0\\n    total_eval_loss = 0\\n    nb_eval_steps = 0\\n\\n    # Evaluate data for one epoch\\n    for batch in validation_dataloader:\\n        \\n        # Unpack this training batch from our dataloader. \\n        #\\n        # As we unpack the batch, we\\'ll also copy each tensor to the GPU using \\n        # the `to` method.\\n        #\\n        # `batch` contains three pytorch tensors:\\n        #   [0]: input ids \\n        #   [1]: attention masks\\n        #   [2]: labels \\n        b_input_ids = batch[0].to(device)\\n        b_input_mask = batch[1].to(device)\\n        b_labels = batch[2].to(device)\\n        \\n        # Tell pytorch not to bother with constructing the compute graph during\\n        # the forward pass, since this is only needed for backprop (training).\\n        with torch.no_grad():        \\n\\n            # Forward pass, calculate logit predictions.\\n            # token_type_ids is the same as the \"segment ids\", which \\n            # differentiates sentence 1 and 2 in 2-sentence tasks.\\n            # The documentation for this `model` function is here: \\n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\\n            # Get the \"logits\" output by the model. The \"logits\" are the output\\n            # values prior to applying an activation function like the softmax.\\n            output = model(b_input_ids, \\n                                   token_type_ids=None, \\n                                   attention_mask=b_input_mask,\\n                                   labels=b_labels)\\n            \\n            loss = output[0]\\n            logits = output[1]\\n            \\n        # Accumulate the validation loss.\\n        total_eval_loss += loss.item()\\n\\n        # Move logits and labels to CPU\\n        logits = logits.detach().cpu().numpy()\\n        label_ids = b_labels.to(\\'cpu\\').numpy()\\n\\n        # Calculate the accuracy for this batch of test sentences, and\\n        # accumulate it over all batches.\\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\\n        \\n\\n    # Report the final accuracy for this validation run.\\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\\n\\n    # Calculate the average loss over all of the batches.\\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\\n    \\n    # Measure how long the validation run took.\\n    validation_time = format_time(time.time() - t0)\\n    \\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\\n    print(\"  Validation took: {:}\".format(validation_time))\\n\\n    # Record all statistics from this epoch.\\n    training_stats.append(\\n        {\\n            \\'epoch\\': epoch_i + 1,\\n            \\'Training Loss\\': avg_train_loss,\\n            \\'Valid. Loss\\': avg_val_loss,\\n            \\'Valid. Accur.\\': avg_val_accuracy,\\n            \\'Training Time\\': training_time,\\n            \\'Validation Time\\': validation_time\\n        }\\n    )\\n\\nprint(\"\")\\nprint(\"Training complete!\")\\n\\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code executes the training and validation loops for the BERT model across specified epochs, logging training and validation loss, accuracy, and timing metrics for each epoch.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.7243838}},\n",
       "     {'cell_id': 21,\n",
       "      'code': '# THE SUMMARY OF THE TRAIN PROCESS\\n\\n# Display floats with two decimal places\\npd.set_option(\\'precision\\', 2)\\n\\n# Create a DataFrame from our training statistics\\ndf_stats = pd.DataFrame(data=training_stats)\\n\\n# Use the \\'epoch\\' as the row index\\ndf_stats = df_stats.set_index(\\'epoch\\')\\n\\n# A hack to force the column headers to wrap\\n#df = df.style.set_table_styles([dict(selector=\"th\",props=[(\\'max-width\\', \\'70px\\')])])\\n\\n# Display the table\\ndf_stats',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code creates and displays a DataFrame summarizing the training statistics, including training loss, validation loss, validation accuracy, and timings for each epoch.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.46057627}},\n",
       "     {'cell_id': 22,\n",
       "      'code': '# PLOT THE VALIDATION LOSS\\n\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport seaborn as sns\\n\\n# Use plot styling from seaborn.\\nsns.set(style=\\'darkgrid\\')\\n\\n# Increase the plot size and font size.\\nsns.set(font_scale=1.5)\\nplt.rcParams[\"figure.figsize\"] = (12,6)\\n\\n# Plot the learning curve.\\nplt.plot(df_stats[\\'Training Loss\\'], \\'b-o\\', label=\"Training\")\\nplt.plot(df_stats[\\'Valid. Loss\\'], \\'g-o\\', label=\"Validation\")\\n\\n# Label the plot.\\nplt.title(\"Training & Validation Loss\")\\nplt.xlabel(\"Epoch\")\\nplt.ylabel(\"Loss\")\\nplt.legend()\\nplt.xticks([1, 2, 3, 4])\\n\\nplt.show()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code visualizes the training and validation loss over the epochs using a line plot to help assess the model's performance.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.9963574}},\n",
       "     {'cell_id': 23,\n",
       "      'code': \"# PREPARE TEST DATA\\n\\n# Load the dataset into a pandas dataframe.\\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\\n\\n# Report the number of sentences.\\nprint('Number of test sentences: {:,}\\\\n'.format(test_data.shape[0]))\\n\\n# Create sentence and label lists\\nsentences = test_data.text.values\\n#labels = test_data.target.values\\n\\n# Tokenize all of the sentences and map the tokens to thier word IDs.\\ninput_ids = []\\nattention_masks = []\\n\\n# For every sentence...\\nfor sent in sentences:\\n    # `encode_plus` will:\\n    #   (1) Tokenize the sentence.\\n    #   (2) Prepend the `[CLS]` token to the start.\\n    #   (3) Append the `[SEP]` token to the end.\\n    #   (4) Map tokens to their IDs.\\n    #   (5) Pad or truncate the sentence to `max_length`\\n    #   (6) Create attention masks for [PAD] tokens.\\n    encoded_dict = tokenizer.encode_plus(\\n                        sent,                      # Sentence to encode.\\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\\n                        max_length = 64,           # Pad & truncate all sentences.\\n                        pad_to_max_length = True,\\n                        return_attention_mask = True,   # Construct attn. masks.\\n                        return_tensors = 'pt',     # Return pytorch tensors.\\n                   )\\n    \\n    # Add the encoded sentence to the list.    \\n    input_ids.append(encoded_dict['input_ids'])\\n    \\n    # And its attention mask (simply differentiates padding from non-padding).\\n    attention_masks.append(encoded_dict['attention_mask'])\\n\\n# Convert the lists into tensors.\\ninput_ids = torch.cat(input_ids, dim=0)\\nattention_masks = torch.cat(attention_masks, dim=0)\\n#labels = torch.tensor(labels)\\n\\n# Set the batch size.  \\nbatch_size = 32  \\n\\n# Create the DataLoader.\\nprediction_data = TensorDataset(input_ids, attention_masks, ) #labels\\nprediction_sampler = SequentialSampler(prediction_data)\\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code prepares the test dataset by loading it, tokenizing the sentences, creating attention masks, converting the data into tensors, and setting up a DataLoader for prediction.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99884653}},\n",
       "     {'cell_id': 24,\n",
       "      'code': \"# GET PREDICTIONS\\n\\nprint('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\\n\\n# Put model in evaluation mode\\nmodel.eval()\\n\\n# Tracking variables \\npredictions = []\\n#true_labels = []\\n\\n# Predict \\nfor batch in prediction_dataloader:\\n  # Add batch to GPU\\n  batch = tuple(t.to(device) for t in batch)\\n  \\n  # Unpack the inputs from our dataloader\\n  b_input_ids, b_input_mask = batch #b_labels\\n  \\n  # Telling the model not to compute or store gradients, saving memory and \\n  # speeding up prediction\\n  with torch.no_grad():\\n      # Forward pass, calculate logit predictions\\n      outputs = model(b_input_ids, token_type_ids=None, \\n                      attention_mask=b_input_mask)\\n\\n  logits = outputs[0]\\n\\n  # Move logits and labels to CPU\\n  logits = logits.detach().cpu().numpy()\\n  label_ids = b_labels.to('cpu').numpy()\\n  \\n  # Store predictions and true labels\\n  predictions.append(logits)\\n  #true_labels.append(label_ids)\\n\\nprint('    DONE.')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code runs the trained BERT model in evaluation mode to predict labels for the test dataset, storing the prediction logits for later use.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.75300366}},\n",
       "     {'cell_id': 25,\n",
       "      'code': '# PREPARE PREDICTIONS FOR SUBMISSION\\n\\n# Combine the results across all batches. \\nflat_predictions = np.concatenate(predictions, axis=0)\\n\\n# For each sample, pick the label (0 or 1) with the higher score.\\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code processes the model's prediction logits by concatenating the results across all batches and determining the final predicted labels for each sample.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.5844843}},\n",
       "     {'cell_id': 26,\n",
       "      'code': \"# SAVE SUBMISSION FILE\\n\\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\\nsubmission.target = flat_predictions\\nsubmission.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code creates a submission file by updating the target labels in a sample submission CSV with the model's predictions and saving it as 'submission.csv'.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99789846}}],\n",
       "    'notebook_id': 20},\n",
       "   'notebook_id': 20},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '# Import packeges\\nimport os\\nimport gc\\nimport re\\nimport time\\nimport warnings\\nimport string\\nimport numpy as np\\nimport pandas as pd\\npd.set_option(\\'display.max_rows\\', 500)\\npd.set_option(\\'display.max_columns\\', 500)\\npd.set_option(\\'display.width\\', 1000)\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom nltk.corpus import stopwords\\nfrom nltk.util import ngrams # function for making ngrams\\nfrom collections import defaultdict\\nimport tensorflow as tf\\nfrom tensorflow.keras.layers import Dense, Input\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import ModelCheckpoint\\nimport tensorflow_hub as hub\\nimport tokenization\\n\\nwarnings.filterwarnings(\"ignore\")\\neng_stopwords = set(stopwords.words(\"english\"))',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports necessary packages and sets up the environment for a machine learning task by configuring pandas display options, importing various libraries for data manipulation, visualization, natural language processing, and deep learning models, while also ignoring warnings and defining English stopwords.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9975701}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\\n\\nprint(\"Training Shape rows = {}, columns = {}\".format(train_df.shape[0],train_df.shape[1]))\\nprint(\"Testing Shape rows = {}, columns = {}\".format(test_df.shape[0],test_df.shape[1]))',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the training, test, and sample submission datasets from CSV files into pandas DataFrames and prints the shape of the training and testing datasets.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99957687}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'print(\"Train columns = {}\".format(train_df.columns))\\nprint(\"Test columns = {}\".format(test_df.columns))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the column names of the training and testing datasets to provide an overview of the available features.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_columns',\n",
       "       'subclass_id': 71,\n",
       "       'predicted_subclass_probability': 0.99450326}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"x=train_df.target.value_counts()\\nsns.barplot(x.index,x)\\nplt.gca().set_ylabel('# of occurrence')\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet counts the occurrences of each target value in the training dataset and creates a bar plot to visualize the distribution of the target variable.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9978562}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'print(\"So there are {} occourance of disastrous twitts and {} occourances of non disastrous\".format(x[1],x[0]))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the number of occurrences of disastrous and non-disastrous tweets in the training dataset based on the target variable distribution.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.91736937}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'train_df.head(10)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first 10 rows of the training dataset to provide an initial look at the data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997634}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'train_df.isnull().sum()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet counts and displays the number of missing (null) values in each column of the training dataset to help identify any incomplete data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.99896073}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'test_df.isnull().sum()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet counts and displays the number of missing (null) values in each column of the testing dataset to help identify any incomplete data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.9990055}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'train_df[train_df.keyword.notnull()].head(10)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet filters the training dataset to display the first 10 rows where the 'keyword' column is not null.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997584}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'train_df[train_df.keyword.notnull()].tail(10)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet filters the training dataset to display the last 10 rows where the 'keyword' column is not null.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997335}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'train_df[train_df.keyword.isnull()].head(10)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet filters the training dataset to display the first 10 rows where the 'keyword' column is null.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9996885}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'keyword_dist = train_df.groupby(\"keyword\")[\\'target\\'].value_counts().unstack(fill_value=0)\\nkeyword_dist = keyword_dist.add_prefix(keyword_dist.columns.name).rename_axis(columns=None).reset_index()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet calculates the distribution of the target variable for each unique keyword in the training dataset and reshapes it into a more readable format by resetting the index.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'groupby',\n",
       "       'subclass_id': 60,\n",
       "       'predicted_subclass_probability': 0.8236565}},\n",
       "     {'cell_id': 12,\n",
       "      'code': \"keyword_dist.sort_values('target1',ascending = False).head(10)\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet sorts the keyword distribution DataFrame in descending order based on the count of disastrous tweets (target=1) and displays the top 10 keywords with the highest occurrence of such tweets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'sort_values',\n",
       "       'subclass_id': 9,\n",
       "       'predicted_subclass_probability': 0.8409343}},\n",
       "     {'cell_id': 13,\n",
       "      'code': \"keyword_dist.sort_values('target0',ascending = False).head(10)\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet sorts the keyword distribution DataFrame in descending order based on the count of non-disastrous tweets (target=0) and displays the top 10 keywords with the highest occurrence of such tweets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'sort_values',\n",
       "       'subclass_id': 9,\n",
       "       'predicted_subclass_probability': 0.7933884}},\n",
       "     {'cell_id': 14,\n",
       "      'code': \"#word count\\ntrain_df['word_count'] = train_df['text'].apply(lambda x : len(str(x).split()))\\ntest_df['word_count'] = test_df['text'].apply(lambda x : len(str(x).split()))\\n#Unique word count\\ntrain_df['unique_word_count'] = train_df['text'].apply(lambda x : len(set(str(x).split())))\\ntest_df['unique_word_count'] = test_df['text'].apply(lambda x : len(set(str(x).split())))\\n#Count of letters\\ntrain_df['count_letters'] = train_df['text'].apply(lambda x : len(str(x)))\\ntest_df['count_letters'] = test_df['text'].apply(lambda x : len(str(x)))\\n#Count of punctuations\\ntrain_df['count_punctuations'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\\ntest_df['count_punctuations'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\\n#count of stopwords\\ntrain_df['stop_word_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\\ntest_df['stop_word_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\\n#Count of hashtag\\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda x : len([c for c in str(x) if c == '#']))\\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x : len([c for c in str(x) if c == '#']))\\n#Count of mentions\\ntrain_df['mention_count'] = train_df['text'].apply(lambda x : len([c for c in str(x) if c=='@']))\\ntest_df['mention_count'] = test_df['text'].apply(lambda x : len([c for c in str(x) if c=='@']))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet adds several new feature columns to both the training and testing datasets by calculating word count, unique word count, letter count, punctuation count, stop word count, hashtag count, and mention count for the 'text' column in each dataset.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9991715}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'plt.figure(figsize=(12,6))\\n## sentenses\\nplt.subplot(121)\\nplt.suptitle(\"Are longer comments more Disastrous\",fontsize=20)\\nsns.violinplot(y=\\'word_count\\',x=\\'target\\', data=train_df,split=True)\\nplt.xlabel(\\'Target?\\', fontsize=12)\\nplt.ylabel(\\'# of words\\', fontsize=12)\\nplt.title(\"Number of words in each comment\", fontsize=15)\\n\\n# words\\nplt.subplot(122)\\nsns.violinplot(y=\\'count_letters\\',x=\\'target\\', data=train_df,split=True,inner=\"quart\")\\nplt.xlabel(\\'Target?\\', fontsize=12)\\nplt.ylabel(\\'# of letters\\', fontsize=12)\\nplt.title(\"Number of letters in each comment\", fontsize=15)\\n\\nplt.show()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a figure with two violin plots to visualize the distribution of the number of words and the number of letters in the text for each target class in the training dataset, examining if longer comments are more likely to be disastrous.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9839205}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'train_df[\\'word_unique_percent\\']=train_df[\\'unique_word_count\\']*100/train_df[\\'word_count\\']\\ntest_df[\\'word_unique_percent\\']=test_df[\\'unique_word_count\\']*100/test_df[\\'word_count\\']\\nplt.figure(figsize=(12,6))\\nplt.subplot(121)\\nplt.title(\"Percentage of unique words of total words in comment\")\\n#sns.boxplot(x=\\'clean\\', y=\\'word_unique_percent\\', data=train_feats)\\nax=sns.kdeplot(train_df[train_df.target == 0].word_unique_percent, label=\"Disastrous\",shade=True,color=\\'r\\')\\nax=sns.kdeplot(train_df[train_df.target == 1].word_unique_percent, label=\" Non Disastrous\")\\nplt.legend()\\nplt.ylabel(\\'Number of occurances\\', fontsize=12)\\nplt.xlabel(\\'Percent unique words\\', fontsize=12)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet calculates the percentage of unique words out of the total word count for each text entry in both the training and testing datasets and then plots a Kernel Density Estimate (KDE) plot to visualize the distribution of this percentage for disastrous and non-disastrous tweets in the training dataset.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.97993535}},\n",
       "     {'cell_id': 17,\n",
       "      'code': \"def generate_ngrams(text, n_gram=1):\\n    token = [token for token in text.lower().split(' ') if token != '' if token not in eng_stopwords]\\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\\n    return [' '.join(ngram) for ngram in ngrams]\\n\\n# Bigrams\\ndisaster_bigrams = defaultdict(int)\\nnondisaster_bigrams = defaultdict(int)\\n\\nfor tweet in train_df[train_df['target']==1]['text']:\\n    for word in generate_ngrams(tweet, n_gram=2):\\n        disaster_bigrams[word] += 1\\n        \\nfor tweet in train_df[train_df['target']==0]['text']:\\n    for word in generate_ngrams(tweet, n_gram=2):\\n        nondisaster_bigrams[word] += 1\\n        \\ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\\n\\nfig, axes = plt.subplots(ncols=2, figsize=(10, 10))\\nplt.tight_layout()\\nsns.barplot(y=df_disaster_bigrams[0].values[:10], x=df_disaster_bigrams[1].values[:10], ax=axes[0], color='cyan')\\nsns.barplot(y=df_nondisaster_bigrams[0].values[:10], x=df_nondisaster_bigrams[1].values[:10], ax=axes[1], color='pink')\\nfor i in range(2):\\n    axes[i].spines['right'].set_visible(False)\\n    axes[i].set_xlabel('')\\n    axes[i].set_ylabel('')\\n    axes[i].tick_params(axis='x', labelsize=10)\\n    axes[i].tick_params(axis='y', labelsize=10)\\naxes[0].set_title('most common bigrams in Disaster Tweets', fontsize=15)\\naxes[1].set_title('most common bigrams in Non-disaster Tweets', fontsize=15)\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet defines a function to generate n-grams, calculates the most common bigrams for both disastrous and non-disastrous tweets, and visualizes the top 10 bigrams in each category using bar plots.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.95510435}},\n",
       "     {'cell_id': 18,\n",
       "      'code': \"# Trigrams\\ndisaster_trigrams = defaultdict(int)\\nnondisaster_trigrams = defaultdict(int)\\n\\nfor tweet in train_df[train_df['target']==1]['text']:\\n    for word in generate_ngrams(tweet, n_gram=3):\\n        disaster_trigrams[word] += 1\\n        \\nfor tweet in train_df[train_df['target']==0]['text']:\\n    for word in generate_ngrams(tweet, n_gram=3):\\n        nondisaster_trigrams[word] += 1\\n        \\ndf_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\\ndf_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])\\n\\nfig, axes = plt.subplots(ncols=2, figsize=(10, 10))\\nplt.tight_layout()\\nsns.barplot(y=df_disaster_trigrams[0].values[:10], x=df_disaster_trigrams[1].values[:10], ax=axes[0], color='cyan')\\nsns.barplot(y=df_nondisaster_trigrams[0].values[:10], x=df_nondisaster_trigrams[1].values[:10], ax=axes[1], color='pink')\\nfor i in range(2):\\n    axes[i].spines['right'].set_visible(False)\\n    axes[i].set_xlabel('')\\n    axes[i].set_ylabel('')\\n    axes[i].tick_params(axis='x', labelsize=10)\\n    axes[i].tick_params(axis='y', labelsize=10)\\naxes[0].set_title('most common trigrams in Disaster Tweets', fontsize=15)\\naxes[1].set_title('most common trigrams in Non-disaster Tweets', fontsize=15)\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet calculates the most common trigrams for both disastrous and non-disastrous tweets and visualizes the top 10 trigrams in each category using bar plots.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.94842196}},\n",
       "     {'cell_id': 19,\n",
       "      'code': '# Refrenced from Gunes Evitan and Vitalii Mokin Notebook\\ndef clean(tweet): \\n            \\n    # Special characters\\n    tweet = re.sub(r\"\\\\x89Û_\", \"\", tweet)\\n    tweet = re.sub(r\"\\\\x89ÛÒ\", \"\", tweet)\\n    tweet = re.sub(r\"\\\\x89ÛÓ\", \"\", tweet)\\n    tweet = re.sub(r\"\\\\x89ÛÏWhen\", \"When\", tweet)\\n    tweet = re.sub(r\"\\\\x89ÛÏ\", \"\", tweet)\\n    tweet = re.sub(r\"China\\\\x89Ûªs\", \"China\\'s\", tweet)\\n    tweet = re.sub(r\"let\\\\x89Ûªs\", \"let\\'s\", tweet)\\n    tweet = re.sub(r\"\\\\x89Û÷\", \"\", tweet)\\n    tweet = re.sub(r\"\\\\x89Ûª\", \"\", tweet)\\n    tweet = re.sub(r\"\\\\x89Û\\\\x9d\", \"\", tweet)\\n    tweet = re.sub(r\"å_\", \"\", tweet)\\n    tweet = re.sub(r\"\\\\x89Û¢\", \"\", tweet)\\n    tweet = re.sub(r\"\\\\x89Û¢åÊ\", \"\", tweet)\\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \\n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\\n    tweet = re.sub(r\"å¨\", \"\", tweet)\\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\\n    \\n    # Contractions\\n    tweet = re.sub(r\"he\\'s\", \"he is\", tweet)\\n    tweet = re.sub(r\"there\\'s\", \"there is\", tweet)\\n    tweet = re.sub(r\"We\\'re\", \"We are\", tweet)\\n    tweet = re.sub(r\"That\\'s\", \"That is\", tweet)\\n    tweet = re.sub(r\"won\\'t\", \"will not\", tweet)\\n    tweet = re.sub(r\"they\\'re\", \"they are\", tweet)\\n    tweet = re.sub(r\"Can\\'t\", \"Cannot\", tweet)\\n    tweet = re.sub(r\"wasn\\'t\", \"was not\", tweet)\\n    tweet = re.sub(r\"don\\\\x89Ûªt\", \"do not\", tweet)\\n    tweet = re.sub(r\"aren\\'t\", \"are not\", tweet)\\n    tweet = re.sub(r\"isn\\'t\", \"is not\", tweet)\\n    tweet = re.sub(r\"What\\'s\", \"What is\", tweet)\\n    tweet = re.sub(r\"haven\\'t\", \"have not\", tweet)\\n    tweet = re.sub(r\"hasn\\'t\", \"has not\", tweet)\\n    tweet = re.sub(r\"There\\'s\", \"There is\", tweet)\\n    tweet = re.sub(r\"He\\'s\", \"He is\", tweet)\\n    tweet = re.sub(r\"It\\'s\", \"It is\", tweet)\\n    tweet = re.sub(r\"You\\'re\", \"You are\", tweet)\\n    tweet = re.sub(r\"I\\'M\", \"I am\", tweet)\\n    tweet = re.sub(r\"shouldn\\'t\", \"should not\", tweet)\\n    tweet = re.sub(r\"wouldn\\'t\", \"would not\", tweet)\\n    tweet = re.sub(r\"i\\'m\", \"I am\", tweet)\\n    tweet = re.sub(r\"I\\\\x89Ûªm\", \"I am\", tweet)\\n    tweet = re.sub(r\"I\\'m\", \"I am\", tweet)\\n    tweet = re.sub(r\"Isn\\'t\", \"is not\", tweet)\\n    tweet = re.sub(r\"Here\\'s\", \"Here is\", tweet)\\n    tweet = re.sub(r\"you\\'ve\", \"you have\", tweet)\\n    tweet = re.sub(r\"you\\\\x89Ûªve\", \"you have\", tweet)\\n    tweet = re.sub(r\"we\\'re\", \"we are\", tweet)\\n    tweet = re.sub(r\"what\\'s\", \"what is\", tweet)\\n    tweet = re.sub(r\"couldn\\'t\", \"could not\", tweet)\\n    tweet = re.sub(r\"we\\'ve\", \"we have\", tweet)\\n    tweet = re.sub(r\"it\\\\x89Ûªs\", \"it is\", tweet)\\n    tweet = re.sub(r\"doesn\\\\x89Ûªt\", \"does not\", tweet)\\n    tweet = re.sub(r\"It\\\\x89Ûªs\", \"It is\", tweet)\\n    tweet = re.sub(r\"Here\\\\x89Ûªs\", \"Here is\", tweet)\\n    tweet = re.sub(r\"who\\'s\", \"who is\", tweet)\\n    tweet = re.sub(r\"I\\\\x89Ûªve\", \"I have\", tweet)\\n    tweet = re.sub(r\"y\\'all\", \"you all\", tweet)\\n    tweet = re.sub(r\"can\\\\x89Ûªt\", \"cannot\", tweet)\\n    tweet = re.sub(r\"would\\'ve\", \"would have\", tweet)\\n    tweet = re.sub(r\"it\\'ll\", \"it will\", tweet)\\n    tweet = re.sub(r\"we\\'ll\", \"we will\", tweet)\\n    tweet = re.sub(r\"wouldn\\\\x89Ûªt\", \"would not\", tweet)\\n    tweet = re.sub(r\"We\\'ve\", \"We have\", tweet)\\n    tweet = re.sub(r\"he\\'ll\", \"he will\", tweet)\\n    tweet = re.sub(r\"Y\\'all\", \"You all\", tweet)\\n    tweet = re.sub(r\"Weren\\'t\", \"Were not\", tweet)\\n    tweet = re.sub(r\"Didn\\'t\", \"Did not\", tweet)\\n    tweet = re.sub(r\"they\\'ll\", \"they will\", tweet)\\n    tweet = re.sub(r\"they\\'d\", \"they would\", tweet)\\n    tweet = re.sub(r\"DON\\'T\", \"DO NOT\", tweet)\\n    tweet = re.sub(r\"That\\\\x89Ûªs\", \"That is\", tweet)\\n    tweet = re.sub(r\"they\\'ve\", \"they have\", tweet)\\n    tweet = re.sub(r\"i\\'d\", \"I would\", tweet)\\n    tweet = re.sub(r\"should\\'ve\", \"should have\", tweet)\\n    tweet = re.sub(r\"You\\\\x89Ûªre\", \"You are\", tweet)\\n    tweet = re.sub(r\"where\\'s\", \"where is\", tweet)\\n    tweet = re.sub(r\"Don\\\\x89Ûªt\", \"Do not\", tweet)\\n    tweet = re.sub(r\"we\\'d\", \"we would\", tweet)\\n    tweet = re.sub(r\"i\\'ll\", \"I will\", tweet)\\n    tweet = re.sub(r\"weren\\'t\", \"were not\", tweet)\\n    tweet = re.sub(r\"They\\'re\", \"They are\", tweet)\\n    tweet = re.sub(r\"Can\\\\x89Ûªt\", \"Cannot\", tweet)\\n    tweet = re.sub(r\"you\\\\x89Ûªll\", \"you will\", tweet)\\n    tweet = re.sub(r\"I\\\\x89Ûªd\", \"I would\", tweet)\\n    tweet = re.sub(r\"let\\'s\", \"let us\", tweet)\\n    tweet = re.sub(r\"it\\'s\", \"it is\", tweet)\\n    tweet = re.sub(r\"can\\'t\", \"cannot\", tweet)\\n    tweet = re.sub(r\"don\\'t\", \"do not\", tweet)\\n    tweet = re.sub(r\"you\\'re\", \"you are\", tweet)\\n    tweet = re.sub(r\"i\\'ve\", \"I have\", tweet)\\n    tweet = re.sub(r\"that\\'s\", \"that is\", tweet)\\n    tweet = re.sub(r\"i\\'ll\", \"I will\", tweet)\\n    tweet = re.sub(r\"doesn\\'t\", \"does not\", tweet)\\n    tweet = re.sub(r\"i\\'d\", \"I would\", tweet)\\n    tweet = re.sub(r\"didn\\'t\", \"did not\", tweet)\\n    tweet = re.sub(r\"ain\\'t\", \"am not\", tweet)\\n    tweet = re.sub(r\"you\\'ll\", \"you will\", tweet)\\n    tweet = re.sub(r\"I\\'ve\", \"I have\", tweet)\\n    tweet = re.sub(r\"Don\\'t\", \"do not\", tweet)\\n    tweet = re.sub(r\"I\\'ll\", \"I will\", tweet)\\n    tweet = re.sub(r\"I\\'d\", \"I would\", tweet)\\n    tweet = re.sub(r\"Let\\'s\", \"Let us\", tweet)\\n    tweet = re.sub(r\"you\\'d\", \"You would\", tweet)\\n    tweet = re.sub(r\"It\\'s\", \"It is\", tweet)\\n    tweet = re.sub(r\"Ain\\'t\", \"am not\", tweet)\\n    tweet = re.sub(r\"Haven\\'t\", \"Have not\", tweet)\\n    tweet = re.sub(r\"Could\\'ve\", \"Could have\", tweet)\\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \\n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \\n            \\n    # Character entity references\\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\\n    \\n    # Typos, slang and informal abbreviations\\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\\n    tweet = re.sub(r\"w/\", \"with\", tweet)\\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\\n    tweet = re.sub(r\"<3\", \"love\", tweet)\\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \\n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\\n    \\n    # Hashtags and usernames\\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \\n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \\n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\\n    tweet = re.sub(r\"GodsLove\", \"God\\'s Love\", tweet)\\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \\n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\\n    tweet = re.sub(r\"\\\\\\\\Allahuakbar\\\\\\\\\", \"Allahu Akbar\", tweet)\\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\\n           \\n    # Urls\\n    tweet = re.sub(r\"https?:\\\\/\\\\/t.co\\\\/[A-Za-z0-9]+\", \"\", tweet)\\n        \\n    # Words with punctuations and special characters\\n    punctuations = \\'@#!?+&*[]-%.:/();$=><|{}^\\' + \"\\'`\"\\n    for p in punctuations:\\n        tweet = tweet.replace(p, f\\' {p} \\')\\n        \\n    # ... and ..\\n    tweet = tweet.replace(\\'...\\', \\' ... \\')\\n    if \\'...\\' not in tweet:\\n        tweet = tweet.replace(\\'..\\', \\' ... \\')      \\n        \\n    # Acronyms\\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\\n    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \\n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \\n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \\n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \\n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \\n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \\n    \\n    # Grouping same words without embeddings\\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\\n    \\n    #Remove Emoji\\n    tweet = re.sub(u\"\\\\U0001F600-\\\\U0001F64F\",\"\", tweet)  # emoticons\\n    tweet = re.sub(u\"\\\\U0001F300-\\\\U0001F5FF\",\"\", tweet)  # symbols & pictographs\\n    tweet = re.sub(u\"\\\\U0001F680-\\\\U0001F6FF\",\"\", tweet)  # transport & map symbols\\n    tweet = re.sub(u\"\\\\U0001F1E0-\\\\U0001F1FF\",\"\", tweet)  # flags (iOS)\\n    tweet = re.sub(u\"\\\\U00002702-\\\\U000027B0\",\"\", tweet)\\n    tweet = re.sub(u\"\\\\U000024C2-\\\\U0001F251\",\"\", tweet)\\n    \\n    return tweet\\n\\ntrain_df[\\'text_cleaned\\'] = train_df[\\'text\\'].apply(lambda s : clean(s))\\ntest_df[\\'text_cleaned\\'] = test_df[\\'text\\'].apply(lambda s : clean(s))',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet defines a text cleaning function to normalize, remove special characters, and handle slang, hashtags, acronyms, and emojis in tweets, then applies this function to create a cleaned version of the 'text' column for both the training and testing datasets.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9979898}},\n",
       "     {'cell_id': 20,\n",
       "      'code': 'def encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        # Tokenise text\\n        text = tokenizer.tokenize(text)\\n        #Reduce 2 slots for start and end tag\\n        text = text[:max_len-2]\\n        #Add start and end tag\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        #Padding to be added\\n        pad_len = max_len - len(input_sequence)\\n        #Get token ids\\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        #add padding\\n        tokens += [0] * pad_len\\n        #Create padding mask with 1\\'s of length of input and 0\\'s with padding length\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        #Create segment ids with all 0\\'s \\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function to encode text data for a BERT-style transformer model by tokenizing the text, adding special tokens and padding, and creating corresponding masks and segment ids, then returns these encoded components as numpy arrays.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9931865}},\n",
       "     {'cell_id': 21,\n",
       "      'code': 'def build_model(bert_layer, max_len=512):\\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\\n\\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\n    clf_output = sequence_output[:, 0, :]\\n    out = Dense(1, activation=\\'sigmoid\\')(clf_output)\\n    \\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\\n    model.compile(Adam(lr=1e-5), loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n    \\n    return model',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines a function to build and compile a binary classification model using a BERT layer, where the model accepts input word ids, masks, and segment ids, processes them through BERT, and outputs a sigmoid-activated prediction, and then compiles the model with the Adam optimizer and binary cross-entropy loss.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9912102}},\n",
       "     {'cell_id': 22,\n",
       "      'code': \"%%time\\n\\nbert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet loads a pre-trained BERT model from TensorFlow Hub and sets it to be trainable, allowing fine-tuning during model training, and measures the time taken for this operation using the Jupyter magic command `%%time`.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.8483843}},\n",
       "     {'cell_id': 23,\n",
       "      'code': 'vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet retrieves the vocabulary file and the do_lower_case property from the loaded BERT model, then initializes a tokenizer using these parameters to tokenize text data appropriately.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.56229645}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'train_input = encode(train_df.text_cleaned.values, tokenizer, max_len=160)\\ntest_input = encode(test_df.text_cleaned.values, tokenizer, max_len=160)\\ntrain_labels = train_df.target.values',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet encodes the cleaned text data from the training and testing datasets, using the previously defined `encode` function with a maximum token length of 160, and extracts the target labels from the training dataset.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.997544}},\n",
       "     {'cell_id': 25,\n",
       "      'code': 'model = build_model(bert_layer, max_len=160)\\nmodel.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet builds a binary classification model with a BERT layer for an input length of 160 tokens and prints a summary of the model architecture.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9372223}},\n",
       "     {'cell_id': 26,\n",
       "      'code': \"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\\n\\ntrain_history = model.fit(\\n    train_input, train_labels,\\n    validation_split=0.2,\\n    epochs=3,\\n    callbacks=[checkpoint],\\n    batch_size=32\\n)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the binary classification model with the encoded training data and labels, using a validation split of 20%, over 3 epochs, saving the best model based on validation loss using a checkpoint callback, and specifies a batch size of 32.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9943159}},\n",
       "     {'cell_id': 27,\n",
       "      'code': \"# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\\n# Prediction by BERT model\\nmodel.load_weights('model.h5')\\ntest_pred_BERT = model.predict(test_input)\\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet loads the best weights saved from model training, uses the model to predict the test data, and rounds the predictions to the nearest integer for evaluation.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.9870588}},\n",
       "     {'cell_id': 28,\n",
       "      'code': 'submission[\\'target\\'] = test_pred_BERT_int\\nsubmission.to_csv(\"submission_BERT.csv\", index=False, header=True)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet assigns the predicted target values to the submission DataFrame and saves it to a CSV file named \"submission_BERT.csv\" with headers.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9993593}}],\n",
       "    'notebook_id': 21},\n",
       "   'notebook_id': 21},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'from collections import Counter\\n\\nimport seaborn as sns\\nimport numpy as np \\nimport pandas as pd\\nfrom matplotlib import pyplot as plt\\nimport spacy\\nfrom tqdm import tqdm\\nfrom spacy.lang.en import English\\nimport torch\\nfrom torch import nn\\nfrom torch.nn.utils.rnn import pad_sequence\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader\\nimport pytorch_lightning as pl\\n\\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.svm import SVC\\nfrom xgboost import XGBClassifier\\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizerFast',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports various libraries and modules necessary for data manipulation, visualization, natural language processing, machine learning, and deep learning tasks.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993081}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"np.random.seed(42)\\n\\n# prettier graphs!\\nplt.style.use('ggplot')\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': \"This code snippet sets a random seed for NumPy to ensure reproducibility and applies a 'ggplot' style to Matplotlib for prettier graphs.\",\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9983991}},\n",
       "     {'cell_id': 2,\n",
       "      'code': \"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet loads the training and testing datasets from specified CSV files into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99975055}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'train.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first five rows of the training dataset to provide an initial glimpse of the data structure and contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997507}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"target_counts = train.target.value_counts()\\nsns.barplot(y=target_counts, x=target_counts.index)\\nplt.ylabel('Samples')\\nplt.title('Target')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays a bar plot to visualize the distribution of the target values in the training dataset using Seaborn and Matplotlib.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99522096}},\n",
       "     {'cell_id': 5,\n",
       "      'code': \"has_kw = ~train.keyword.isna()\\nfig, ax = plt.subplots(1, 2, sharey=True)\\ntrain[has_kw]\\nsns.countplot(data=train[has_kw], x='target', ax=ax[0])\\nax[0].set_title('With keyword')\\nsns.countplot(data=train[~has_kw], x='target', ax=ax[1])\\nax[1].set_title('Without keyword')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays two count plots to compare the distribution of target values in the training dataset for samples with and without associated keywords.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.98656905}},\n",
       "     {'cell_id': 6,\n",
       "      'code': \"has_loc = ~train.location.isna()\\nsns.countplot(x=has_loc)\\nplt.xlabel('Has location')\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays a count plot to visualize the number of samples in the training dataset that have associated location information.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99126256}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'loc_count = train.location.value_counts()\\ntop_loc = loc_count.iloc[:50]\\nplt.subplots(figsize=(20, 8))\\nplt.xticks(rotation=80)\\nsns.barplot(x=top_loc.index, y=top_loc)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays a bar plot to visualize the top 50 most frequent locations in the training dataset, with rotated x-axis labels for readability.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99783665}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"min_freq = 5\\nabove_threshold = train.location.value_counts() > min_freq\\nfrequent_places = above_threshold.index[above_threshold]\\ndata = train[train.location.isin(frequent_places)].location\\nprint(f'{data.nunique()} unique locations with more than {min_freq} occurrences')\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet identifies locations in the training dataset that occur more than a specified minimum frequency, filters the dataset to include only these frequently occurring locations, and prints the number of unique locations meeting this criteria.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_unique_values',\n",
       "       'subclass_id': 54,\n",
       "       'predicted_subclass_probability': 0.9539694}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"train.drop(['location', 'keyword'], axis=1, inplace=True)\\ntest.drop(['location', 'keyword'], axis=1, inplace=True)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet removes the 'location' and 'keyword' columns from both the training and testing datasets.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.9991115}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'train.text.isna().sum(), test.text.isna().sum()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet calculates and displays the number of missing values in the 'text' column of both the training and testing datasets.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.9989147}},\n",
       "     {'cell_id': 11,\n",
       "      'code': \"nlp = English()\\ntokenizer = nlp.tokenizer\\ntokens = tokenizer('This is a test!')\\nprint(tokens)\\nprint(type(tokens))\\nprint([t.text for t in tokens])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet initializes a tokenizer from the spaCy English model, tokenizes a sample sentence, and prints the resulting tokens and their types, as well as the text of each token.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.46164313}},\n",
       "     {'cell_id': 12,\n",
       "      'code': 'text = \"Don\\'t split #hashtags!\"\\nprint(\\'Before:\\', [t for t in tokenizer(text)])\\n\\nprefixes = list(nlp.Defaults.prefixes)\\nprefixes.remove(\\'#\\')\\nprefix_regex = spacy.util.compile_prefix_regex(prefixes)\\ntokenizer.prefix_search = prefix_regex.search\\n\\nprint(\\'After:\\', [t for t in tokenizer(text)])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet customizes the spaCy tokenizer to avoid splitting hashtags by modifying the prefix handling, and demonstrates the effect by tokenizing a sample sentence before and after the modification.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.5677337}},\n",
       "     {'cell_id': 13,\n",
       "      'code': \"text = 'This is  a test\\\\n , ok?'\\nprint('All tokens:', [t.text for t in tokenizer(text)])\\n\\nprint('Check for is_space():', [t.text for t in tokenizer(text) if not t.is_space])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet tokenizes a sample text, printing all tokens and then printing tokens that are not whitespace, using the spaCy tokenizer and filtering based on the `is_space` attribute.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9729691}},\n",
       "     {'cell_id': 14,\n",
       "      'code': \"train['tokens'] = train['text'].apply(lambda row: [t.text.lower() for t in tokenizer(row) if not t.is_space])\\ntest['tokens'] = test['text'].apply(lambda row: [t.text.lower() for t in tokenizer(row) if not t.is_space])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet adds a new 'tokens' column to both the training and testing datasets by tokenizing the 'text' column, converting tokens to lowercase, and excluding whitespace tokens using the spaCy tokenizer.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'data_type_conversions',\n",
       "       'subclass_id': 16,\n",
       "       'predicted_subclass_probability': 0.5443664}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'train.sample(10)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays 10 random samples from the training dataset to give an overview of the data after transformations.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99975437}},\n",
       "     {'cell_id': 16,\n",
       "      'code': \"train['num_tokens'] = train.tokens.apply(len)\\nplt.hist(train.num_tokens, bins=20)\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet adds a 'num_tokens' column to the training dataset, which counts the number of tokens in each entry, and then creates and displays a histogram to visualize the distribution of token counts.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.99763453}},\n",
       "     {'cell_id': 17,\n",
       "      'code': \"inds40 = train.num_tokens <= 40\\nfig, ax = plt.subplots(figsize=(16, 8))\\nplt.hist(train[inds40 & train.target].num_tokens, bins=20, alpha=0.5, label='Positive', density=True)\\nplt.hist(train[inds40 & ~train.target].num_tokens, bins=20, alpha=0.5, label='Negative', density=True)\\nplt.legend()\\nplt.title('Tweet length distribution')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays a histogram to compare the distribution of tweet lengths (number of tokens) for positive and negative target classes, limited to tweets with 40 or fewer tokens.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9983352}},\n",
       "     {'cell_id': 18,\n",
       "      'code': 'from sklearn.feature_extraction.text import CountVectorizer\\n\\n# min and max document frequency (ratio of documents containing that token)\\nmin_df = 5\\nmax_df = 0.6\\n\\n# limit vocabulary size as a function of the training data\\nmax_features = len(train) * 2\\n\\nvectorizer = CountVectorizer(lowercase=False, tokenizer=lambda x: x, min_df=min_df, max_df=max_df, max_features=max_features, binary=True)\\ntrain_bow = vectorizer.fit_transform(train.tokens)\\ntrain_bow',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet initializes a `CountVectorizer` with specified parameters such as minimum and maximum document frequency, transforms the tokenized texts in the training dataset into a binary bag-of-words matrix, and stores the result.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.97597075}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'def plot_top_values(data, k, names, xlabel=None, ylabel=None, use_abs=False):\\n    \"\"\"\\n    Function to plot a barplot with counts of the top k items in data and their corresponding names.\\n    \\n    Args:\\n        data: a numpy array\\n        k: int\\n        names: list of strings corresponding to the positions in data\\n        use_abs: if True, take the highest absolute values\\n    \"\"\"\\n    if use_abs:\\n        inds = np.abs(data).argsort()\\n    else:\\n        inds = data.argsort()\\n            \\n    # inverted argsort and top k\\n    top_inds = inds[::-1][:k]\\n    top_values = data[top_inds]\\n    top_names = [names[i] for i in top_inds]\\n    \\n    fig, ax = plt.subplots(figsize=(16, 8))\\n    plt.bar(np.arange(k), top_values)\\n    if ylabel:\\n        ax.set_ylabel(ylabel)\\n    if xlabel:\\n        ax.set_xlabel(xlabel)\\n    ax.set_xticks(np.arange(k))\\n    ax.set_xticklabels(top_names, rotation=80)\\n    fig.tight_layout()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet defines a function to create and display a bar plot of the counts of the top k items in a given data array, with options to use absolute values and to label the x and y axes.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9957224}},\n",
       "     {'cell_id': 20,\n",
       "      'code': \"k = 50\\n\\nvocab = vectorizer.get_feature_names()\\nword_count = train_bow.toarray().sum(0)\\n\\nplot_top_values(word_count, k, vocab, 'Count', 'Type')\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet retrieves the vocabulary from the `CountVectorizer`, calculates the word count for each token in the training dataset, and plots a bar plot of the top 50 most frequent tokens using the previously defined `plot_top_values` function.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.97462577}},\n",
       "     {'cell_id': 21,\n",
       "      'code': \"x = train_bow\\ny = train['target']\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet assigns the bag-of-words matrix to variable `x` and the target column to variable `y` for further use in the modeling process.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.99928766}},\n",
       "     {'cell_id': 22,\n",
       "      'code': \"majority = y.mode()[0] == y\\nprint(f'Majority class baseline: {majority.mean()}')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet calculates and prints the accuracy of a baseline model that always predicts the majority class in the target column.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.91397643}},\n",
       "     {'cell_id': 23,\n",
       "      'code': \"classifier = LogisticRegression()\\ncv_scores = cross_val_score(classifier, x, y, scoring='f1', cv=10, n_jobs=-1)\\nprint(f'Mean F1: {cv_scores.mean()}')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates a logistic regression classifier using 10-fold cross-validation and prints the mean F1 score.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.97591215}},\n",
       "     {'cell_id': 24,\n",
       "      'code': \"k = 50\\nclassifier = LogisticRegression(max_iter=500)\\nclassifier.fit(x, y)\\nplot_top_values(classifier.coef_[0], k, vocab, 'Type', 'Weight', use_abs=True)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet trains a logistic regression classifier on the entire training dataset, then visualizes the top 50 most influential features by plotting their absolute weights using the `plot_top_values` function.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.7323191}},\n",
       "     {'cell_id': 25,\n",
       "      'code': 'def get_rows_containing(data, term):\\n    \"\"\"Return rows containing a term\"\"\"\\n    has_term = data.tokens.apply(lambda row: term in row)\\n    return data[has_term]\\n\\nterms = [\\'bags\\', \\'australia\\']\\nfor term in terms:\\n    rows = get_rows_containing(train, term)\\n    print(f\\'Distribution containing {term}:\\')\\n    print(rows.target.value_counts())\\n    for i, row in rows.sample(5).iterrows():\\n        print(row.target, row.text)\\n    print()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet defines a function to return rows in the dataset that contain a specific term, and then for a list of terms, it prints the target value distribution of the rows containing each term along with some sample texts.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.57849276}},\n",
       "     {'cell_id': 26,\n",
       "      'code': \"from sklearn.feature_selection import chi2, SelectKBest\\n\\nnum_features = [1000, 500, 250, 100, 50]\\nf1 = []\\nfor k in num_features:\\n    selector = SelectKBest(chi2, k=k)\\n    x_selected = selector.fit_transform(x, y)\\n    scores = cross_val_score(classifier, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\\n    f1.append(scores.mean())\\n\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates the performance of a logistic regression classifier using cross-validation for different numbers of top features selected by the Chi-squared test, recording the mean F1 score for each feature count.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_params',\n",
       "       'subclass_id': 2,\n",
       "       'predicted_subclass_probability': 0.48363486}},\n",
       "     {'cell_id': 27,\n",
       "      'code': \"ticks = np.arange(len(f1))\\nplt.plot(ticks, f1)\\nplt.xticks(ticks, [str(k) for k in num_features])\\nplt.title('F1 per number of features (chi2 selector)')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays a line plot visualizing the mean F1 scores for different numbers of top features selected by the Chi-squared test, showing the influence of feature selection on model performance.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.91531163}},\n",
       "     {'cell_id': 28,\n",
       "      'code': \"selector = SelectKBest(chi2, k=250)\\nx_selected = selector.fit_transform(x, y)\\nvocab = [vocab[i] for i, selected in enumerate(selector.get_support()) if selected]\\nclassifier.fit(x_selected, y)\\nplot_top_values(classifier.coef_[0], k, vocab, 'Type', 'Weight', use_abs=True)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet selects the top 250 features using the Chi-squared test, retrains the logistic regression classifier with these selected features, and plots the top 50 most influential features by their absolute weights using the `plot_top_values` function.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.58552986}},\n",
       "     {'cell_id': 29,\n",
       "      'code': 'rows = get_rows_containing(train, \\'ebay\\')\\nsns.countplot(x=\\'target\\', data=rows)\\nplt.title(\\'Target distribution containing \"ebay\"\\')\\nplt.show()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet retrieves rows from the training dataset that contain the term 'ebay' and creates a count plot to visualize the distribution of the target variable for these rows.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9911773}},\n",
       "     {'cell_id': 30,\n",
       "      'code': \"regularization = [1, 0.1, 0.01, 0.001, 0.0001]\\nl1_scores = []\\nl2_scores = []\\nl1_std = []\\nl2_std = []\\n\\nfor value in regularization:\\n    log_reg = LogisticRegression(C=value)\\n    results = cross_val_score(log_reg, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\\n    l2_scores.append(results.mean())\\n    l2_std.append(results.std())\\n    \\n    alpha = 1 / (2 * value)  # as defined in sklearn\\n    ridge = RidgeClassifier(alpha=alpha)\\n    results = cross_val_score(ridge, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\\n    l1_scores.append(results.mean())\\n    l1_std.append(results.std())\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates the performance of logistic regression and ridge classifiers using L2 and L1 regularization, respectively, across different regularization values, recording the mean and standard deviation of the F1 scores using 10-fold cross-validation.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.39513838}},\n",
       "     {'cell_id': 31,\n",
       "      'code': \"n = np.arange(len(regularization)) + 1\\nfig, ax = plt.subplots(figsize=(14, 6))\\nwidth = 0.4\\n\\nax.bar(n, l1_scores, width, label='L1 reg', yerr=l1_std)\\nax.bar(n + width, l2_scores, width, label='L2 reg', yerr=l2_std)\\nax.set_xlabel('Regularization (lower is stronger)')\\nax.set_ylabel('Mean F1')\\nax.set_xticks(n + width / 2)\\nax.set_xticklabels([str(val) for val in regularization])\\nax.legend(loc='best')\\n\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays a bar plot comparing the mean F1 scores and standard deviations for logistic regression with L2 regularization and ridge classifier with L1 regularization across various regularization strengths.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.97958606}},\n",
       "     {'cell_id': 32,\n",
       "      'code': \"print(f'Best baseline F1: {l2_scores[1]}')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet prints the best F1 score from the previous evaluation, specifically selecting the second value from the list of F1 scores for logistic regression with L2 regularization.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.5057336}},\n",
       "     {'cell_id': 33,\n",
       "      'code': \"# min and max document frequency (ratio of documents containing that token)\\nmin_df = 10\\nmax_df = 0.6\\n\\n# limit vocabulary size as a function of the training data\\nmax_features = len(train) * 2\\n\\n# single words to 3-grams\\nngram_range = (1, 3)\\n\\nvectorizer = CountVectorizer(lowercase=False, tokenizer=lambda x: x, min_df=min_df, max_df=max_df, max_features=max_features, binary=True, ngram_range=ngram_range)\\nx = train_bow = vectorizer.fit_transform(train.tokens)\\n\\nvocab = vectorizer.get_feature_names()\\nword_count = train_bow.toarray().sum(0)\\n\\nplot_top_values(word_count, k, vocab, 'Count', 'Type')\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet reconfigures the `CountVectorizer` to use single words to 3-grams and updates the minimum and maximum document frequency, re-transforms the tokenized texts into a bag-of-words matrix, and then visualizes the top 50 most frequent n-grams.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'relationship',\n",
       "       'subclass_id': 81,\n",
       "       'predicted_subclass_probability': 0.74612176}},\n",
       "     {'cell_id': 34,\n",
       "      'code': 'x.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet outputs the shape of the bag-of-words matrix `x` to provide insight into the number of samples and features after transformation.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9995432}},\n",
       "     {'cell_id': 35,\n",
       "      'code': \"classifier = LogisticRegression(C=0.1)\\nselector = SelectKBest(chi2, k=500)\\nx = selector.fit_transform(x, y)\\ncv_scores = cross_validate(classifier, x, y, scoring='f1', cv=10, n_jobs=-1, return_train_score=True)\\nmean_f1 = cv_scores['test_score'].mean()\\nprint(f'Mean F1: {mean_f1}')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet applies Chi-squared feature selection to retain the top 500 features, trains a logistic regression classifier with L2 regularization on this feature subset, and evaluates the model with 10-fold cross-validation, printing the mean F1 score.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.98264277}},\n",
       "     {'cell_id': 36,\n",
       "      'code': 'def plot_model_score(train_scores, valid_scores):\\n    \"\"\"Plot train and validation score for comparison and checking overfitting\"\"\"\\n    mean_train = train_scores.mean()\\n    mean_valid = valid_scores.mean()\\n    fig, ax = plt.subplots()\\n    plt.bar(0, mean_train, yerr=train_scores.std())\\n    plt.bar(1, mean_valid, yerr=valid_scores.std())\\n    ax.text(0, mean_train + 0.01, f\\'{mean_train:.4f}\\')\\n    ax.text(1, mean_valid + 0.01, f\\'{mean_valid:.4f}\\')\\n    plt.title(\\'Model F1 and standard deviation\\')\\n    plt.xticks([0, 1], [\\'Train\\', \\'Validation\\'])\\n    ymin = np.min([mean_train, mean_valid]) * 0.8\\n    plt.ylim(bottom=ymin)\\n    plt.show()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet defines a function to create and display a bar plot comparing the mean and standard deviation of model F1 scores on training and validation datasets to check for potential overfitting.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.91516846}},\n",
       "     {'cell_id': 37,\n",
       "      'code': \"plot_model_score(cv_scores['train_score'], cv_scores['test_score'])\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet uses the previously defined `plot_model_score` function to create and display a bar plot comparing the mean and standard deviation of the logistic regression model's F1 scores on the training and validation datasets from 10-fold cross-validation.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.977454}},\n",
       "     {'cell_id': 38,\n",
       "      'code': \"c = RandomForestClassifier(n_estimators=100, n_jobs=-1)\\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates a RandomForestClassifier using 8-fold cross-validation and plots the mean and standard deviation of the F1 scores on the training and validation datasets to compare their performance and check for overfitting.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.7403155}},\n",
       "     {'cell_id': 39,\n",
       "      'code': \"c = RandomForestClassifier(n_estimators=100, min_samples_leaf=3)\\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates a RandomForestClassifier with the parameter `min_samples_leaf=3` using 8-fold cross-validation and plots the mean and standard deviation of the F1 scores on the training and validation datasets to compare their performance and check for overfitting.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.8847018}},\n",
       "     {'cell_id': 40,\n",
       "      'code': \"c = RandomForestClassifier(n_estimators=500, min_samples_split=10)\\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates a RandomForestClassifier with the parameters `n_estimators=500` and `min_samples_split=10` using 8-fold cross-validation, then plots the mean and standard deviation of the F1 scores on the training and validation datasets to assess performance and overfitting.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.88807696}},\n",
       "     {'cell_id': 41,\n",
       "      'code': \"c = RandomForestClassifier(n_estimators=200, min_samples_split=5, max_depth=50)\\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates a RandomForestClassifier with the parameters `n_estimators=200`, `min_samples_split=5`, and `max_depth=50` using 8-fold cross-validation, then plots the mean and standard deviation of the F1 scores on the training and validation datasets to assess performance and overfitting.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.41321388}},\n",
       "     {'cell_id': 42,\n",
       "      'code': \"c = XGBClassifier()\\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet evaluates an XGBClassifier using 8-fold cross-validation and plots the mean and standard deviation of the F1 scores on the training and validation datasets to compare their performance and check for overfitting.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.8236538}},\n",
       "     {'cell_id': 43,\n",
       "      'code': \"test_bow = vectorizer.transform(test.tokens)\\ntest_bow = selector.transform(test_bow)\\nclassifier = LogisticRegression(C=0.1)\\n\\n# use the whole training dataset now\\nclassifier.fit(x, y)\\npredicted = classifier.predict(test_bow)\\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted})\\nsubmission.to_csv('bow-linear.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet transforms the test dataset using the previously fitted `CountVectorizer` and `SelectKBest` selector, trains a logistic regression classifier on the entire training dataset, predicts the target for the test dataset, and exports the results to a CSV file.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99929416}},\n",
       "     {'cell_id': 44,\n",
       "      'code': \"filename = '/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt'\\nword_dict = {}\\nembeddings = []\\nwith open(filename, 'r') as f:\\n    for line in tqdm(f, total=400000):\\n        word, vector_string = line.split(' ', 1)\\n        vector = [float(value) for value in vector_string.split()]\\n        embeddings.append(vector)\\n        word_dict[word] = len(word_dict)\\n\\nembeddings = torch.tensor(embeddings)\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads pre-trained GloVe word vectors from a text file, constructs a dictionary mapping words to vector indices, and stores the embeddings in a tensor for further use.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9478619}},\n",
       "     {'cell_id': 45,\n",
       "      'code': 'print(embeddings.shape)\\nprint(len(word_dict))',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the shape of the embeddings tensor and the length of the word dictionary to verify the dimensions and size of the extracted GloVe word vectors.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9995413}},\n",
       "     {'cell_id': 46,\n",
       "      'code': \"oov_count = Counter()\\nall_tokens = []\\n\\nfor row in train.tokens:\\n    tokens = [t[1:] if t.startswith('#') else t for t in row]\\n    all_tokens.append(tokens)\\n    oov_count.update(set(t for t in tokens if t not in word_dict))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet processes the tokenized text by stripping hashtags and counts the out-of-vocabulary (OOV) tokens not found in the GloVe word dictionary while aggregating all processed tokens.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.68699807}},\n",
       "     {'cell_id': 47,\n",
       "      'code': \"test_tokens = []\\nfor row in test.tokens:\\n    tokens = [t[1:] if t.startswith('#') else t for t in row]\\n    test_tokens.append(tokens)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet processes the tokenized text in the test dataset by stripping hashtags and storing the modified tokens for further analysis.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.9887138}},\n",
       "     {'cell_id': 48,\n",
       "      'code': 'oov_count.most_common(10)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the ten most common out-of-vocabulary (OOV) tokens that were not found in the GloVe word dictionary from the training dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.94204676}},\n",
       "     {'cell_id': 49,\n",
       "      'code': 'words_to_add = [w for w in oov_count if oov_count[w] > 2]\\nfor word in words_to_add:\\n    word_dict[word] = len(word_dict)\\n\\nnew_vectors = torch.zeros((len(words_to_add), embeddings.shape[1]))\\nembeddings = torch.cat([embeddings, new_vectors], dim=0)\\nprint(len(word_dict), embeddings.shape)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet adds frequently occurring out-of-vocabulary (OOV) tokens to the word dictionary, expands the embeddings tensor with zero vectors for these new words, and prints the updated dictionary size and embeddings tensor shape.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'concatenate',\n",
       "       'subclass_id': 11,\n",
       "       'predicted_subclass_probability': 0.89962006}},\n",
       "     {'cell_id': 50,\n",
       "      'code': 'len(oov_count)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the total number of unique out-of-vocabulary (OOV) tokens identified in the training dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9988147}},\n",
       "     {'cell_id': 51,\n",
       "      'code': 'def convert_to_indices(all_tokens):\\n    word_indices = []\\n\\n    for tokens in all_tokens:\\n        tweet_inds = torch.tensor([word_dict[t] for t in tokens if t in word_dict], dtype=torch.long)\\n        word_indices.append(tweet_inds)\\n    \\n    return word_indices\\n\\nword_indices = convert_to_indices(all_tokens)\\ntest_word_indices = convert_to_indices(test_tokens)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function to convert tokens into their corresponding indices based on the word dictionary, and applies this function to both the training and test datasets, storing the resulting indices for further use.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.92438155}},\n",
       "     {'cell_id': 52,\n",
       "      'code': 'class BagOfEmbeddingsClassifier(pl.LightningModule):\\n    def __init__(self, embeddings, learning_rate=0.001, l2=0.001):\\n        super().__init__()\\n        self.learning_rate = learning_rate\\n        self.l2 = l2\\n        \\n        vocab_size, embedding_dim = embeddings.shape\\n        self.embedding_bag = nn.EmbeddingBag.from_pretrained(embeddings, freeze=False)\\n        \\n        # a single output value determines the probability of class 1 with a sigmoid function\\n        self.linear = nn.Linear(embedding_dim, 1, bias=True)\\n    \\n    def forward(self, x):\\n        \"\"\"x is a list of tensors with any shape\"\"\"\\n        # embedding bag operates with a single tensor of concatenated inputs and another of offsets\\n        lengths = torch.tensor([0] + [len(sample) for sample in x[:-1]])\\n        offsets = lengths.cumsum(0).to(x[0].device)\\n        x = torch.cat(x)\\n        embedded = self.embedding_bag(x, offsets)\\n        logits = self.linear(embedded).squeeze(-1)\\n        return logits\\n    \\n    def _get_loss_and_acc(self, logits, y):\\n        \"\"\"Internal function\"\"\"\\n        predicted = logits > 0\\n        acc = (predicted == y).float().mean()\\n        loss = F.binary_cross_entropy_with_logits(logits, y.float())\\n        \\n        return loss, acc\\n    \\n    def on_fit_start(self):        \\n        self.train_losses = []\\n        self.train_accs = []\\n        self.valid_losses = []\\n        self.valid_accs = []\\n        \\n        self.reset_metrics()\\n    \\n    def reset_metrics(self):\\n        self.partial_train_losses = []\\n        self.partial_train_accs = []\\n        self.partial_valid_losses = []\\n        self.partial_valid_accs = []\\n    \\n    def on_validation_end(self):\\n        self.train_losses.append(np.array(self.partial_train_losses).mean())\\n        self.train_accs.append(np.array(self.partial_train_accs).mean())\\n        self.valid_losses.append(np.array(self.partial_valid_losses).mean())\\n        self.valid_accs.append(np.array(self.partial_valid_accs).mean())\\n        self.reset_metrics()\\n    \\n    def training_step(self, batch, batch_idx):\\n        \"\"\"\\n        batch is a tuple (x, y)\\n        x is a list of tensors as in forward\\n        y is a tensor with the classes\\n        \"\"\"\\n        x, y = batch\\n        logits = self(x)\\n        loss, acc = self._get_loss_and_acc(logits, y)\\n        \\n        # ideally we\\'d use tensorboard to see the graphs, but currently it is disabled in Kaggle\\n        # so we resort to manually plotting\\n#         self.log(\\'train_loss\\', loss)\\n#         self.log(\\'train_acc\\', acc)\\n        self.partial_train_losses.append(loss.detach().cpu().numpy())\\n        self.partial_train_accs.append(acc.detach().cpu().numpy())\\n        \\n        return loss\\n    \\n    def validation_step(self, batch, batch_idx):\\n        x, y = batch\\n        logits = self(x)\\n        loss, acc = self._get_loss_and_acc(logits, y)\\n        \\n#         self.log(\\'valid_loss\\', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\\n#         self.log(\\'valid_acc\\', acc)\\n        self.partial_valid_losses.append(loss.detach().cpu().numpy())\\n        self.partial_valid_accs.append(acc.detach().cpu().numpy())\\n        \\n        return loss\\n    \\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.l2)\\n        return optimizer',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines a custom PyTorch Lightning module named `BagOfEmbeddingsClassifier`, which uses a Bag-of-Embeddings model for binary classification, and includes methods for training, validation, optimization, and tracking of performance metrics.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.94578296}},\n",
       "     {'cell_id': 53,\n",
       "      'code': \"def plot_model_performance(model):\\n    fig, ax = plt.subplots(2, 1, figsize=(16, 8))\\n    ax[0].set_title('Loss')\\n    ax[1].set_title('Accuracy')\\n\\n    n = np.arange(len(model.train_losses))\\n    ax[0].plot(n, model.train_losses, 'bo', label='Train', linestyle='--')\\n    ax[1].plot(n, model.train_accs, 'bo', linestyle='--')\\n    ax[0].plot(n, model.valid_losses, 'ro', label='Validation', linestyle='--')\\n    ax[1].plot(n, model.valid_accs, 'ro', linestyle='--')\\n    ax[0].legend()\\n    plt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet defines a function to plot the training and validation loss and accuracy of a given model over epochs, providing insights into the model's performance and potential overfitting or underfitting issues.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.9959293}},\n",
       "     {'cell_id': 54,\n",
       "      'code': 'def collate_as_list(samples):\\n    \"\"\"Function for the DataLoader to combine samples in a batch. Each sample is a (x, y) pair.\"\"\"\\n    x, y = list(zip(*samples))\\n    if y[0] is None:\\n        return x\\n    return x, torch.tensor(y).float()\\n\\n\\nclass WordIndexDataset(Dataset):\\n    def __init__(self, x, y=None):\\n        self.x = x\\n        self.y = y\\n    \\n    def __getitem__(self, i):\\n        if self.y is not None:\\n            return self.x[i], self.y[i]\\n        else:\\n            return self.x[i], None\\n    \\n    def __len__(self):\\n        return len(self.x)\\n',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines a custom dataset class `WordIndexDataset` for PyTorch, which stores samples as lists of word indices and their corresponding labels, and also provides a collate function `collate_as_list` for combining samples into batches, making it compatible with the DataLoader.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.7811093}},\n",
       "     {'cell_id': 55,\n",
       "      'code': 'validation_size = int(0.1 * len(train))\\nvalidation_inds = np.random.choice(np.arange(len(train)), size=validation_size, replace=False)\\nis_train = np.ones(len(train), dtype=np.bool)\\nis_train[validation_inds] = False\\n\\n# use an object array since we have varied size tensors\\ntweets = np.array(word_indices, dtype=object)\\ntarget = train.target.to_numpy()\\n# train_tweets, valid_tweets, train_target, valid_target = train_test_split(tweets, target, test_size=0.1, stratify=target)\\ntrain_tweets = tweets[is_train].tolist()\\ntrain_target = target[is_train]\\nvalid_tweets = tweets[~is_train].tolist()\\nvalid_target = target[~is_train]\\n\\ntrain_data = WordIndexDataset(train_tweets, train_target)\\nvalid_data = WordIndexDataset(valid_tweets, valid_target)\\ntest_data = WordIndexDataset(test_word_indices)\\ntrain_loader = DataLoader(train_data, batch_size=32, collate_fn=collate_as_list)\\nvalid_loader = DataLoader(valid_data, batch_size=256, collate_fn=collate_as_list)\\ntest_loader = DataLoader(test_data, batch_size=256, collate_fn=collate_as_list)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the training dataset into training and validation sets, creates custom `WordIndexDataset` instances for each subset as well as the test dataset, and initializes DataLoader objects for efficient batch processing during model training and evaluation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.46862042}},\n",
       "     {'cell_id': 56,\n",
       "      'code': 'model = BagOfEmbeddingsClassifier(embeddings, 0.001, l2=0)\\nbatch = next(iter(train_loader))\\n\\n# batch is x, y\\nlogits = model(batch[0])\\nprint(logits)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This code snippet initializes an instance of the `BagOfEmbeddingsClassifier` with pre-trained embeddings and specified hyperparameters, retrieves a batch from the training DataLoader, and prints the logits produced by the model's forward pass.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9270925}},\n",
       "     {'cell_id': 57,\n",
       "      'code': 'trainer = pl.Trainer(gpus=1, max_epochs=5, val_check_interval=0.5)\\ntrainer.fit(model, train_loader, valid_loader)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes a PyTorch Lightning `Trainer` for GPU training and specifies training parameters, then fits the `BagOfEmbeddingsClassifier` model on the training data and evaluates it on the validation data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.999678}},\n",
       "     {'cell_id': 58,\n",
       "      'code': 'plot_model_performance(model)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet uses the previously defined `plot_model_performance` function to create and display plots of the training and validation loss and accuracy over epochs for the `BagOfEmbeddingsClassifier` model.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.8608386}},\n",
       "     {'cell_id': 59,\n",
       "      'code': 'model = BagOfEmbeddingsClassifier(embeddings, 0.001, l2=0.0001)\\ntrainer = pl.Trainer(gpus=1, max_epochs=6, val_check_interval=0.5)\\ntrainer.fit(model, train_loader, valid_loader)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes a new instance of the `BagOfEmbeddingsClassifier` with a different regularization parameter, sets up a PyTorch Lightning `Trainer` with specific training parameters, and trains the model on the training data while evaluating it on the validation data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9926635}},\n",
       "     {'cell_id': 60,\n",
       "      'code': 'plot_model_performance(model)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy over epochs for the retrained `BagOfEmbeddingsClassifier` model.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.8608386}},\n",
       "     {'cell_id': 61,\n",
       "      'code': '# trainer.predict returns a list with batch results\\nlogits = np.concatenate(trainer.predict(model, test_loader))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet uses the trained `BagOfEmbeddingsClassifier` model to predict logits for the test dataset, concatenating the results from each batch returned by the PyTorch Lightning `Trainer`.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.99380153}},\n",
       "     {'cell_id': 62,\n",
       "      'code': \"predicted = logits > 0\\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted.astype(np.int)})\\nsubmission.to_csv('embeddings.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet converts the predicted logits into binary target values, creates a DataFrame with the test IDs and predicted targets, and exports the results to a CSV file named 'embeddings.csv'.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9992009}},\n",
       "     {'cell_id': 63,\n",
       "      'code': \"pretrained_name = 'distilroberta-base'\\ntokenizer = RobertaTokenizerFast.from_pretrained(pretrained_name)\\nroberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet initializes a tokenizer and a pre-trained RoBERTa model for sequence classification, specifying the number of labels for classification tasks.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.9948885}},\n",
       "     {'cell_id': 64,\n",
       "      'code': \"# create tensors of variable sizes\\n# note that the tokenizer returns a tensor with shape [1, num_tokens]\\ntrain_tokens = train.text[is_train].apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\\nvalid_tokens = train.text[~is_train].apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\\ntest_tokens = test.text.apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\\n\\n# add padding to have a fixed size matrix. With bigger datasets we should be careful about memory usage, but this is small enough to skip this kind of optimization\\npadding = tokenizer.pad_token_id\\nx_train = pad_sequence(train_tokens, batch_first=True, padding_value=padding)\\nx_valid = pad_sequence(valid_tokens, batch_first=True, padding_value=padding)\\nx_test = pad_sequence(test_tokens, batch_first=True, padding_value=padding)\\n\\nx_train_mask = x_train != padding\\nx_valid_mask = x_valid != padding\\nx_test_mask = x_test != padding\\nprint(f'x_train shape: {x_train.shape}, x_valid shape: {x_valid.shape}, x_test shape: {x_test.shape}')\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet tokenizes the text from the training, validation, and test datasets using the RoBERTa tokenizer, pads the sequences to equal lengths, and creates attention masks to differentiate between real tokens and padding tokens, then prints the shapes of the resulting tensors.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9920512}},\n",
       "     {'cell_id': 65,\n",
       "      'code': 'train_data = TensorDataset(x_train, x_train_mask, torch.tensor(train_target))\\nvalid_data = TensorDataset(x_valid, x_valid_mask, torch.tensor(valid_target))\\ntest_data = TensorDataset(x_test, x_test_mask)\\n\\ntrain_loader = DataLoader(train_data, batch_size=32)\\nvalid_loader = DataLoader(valid_data, batch_size=256)\\ntest_loader = DataLoader(test_data, batch_size=256)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet creates `TensorDataset` objects for the training, validation, and test datasets incorporating the tokenized texts and their corresponding masks, and initializes DataLoader objects for efficient batch processing during model training and evaluation.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.97746176}},\n",
       "     {'cell_id': 66,\n",
       "      'code': \"class TransformerWrapper(pl.LightningModule):\\n    def __init__(self, transformer, learning_rate=0.001, l2=0.0001):\\n        super().__init__()\\n        self.model = transformer\\n        self.learning_rate = learning_rate\\n        self.l2 = l2\\n    \\n    def forward(self, batch):\\n        x, mask = batch\\n        output = self.model(x, mask)\\n        return output.logits\\n    \\n    def training_step(self, batch, batch_idx):\\n        loss, acc = self._get_loss_and_acc(batch)\\n        self.partial_train_losses.append(loss.detach().cpu().numpy())\\n        self.partial_train_accs.append(acc.detach().cpu().numpy())\\n        \\n        return loss\\n    \\n    def _get_loss_and_acc(self, batch):\\n        x, mask, y = batch\\n        output = self.model(x, mask, labels=y)\\n        loss = output.loss\\n        logits = output.logits\\n        \\n        predicted = logits.argmax(1)\\n        acc = (predicted == y).float().mean()\\n        \\n        return loss, acc\\n    \\n    # these functions are copied from the BagOfWords class to allow ploting without tensorboard\\n    # ideally, we'd inherit from a common base class. well, ideally we'd have access to tensorboard and none of this would exist :)\\n    def on_fit_start(self):        \\n        self.train_losses = []\\n        self.train_accs = []\\n        self.valid_losses = []\\n        self.valid_accs = []\\n        \\n        self.reset_metrics()\\n    \\n    def reset_metrics(self):\\n        self.partial_train_losses = []\\n        self.partial_train_accs = []\\n        self.partial_valid_losses = []\\n        self.partial_valid_accs = []\\n    \\n    def on_validation_end(self):\\n        self.train_losses.append(np.array(self.partial_train_losses).mean())\\n        self.train_accs.append(np.array(self.partial_train_accs).mean())\\n        self.valid_losses.append(np.array(self.partial_valid_losses).mean())\\n        self.valid_accs.append(np.array(self.partial_valid_accs).mean())\\n        self.reset_metrics()\\n        \\n    def validation_step(self, batch, batch_idx):\\n        loss, acc = self._get_loss_and_acc(batch)\\n        \\n#         self.log('valid_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\\n#         self.log('valid_acc', acc)\\n        self.partial_valid_losses.append(loss.cpu().numpy())\\n        self.partial_valid_accs.append(acc.cpu().numpy())\\n        \\n        return loss\\n    \\n    def configure_optimizers(self):\\n        # to make it lighter, fine tune only the classifier on top of the language model\\n        parameters = [p[1] for p in self.model.named_parameters() if p[0].startswith('classifier')]\\n        optimizer = torch.optim.AdamW(parameters, lr=self.learning_rate, weight_decay=self.l2)\\n        return optimizer\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines a PyTorch Lightning module named `TransformerWrapper` to train and evaluate transformer-based models, incorporating methods for forward pass, training step, validation step, loss and accuracy calculation, optimizer configuration, and tracking of training metrics.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.48894703}},\n",
       "     {'cell_id': 67,\n",
       "      'code': 'model = TransformerWrapper(roberta, 0.001, l2=0)\\ntrainer = pl.Trainer(gpus=1, max_epochs=6, val_check_interval=0.5)\\ntrainer.fit(model, train_loader, valid_loader)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes a `TransformerWrapper` model with the RoBERTa architecture and specified hyperparameters, sets up a PyTorch Lightning `Trainer`, and trains the model on the training dataset while evaluating it on the validation dataset.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9910779}},\n",
       "     {'cell_id': 68,\n",
       "      'code': 'plot_model_performance(model)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy over epochs for the `TransformerWrapper` model.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.8608386}},\n",
       "     {'cell_id': 69,\n",
       "      'code': 'roberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)\\nmodel = TransformerWrapper(roberta, 0.01, l2=0)\\ntrainer = pl.Trainer(gpus=1, max_epochs=4, val_check_interval=0.5)\\ntrainer.fit(model, train_loader, valid_loader)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet reinitializes the RoBERTa model for sequence classification and the `TransformerWrapper` with different learning rate and number of epochs, sets up a PyTorch Lightning `Trainer`, and trains the model on the training dataset while evaluating it on the validation dataset.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.98994815}},\n",
       "     {'cell_id': 70,\n",
       "      'code': 'plot_model_performance(model)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy over epochs for the newly trained `TransformerWrapper` model.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.8608386}},\n",
       "     {'cell_id': 71,\n",
       "      'code': 'roberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)\\n\\ntrain_loader = DataLoader(train_data, batch_size=128)\\n\\nmodel = TransformerWrapper(roberta, 0.005, l2=0)\\ntrainer = pl.Trainer(gpus=1, max_epochs=4, val_check_interval=0.5)\\ntrainer.fit(model, train_loader, valid_loader)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet reinitializes the RoBERTa model for sequence classification and the `TransformerWrapper`, sets up a DataLoader with a larger batch size, creates a PyTorch Lightning `Trainer`, and trains the model on the entire training dataset with new hyperparameters while evaluating it on the validation dataset.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.898724}},\n",
       "     {'cell_id': 72,\n",
       "      'code': 'plot_model_performance(model)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet uses the `plot_model_performance` function to create and display plots of the training and validation loss and accuracy over epochs for the retrained `TransformerWrapper` model with updated batch size and hyperparameters.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'learning_history',\n",
       "       'subclass_id': 35,\n",
       "       'predicted_subclass_probability': 0.8608386}},\n",
       "     {'cell_id': 73,\n",
       "      'code': \"# trainer.predict returns a list with batch results\\nlogits = np.concatenate(trainer.predict(model, test_loader), axis=0)\\npredicted = logits.argmax(1)\\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted})\\nsubmission.to_csv('roberta.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet uses the trained `TransformerWrapper` model to predict logits for the test dataset, converts the logits to predicted target classes, creates a DataFrame with the test IDs and predictions, and exports the results to a CSV file named 'roberta.csv'.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9993868}},\n",
       "     {'cell_id': 74,\n",
       "      'code': '!head *.csv',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet utilizes a shell command to display the first few lines of the CSV files generated during the notebook execution to verify their contents.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99973243}}],\n",
       "    'notebook_id': 22},\n",
       "   'notebook_id': 22},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import numpy as np\\nimport pandas as pd',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports the numpy and pandas libraries.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993284}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"df = pd.read_csv('../input/nlp-getting-started/train.csv')\\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads in training and test datasets from CSV files using pandas and stores them in dataframes `df` and `test`.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9997588}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'df.head()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first few rows of the dataframe `df` to provide an overview of the data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997553}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'df.info()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet provides a summary of the dataframe `df`, including the data types and non-null counts of each column.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.9992442}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'df.duplicated().sum()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet calculates and returns the total number of duplicate rows in the dataframe `df`.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_duplicates',\n",
       "       'subclass_id': 38,\n",
       "       'predicted_subclass_probability': 0.8993749}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'df = df.drop_duplicates().reset_index(drop = True)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet removes duplicate rows from the dataframe `df` and resets the index.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'remove_duplicates',\n",
       "       'subclass_id': 19,\n",
       "       'predicted_subclass_probability': 0.8511636}},\n",
       "     {'cell_id': 6,\n",
       "      'code': \"df['target'].value_counts()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet calculates and displays the count of unique values in the 'target' column of the dataframe `df`.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9995012}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"df[df['target'] == 0][:1]\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet filters the dataframe `df` to select the first record where the 'target' column value is 0.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'filter',\n",
       "       'subclass_id': 14,\n",
       "       'predicted_subclass_probability': 0.98627037}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"df[df['target'] == 1][:1]\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet filters the dataframe `df` to select the first record where the 'target' column value is 1.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'filter',\n",
       "       'subclass_id': 14,\n",
       "       'predicted_subclass_probability': 0.98621887}},\n",
       "     {'cell_id': 9,\n",
       "      'code': 'df.isnull().sum()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet calculates and displays the total number of missing values in each column of the dataframe `df`.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.9985019}},\n",
       "     {'cell_id': 10,\n",
       "      'code': \"df['keyword'].value_counts()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet calculates and displays the count of unique values in the 'keyword' column of the dataframe `df`.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9995165}},\n",
       "     {'cell_id': 11,\n",
       "      'code': \"df['location'].value_counts()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet calculates and displays the count of unique values in the 'location' column of the dataframe `df`.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9994772}},\n",
       "     {'cell_id': 12,\n",
       "      'code': 'import matplotlib.pyplot as plt\\n%matplotlib inline\\nimport seaborn as sns',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports the matplotlib and seaborn libraries for plotting and configures Jupyter Notebook to display inline plots.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.99939334}},\n",
       "     {'cell_id': 13,\n",
       "      'code': '#Most Common Words',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This comment likely indicates the beginning of a section where the most common words in the dataset will be visualized.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.74166125}},\n",
       "     {'cell_id': 14,\n",
       "      'code': \"# plt.figure(figsize=(10,8))\\n# sns.barplot(x = df['keyword'].value_counts().head(5).index, y = df['keyword'].value_counts().head(5))\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet (currently commented out) would create a bar plot showing the top 5 most common keywords in the dataframe `df` using seaborn if uncommented.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.97318214}},\n",
       "     {'cell_id': 15,\n",
       "      'code': \"plt.figure(figsize= (9,6))\\nsns.countplot(y = df.keyword, order = df.keyword.value_counts().iloc[:15].index)\\nplt.title('Top 15 Keyword')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a count plot displaying the top 15 most common keywords in the dataframe `df` using seaborn, along with a title for the plot.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9776137}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'sns.countplot(y = df.target)',\n",
       "      'class': 'Visualization',\n",
       "      'desc': \"This code snippet creates a count plot displaying the distribution of values in the 'target' column of the dataframe `df` using seaborn.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.98655856}},\n",
       "     {'cell_id': 17,\n",
       "      'code': '#Top 10 words in Disasterous and Non-Disasterous tweets',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This comment likely indicates the beginning of a section where the top 10 words in disasterous and non-disasterous tweets will be visualized.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.7826869}},\n",
       "     {'cell_id': 18,\n",
       "      'code': \"w_nd = df[df.target == 0].keyword.value_counts().head(10)\\nsns.barplot(w_nd, w_nd.index, color = 'c')\\nplt.title('Top keyword for Disaster tweet')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a bar plot displaying the top 10 most common keywords in non-disasterous tweets using seaborn, along with a title for the plot.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.8726834}},\n",
       "     {'cell_id': 19,\n",
       "      'code': \"nw_nd = df[df.target == 1].keyword.value_counts().head(10)\\nsns.barplot(nw_nd, nw_nd.index, color = 'y')\\nplt.title('Top keyword for Non-Disaster tweet')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a bar plot displaying the top 10 most common keywords in disasterous tweets using seaborn, along with a title for the plot.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9795735}},\n",
       "     {'cell_id': 20,\n",
       "      'code': 'a = []\\nfor i in w_nd:\\n    if i in nw_nd:\\n        a.apeend(i)\\na\\n#there is no common item on both group of tweets',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet creates a list `a` to compile common keywords between the top 10 keywords in disasterous and non-disasterous tweets, but it contains an error (incorrect method name `apeend` instead of `append` in the loop) and aims to show that there are no common items.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.3016006}},\n",
       "     {'cell_id': 21,\n",
       "      'code': \"#check no of unique keyword and location\\nprint(df.keyword.nunique())\\ndf['location'].nunique()\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This code snippet prints the number of unique values in the 'keyword' column and calculates the number of unique values in the 'location' column of the dataframe `df`.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_unique_values',\n",
       "       'subclass_id': 54,\n",
       "       'predicted_subclass_probability': 0.9503388}},\n",
       "     {'cell_id': 22,\n",
       "      'code': \"#Most Common Location\\nplt.figure(figsize = (9,6))\\nsns.countplot(y = df.location, order = df.location.value_counts().iloc[:15].index)\\nplt.title('Top 15 Location')\\nplt.show()\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates a count plot displaying the top 15 most common locations in the dataframe `df` using seaborn, along with a title for the plot.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.97139984}},\n",
       "     {'cell_id': 23,\n",
       "      'code': '#Cleaning the Data',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This comment likely indicates the beginning of a section where data cleaning operations will be performed.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.892342}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'import re\\n#Conver lowercase remove punctuation and Character and then strip \\ntext = df[\"text\"].iloc[0]\\nprint(text)\\ntext = re.sub(r\\'[^\\\\w\\\\s]\\', \\'\\', str(text).lower().strip())\\ntxt = text.split()\\nprint(txt)\\n\\n',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet imports the `re` module, converts the first tweet text to lowercase, removes punctuation and unnecessary characters, strips leading/trailing whitespace, and then splits the text into words, to clean the data.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9692003}},\n",
       "     {'cell_id': 25,\n",
       "      'code': '#remove stopwords\\nimport nltk\\nlst_stopwords = nltk.corpus.stopwords.words(\"english\")\\ntxt = [word for word in txt if word not in lst_stopwords]\\nprint(txt)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet imports the `nltk` library, retrieves a list of English stopwords, and filters out these stopwords from the previously processed text.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9619886}},\n",
       "     {'cell_id': 26,\n",
       "      'code': '#stemming\\nps = nltk.stem.porter.PorterStemmer()\\nprint([ps.stem(word) for word in txt])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet creates a PorterStemmer object and applies stemming to each word in the previously filtered text using the Porter stemming algorithm.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.23080209}},\n",
       "     {'cell_id': 27,\n",
       "      'code': '#Lemmentization\\nlem = nltk.stem.wordnet.WordNetLemmatizer()\\nprint([lem.lemmatize(word) for word in txt])',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet creates a WordNetLemmatizer object and applies lemmatization to each word in the previously filtered text.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'define_search_model',\n",
       "       'subclass_id': 82,\n",
       "       'predicted_subclass_probability': 0.22034359}},\n",
       "     {'cell_id': 28,\n",
       "      'code': '#to apply all the technique to all the records on dataset\\ndef utils_preprocess_text(text, flg_stemm=True, flg_lemm =True, lst_stopwords=None ):\\n    text = re.sub(r\\'[^\\\\w\\\\s]\\', \\'\\', str(text).lower().strip())\\n    \\n    #tokenization(convert from string to List)\\n    lst_text = text.split()\\n    #remove stopwords\\n    if lst_stopwords is not None:\\n        lst_text = [word for word in lst_text if word not in\\n                   lst_stopwords]\\n        \\n     #stemming\\n    if flg_stemm == True:\\n        ps = nltk.stem.porter.PorterStemmer()\\n        lst_text = [ps.stem(word) for word in lst_text]\\n        \\n    #Lemmentization\\n    if flg_lemm == True:\\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\\n        lst_text = [lem.lemmatize(word) for word in lst_text]\\n        \\n    # back to string from list\\n    text = \" \".join(lst_text)\\n    return text\\n    ',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function named `utils_preprocess_text` that preprocesses text by converting to lowercase, removing punctuation, tokenizing, removing stopwords, and optionally applying stemming and lemmatization, then joining the tokens back into a single string.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.93941545}},\n",
       "     {'cell_id': 29,\n",
       "      'code': \"#apply dataset\\ndf['clean_text'] = df['text'].apply(lambda x: utils_preprocess_text(x, flg_stemm = False, flg_lemm=True))\\ntest['clean_text'] = test['text'].apply(lambda x: utils_preprocess_text(x, flg_stemm = False, flg_lemm=True))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies the `utils_preprocess_text` function to the 'text' column of both the `df` and `test` dataframes to create a new column 'clean_text' with the preprocessed text, disabling stemming and enabling lemmatization.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.96186244}},\n",
       "     {'cell_id': 30,\n",
       "      'code': '#Target Encoding\\n',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This comment likely indicates the beginning of a section focused on encoding the target variable.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.4920049}},\n",
       "     {'cell_id': 31,\n",
       "      'code': \"import category_encoders as ce\\n\\n# Target encoding\\nfeatures = ['keyword', 'location']\\nencoder = ce.TargetEncoder(cols=features)\\nencoder.fit(df[features],df['target'])\\n\\ndf = df.join(encoder.transform(df[features]).add_suffix('_target'))\\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))\\n\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet uses the `category_encoders` library to perform target encoding on the 'keyword' and 'location' columns and then adds these encoded features to both the `df` and `test` dataframes with a '_target' suffix.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9991437}},\n",
       "     {'cell_id': 32,\n",
       "      'code': 'df.isnull().sum()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet calculates and displays the total number of missing values in each column of the dataframe `df`.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_missing_values',\n",
       "       'subclass_id': 39,\n",
       "       'predicted_subclass_probability': 0.9985019}},\n",
       "     {'cell_id': 33,\n",
       "      'code': \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \\n# Only include >=10 occurrences\\n# Have unigrams and bigrams\\ntext_vec = vec_text.fit_transform(df['clean_text'])\\ntext_vec_test = vec_text.transform(test['clean_text'])\\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\\nprint (X_train_text.shape)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This code snippet applies TF-IDF vectorization to the 'clean_text' column of both the `df` and `test` dataframes, converting text data into numerical features and creating dataframes `X_train_text` and `X_test_text`.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.6342874}},\n",
       "     {'cell_id': 34,\n",
       "      'code': \"df = df.join(X_train_text, rsuffix='_text')\\ntest = test.join(X_test_text, rsuffix='_text')\\n\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet joins the TF-IDF vectorized features (from `X_train_text` and `X_test_text`) to the original `df` and `test` dataframes, respectively.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'merge',\n",
       "       'subclass_id': 32,\n",
       "       'predicted_subclass_probability': 0.99786144}},\n",
       "     {'cell_id': 35,\n",
       "      'code': 'df.head(1)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet displays the first row of the dataframe `df` to provide an overview of its current state after transformations.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997688}},\n",
       "     {'cell_id': 36,\n",
       "      'code': '#Logistic Regression',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This comment likely indicates the beginning of a section where a logistic regression model will be trained.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'commented',\n",
       "       'subclass_id': 76,\n",
       "       'predicted_subclass_probability': 0.962167}},\n",
       "     {'cell_id': 37,\n",
       "      'code': \"from sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\nfeatures_to_drop = ['id', 'keyword','location', 'text','clean_text' ]\\nscaler = MinMaxScaler()\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet imports necessary classes for logistic regression and normalization, defines a list of features to drop from the dataframes, and creates an instance of `MinMaxScaler` for scaling features.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'normalization',\n",
       "       'subclass_id': 18,\n",
       "       'predicted_subclass_probability': 0.9946407}},\n",
       "     {'cell_id': 38,\n",
       "      'code': \"X_train = df.drop(columns = features_to_drop + ['target'])\\nX_test = test.drop(columns = features_to_drop)\\ny_train = df.target\\nlr = LogisticRegression(solver = 'liblinear', random_state = 777)\\n\\npipeline = Pipeline([('scale', scaler), ('lr',lr),])\\npipeline.fit(X_train, y_train)\\ny_test = pipeline.predict(X_test)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet prepares the training and test datasets by dropping unnecessary columns, defines the target variable, creates a logistic regression model, sets up a pipeline to scale the features and fit the model, and then trains the logistic regression model on the training data before making predictions on the test data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'find_best_model_class',\n",
       "       'subclass_id': 3,\n",
       "       'predicted_subclass_probability': 0.2473382}},\n",
       "     {'cell_id': 39,\n",
       "      'code': 'y_test[:10]',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet displays the first 10 predictions from the logistic regression model applied to the test data.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.99966455}},\n",
       "     {'cell_id': 40,\n",
       "      'code': \"sub_sample = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\\n\\nsubmit = sub_sample.copy()\\nsubmit.target = y_test\\nsubmit.to_csv('submit.csv',index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet reads in a sample submission CSV file, creates a copy of it, updates the 'target' column with the model's predictions, and then saves the updated dataframe to a new CSV file named 'submit.csv'.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.999146}},\n",
       "     {'cell_id': 41,\n",
       "      'code': \"print ('Training accuracy: %.4f' % pipeline.score(X_train, y_train))\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet calculates and prints the accuracy of the logistic regression model on the training dataset.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.8493974}},\n",
       "     {'cell_id': 42,\n",
       "      'code': \"from sklearn.metrics import f1_score\\n\\nprint ('Training f-1 score: %.4f' % f1_score(y_train, pipeline.predict(X_train)))\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet calculates and prints the F1 score of the logistic regression model on the training dataset.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.5244426}},\n",
       "     {'cell_id': 43,\n",
       "      'code': '# Confusion matrix\\nfrom sklearn.metrics import confusion_matrix\\npd.DataFrame(confusion_matrix(y_train, pipeline.predict(X_train)))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet imports the `confusion_matrix` function, computes the confusion matrix for the logistic regression model on the training dataset, and displays it as a pandas dataframe.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.91332203}}],\n",
       "    'notebook_id': 23},\n",
       "   'notebook_id': 23},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '# Octopus ML pakage - github.com/gershonc/octopus-ml\\n!pip install octopus-ml',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': \"This cell installs the 'octopus-ml' package from GitHub using pip.\",\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'install_modules',\n",
       "       'subclass_id': 87,\n",
       "       'predicted_subclass_probability': 0.99379325}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'import warnings\\nwarnings.simplefilter(\"ignore\")\\nimport seaborn as sns \\nimport matplotlib.pyplot as plt\\nimport time\\nimport pandas as pd\\nimport numpy as np\\nimport lightgbm as lgb\\nimport tracemalloc\\nfrom pandas_summary import DataFrameSummary\\nfrom sklearn.metrics import classification_report\\n\\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\\n\\n%matplotlib inline\\nsns.set_style(\"whitegrid\")\\n\\npd.set_option(\\'display.max_columns\\', None)  # or 1000\\npd.set_option(\\'display.max_rows\\', None)  # or 1000\\npd.set_option(\\'display.max_colwidth\\', -1)  # or 199\\n\\n#check out https://github.com/gershonc/octopus-ml\\nimport octopus_ml as oc\\n\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.multiclass import OneVsRestClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This cell imports various libraries and modules required for data analysis, visualization, machine learning, and sets up the environment configurations for displaying data.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9992041}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This cell reads the training and test datasets from CSV files located at specified paths into Pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99975425}},\n",
       "     {'cell_id': 3,\n",
       "      'code': 'train_df.head(5)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This cell displays the first five rows of the training dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997615}},\n",
       "     {'cell_id': 4,\n",
       "      'code': '# DataFrane Summary by pandas summary package (extension of pandas.describe method) \\ndfs = DataFrameSummary(train_df)\\ndfs.summary()',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This cell generates and displays a detailed summary of the training dataset, including descriptive statistics and data types, using the `DataFrameSummary` class from the pandas_summary package.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.988304}},\n",
       "     {'cell_id': 5,\n",
       "      'code': '# Target distribution analysis\\nfig, ax =plt.subplots(1,2)\\n\\n\\nplt.style.use(\\'fivethirtyeight\\')\\nplt.figure(figsize=(3,4))\\nsns.set_context(\"paper\", font_scale=1.2)                                                  \\nsns.countplot(\\'target\\',data=train_df, ax=ax[0])\\ntrain_df[\\'target\\'].value_counts().plot.pie(explode=[0,0.2],autopct=\\'%1.2f%%\\',ax=ax[1])\\nfig.show()',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This cell creates visualizations to analyze the distribution of the target variable in the training dataset, including a count plot and a pie chart.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9649334}},\n",
       "     {'cell_id': 6,\n",
       "      'code': \"def wordcount(x):\\n    length = len(str(x).split())\\n    return length\\ndef charcount(x):\\n    s = x.split()\\n    x = ''.join(s)\\n    return len(x)\\n\\ndef hashtag_count(x):\\n    l = len([t for t in x.split() if t.startswith('#')])\\n    return l\\n\\ndef mentions_count(x):\\n    l = len([t for t in x.split() if t.startswith('@')])\\n    return l\\n\\n\\ntrain_df['char_count'] = train_df['text'].apply(lambda x: charcount(x))\\ntrain_df['word_count'] = train_df['text'].apply(lambda x: wordcount(x))\\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda x: hashtag_count(x))\\ntrain_df['mention_count'] = train_df['text'].apply(lambda x: mentions_count(x))\\ntrain_df['length']=train_df['text'].apply(len)\\n\\ntest_df['char_count'] = test_df['text'].apply(lambda x: charcount(x))\\ntest_df['word_count'] = test_df['text'].apply(lambda x: wordcount(x))\\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x: hashtag_count(x))\\ntest_df['mention_count'] = test_df['text'].apply(lambda x: mentions_count(x))\\ntest_df['length']=test_df['text'].apply(len)\\n\\ntrain_df.head(2)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell defines functions to compute various text-related features (character count, word count, hashtag count, mention count, and text length) and then applies these functions to the text columns in both the training and test datasets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9946966}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"sns.displot(data = train_df, kind = 'hist', x = 'length', hue = 'target', multiple = 'stack',bins=50,height = 5, aspect = 1.9)\\n\\n# The distibution of tweet text length vs target - there is a correlation between tweet length and target \",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This cell creates a histogram to visualize the distribution of tweet text length in the training dataset, segmented by the target variable.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9802619}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"sns.displot(data = train_df, kind = 'hist', x = 'hashtag_count', hue = 'target', multiple = 'stack',bins=50,height = 5, aspect = 1.9)\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This cell creates a histogram to visualize the distribution of hashtag counts in the training dataset, segmented by the target variable.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9985002}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"sns.displot(data = train_df, kind = 'hist', x = 'word_count', hue = 'target', multiple = 'stack',bins=50,height = 5, aspect = 1.9)\\n\",\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This cell creates a histogram to visualize the distribution of word counts in the training dataset, segmented by the target variable.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9985385}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'duplicates = pd.concat(x for _, x in train_df.groupby([\"text\"]) if len(x) > 1)\\n\\n#with pd.option_context(\"display.max_rows\", None, \"max_colwidth\", 80):\\n#    display(duplicates[[\"id\", \"target\", \"text\"]])',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': \"This cell identifies and concatenates duplicate rows in the training dataset based on the 'text' column, facilitating the analysis of duplicated entries.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'concatenate',\n",
       "       'subclass_id': 11,\n",
       "       'predicted_subclass_probability': 0.87380403}},\n",
       "     {'cell_id': 11,\n",
       "      'code': '# Taken from - Craig Thomas https://www.kaggle.com/craigmthomas/logistic-regression-lightgbm-fe\\ntrain_df.drop(\\n    [\\n        6449, 7034, 3589, 3591, 3597, 3600, 3603, 3604, 3610, 3613, 3614, 119, 106, 115,\\n        2666, 2679, 1356, 7609, 3382, 1335, 2655, 2674, 1343, 4291, 4303, 1345, 48, 3374,\\n        7600, 164, 5292, 2352, 4308, 4306, 4310, 1332, 1156, 7610, 2441, 2449, 2454, 2477,\\n        2452, 2456, 3390, 7611, 6656, 1360, 5771, 4351, 5073, 4601, 5665, 7135, 5720, 5723,\\n        5734, 1623, 7533, 7537, 7026, 4834, 4631, 3461, 6366, 6373, 6377, 6378, 6392, 2828,\\n        2841, 1725, 3795, 1251, 7607\\n    ], inplace=True\\n)\\n\\ntrain_df.drop(\\n    [\\n        4290, 4299, 4312, 4221, 4239, 4244, 2830, 2831, 2832, 2833, 4597, 4605, 4618, 4232, 4235, 3240,\\n        3243, 3248, 3251, 3261, 3266, 4285, 4305, 4313, 1214, 1365, 6614, 6616, 1197, 1331, 4379, 4381,\\n        4284, 4286, 4292, 4304, 4309, 4318, 610, 624, 630, 634, 3985, 4013, 4019, 1221, 1349, 6091, 6094, \\n        6103, 6123, 5620, 5641\\n    ], inplace=True\\n)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell removes specific rows from the training dataset based on their indices, likely to eliminate noise or irrelevant data as a preprocessing step.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'drop_column',\n",
       "       'subclass_id': 10,\n",
       "       'predicted_subclass_probability': 0.9402679}},\n",
       "     {'cell_id': 12,\n",
       "      'code': '## for data\\nimport json\\nimport pandas as pd\\nimport numpy as np\\n## for plotting\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n## for processing\\nimport re\\nimport nltk\\n## for bag-of-words\\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\\n## for explainer\\nfrom lime import lime_text\\n## for word embedding\\nimport gensim\\nimport gensim.downloader as gensim_api\\n## for deep learning\\nfrom tensorflow.keras import models, layers, preprocessing as kprocessing\\nfrom tensorflow.keras import backend as K\\n## for bert language model\\nimport transformers\\nimport unicodedata',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This cell imports additional libraries and modules for data processing, plotting, machine learning, deep learning, and language modeling.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.9993574}},\n",
       "     {'cell_id': 13,\n",
       "      'code': 'def preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\\n    \\n    text = re.sub(r\\'[^\\\\w\\\\s]\\', \\'\\', str(text).lower().strip())\\n    lst_text = text.split()\\n    if lst_stopwords is not None:\\n        lst_text = [word for word in lst_text if word not in \\n                    lst_stopwords]\\n                \\n    ## Stemming (remove -ing, -ly, ...)\\n    if flg_stemm == True:\\n        ps = nltk.stem.porter.PorterStemmer()\\n        lst_text = [ps.stem(word) for word in lst_text]\\n\\n    if flg_lemm == True:\\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\\n        lst_text = [lem.lemmatize(word) for word in lst_text]\\n            \\n                            \\n    ## back to string from list\\n    text = \" \".join(lst_text)\\n    return text',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell defines a function to preprocess text by converting it to lowercase, removing punctuation, optional stopword removal, and optional stemming or lemmatization.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9333879}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'lst_stopwords = nltk.corpus.stopwords.words(\"english\")\\n#lst_stopwords\\n',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell retrieves a list of English stopwords using the NLTK library and assigns it to the variable `lst_stopwords`.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9620199}},\n",
       "     {'cell_id': 15,\n",
       "      'code': 'contractions = { \\n\"ain\\'t\": \"am not\",\\n\"aren\\'t\": \"are not\",\\n\"can\\'t\": \"cannot\",\\n\"can\\'t\\'ve\": \"cannot have\",\\n\"\\'cause\": \"because\",\\n\"could\\'ve\": \"could have\",\\n\"couldn\\'t\": \"could not\",\\n\"couldn\\'t\\'ve\": \"could not have\",\\n\"didn\\'t\": \"did not\",\\n\"doesn\\'t\": \"does not\",\\n\"don\\'t\": \"do not\",\\n\"hadn\\'t\": \"had not\",\\n\"hadn\\'t\\'ve\": \"had not have\",\\n\"hasn\\'t\": \"has not\",\\n\"haven\\'t\": \"have not\",\\n\"he\\'d\": \"he would\",\\n\"he\\'d\\'ve\": \"he would have\",\\n\"he\\'ll\": \"he will\",\\n\"he\\'ll\\'ve\": \"he will have\",\\n\"he\\'s\": \"he is\",\\n\"how\\'d\": \"how did\",\\n\"how\\'d\\'y\": \"how do you\",\\n\"how\\'ll\": \"how will\",\\n\"how\\'s\": \"how does\",\\n\"i\\'d\": \"i would\",\\n\"i\\'d\\'ve\": \"i would have\",\\n\"i\\'ll\": \"i will\",\\n\"i\\'ll\\'ve\": \"i will have\",\\n\"i\\'m\": \"i am\",\\n\"i\\'ve\": \"i have\",\\n\"isn\\'t\": \"is not\",\\n\"it\\'d\": \"it would\",\\n\"it\\'d\\'ve\": \"it would have\",\\n\"it\\'ll\": \"it will\",\\n\"it\\'ll\\'ve\": \"it will have\",\\n\"it\\'s\": \"it is\",\\n\"let\\'s\": \"let us\",\\n\"ma\\'am\": \"madam\",\\n\"mayn\\'t\": \"may not\",\\n\"might\\'ve\": \"might have\",\\n\"mightn\\'t\": \"might not\",\\n\"mightn\\'t\\'ve\": \"might not have\",\\n\"must\\'ve\": \"must have\",\\n\"mustn\\'t\": \"must not\",\\n\"mustn\\'t\\'ve\": \"must not have\",\\n\"needn\\'t\": \"need not\",\\n\"needn\\'t\\'ve\": \"need not have\",\\n\"o\\'clock\": \"of the clock\",\\n\"oughtn\\'t\": \"ought not\",\\n\"oughtn\\'t\\'ve\": \"ought not have\",\\n\"shan\\'t\": \"shall not\",\\n\"sha\\'n\\'t\": \"shall not\",\\n\"shan\\'t\\'ve\": \"shall not have\",\\n\"she\\'d\": \"she would\",\\n\"she\\'d\\'ve\": \"she would have\",\\n\"she\\'ll\": \"she will\",\\n\"she\\'ll\\'ve\": \"she will have\",\\n\"she\\'s\": \"she is\",\\n\"should\\'ve\": \"should have\",\\n\"shouldn\\'t\": \"should not\",\\n\"shouldn\\'t\\'ve\": \"should not have\",\\n\"so\\'ve\": \"so have\",\\n\"so\\'s\": \"so is\",\\n\"that\\'d\": \"that would\",\\n\"that\\'d\\'ve\": \"that would have\",\\n\"that\\'s\": \"that is\",\\n\"there\\'d\": \"there would\",\\n\"there\\'d\\'ve\": \"there would have\",\\n\"there\\'s\": \"there is\",\\n\"they\\'d\": \"they would\",\\n\"they\\'d\\'ve\": \"they would have\",\\n\"they\\'ll\": \"they will\",\\n\"they\\'ll\\'ve\": \"they will have\",\\n\"they\\'re\": \"they are\",\\n\"they\\'ve\": \"they have\",\\n\"to\\'ve\": \"to have\",\\n\"wasn\\'t\": \"was not\",\\n\" u \": \" you \",\\n\" ur \": \" your \",\\n\" n \": \" and \",\\n\"won\\'t\": \"would not\",\\n\\'dis\\': \\'this\\',\\n\\'bak\\': \\'back\\',\\n\\'brng\\': \\'bring\\'}\\n\\ndef cont_to_exp(x):\\n    if type(x) is str:\\n        for key in contractions:\\n            value = contractions[key]\\n            x = x.replace(key, value)\\n        return x\\n    else:\\n        return x\\n    \\ntrain_df[\\'text_clean\\'] = train_df[\\'text\\'].apply(lambda x: cont_to_exp(x))\\ntest_df[\\'text_clean\\'] = test_df[\\'text\\'].apply(lambda x: cont_to_exp(x))\\n\\n\\ndef remove_emails(x):\\n     return re.sub(r\\'([a-z0-9+._-]+@[a-z0-9+._-]+\\\\.[a-z0-9+_-]+)\\',\"\", x)\\n\\n\\ndef remove_urls(x):\\n    return re.sub(r\\'(http|https|ftp|ssh)://([\\\\w_-]+(?:(?:\\\\.[\\\\w_-]+)+))([\\\\w.,@?^=%&:/~+#-]*[\\\\w@?^=%&/~+#-])?\\', \\'\\' , x)\\n\\ndef remove_rt(x):\\n    return re.sub(r\\'\\\\brt\\\\b\\', \\'\\', x).strip()\\n\\ndef remove_special_chars(x):\\n    x = re.sub(r\\'[^\\\\w ]+\\', \"\", x)\\n    x = \\' \\'.join(x.split())\\n    return x\\n\\n\\ndef remove_accented_chars(x):\\n    x = unicodedata.normalize(\\'NFKD\\', x).encode(\\'ascii\\', \\'ignore\\').decode(\\'utf-8\\', \\'ignore\\')\\n    return x\\n\\n\\n\\ntrain_df[\\'text_clean\\'] = train_df[\\'text_clean\\'].apply(lambda x: remove_emails(x))\\ntrain_df[\\'text_clean\\'] = train_df[\\'text_clean\\'].apply(lambda x: remove_urls(x))\\ntrain_df[\\'text_clean\\'] = train_df[\\'text_clean\\'].apply(lambda x: remove_rt(x))\\ntrain_df[\\'text_clean\\'] = train_df[\\'text_clean\\'].apply(lambda x: remove_special_chars(x))\\ntrain_df[\\'text_clean\\'] = train_df[\\'text_clean\\'].apply(lambda x: remove_accented_chars(x))',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This cell defines functions to expand contractions and remove emails, URLs, 'rt', special characters, and accented characters from text, applying these functions to clean the text in both the training and test datasets.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.99555004}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: preprocess_text(x, flg_stemm=True, flg_lemm=False, lst_stopwords=lst_stopwords))\\ntrain_df.head()',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This cell applies the `preprocess_text` function to the 'text_clean' column of the training dataset with stemming enabled and lemmatization disabled, and displays the first few rows of the updated DataFrame.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9974553}},\n",
       "     {'cell_id': 17,\n",
       "      'code': \"vec=TfidfVectorizer(max_features = 10000,ngram_range=(1,4))\\nvec.fit(train_df['text_clean'])\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell initializes a `TfidfVectorizer` to transform text data into a matrix of TF-IDF features, configured to consider up to 10,000 features and ngrams ranging from 1 to 4, and then fits the vectorizer to the cleaned text in the training dataset.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.5606868}},\n",
       "     {'cell_id': 18,\n",
       "      'code': \"matrix = vec.transform(train_df['text_clean']).toarray()\\nfeatures = vec.get_feature_names()\\nmatrix_df = pd.DataFrame(data=matrix, columns=features)\\n\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell transforms the cleaned text in the training dataset into a TF-IDF matrix, retrieves the feature names, and stores the resulting matrix in a DataFrame with the feature names as column headers.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.99546874}},\n",
       "     {'cell_id': 19,\n",
       "      'code': 'matrix_df.head(2)',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This cell displays the first two rows of the DataFrame containing the TF-IDF features extracted from the cleaned text in the training dataset.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997633}},\n",
       "     {'cell_id': 20,\n",
       "      'code': 'matrix_df.shape',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This cell outputs the shape of the DataFrame containing the TF-IDF features, indicating the number of rows and columns.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_shape',\n",
       "       'subclass_id': 58,\n",
       "       'predicted_subclass_probability': 0.9996574}},\n",
       "     {'cell_id': 21,\n",
       "      'code': \"matrix_df['length']=train_df['length']\\nmatrix_df['char_count']=train_df['char_count']\\nmatrix_df['word_count']=train_df['word_count']\\nmatrix_df['hashtag_count']=train_df['hashtag_count']\\nmatrix_df['mention_count']=train_df['mention_count']\\ny=train_df['target']\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell adds the additional computed text features (length, char_count, word_count, hashtag_count, mention_count) to the TF-IDF features DataFrame and assigns the target variable to `y`.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.8321087}},\n",
       "     {'cell_id': 22,\n",
       "      'code': 'params = {\\n        \\'boosting_type\\': \\'gbdt\\',\\n        \\'objective\\': \\'binary\\',\\n        \\'metric\\': \\'auc\\',\\n        \\'learning_rate\\': 0.01,\\n        \\'num_leaves\\':32,\\n        \\'subsample\\': 1,\\n        #\\'colsample_bytree\\': 0.25,\\n        #\\'reg_alpha\\': 0,\\n        #\\'reg_lambda\\': 1,\\n        #\\'scale_pos_weight\\': 5,\\n        \\'n_estimators\\': 10000,\\n        \\'verbose\\': -1,\\n        \\'max_depth\\': -1,\\n        \\'seed\\':100, \\n        \\'colsample_bytree\\':0.4,\\n        \\'force_col_wise\\': True\\n\\n\\n}\\n\"\"\"\\n    boosting_type=\\'gbdt\\', class_weight=None, colsample_bytree=0.4,\\n               importance_type=\\'split\\', learning_rate=0.04, max_depth=-1,\\n               metric=\\'auc\\', min_child_samples=20, min_child_weight=0.001,\\n               min_split_gain=0.0, n_estimators=1500, n_jobs=-1, num_leaves=31,\\n               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\\n               silent=True, subsample=1.0, subsample_for_bin=200000,\\n               subsample_freq=0 \\n\"\"\"\\nmetrics = oc.cv_adv(matrix_df,y,0.5,2000,shuffle=True,params=params)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"This cell sets the parameters for training a LightGBM model with a specified configuration and performs cross-validation using the `cv_adv` method from the octopus-ml package to evaluate the model's performance metrics.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'init_hyperparams',\n",
       "       'subclass_id': 59,\n",
       "       'predicted_subclass_probability': 0.99348336}},\n",
       "     {'cell_id': 23,\n",
       "      'code': \"oc.cv_plot(metrics['f1_weighted'],metrics['f1_macro'],metrics['f1_positive'],'Titanic Kaggle competition')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This cell uses the `cv_plot` function from the octopus-ml package to visualize the cross-validation performance metrics (weighted F1, macro F1, and positive class F1 scores) for the model, labeling the plot as 'Titanic Kaggle competition'.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.64448214}},\n",
       "     {'cell_id': 24,\n",
       "      'code': \"print(classification_report(metrics['y'], metrics['predictions_folds']))\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell prints a detailed classification report comparing the true labels with the predicted labels from the cross-validation folds, including precision, recall, and F1 scores.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.9977271}},\n",
       "     {'cell_id': 25,\n",
       "      'code': \"oc.roc_curve_plot(metrics['y'], metrics['predictions_proba'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell generates and displays an ROC curve plot to evaluate the performance of the model by visualizing the true positive rate against the false positive rate using the true labels and predicted probabilities.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.67355084}},\n",
       "     {'cell_id': 26,\n",
       "      'code': \"oc.confusion_matrix_plot(metrics['y'], metrics['predictions_folds'])\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell generates and displays a confusion matrix plot to evaluate the performance of the model by visualizing the number of correct and incorrect predictions across different classes using the true labels and predicted labels from the cross-validation folds.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.65878206}},\n",
       "     {'cell_id': 27,\n",
       "      'code': \"feature_imp_list=oc.plot_imp(metrics['final_clf'],matrix_df,'LightGBM Mortality Kaggle',num=40)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This cell plots the importance of the top 40 features using the trained LightGBM model and the TF-IDF features dataframe, labeling the plot as 'LightGBM Mortality Kaggle', and stores the feature importance list in `feature_imp_list`.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9963425}},\n",
       "     {'cell_id': 28,\n",
       "      'code': \"oc.preds_distribution(metrics['y'], metrics['predictions_proba'], bins=40)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell generates and displays a histogram to visualize the distribution of the predicted probabilities from the model, segmented by the true labels, using 40 bins.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.558016}},\n",
       "     {'cell_id': 29,\n",
       "      'code': \"top_features=feature_imp_list.sort_values(by='Value', ascending=False).head(20)\\ntop_features\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell sorts the feature importance list by value in descending order and displays the top 20 most important features for the model.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'sort_values',\n",
       "       'subclass_id': 9,\n",
       "       'predicted_subclass_probability': 0.992605}},\n",
       "     {'cell_id': 30,\n",
       "      'code': \"list_for_correlations=top_features['Feature'].to_list()\\nlist_for_correlations.append('target')\\noc.correlations(matrix_df,list_for_correlations)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell creates a list of the top 20 most important features along with the target variable, and then uses the `correlations` function from the octopus-ml package to compute and display the correlation matrix.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9603935}},\n",
       "     {'cell_id': 31,\n",
       "      'code': \"def Kaggle_submission(file_name,model,test_data,ids_list):\\n    #if TARGET in test_data.columns:\\n    #    test_data.drop([TARGET],axis=1,inplace=True)\\n    #test_pred=model.predict(test_data)[:,1]\\n    test_pred=model.predict(test_data)\\n    predictions = []\\n    predictions = oc.adjusted_classes(test_pred, 0.5)\\n\\n    submit=pd.DataFrame()\\n    submit['id'] = ids_list\\n    submit['target'] = predictions\\n    submit.to_csv(file_name,index=False)\\n    return submit\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This cell defines a function for creating a Kaggle submission file by predicting the target values using the provided model on the test data, adjusting the predictions based on a threshold, and saving the results along with the corresponding IDs to a CSV file.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9981369}},\n",
       "     {'cell_id': 32,\n",
       "      'code': 'test_df[\"text_clean\"]=test_df[\\'text\\']\\ntest_df[\\'text_clean\\'] = test_df[\\'text_clean\\'].apply(lambda x: remove_emails(x))\\ntest_df[\\'text_clean\\'] = test_df[\\'text_clean\\'].apply(lambda x: remove_urls(x))\\ntest_df[\\'text_clean\\'] = test_df[\\'text_clean\\'].apply(lambda x: remove_rt(x))\\ntest_df[\\'text_clean\\'] = test_df[\\'text_clean\\'].apply(lambda x: remove_special_chars(x))\\ntest_df[\\'text_clean\\'] = test_df[\\'text_clean\\'].apply(lambda x: remove_accented_chars(x))\\n\\ntest_df[\"text_clean\"] = test_df[\"text\"].apply(lambda x: preprocess_text(x, flg_stemm=True, flg_lemm=False, lst_stopwords=lst_stopwords))\\ntest_df[\\'length\\']=test_df[\\'text\\'].apply(len)\\n\\ntest_df.head()\\n\\n#vec=TfidfVectorizer(max_features = 20000,ngram_range=(1,4))\\n#vec.fit(test_df[\\'text_clean\\'])\\n\\n\\n\\nmatrix = vec.transform(test_df[\\'text_clean\\']).toarray()\\nfeatures = vec.get_feature_names()\\nmatrix_df = pd.DataFrame(data=matrix, columns=features)\\n\\nmatrix_df[\\'length\\']=test_df[\\'length\\']\\nmatrix_df[\\'char_count\\']=test_df[\\'char_count\\']\\nmatrix_df[\\'word_count\\']=test_df[\\'word_count\\']\\nmatrix_df[\\'hashtag_count\\']=test_df[\\'hashtag_count\\']\\nmatrix_df[\\'mention_count\\']=test_df[\\'mention_count\\']',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': \"This cell performs a series of text preprocessing steps on the test dataset, including email, URL, 'rt', special character, and accented character removal, followed by text preprocessing with stemming, and then transforms the cleaned text into a TF-IDF matrix, ultimately adding the additional computed features (length, char_count, word_count, hashtag_count, mention_count) to the matrix DataFrame.\",\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.99814296}},\n",
       "     {'cell_id': 33,\n",
       "      'code': \"test_pred=metrics['final_clf'].predict(matrix_df)\\npredictions = []\\n#predictions = oc.adjusted_classes(test_pred, 0.5)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell uses the final trained classifier to predict the target values for the test dataset and stores these predictions in a variable.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.9896577}},\n",
       "     {'cell_id': 34,\n",
       "      'code': '!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This cell downloads the `tokenization.py` script from the TensorFlow models repository using `wget`, necessary for BERT tokenization.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_url',\n",
       "       'subclass_id': 42,\n",
       "       'predicted_subclass_probability': 0.8866123}},\n",
       "     {'cell_id': 35,\n",
       "      'code': 'import tensorflow as tf\\nfrom tensorflow.keras.layers import Dense, Input\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import ModelCheckpoint\\nimport tensorflow_hub as hub\\n\\nimport tokenization',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This cell imports various modules from TensorFlow, TensorFlow Hub, and the recently downloaded `tokenization.py` script, which are essential for creating and training a BERT-based model.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99932015}},\n",
       "     {'cell_id': 36,\n",
       "      'code': '#Credit: https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\\ndef bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell defines a function to encode texts using BERT tokenization by converting text into tokens, padding and segmenting them to a specified maximum length, and returning arrays of tokens, masks, and segment IDs.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9645982}},\n",
       "     {'cell_id': 37,\n",
       "      'code': 'def build_model(bert_layer, max_len=512):\\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\\n\\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\n    clf_output = sequence_output[:, 0, :]\\n    \\n    if Dropout_num == 0:\\n        # Without Dropout\\n        out = Dense(1, activation=\\'sigmoid\\')(clf_output)\\n    else:\\n        # With Dropout(Dropout_num), Dropout_num > 0\\n        x = Dropout(Dropout_num)(clf_output)\\n        out = Dense(1, activation=\\'sigmoid\\')(x)\\n\\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\\n    model.compile(Adam(lr=learning_rate), loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n    \\n    return model',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This cell defines a function to build a BERT-based binary classification model, which includes input layers for token IDs, masks, and segment IDs, followed by the BERT layer and a dense output layer with an optional dropout layer, configuring the model with binary cross-entropy loss and Adam optimizer.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9445593}},\n",
       "     {'cell_id': 38,\n",
       "      'code': '# Load BERT from the Tensorflow Hub\\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\\nbert_layer = hub.KerasLayer(module_url, trainable=True)',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This cell loads a pre-trained BERT model from TensorFlow Hub, making it trainable for fine-tuning.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.87859446}},\n",
       "     {'cell_id': 39,\n",
       "      'code': '# Load tokenizer from the bert layer\\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This cell initializes the BERT tokenizer by loading the vocabulary file and case information from the loaded BERT layer.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.4494561}},\n",
       "     {'cell_id': 40,\n",
       "      'code': '# Encode the text into tokens, masks, and segment flags\\ntrain_input = bert_encode(train_df.text_clean.values, tokenizer, max_len=160)\\ntest_input = bert_encode(test_df.text_clean.values, tokenizer, max_len=160)\\ntrain_labels = train_df.target.values',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This cell encodes the cleaned text from the training and test datasets into arrays of tokens, masks, and segment IDs using the BERT tokenizer, setting a maximum length of 160, and extracts the target labels for the training data.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9929838}},\n",
       "     {'cell_id': 41,\n",
       "      'code': 'random_state_split = 2\\nDropout_num = 0\\nlearning_rate = 6e-6\\nvalid = 0.2\\nepochs_num = 3\\nbatch_size_num = 16\\ntarget_corrected = False\\ntarget_big_corrected = False\\n\\n# Build BERT model with my tuning\\nmodel_BERT = build_model(bert_layer, max_len=160)\\nmodel_BERT.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This cell sets various hyperparameters and configurations for training, then builds and summarizes a BERT-based model with the specified maximum input length of 160 tokens.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.66016126}},\n",
       "     {'cell_id': 42,\n",
       "      'code': \"checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\\n\\ntrain_history = model_BERT.fit(\\n    train_input, train_labels,\\n    validation_split = valid,\\n    epochs = epochs_num, # recomended 3-5 epochs\\n    callbacks=[checkpoint],\\n    batch_size = batch_size_num\\n)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This cell trains the BERT model on the encoded training data and labels with specified validation split, epochs, and batch size, while saving the best model based on validation loss using a checkpoint callback.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.99751294}},\n",
       "     {'cell_id': 43,\n",
       "      'code': \"model_BERT.load_weights('model_BERT.h5')\\ntest_pred_BERT = model_BERT.predict(test_input)\\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell loads the best weights into the trained BERT model and generates predictions on the encoded test data, rounding these predictions to integers for final classification output.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.991269}},\n",
       "     {'cell_id': 44,\n",
       "      'code': \"train_pred_BERT = model_BERT.predict(train_input)\\ntrain_pred_BERT_int = train_pred_BERT.round().astype('int')\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This cell generates predictions on the encoded training data using the trained BERT model and rounds these predictions to integers for comparison to the true labels.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.990164}},\n",
       "     {'cell_id': 45,\n",
       "      'code': \"submit=pd.DataFrame()\\nsubmit['id'] = test_df['id'].tolist()\\nsubmit['target'] = test_pred_BERT_int\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This cell creates a DataFrame for submission, containing the IDs from the test dataset and the corresponding integer predictions from the trained BERT model.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.90852714}},\n",
       "     {'cell_id': 46,\n",
       "      'code': \"submit.to_csv('BERT_model_v3.csv',index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This cell exports the submission DataFrame to a CSV file named 'BERT_model_v3.csv' without including the DataFrame index.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.9991627}},\n",
       "     {'cell_id': 47,\n",
       "      'code': 'submit.head(3)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This cell displays the first three rows of the submission DataFrame.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.9997651}}],\n",
       "    'notebook_id': 24},\n",
       "   'notebook_id': 24},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': 'import random\\n\\nimport pandas as pd\\nimport numpy as np \\nfrom scipy.special import softmax\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import (roc_auc_score, classification_report, \\n                             confusion_matrix)\\nimport tensorflow as tf\\nfrom transformers import BertTokenizer\\nfrom transformers import TFBertForSequenceClassification\\nfrom transformers import AutoConfig\\n',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports various libraries and modules required for data manipulation, visualization, model training, and evaluation tasks, including pandas, numpy, seaborn, matplotlib, scikit-learn, tensorflow, and transformers.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99931514}},\n",
       "     {'cell_id': 1,\n",
       "      'code': \"# The name of the BERT model used\\nPRETRAINED_MODEL_NAME = 'bert-base-uncased'\\n# The number of labels of the target variable\\nLABELS_NUMBER = 2\\n\\n# The max lenght of text can be up to 512 for BERT\\nMAX_LENGHT = 512\\n\\nBATCH_SIZE = 6\\nLEARNING_RATE = 2e-5\\nEPOCHS_NUMBER = 1\\n\\nN_PREDICTIONS_TO_SHOW = 10\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet defines constants related to the BERT model configuration, including the model name, number of labels, maximum text length, batch size, learning rate, number of epochs, and the number of predictions to display.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.99904436}},\n",
       "     {'cell_id': 2,\n",
       "      'code': \"train_data = pd.read_csv('../input/nlp-getting-started/train.csv')\\nprint(train_data.shape)\\ntrain_data.head(3)\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the training data from a CSV file into a pandas DataFrame and prints its shape and the first three rows to verify the loaded data.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9993906}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"# load test dataset\\ntest_data = pd.read_csv('../input/nlp-getting-started/test.csv')\\nprint(test_data.shape)\\ntest_data.head(3)\",\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet reads the test data from a CSV file into a pandas DataFrame and prints its shape and the first three rows to verify the loaded data.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99944836}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'for tweet_index in range(1,30,5):\\n    print(f\\'Text of the tweet: {train_data[\"text\"][tweet_index]}\\')\\n    print(f\\'Target: {\"Real disaster\" if train_data[\"target\"][tweet_index]==1 else \"Not real disaster\"}\\\\n\\')',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prints the text and corresponding target label of selected tweets from the training data, iterating through the dataset at intervals of 5 between indices 1 and 30, to give a sense of the tweet content and classification.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table_attributes',\n",
       "       'subclass_id': 40,\n",
       "       'predicted_subclass_probability': 0.29876474}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'sns.countplot(train_data[\"target\"])',\n",
       "      'class': 'Visualization',\n",
       "      'desc': 'This code snippet creates and displays a count plot using seaborn to show the distribution of the target variable in the training data.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'distribution',\n",
       "       'subclass_id': 33,\n",
       "       'predicted_subclass_probability': 0.9974095}},\n",
       "     {'cell_id': 6,\n",
       "      'code': '# Get the Bert tokenizer\\ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, \\n                                          do_lower_case=True)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet initializes the BERT tokenizer from the pretrained BERT model specified by `PRETRAINED_MODEL_NAME`, with the `do_lower_case` parameter set to True, to use for tokenizing text data.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.9954691}},\n",
       "     {'cell_id': 7,\n",
       "      'code': \"# Print some words of the vocabulary\\nvocabulary = tokenizer.get_vocab()\\nprint(f'Size of the vocabulary: {len(vocabulary)}')\\nprint(f'Some tokens of the vocabulary: {list(vocabulary.keys())[5000:5010]}')\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet retrieves the vocabulary from the BERT tokenizer, prints its size, and displays a sample of tokens from the vocabulary to understand its structure and content.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9397918}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'def prepare_sequence(text):\\n    \"\"\"\\n    Tokenize and prepare a sequence for the model. It tokenizes the text sequence\\n    adding special tokens ([CLS], [SEP]), padding  to the max length and truncate \\n    reviews longer than the max length.\\n    Return the token IDs, the segment IDs and the mask IDs.\\n    \"\"\"\\n\\n    prepared_sequence = tokenizer.encode_plus(\\n                            text, \\n                            add_special_tokens = True, \\n                            max_length = MAX_LENGHT, \\n                            padding = \\'max_length\\',\\n                            return_attention_mask = True\\n                            )\\n    return prepared_sequence',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines a function `prepare_sequence` that tokenizes input text sequences using the BERT tokenizer, adds special tokens, pads to a specified maximum length, and returns token IDs, segment IDs, and attention masks, essential for modeling with BERT.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9985875}},\n",
       "     {'cell_id': 9,\n",
       "      'code': '# Prepare a test sentence\\ntest_sentence = \\'Is this jacksonville?\\'\\ntest_sentence_encoded = prepare_sequence(test_sentence)\\ntoken_ids = test_sentence_encoded[\"input_ids\"]\\nprint(f\\'Test sentence:   {test_sentence}\\')\\nprint(f\\'Keys:            {test_sentence_encoded.keys()}\\')\\nprint(f\\'Tokens:          {tokenizer.convert_ids_to_tokens(token_ids)[:12]}\\')\\nprint(f\\'Token IDs:       {token_ids[:12]}\\')\\nprint(f\\'Segment IDs:     {test_sentence_encoded[\"token_type_ids\"][:12]}\\')\\nprint(f\\'Mask IDs         {test_sentence_encoded[\"attention_mask\"][:12]}\\')\\nprint(f\\'Input dimension: {len(token_ids)}\\')',\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet prepares and tokenizes a test sentence using the `prepare_sequence` function, and then prints out its tokens, token IDs, segment IDs, attention mask IDs, and the total number of tokens to verify the correctness of the tokenization process.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.42718402}},\n",
       "     {'cell_id': 10,\n",
       "      'code': 'def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\\n    \"\"\"\\n    Map to the expected input to TFBertForSequenceClassification.\\n    \"\"\"\\n    mapped_example = {\\n        \"input_ids\": input_ids,\\n        \"token_type_ids\": token_type_ids,\\n        \"attention_mask\": attention_masks,\\n    }\\n    return mapped_example, label \\n\\ndef encode_examples(texts_and_labels):\\n    \"\"\"\\n    Prepare all sequences of text and build TF dataset.\\n    \"\"\"\\n\\n    input_ids_list = []\\n    token_type_ids_list = []\\n    attention_mask_list = []\\n    label_list = []\\n        \\n    for text, label in texts_and_labels:\\n\\n        bert_input = prepare_sequence(text)\\n\\n        input_ids_list.append(bert_input[\\'input_ids\\'])\\n        token_type_ids_list.append(bert_input[\\'token_type_ids\\'])\\n        attention_mask_list.append(bert_input[\\'attention_mask\\'])\\n        label_list.append([label])\\n\\n    # Create TF dataset\\n    dataset = tf.data.Dataset.from_tensor_slices(\\n        (input_ids_list, attention_mask_list, token_type_ids_list,\\n         label_list)\\n    )\\n    # Map to the expected input to TFBertForSequenceClassification\\n    dataset_mapped = dataset.map(map_example_to_dict)\\n    return dataset_mapped',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines two functions: `map_example_to_dict`, which maps inputs to the format expected by `TFBertForSequenceClassification`, and `encode_examples`, which prepares all text sequences and constructs a TensorFlow dataset by tokenizing the texts and mapping them to the necessary input format.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9974004}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'X = train_data[\"text\"]\\ny = train_data[\"target\"]',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet extracts the text data and target labels from the training dataset into separate variables `X` for the text and `y` for the labels, preparing them for further processing or modeling.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'prepare_x_and_y',\n",
       "       'subclass_id': 21,\n",
       "       'predicted_subclass_probability': 0.99927586}},\n",
       "     {'cell_id': 12,\n",
       "      'code': '# Split the training dataset for training and test\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, \\n                                                    random_state=1)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet splits the dataset into training and validation sets with 10% of the data reserved for validation, ensuring the split is reproducible by using a specified random state.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.99780315}},\n",
       "     {'cell_id': 13,\n",
       "      'code': \"n_training_examples = X_train.shape[0]\\nn_positive_training_examples = y_train.value_counts()[1]\\nn_negative_training_examples = y_train.value_counts()[0]\\nprint(f'Number examples in training dataset: {n_training_examples}')\\nprint(f'Number of positive examples in training dataset: {n_positive_training_examples}')\\nprint(f'Number of negative examples in training dataset: {n_negative_training_examples}')\",\n",
       "      'class': 'Exploratory_Data_Analysis',\n",
       "      'desc': 'This code snippet calculates and prints the total number of training examples, as well as the number of positive and negative examples in the training dataset, to understand the class distribution.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'count_values',\n",
       "       'subclass_id': 72,\n",
       "       'predicted_subclass_probability': 0.9978163}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'train_dataset = list(zip(X_train, y_train))\\nval_dataset = list(zip(X_val, y_val))',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet combines the text and target label pairs for the training and validation datasets into lists of tuples, preparing them for further processing or model training.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'define_variables',\n",
       "       'subclass_id': 77,\n",
       "       'predicted_subclass_probability': 0.7672821}},\n",
       "     {'cell_id': 15,\n",
       "      'code': '# Prepare sequences of text and build TF train dataset\\nds_train_encoded = encode_examples(train_dataset).shuffle(10000).batch(BATCH_SIZE)\\n\\n# Prepare sequences of text and build TF validation dataset\\nds_val_encoded = encode_examples(val_dataset).batch(BATCH_SIZE)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet encodes the training and validation datasets into the format needed for the BERT model using the `encode_examples` function, shuffles the training dataset, and batches both datasets according to the specified batch size.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.8710919}},\n",
       "     {'cell_id': 16,\n",
       "      'code': 'def get_model():\\n    # Define the configuration of the model\\n    config = AutoConfig.from_pretrained(PRETRAINED_MODEL_NAME,\\n                                        hidden_dropout_prob=0.2,\\n                                        num_labels=LABELS_NUMBER)\\n    # Model initialization\\n    model = TFBertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, \\n                                                            config=config)\\n    return model',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines a function `get_model` that initializes and returns a BERT model for sequence classification with a specified dropout probability and the number of labels, based on a pre-trained model configuration.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.98904186}},\n",
       "     {'cell_id': 17,\n",
       "      'code': \"# Model initialization\\nmodel = get_model()\\n\\n# Define the optimizer, the loss function and metrics\\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\\n# Compile the model\\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet initializes the BERT model, defines the optimizer, loss function, and accuracy metric, and then compiles the model to prepare it for training.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.9943469}},\n",
       "     {'cell_id': 18,\n",
       "      'code': \"# Scaling by total/2 helps keep the loss to a similar magnitude.\\n# The sum of the weights of all examples stays the same.\\nweight_for_0 = (1 / n_negative_training_examples)*(n_training_examples)/2.0 \\nweight_for_1 = (1 / n_positive_training_examples)*(n_training_examples)/2.0\\n\\nclass_weight = {0: weight_for_0, 1: weight_for_1}\\n\\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet calculates class weights to handle class imbalances in the training data by scaling the weights inversely proportional to class frequencies, and then prints the computed weights for each class.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.44395435}},\n",
       "     {'cell_id': 19,\n",
       "      'code': '# Train the model\\nmodel.fit(ds_train_encoded, epochs=EPOCHS_NUMBER, validation_data=ds_val_encoded,\\n          class_weight = class_weight)',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the BERT model on the encoded training dataset for a specified number of epochs, using the validation dataset for evaluation, and applies class weights to address class imbalances.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9996909}},\n",
       "     {'cell_id': 20,\n",
       "      'code': '# Get predictions in the validation dataset\\nval_predictions = model.predict(ds_val_encoded)\\nval_probabilities = softmax(val_predictions[0], axis=1)\\ny_val_predictions = np.argmax(val_probabilities, axis=1).flatten()',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet generates predictions on the validation dataset using the trained BERT model, applies the softmax function to obtain probabilities, and determines the predicted class labels by selecting the highest probability for each instance.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.9947659}},\n",
       "     {'cell_id': 21,\n",
       "      'code': '# Compute metrics to evaluate the model\\nclassification_metrics = classification_report(y_val, y_val_predictions)\\n# Compute the area under the ROC curve\\narea_under_the_curve = roc_auc_score(y_val, val_probabilities[:,1:2], multi_class=\"ovr\")\\n# Compute the confusion matrix\\nerror_matrix = confusion_matrix(y_val, y_val_predictions)\\nprint(f\\'Area under the ROC curve: {area_under_the_curve}\\')\\nprint(f\\'Classification metrics:\\\\n{classification_metrics}\\')\\n# Plot the confusion matrix\\nax = plt.axes()\\nsns.heatmap(error_matrix, annot=True, fmt=\"d\")\\nax.set_title(\\'Confusion matrix Validation set\\')',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet computes various metrics to evaluate the model's performance on the validation set, including the classification report, ROC AUC score, and confusion matrix, and then visualizes the confusion matrix using a heatmap.\",\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'heatmap',\n",
       "       'subclass_id': 80,\n",
       "       'predicted_subclass_probability': 0.68382823}},\n",
       "     {'cell_id': 22,\n",
       "      'code': '# Show some predictions in the validation dataset\\nfor i in random.sample(range(len(val_dataset)), k=N_PREDICTIONS_TO_SHOW):\\n    print(f\\'\\\\nText:       {X_test.values[i]}\\')\\n    print(f\\'Ground truth: {\"Real disaster\" if y_val.values[i]==1 else \"Not real disaster\"}\\')\\n    print(f\\'Predicted:    {\"Real disaster\" if y_val_predictions[i]==1 else \"Not real disaster\"}\\')',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': \"This code snippet randomly selects and displays a specified number of text examples from the validation dataset, along with their ground truth labels and the model's predicted labels, to qualitatively assess the model's performance.\",\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'show_table',\n",
       "       'subclass_id': 41,\n",
       "       'predicted_subclass_probability': 0.49180317}},\n",
       "     {'cell_id': 23,\n",
       "      'code': 'def encode_test_examples(texts):\\n    \"\"\"\\n    Prepare all sequences of text and build TF dataset.\\n    \"\"\"\\n\\n    input_ids_list = []\\n    token_type_ids_list = []\\n    attention_mask_list = []\\n        \\n    for text in texts:\\n\\n        bert_input = prepare_sequence(text)\\n\\n        input_ids_list.append(bert_input[\\'input_ids\\'])\\n        token_type_ids_list.append(bert_input[\\'token_type_ids\\'])\\n        attention_mask_list.append(bert_input[\\'attention_mask\\'])\\n\\n    # Create TF dataset\\n    dataset = tf.data.Dataset.from_tensor_slices(\\n        (input_ids_list, attention_mask_list, token_type_ids_list)\\n    )\\n    # Map to the expected input to TFBertForSequenceClassification\\n    dataset_mapped = dataset.map(map_test_example_to_dict)\\n    return dataset_mapped\\n\\ndef map_test_example_to_dict(input_ids, attention_masks, token_type_ids):\\n    \"\"\"\\n    Map to the expected input to TFBertForSequenceClassification.\\n    \"\"\"\\n    mapped_example = {\\n        \"input_ids\": input_ids,\\n        \"token_type_ids\": token_type_ids,\\n        \"attention_mask\": attention_masks,\\n    }\\n    return mapped_example',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet defines two functions: `encode_test_examples`, which prepares sequences of text for the test dataset and constructs a TensorFlow dataset, and `map_test_example_to_dict`, which maps the test inputs to the format expected by `TFBertForSequenceClassification`.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9800521}},\n",
       "     {'cell_id': 24,\n",
       "      'code': 'X_test = test_data[\"text\"]\\ntest_dataset = list(X_test)\\nds_test_encoded = encode_test_examples(test_dataset).batch(BATCH_SIZE)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet extracts the text data from the test dataset, converts it into a list, encodes the test dataset using the `encode_test_examples` function, and batches it according to the specified batch size.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9627081}},\n",
       "     {'cell_id': 25,\n",
       "      'code': 'test_predictions = model.predict(ds_test_encoded)\\ntest_probabilities = softmax(test_predictions[0], axis=1)\\ny_test_predictions = np.argmax(test_probabilities, axis=1).flatten()',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet generates predictions on the encoded test dataset using the trained BERT model, converts the predicted logits to probabilities using the softmax function, and determines the final predicted class labels by selecting the highest probability for each instance.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.99444926}},\n",
       "     {'cell_id': 26,\n",
       "      'code': '# Copy the results to a pandas dataframe with an \"id\" column and a \"target\" column\\nfinal_submission = pd.DataFrame( data={\"id\":test_data[\"id\"], \"target\":y_test_predictions})\\n# Save the submission file\\nfinal_submission.to_csv(\"submissionTweets.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'This code snippet creates a pandas DataFrame containing the test predictions and corresponding ids, and then saves this DataFrame to a CSV file named \"submissionTweets.csv\" for submission or further use.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99925834}}],\n",
       "    'notebook_id': 25},\n",
       "   'notebook_id': 25},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the read-only \"../input/\" directory\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \\n# You can also write temporary files to /kaggle/temp/, but they won\\'t be saved outside of the current session',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code imports necessary libraries like numpy and pandas for data processing and lists all files in the input directory of the Kaggle environment.',\n",
       "      'testing': {'class': 'Exploratory_Data_Analysis',\n",
       "       'subclass': 'list_files',\n",
       "       'subclass_id': 88,\n",
       "       'predicted_subclass_probability': 0.99921954}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'train_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\\nsubmission_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'The code reads the train, test, and sample submission CSV files from the Kaggle input directory into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.9997112}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'import hashlib\\n\\nimport spacy\\nimport sklearn\\nfrom sklearn.svm import SVC\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import f1_score',\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'The code imports various libraries and modules required for natural language processing, machine learning model training, evaluation, and data preprocessing tasks.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'import_modules',\n",
       "       'subclass_id': 22,\n",
       "       'predicted_subclass_probability': 0.99932015}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"nlp = spacy.load('en')\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code loads the spaCy language model for English, which is used for natural language processing tasks.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'load_pretrained',\n",
       "       'subclass_id': 30,\n",
       "       'predicted_subclass_probability': 0.9950819}},\n",
       "     {'cell_id': 4,\n",
       "      'code': \"# remove stopwords,punct\\n# remove duplicate tweet\\ntexts = []\\nlabels = []\\ntexts_md5 = set()\\nfor target, doc in zip(train_raw.target, nlp.pipe(train_raw.text)):\\n    tokens = [token.lemma_ for token in doc if token.is_stop is False and token.is_punct is False and token.is_space is False]\\n    temp_text = ' '.join(tokens)\\n    # remove duplicate\\n    md5 = hashlib.md5()\\n    md5.update(temp_text.encode('utf-8'))\\n    text_md5 = md5.hexdigest()\\n    if text_md5 not in texts_md5:\\n        texts.append(temp_text)\\n        labels.append(target)\\n        texts_md5.add(text_md5)\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code processes the training text data by removing stop words, punctuation, and duplicate tweets while lemmatizing the remaining tokens, and stores the processed texts and their corresponding labels, avoiding duplicates using their MD5 hash values.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.9552063}},\n",
       "     {'cell_id': 5,\n",
       "      'code': \"tests = []\\nfor doc in nlp.pipe(test_raw.text):\\n    tokens = [token.lemma_ for token in doc if token.is_stop is False and token.is_punct is False and token.is_space is False]\\n    tests.append(' '.join(tokens))\",\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code processes the test text data by removing stop words, punctuation, and lemmatizing the remaining tokens, and stores the processed texts.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'string_transform',\n",
       "       'subclass_id': 78,\n",
       "       'predicted_subclass_probability': 0.9769126}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'tf_idf = TfidfVectorizer(max_features=10000).fit(texts)\\ntrain = tf_idf.transform(texts)\\ntest = tf_idf.transform(tests)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code fits a TF-IDF vectorizer to the processed training texts and transforms both the training and test texts into their respective TF-IDF representations.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'feature_engineering',\n",
       "       'subclass_id': 8,\n",
       "       'predicted_subclass_probability': 0.9555307}},\n",
       "     {'cell_id': 7,\n",
       "      'code': 'X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.3)',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'The code splits the transformed training data and corresponding labels into training and testing subsets with 30% of the data reserved for testing.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.99791616}},\n",
       "     {'cell_id': 8,\n",
       "      'code': 'param_grid = {\\n    \"gamma\" : [0.001,0.01,1,10,100],\\n    \"C\":[0.001,0.01,1,10,100],\\n    \\'kernel\\' : [\\'poly\\', \"linear\", \\'sigmoid\\', \\'rbf\\']\\n}',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code defines a parameter grid to be used for hyperparameter tuning of an SVM model, specifying different values for `gamma`, `C`, and `kernel`.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'define_search_space',\n",
       "       'subclass_id': 5,\n",
       "       'predicted_subclass_probability': 0.99403256}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"svc = SVC()\\ngrid_searcher = GridSearchCV(svc, param_grid, cv=5, scoring='f1')\\ngrid_searcher.fit(X_train, y_train)\\ngrid_searcher.best_params_\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code initializes an SVM classifier, performs 5-fold cross-validated grid search to find the best hyperparameters based on the F1 score, and retrieves the best parameters.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_on_grid',\n",
       "       'subclass_id': 6,\n",
       "       'predicted_subclass_probability': 0.99040365}},\n",
       "     {'cell_id': 10,\n",
       "      'code': \"best_params= {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\\nsvc = SVC(**best_params)\\nscores = cross_val_score(svc,X_train, y_train, cv=5, scoring='f1')\\nprint(scores)\\nprint(sum(scores)/len(scores))\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'The code initializes an SVM classifier with the best found hyperparameters, performs 5-fold cross-validation on the training data to evaluate its F1 score, prints the individual fold scores, and calculates the average F1 score.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'compute_train_metric',\n",
       "       'subclass_id': 28,\n",
       "       'predicted_subclass_probability': 0.9838514}},\n",
       "     {'cell_id': 11,\n",
       "      'code': 'val_texts = [\"A happy day!\", \\'An earthquake happened!\\']\\nval_data = tf_idf.transform(val_texts)\\nsvc.fit(X_train, y_train)\\nprint(svc.predict(val_data))',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': \"The code transforms sample validation texts into TF-IDF features, fits the SVM model using the training data, and prints the model's predictions for the sample validation texts.\",\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.6037878}},\n",
       "     {'cell_id': 12,\n",
       "      'code': 'y_hat = svc.predict(test)',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'The code uses the trained SVM model to predict the labels for the test data set.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.99424076}},\n",
       "     {'cell_id': 13,\n",
       "      'code': \"submission = pd.DataFrame({\\n    'id': test_raw.id,\\n    'target':y_hat\\n})\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'The code creates a DataFrame for the submission file, containing the IDs from the test data and the predicted target labels.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'create_dataframe',\n",
       "       'subclass_id': 12,\n",
       "       'predicted_subclass_probability': 0.9941958}},\n",
       "     {'cell_id': 14,\n",
       "      'code': 'submission.to_csv(\"my_submission_linear.csv\", index=False)',\n",
       "      'class': 'Data_Export',\n",
       "      'desc': 'The code exports the submission DataFrame to a CSV file named \"my_submission_linear.csv\" without including the DataFrame index.',\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99924576}}],\n",
       "    'notebook_id': 26},\n",
       "   'notebook_id': 26},\n",
       "  {'cells': {'cells': [{'cell_id': 0,\n",
       "      'code': \"import os\\nimport time\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nimport transformers\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.layers import Dense, Input\\nfrom tensorflow.keras.callbacks import ModelCheckpoint\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\",\n",
       "      'class': 'Imports_and_Environment',\n",
       "      'desc': 'This code snippet imports necessary libraries and modules for data manipulation, model building, and evaluation while setting up the environment to ignore warning messages.',\n",
       "      'testing': {'class': 'Imports_and_Environment',\n",
       "       'subclass': 'set_options',\n",
       "       'subclass_id': 23,\n",
       "       'predicted_subclass_probability': 0.9989656}},\n",
       "     {'cell_id': 1,\n",
       "      'code': 'def bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n    \\n    return np.array(all_tokens)\\n\\ndef build_model(transformer, max_len=512):\\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    sequence_output = transformer(input_word_ids)[0]\\n    cls_token = sequence_output[:, 0, :]\\n    out = Dense(1, activation=\\'sigmoid\\')(cls_token)\\n    \\n    model = Model(inputs=input_word_ids, outputs=out)\\n    model.compile(Adam(lr=1e-5), loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n    \\n    return model',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet defines functions to encode text using a BERT tokenizer and to build a binary classification model using a pre-trained transformer model.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'categorify',\n",
       "       'subclass_id': 20,\n",
       "       'predicted_subclass_probability': 0.7754506}},\n",
       "     {'cell_id': 2,\n",
       "      'code': 'train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest  = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")',\n",
       "      'class': 'Data_Extraction',\n",
       "      'desc': 'This code snippet loads the training, testing, and sample submission datasets from CSV files into pandas DataFrames.',\n",
       "      'testing': {'class': 'Data_Extraction',\n",
       "       'subclass': 'load_from_csv',\n",
       "       'subclass_id': 45,\n",
       "       'predicted_subclass_probability': 0.99972814}},\n",
       "     {'cell_id': 3,\n",
       "      'code': \"%%time\\n# Distil Bert Base\\n# model_to_use = 'distilbert-base-uncased'\\n\\n# model_to_use = 'albert-base-v1'\\n# model_to_use = 'albert-large-v1'\\n# model_to_use = 'albert-xlarge-v1'\\n# model_to_use = 'albert-xxlarge-v1'\\n\\nmodel_to_use = 'bert-base-uncased'\\n# model_to_use = 'bert-large-uncased'\\n\\n# model_to_use = 'roberta-base'\\n# model_to_use = 'roberta-large'\\n\\nif model_to_use.split('-')[0] == 'distilbert':\\n    transformer_layer = transformers.TFDistilBertModel.from_pretrained(model_to_use)\\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(model_to_use)\\n    \\nif model_to_use.split('-')[0] == 'albert':\\n    transformer_layer = transformers.TFAlbertModel.from_pretrained(model_to_use)\\n    tokenizer = transformers.AlbertTokenizer.from_pretrained(model_to_use)\\n    \\nif model_to_use.split('-')[0] == 'bert':\\n    transformer_layer = transformers.TFBertModel.from_pretrained(model_to_use)\\n    tokenizer = transformers.BertTokenizer.from_pretrained(model_to_use)\\n    \\nif model_to_use.split('-')[0] == 'roberta':\\n    transformer_layer = transformers.TFRobertaModel.from_pretrained(model_to_use)\\n    tokenizer = transformers.RobertaTokenizer.from_pretrained(model_to_use)\",\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet selects and loads a pre-trained transformer model along with its corresponding tokenizer based on a specified model name.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'choose_model_class',\n",
       "       'subclass_id': 4,\n",
       "       'predicted_subclass_probability': 0.6072228}},\n",
       "     {'cell_id': 4,\n",
       "      'code': 'max_seq_len = 160\\n\\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=max_seq_len)\\ntest_input  = bert_encode(test.text.values, tokenizer, max_len=max_seq_len)\\ntrain_label = train.target.values\\n\\n# Data split\\nX_train, X_test, y_train, y_test = train_test_split(train_input, \\n                                                    train_label, \\n                                                    test_size=0.25,\\n                                                    random_state=42, \\n                                                    shuffle=True)\\n# X_train.shape, X_test.shape = ((5709, 160), (1904, 160))',\n",
       "      'class': 'Data_Transform',\n",
       "      'desc': 'This code snippet encodes the text data from the train and test datasets using the BERT tokenizer, derives training labels, and then splits the encoded training data into training and validation sets.',\n",
       "      'testing': {'class': 'Data_Transform',\n",
       "       'subclass': 'split',\n",
       "       'subclass_id': 13,\n",
       "       'predicted_subclass_probability': 0.99449795}},\n",
       "     {'cell_id': 5,\n",
       "      'code': 'def metrics(y_true, y_pred):\\n    print(\"\\\\nF1-score: \", round(f1_score(y_true, y_pred), 2))\\n    print(\"Precision: \", round(precision_score(y_true, y_pred), 2))\\n    print(\"Recall: \", round(recall_score(y_true, y_pred), 2))',\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet defines a function to calculate and print the F1-score, precision, and recall metrics for true and predicted labels.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'compute_test_metric',\n",
       "       'subclass_id': 49,\n",
       "       'predicted_subclass_probability': 0.998026}},\n",
       "     {'cell_id': 6,\n",
       "      'code': 'model = build_model(transformer_layer, max_len=160)\\nmodel.summary()',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet builds a binary classification model using the previously defined `build_model` function and prints its summary.',\n",
       "      'testing': {'class': 'Visualization',\n",
       "       'subclass': 'model_coefficients',\n",
       "       'subclass_id': 79,\n",
       "       'predicted_subclass_probability': 0.9385333}},\n",
       "     {'cell_id': 7,\n",
       "      'code': '# Training\\nstart_time = time.time()\\ntrain_history = model.fit(X_train, y_train, epochs = 3, batch_size = 8)\\nend_time = time.time()\\nprint(\"\\\\n=>Training time :\", round(end_time - start_time, 1), \\'s\\')',\n",
       "      'class': 'Model_Train',\n",
       "      'desc': 'This code snippet trains the model on the training data for three epochs with a batch size of eight and prints the training duration.',\n",
       "      'testing': {'class': 'Model_Train',\n",
       "       'subclass': 'train_model',\n",
       "       'subclass_id': 7,\n",
       "       'predicted_subclass_probability': 0.9996803}},\n",
       "     {'cell_id': 8,\n",
       "      'code': \"# Validation\\nstart_time = time.time()\\ntest_pred = model.predict(X_test, verbose=1).round().astype(int)\\nend_time = time.time()\\n\\nprint('\\\\n=>Average Inference Time :', round((end_time - start_time) / len(test_pred) * 1000, 1), 'ms')\\nmetrics(y_test, test_pred)\",\n",
       "      'class': 'Model_Evaluation',\n",
       "      'desc': 'This code snippet performs predictions on the validation set, calculates the average inference time, and evaluates the predictions using predefined metrics.',\n",
       "      'testing': {'class': 'Model_Evaluation',\n",
       "       'subclass': 'predict_on_test',\n",
       "       'subclass_id': 48,\n",
       "       'predicted_subclass_probability': 0.88822424}},\n",
       "     {'cell_id': 9,\n",
       "      'code': \"submission['target'] = model.predict(test_input, verbose=1).round().astype(int)\\nsubmission.to_csv('submission.csv', index=False)\",\n",
       "      'class': 'Data_Export',\n",
       "      'desc': \"This code snippet generates predictions for the test set, assigns them to the 'target' column of the submission DataFrame, and saves the submission DataFrame to a CSV file.\",\n",
       "      'testing': {'class': 'Data_Export',\n",
       "       'subclass': 'save_to_csv',\n",
       "       'subclass_id': 25,\n",
       "       'predicted_subclass_probability': 0.99906117}}],\n",
       "    'notebook_id': 27},\n",
       "   'notebook_id': 27}],\n",
       " 'metadata': {'labels': ['Data_Transform',\n",
       "   'Data_Extraction',\n",
       "   'Visualization',\n",
       "   'Model_Train',\n",
       "   'Model_Evaluation',\n",
       "   'Imports_and_Environment',\n",
       "   'Data_Export',\n",
       "   'Exploratory_Data_Analysis'],\n",
       "  'accuracy': 74.71655328798185}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [02:31<00:00,  5.42s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys; sys.path.insert(0, \"../\")\n",
    "from Clusterers.clusterer import ClassCluster\n",
    "with open('../../../secrets/api_key.txt', 'r') as f: api_key = f'{f.read()}'\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "clusterer = ClassCluster()\n",
    "for notebook in tqdm(data[\"notebooks\"]):\n",
    "    for cell in notebook[\"cells\"][\"cells\"]:\n",
    "        cell[\"embedding\"] = clusterer.embed_cell(cell[\"code\"], cell[\"desc\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
