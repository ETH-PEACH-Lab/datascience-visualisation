{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "notebook_file = \"/home/ryounis/Documents/Zurich/PEACHLab/datascience-visualisation/data/datasets/Natural Language Processing with Disaster Tweets/classified_notebooks.json\"\n",
    "with open(notebook_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "LABELS = data[\"metadata\"][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryounis/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 28/28 [00:28<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Transform: 246\n",
      "Data_Extraction: 55\n",
      "Visualization: 82\n",
      "Model_Train: 120\n",
      "Model_Evaluation: 88\n",
      "Imports_and_Environment: 80\n",
      "Data_Export: 38\n",
      "Exploratory_Data_Analysis: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys; sys.path.insert(0, \"../\")\n",
    "from Clusterers.clusterer import ClassCluster\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('../../../secrets/api_key.txt', 'r') as f: api_key = f'{f.read()}'\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "clusterer = ClassCluster()\n",
    "grouped_cells = {label: [] for label in LABELS}\n",
    "for notebook in tqdm(data[\"notebooks\"]):\n",
    "    for cell in notebook[\"cells\"]:\n",
    "        cell[\"embedding\"] = clusterer.embed_cell(cell[\"code\"], cell[\"desc\"])\n",
    "        grouped_cells[cell[\"class\"]].append(cell)\n",
    "        \n",
    "for label, cells in grouped_cells.items():\n",
    "    print(f'{label}: {len(cells)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = grouped_cells[\"Data_Transform\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = np.array([cell[\"embedding\"] for cell in cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryounis/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.4911424 , -20.977583  ],\n",
       "       [  4.1030197 ,  -8.340393  ],\n",
       "       [  4.1923532 ,  -8.065796  ],\n",
       "       [  3.9924166 ,  -8.613281  ],\n",
       "       [  3.0819228 ,   7.3527956 ],\n",
       "       [ -6.8498898 ,   9.318042  ],\n",
       "       [ -8.514273  ,   4.8970323 ],\n",
       "       [  3.0791552 ,   7.351713  ],\n",
       "       [  2.7196898 ,   3.7462842 ],\n",
       "       [ -3.08045   ,  15.636194  ],\n",
       "       [ -8.1040535 ,   7.3929768 ],\n",
       "       [ -7.6331873 ,   6.8954935 ],\n",
       "       [ -8.570385  ,  -1.3460537 ],\n",
       "       [  0.40983674,  -5.7024317 ],\n",
       "       [  9.1918955 ,   4.253837  ],\n",
       "       [ -0.5927596 ,  16.993883  ],\n",
       "       [ -0.7142469 ,  16.78827   ],\n",
       "       [ -5.877225  ,   4.5820756 ],\n",
       "       [ -4.0901    ,  -3.8603642 ],\n",
       "       [ -0.53378254,  14.493711  ],\n",
       "       [  8.059087  ,   8.669608  ],\n",
       "       [  8.057986  ,   8.684082  ],\n",
       "       [  4.608428  ,   1.8113874 ],\n",
       "       [  4.398058  ,  11.601454  ],\n",
       "       [  4.7468767 ,   1.7341857 ],\n",
       "       [ -2.359604  ,  -5.2552366 ],\n",
       "       [ -5.0885825 ,  -3.835769  ],\n",
       "       [ -8.775151  ,   0.23621811],\n",
       "       [ -3.3844588 ,  -5.992502  ],\n",
       "       [  3.2120152 ,   6.0106926 ],\n",
       "       [  9.688266  ,  12.806893  ],\n",
       "       [ -3.1561413 ,   1.4054289 ],\n",
       "       [-15.602245  ,   5.2613587 ],\n",
       "       [  9.991606  ,  12.725576  ],\n",
       "       [  8.974494  ,  13.190348  ],\n",
       "       [  4.826329  ,   6.3154764 ],\n",
       "       [  8.951937  ,  -2.9944313 ],\n",
       "       [ -8.518968  ,   8.856438  ],\n",
       "       [  8.6084    , -10.549362  ],\n",
       "       [  4.0857463 , -13.081872  ],\n",
       "       [ -0.4260254 ,  -7.8928137 ],\n",
       "       [ -4.620434  ,   8.135983  ],\n",
       "       [ -7.299626  ,   2.925058  ],\n",
       "       [ -5.763961  ,   6.1743054 ],\n",
       "       [ -7.429165  ,   2.1752853 ],\n",
       "       [  2.8559005 , -11.290524  ],\n",
       "       [ 10.973705  ,   1.8633778 ],\n",
       "       [  0.4489207 ,   3.6364226 ],\n",
       "       [  1.3829736 , -11.470097  ],\n",
       "       [-15.020634  ,  10.6184435 ],\n",
       "       [ -4.482664  ,  -1.149621  ],\n",
       "       [ -3.1204236 ,  11.179773  ],\n",
       "       [ -6.6073055 ,  17.159983  ],\n",
       "       [-10.6750555 ,  15.300937  ],\n",
       "       [ 11.033347  , -19.073463  ],\n",
       "       [  2.8475358 ,  -4.384909  ],\n",
       "       [ -2.0854921 ,  -4.72011   ],\n",
       "       [ -4.7375097 ,  14.91324   ],\n",
       "       [ -3.708754  ,  13.551936  ],\n",
       "       [ -5.088515  ,  10.501     ],\n",
       "       [  1.2664462 ,   4.9167223 ],\n",
       "       [ 12.033732  , -11.541703  ],\n",
       "       [  4.4662647 , -11.199044  ],\n",
       "       [  4.217767  , -11.674744  ],\n",
       "       [  1.7360191 , -17.638857  ],\n",
       "       [  3.0798044 , -12.776994  ],\n",
       "       [ -7.6380515 ,  -6.1609936 ],\n",
       "       [  4.6887584 ,  -0.03914154],\n",
       "       [  6.533724  ,   0.5754346 ],\n",
       "       [  3.6544302 ,  14.597061  ],\n",
       "       [  6.5074906 , -11.858356  ],\n",
       "       [  3.6429496 , -14.0352545 ],\n",
       "       [  4.6890736 , -14.359901  ],\n",
       "       [  2.437911  ,  15.724731  ],\n",
       "       [  6.7900786 , -12.196148  ],\n",
       "       [  3.6858928 , -15.164356  ],\n",
       "       [  5.665966  , -13.516985  ],\n",
       "       [  6.4296026 ,   2.0189128 ],\n",
       "       [  6.6545553 ,  -3.797109  ],\n",
       "       [ -4.316378  ,  -9.482368  ],\n",
       "       [ -8.911797  ,   1.6495987 ],\n",
       "       [ -0.63847905, -13.204003  ],\n",
       "       [ -0.76524526, -13.645268  ],\n",
       "       [ -0.17642057, -13.072916  ],\n",
       "       [ 11.011986  , -12.729398  ],\n",
       "       [ 11.417535  , -12.632001  ],\n",
       "       [  8.036204  , -13.789354  ],\n",
       "       [  1.8697172 , -16.608606  ],\n",
       "       [ -0.9634768 , -10.6177    ],\n",
       "       [  7.4331403 , -15.697743  ],\n",
       "       [  7.4531984 , -15.748064  ],\n",
       "       [-14.553859  ,   8.409329  ],\n",
       "       [ -0.9956612 , -10.540956  ],\n",
       "       [  4.06037   , -18.676212  ],\n",
       "       [-14.943383  ,  14.092121  ],\n",
       "       [-14.939403  ,  14.079824  ],\n",
       "       [-18.200945  ,  11.445278  ],\n",
       "       [ -9.724468  ,   7.5347643 ],\n",
       "       [ -2.0417278 ,   1.0397137 ],\n",
       "       [ -3.5632493 ,   9.787016  ],\n",
       "       [ -3.019399  ,   4.925535  ],\n",
       "       [ -2.8653598 ,  16.01131   ],\n",
       "       [  6.809896  ,  -4.8141136 ],\n",
       "       [ 11.776925  ,   7.7577524 ],\n",
       "       [  1.9960078 ,  13.7166195 ],\n",
       "       [  6.4345527 ,   3.175998  ],\n",
       "       [  7.836699  ,   2.94682   ],\n",
       "       [  7.923975  ,   3.3495839 ],\n",
       "       [ -6.9299664 ,  16.114973  ],\n",
       "       [  1.0369406 , -15.495145  ],\n",
       "       [ 11.435338  ,   7.9085197 ],\n",
       "       [  1.6798513 , -15.29268   ],\n",
       "       [ -6.7668676 ,   7.6278934 ],\n",
       "       [ -0.31787205, -16.429451  ],\n",
       "       [-12.75347   ,   6.946363  ],\n",
       "       [-10.077307  ,  13.118757  ],\n",
       "       [  1.879407  ,   5.276279  ],\n",
       "       [-13.285581  ,  11.40872   ],\n",
       "       [-10.95551   ,  16.004637  ],\n",
       "       [ -5.2960396 ,  -4.284764  ],\n",
       "       [ -7.8030224 ,  -2.3838074 ],\n",
       "       [ -7.8998413 ,  10.98991   ],\n",
       "       [-13.45829   ,  11.500038  ],\n",
       "       [ -2.098795  ,   5.916464  ],\n",
       "       [  7.586321  , -11.763202  ],\n",
       "       [-18.473793  ,  10.631288  ],\n",
       "       [-13.788611  ,   9.919141  ],\n",
       "       [  2.19227   , -14.340541  ],\n",
       "       [ -3.6463225 ,  -1.8648857 ],\n",
       "       [-10.334268  ,   1.9126722 ],\n",
       "       [-12.095957  ,   3.491854  ],\n",
       "       [  2.2510223 ,   2.7288682 ],\n",
       "       [-12.055382  ,   3.435962  ],\n",
       "       [-14.849721  ,   1.5755281 ],\n",
       "       [  4.8172483 , -19.745558  ],\n",
       "       [ 10.158436  ,  -1.7382355 ],\n",
       "       [ 13.299309  ,   0.98543173],\n",
       "       [ 13.281995  ,   0.9916169 ],\n",
       "       [  0.72051316,  14.756723  ],\n",
       "       [  6.7249846 ,  -6.8803954 ],\n",
       "       [  4.87239   ,  -5.288677  ],\n",
       "       [ -0.8552658 ,  -7.4717793 ],\n",
       "       [  4.8024654 , -19.83588   ],\n",
       "       [  6.658818  ,  -6.8609366 ],\n",
       "       [  9.316136  , -15.086629  ],\n",
       "       [ -2.7672415 ,   6.345767  ],\n",
       "       [ -1.8851329 ,  -1.8641586 ],\n",
       "       [ -8.098972  ,  12.462139  ],\n",
       "       [ -2.795406  ,  -1.971202  ],\n",
       "       [-14.045732  ,   1.6019909 ],\n",
       "       [  0.8217532 ,  -4.073481  ],\n",
       "       [  3.6103487 ,  -3.1428638 ],\n",
       "       [ -1.4771562 ,  -3.8703377 ],\n",
       "       [  3.2463152 ,  -3.5963917 ],\n",
       "       [  0.08053005,  -3.18523   ],\n",
       "       [-10.749737  ,  10.900685  ],\n",
       "       [  4.740876  ,   4.922604  ],\n",
       "       [  5.7181535 , -19.112741  ],\n",
       "       [  0.5346666 ,   6.0908203 ],\n",
       "       [  5.3542027 ,   7.8866687 ],\n",
       "       [  6.0174384 ,   8.873576  ],\n",
       "       [  1.8808582 , -12.587631  ],\n",
       "       [ -0.703811  ,   7.4264717 ],\n",
       "       [ -1.619957  , -18.870045  ],\n",
       "       [ -1.0030869 ,   7.5051756 ],\n",
       "       [ -1.2651656 ,   8.957152  ],\n",
       "       [  4.0416107 ,   8.821385  ],\n",
       "       [ -1.5951645 ,   9.414739  ],\n",
       "       [ -0.58932036,   4.332915  ],\n",
       "       [-12.248752  ,  13.24694   ],\n",
       "       [ -9.71812   ,   4.107433  ],\n",
       "       [ -0.6836764 ,   5.470822  ],\n",
       "       [ -9.730919  ,   4.255055  ],\n",
       "       [  2.4184358 ,  -9.763221  ],\n",
       "       [ -3.4276917 ,  13.549579  ],\n",
       "       [  1.765311  ,  -8.313595  ],\n",
       "       [  2.752961  ,  -8.94554   ],\n",
       "       [  1.9034454 ,  -9.517391  ],\n",
       "       [  2.689214  ,  -7.0995936 ],\n",
       "       [ -3.8533013 ,   3.5139039 ],\n",
       "       [ -6.051204  ,   9.825768  ],\n",
       "       [ 12.406368  ,  -7.9197106 ],\n",
       "       [ -3.6775718 ,   5.1770234 ],\n",
       "       [  5.2594748 ,  -9.755101  ],\n",
       "       [  6.970217  ,  -9.642674  ],\n",
       "       [ -2.02417   ,   4.354738  ],\n",
       "       [ -1.629808  ,   2.5375261 ],\n",
       "       [ -5.794597  ,  -0.87719995],\n",
       "       [ -1.3985996 ,   2.8299544 ],\n",
       "       [ -6.3908    ,  -1.3628763 ],\n",
       "       [ -1.3097304 ,   2.7198236 ],\n",
       "       [ -6.1134553 ,  -1.2944112 ],\n",
       "       [ -6.983636  ,   5.7963862 ],\n",
       "       [ -6.365953  ,  -0.4468835 ],\n",
       "       [ -8.0384245 ,  12.260972  ],\n",
       "       [ -7.1103816 ,  -0.80238575],\n",
       "       [  1.3807209 ,   9.673805  ],\n",
       "       [  9.528444  ,  -3.7490685 ],\n",
       "       [ 12.02499   ,  -2.6664767 ],\n",
       "       [ 12.035615  ,  -2.6415079 ],\n",
       "       [-13.088502  ,  -2.1748838 ],\n",
       "       [ 11.022153  , -19.07696   ],\n",
       "       [  8.693322  ,   1.82369   ],\n",
       "       [  7.9896755 ,  -8.05356   ],\n",
       "       [ 11.157551  ,   4.36402   ],\n",
       "       [  1.3930172 ,  13.793808  ],\n",
       "       [  9.36695   ,  -7.4447412 ],\n",
       "       [-10.358156  ,  -5.4107537 ],\n",
       "       [-11.374082  ,   6.6224036 ],\n",
       "       [ -6.2236657 ,  -8.37891   ],\n",
       "       [  1.8247467 ,   0.0640066 ],\n",
       "       [  2.0765707 ,  -0.92514175],\n",
       "       [ -2.8569844 , -18.252577  ],\n",
       "       [ -8.233906  , -12.120044  ],\n",
       "       [ -2.9680934 , -17.726389  ],\n",
       "       [ -8.235794  , -12.138669  ],\n",
       "       [ -9.888899  ,  -5.9012775 ],\n",
       "       [ -5.4648843 ,  -6.313779  ],\n",
       "       [  7.1719913 ,   5.128985  ],\n",
       "       [ -2.8664668 , -17.762976  ],\n",
       "       [  1.4946516 , -20.869478  ],\n",
       "       [ -4.87249   ,  13.326306  ],\n",
       "       [  8.921921  ,  -1.7610298 ],\n",
       "       [ -5.868545  ,  16.927528  ],\n",
       "       [ -2.9250617 , -20.127605  ],\n",
       "       [ -3.5244508 ,  14.488954  ],\n",
       "       [  7.990937  ,  -6.3916745 ],\n",
       "       [  1.8038685 ,   2.8635237 ],\n",
       "       [  0.22786504,   1.2459419 ],\n",
       "       [-18.572382  ,   9.971396  ],\n",
       "       [  0.13713798,   1.6884476 ],\n",
       "       [ -4.3648643 ,   9.670094  ],\n",
       "       [ -3.5945346 ,   7.683328  ],\n",
       "       [ -0.1997497 ,  -2.337449  ],\n",
       "       [ -3.8549535 ,  16.565983  ],\n",
       "       [  6.3884635 ,  -8.076441  ],\n",
       "       [ -5.712761  ,   1.0717326 ],\n",
       "       [ -5.0437927 ,   4.5406694 ],\n",
       "       [  2.600987  ,  12.52804   ],\n",
       "       [ -0.3566282 ,  10.303238  ],\n",
       "       [-15.547438  ,  12.610553  ],\n",
       "       [-10.982545  ,   9.457242  ],\n",
       "       [ -9.235936  ,  12.762201  ],\n",
       "       [ -6.0983567 ,   1.064768  ],\n",
       "       [  0.6762013 ,  10.003982  ],\n",
       "       [  0.46581823,   7.8991637 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=3000)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "reduced_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 65)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=0.9)\n",
    "\n",
    "# Fit the PCA model to the embeddings\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Print the shape of the reduced embeddings\n",
    "print(reduced_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster -1: 51\n",
      "Cluster 0: 13\n",
      "Cluster 1: 21\n",
      "Cluster 2: 47\n",
      "Cluster 3: 36\n",
      "Cluster 4: 78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "import numpy as np\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    " \n",
    "# reduced_embeddings = pairwise.cosine_distances(reduced_embeddings)\n",
    "clusterer = HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    min_samples=2,\n",
    "    cluster_selection_epsilon=0,\n",
    "    max_cluster_size=None,\n",
    "    alpha=1,\n",
    "    # metric=\"precomputed\"\n",
    ")\n",
    "\n",
    "clusterer.fit(reduced_embeddings)\n",
    "labels = clusterer.labels_\n",
    "for i, n in  enumerate(np.bincount([label+1 for label in labels])):\n",
    "    print(f\"Cluster {i-1}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster -1: 5\n",
      "Cluster 0: 31\n",
      "Cluster 1: 15\n"
     ]
    }
   ],
   "source": [
    "cluster_indices = np.where(labels == -1)[0]\n",
    "cluster_points = reduced_embeddings[cluster_indices]\n",
    "cluster_points = pairwise.cosine_distances(cluster_points)\n",
    "clusterer.fit(cluster_points)\n",
    "outlier_labels = clusterer.labels_\n",
    "for i, n in  enumerate(np.bincount([label+1 for label in outlier_labels])):\n",
    "    print(f\"Cluster {i-1}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster -1: 5\n",
      "Cluster 0: 13\n",
      "Cluster 1: 21\n",
      "Cluster 2: 47\n",
      "Cluster 3: 36\n",
      "Cluster 4: 78\n",
      "Cluster 5: 31\n",
      "Cluster 6: 15\n"
     ]
    }
   ],
   "source": [
    "max_cluster = max(labels)+1\n",
    "for i, idx in enumerate(cluster_indices):\n",
    "    if outlier_labels[i] != -1:\n",
    "        labels[idx] = max_cluster + outlier_labels[i]\n",
    "\n",
    "for i, n in  enumerate(np.bincount([label+1 for label in labels])):\n",
    "    print(f\"Cluster {i-1}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:06<00:00,  1.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': 'Natural Language Processing with spaCy and NLTK',\n",
       " '1': 'Text Preprocessing with NLP Methods in Python',\n",
       " '2': 'Data Preprocessing with NLTK and Pandas',\n",
       " '3': 'Data Preparation and TensorFlow Models',\n",
       " '4': 'Text Preprocessing and Vectorization Techniques',\n",
       " '5': 'Data Processing with Pandas, NLTK, TfidfVectorizer',\n",
       " '6': 'Text Preprocessing with TensorFlow and CountVectorizer',\n",
       " '-1': 'Data Preprocessing and Encoding with Dictionaries'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Clusterers.title_generator import TitleGenerator\n",
    "\n",
    "title_generator = TitleGenerator()\n",
    "descs = [cell[\"desc\"] for cell in cells]\n",
    "titles = title_generator.generate_titles_from_descs(labels, descs)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE visualization of clusters')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Add titles for each cluster\n",
    "for i, label in enumerate(np.unique(labels)):\n",
    "    if label != -1:\n",
    "        plt.text(reduced_embeddings[labels == label, 0].mean(), reduced_embeddings[labels == label, 1].mean(), titles[str(label)], fontsize=8, ha='center', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score:           0.24592193961143494 (how similar a point is to its own cluster (cohesion) compared to other clusters (separation))\n",
      "Calinski-Harabasz Index:    103.57980725673966 (ratio of the sum of between-cluster dispersion and within-cluster dispersion)\n",
      "Davies-Bouldin Index:       1.5289414279040352 (average similarity ratio of each cluster with the cluster that is most similar to it)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "sil_score = silhouette_score(reduced_embeddings, labels)\n",
    "ch_index = calinski_harabasz_score(reduced_embeddings, labels)\n",
    "db_index = davies_bouldin_score(reduced_embeddings, labels)\n",
    "print(f\"Silhouette Score:           {sil_score} (how similar a point is to its own cluster (cohesion) compared to other clusters (separation))\")\n",
    "print(f\"Calinski-Harabasz Index:    {ch_index} (ratio of the sum of between-cluster dispersion and within-cluster dispersion)\")\n",
    "print(f\"Davies-Bouldin Index:       {db_index} (average similarity ratio of each cluster with the cluster that is most similar to it)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_df = train_df.drop(columns=[\"id\", \"keyword\", \"location\"])</td>\n",
       "      <td>This code snippet drops the \"id\", \"keyword\", and \"location\" columns from the training DataFrame to focus on the relevant data for the analysis.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def remove_URL(text):\\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\\n    return url.sub(r'',text)\\n\\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_URL)\\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_URL)</td>\n",
       "      <td>This code snippet defines a function to remove URLs from text and applies this function to the \"text\" column in both the training and test DataFrames to clean the data.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def remove_html(text):\\n    html=re.compile(r'&lt;.*?&gt;')\\n    return html.sub(r'',text)\\n\\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_html)\\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_html)</td>\n",
       "      <td>This code snippet defines a function to remove HTML tags from text and applies this function to the \"text\" column in both the training and test DataFrames to further clean the data.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def remove_emoji(text):\\n    emoji_pattern = re.compile(\"[\"\\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols &amp; pictographs\\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport &amp; map symbols\\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\\n                           u\"\\U00002702-\\U000027B0\"\\n                           u\"\\U000024C2-\\U0001F251\"\\n                           \"]+\", flags=re.UNICODE)\\n    return emoji_pattern.sub(r'', text)\\n\\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_emoji)\\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_emoji)</td>\n",
       "      <td>This code snippet defines a function to remove emojis from text and applies this function to the \"text\" column in both the training and test DataFrames to clean the data further.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_tensor = tokenizer(list(train_df[\"text\"]), padding=\"max_length\",\\n                        truncation=True, max_length=30,\\n                        return_tensors=\"pt\")[\"input_ids\"]</td>\n",
       "      <td>This code snippet tokenizes the text data from the training DataFrame, padding to a maximum length, truncating where necessary, and converting the tokens into tensors suitable for model input.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>class TweetDataset:\\n    def __init__(self, tensors, targ, ids):\\n        self.text = tensors[ids, :]\\n        self.targ = targ[ids].reset_index(drop=True)\\n    \\n    def __len__(self):\\n        return len(self.text)\\n    \\n    def __getitem__(self, idx):\\n        \\n        t = self.text[idx]\\n        y = self.targ[idx]\\n        \\n        return t, tensor(y)</td>\n",
       "      <td>This code snippet defines a custom dataset class, `TweetDataset`, which takes tokenized text tensors, target labels, and indices to create a dataset that can be utilized in model training and evaluation processes.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_ids, valid_ids = RandomSplitter()(train_df)\\n\\n\\ntarget = train_df[\"target\"]\\n\\ntrain_ds = TweetDataset(train_tensor, target, train_ids)\\nvalid_ds = TweetDataset(train_tensor, target, valid_ids)\\n\\ntrain_dl = DataLoader(train_ds, bs=64)\\nvalid_dl = DataLoader(valid_ds, bs=512)\\ndls = DataLoaders(train_dl, valid_dl).to(\"cuda\")</td>\n",
       "      <td>This code snippet splits the training DataFrame into training and validation sets, creates corresponding datasets using the custom `TweetDataset` class, wraps these datasets in DataLoader objects with specified batch sizes, and then combines them into a DataLoaders object, moving them to the GPU for training.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_tensor = tokenizer(list(test_df[\"text\"]),\\n                        padding=\"max_length\",\\n                        truncation=True,\\n                        max_length=30,\\n                        return_tensors=\"pt\")[\"input_ids\"]</td>\n",
       "      <td>This code snippet tokenizes the text data from the test DataFrame, padding to a maximum length, truncating where necessary, and converting the tokens into tensors suitable for model inference.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>class TestDS:\\n    def __init__(self, tensors):\\n        self.tensors = tensors\\n    \\n    def __len__(self):\\n        return len(self.tensors)\\n    \\n    def __getitem__(self, idx):\\n        t = self.tensors[idx]\\n        return t, tensor(0)\\n\\ntest_dl = DataLoader(TestDS(test_tensor), bs=128)</td>\n",
       "      <td>This code snippet defines a custom dataset class, `TestDS`, for the tokenized test data and creates a DataLoader with a specified batch size to facilitate model inference.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stop_words=nltk.corpus.stopwords.words('english')\\ni=0\\n#sc=SpellChecker()\\n#data=pd.concat([train,test])\\nwnl=WordNetLemmatizer()\\nstemmer=SnowballStemmer('english')\\nfor doc in train.text:\\n    doc=re.sub(r'https?://\\S+|www\\.\\S+','',doc)\\n    doc=re.sub(r'&lt;.*?&gt;','',doc)\\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\\n    doc=contractions.fix(doc)\\n    tokens=nltk.word_tokenize(doc)\\n    filtered=[token for token in tokens if token not in stop_words]\\n    doc=' '.join(filtered)\\n    train.text[i]=doc\\n    i+=1\\ni=0\\nfor doc in test.text:\\n    doc=re.sub(r'https?://\\S+|www\\.\\S+','',doc)\\n    doc=re.sub(r'&lt;.*?&gt;','',doc)\\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\\n    doc=contractions.fix(doc)\\n    tokens=nltk.word_tokenize(doc)\\n    filtered=[token for token in tokens if token not in stop_words]\\n    doc=' '.join(filtered)\\n    test.text[i]=doc\\n    i+=1</td>\n",
       "      <td>This code processes the text data in the 'text' column of the training and testing datasets by removing URLs, HTML tags, non-alphabetic characters, and stop words, and applying lemmatization and contraction fixes to standardize and prepare the text for further analysis.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>from sklearn.feature_extraction.text import CountVectorizer\\ncv=CountVectorizer(ngram_range=(1,1)) \\n\\n#    ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, \\n#    and (2, 2) means only bigrams.\\n\\ncv_matrix=cv.fit_transform(train.text).toarray()\\ntrain_df=pd.DataFrame(cv_matrix,columns=cv.get_feature_names())\\ntest_df=pd.DataFrame(cv.transform(test.text).toarray(),columns=cv.get_feature_names())\\ntrain_df.head()</td>\n",
       "      <td>This code uses the `CountVectorizer` to convert the processed text data from the training and testing datasets into a matrix of token counts (unigrams) and then creates DataFrames from these matrices for further analysis.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from sklearn.feature_extraction.text import TfidfVectorizer\\ntfidf=TfidfVectorizer(ngram_range=(1,1),use_idf=True)\\nmat=tfidf.fit_transform(train.text).toarray()\\ntrain_df=pd.DataFrame(mat,columns=tfidf.get_feature_names())\\ntest_df=pd.DataFrame(tfidf.transform(test.text).toarray(),columns=tfidf.get_feature_names())\\ntrain_df.head()</td>\n",
       "      <td>This code utilizes the `TfidfVectorizer` to convert the processed text data from the training and testing datasets into TF-IDF feature matrices for unigrams and then creates DataFrames from these matrices for further analysis.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>from sklearn.model_selection import train_test_split\\n\\nX = train['text'] + ' ' +  train['keyword'].astype(str) + ' ' +  train['location'].astype(str) # the features we want to analyze\\nylabels = train['target'] # the labels, or answers, we want to test against\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)</td>\n",
       "      <td>This code snippet constructs feature data by concatenating text, keyword, and location columns and splits the data into training and testing sets with the corresponding targets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>my_submission_preds = pipe.predict(test['text']+ ' ' +  test['keyword'].astype(str) + ' ' +  test['location'].astype(str))\\n\\nmy_submission = pd.DataFrame({\"id\":test['id'], 'target':my_submission_preds})</td>\n",
       "      <td>This code snippet generates predictions for the test dataset using the trained pipeline and creates a submission DataFrame with 'id' and corresponding predicted 'target' values.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nlp = spacy.load('en')</td>\n",
       "      <td>This code snippet loads the English language model from the spaCy library for natural language processing tasks.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td># remove stopwords,punct\\n# remove duplicate tweet\\ntexts = []\\nlabels = []\\ntexts_md5 = set()\\nfor target, doc in zip(train_raw.target, nlp.pipe(train_raw.text)):\\n    tokens = [token.lemma_ for token in doc if token.is_stop is False and token.is_punct is False and token.is_space is False]\\n    temp_text = ' '.join(tokens)\\n    # remove duplicate\\n    md5 = hashlib.md5()\\n    md5.update(temp_text.encode('utf-8'))\\n    text_md5 = md5.hexdigest()\\n    if text_md5 not in texts_md5:\\n        texts.append(temp_text)\\n        labels.append(target)\\n        texts_md5.add(text_md5)</td>\n",
       "      <td>This code snippet processes the training data by removing stopwords and punctuation, lemmatizing tokens, and removing duplicate tweets based on their MD5 hashes.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tests = []\\nfor doc in nlp.pipe(test_raw.text):\\n    tokens = [token.lemma_ for token in doc if token.is_stop is False and token.is_punct is False and token.is_space is False]\\n    tests.append(' '.join(tokens))</td>\n",
       "      <td>This code snippet processes the testing data by removing stopwords, punctuation, lemmatizing tokens, and then converting tokens back into strings.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tf_idf = TfidfVectorizer(max_features=10000).fit(texts)\\ntrain = tf_idf.transform(texts)\\ntest = tf_idf.transform(tests)</td>\n",
       "      <td>This code snippet creates a TF-IDF Vectorizer with a maximum of 10,000 features, fits it to the training texts, and then transforms both the training and testing texts into TF-IDF features.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.3)</td>\n",
       "      <td>This code snippet splits the training DataFrame and corresponding labels into training and testing subsets with 30% of the data allocated to the test set.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>def remove_URL(text):\\n    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\\n    return url.sub(r\"\", text)\\n\\ndef remove_punct(text):\\n    translator = str.maketrans(\"\", \"\", string.punctuation)\\n    return text.translate(translator)</td>\n",
       "      <td>This code defines two functions to preprocess text: one to remove URLs using regular expressions and another to remove punctuation using string translation.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>#regex pattern to remove links\\npattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\\n#for train\\nfor t in train.text:\\n    matches = pattern.findall(t)\\n    for match in matches:\\n        print(t)\\n        print('After Transformation:')\\n        print(pattern.sub(r\"\", t))\\n    if len(matches) &gt; 0:\\n        break</td>\n",
       "      <td>This code snippet demonstrates how URLs in the text of the training dataset can be identified and removed using a regular expression pattern, displaying an example transformation.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>#for test:\\nfor t in test.text:\\n    matches = pattern.findall(t)\\n    for match in matches:\\n        print(t)\\n        print('After Transformation:')\\n        print(pattern.sub(r\"\", t))\\n    if len(matches) &gt; 0:\\n        break</td>\n",
       "      <td>This code snippet demonstrates how URLs in the text of the testing dataset can be identified and removed using a regular expression pattern, displaying an example transformation.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>#preprocess data frames:\\n#train\\ntrain[\"text\"] = train.text.map(remove_URL) \\ntrain[\"text\"] = train.text.map(remove_punct)\\n#test\\ntest[\"text\"] = test.text.map(remove_URL) \\ntest[\"text\"] = test.text.map(remove_punct)</td>\n",
       "      <td>This code snippet preprocesses the text data in both the training and testing datasets by applying the previously defined functions to remove URLs and punctuation from the text.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td># remove stopwords\\nnltk.download('stopwords')\\n\\nstop = set(stopwords.words(\"english\"))\\n\\ndef remove_stopwords(text):\\n    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\\n    return \" \".join(filtered_words)</td>\n",
       "      <td>This code snippet downloads the list of English stopwords, defines a set of these stopwords, and creates a function to remove these stopwords from the given text.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>#train\\ntrain[\"text\"] = train.text.map(remove_stopwords)\\n#test\\ntest[\"text\"] = test.text.map(remove_stopwords)</td>\n",
       "      <td>This code snippet preprocesses the text data in both the training and testing datasets by applying the previously defined function to remove stopwords from the text.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td># Count unique words\\ndef counter_word(text_col):\\n    count = Counter()\\n    for text in text_col.values:\\n        for word in text.split():\\n            count[word] += 1\\n    return count\\n\\n\\ncounter = counter_word(train.text)</td>\n",
       "      <td>This code snippet defines a function to count the occurrence of each unique word in a text column and applies it to the text column of the training dataset.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td># Split dataset into training and validation set\\nX = train.text\\ny = train.target\\ntrain_sentences, val_sentences , train_labels, val_labels = train_test_split(X, y, test_size=0.2)</td>\n",
       "      <td>This code splits the text and target columns of the training dataset into training and validation sets using an 80/20 split.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>#train/val\\ntrain_sentences = train_sentences.to_numpy()\\ntrain_labels = train_labels.to_numpy()\\nval_sentences = val_sentences.to_numpy()\\nval_labels = val_labels.to_numpy()</td>\n",
       "      <td>This code converts the training and validation sentences and labels from pandas Series to NumPy arrays.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>#test\\ntest_sentences = test.text.to_numpy()</td>\n",
       "      <td>This code converts the text column of the testing dataset from a pandas Series to a NumPy array.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td># Tokenize\\n# vectorize a text corpus by turning each text into a sequence of integers\\n\\ntokenizer = Tokenizer(num_words=num_unique_words)\\ntokenizer.fit_on_texts(train_sentences) # fit only to training</td>\n",
       "      <td>This code snippet initializes a tokenizer configured to use the number of unique words as the vocabulary size and fits it on the training sentences to convert text into sequences of integers.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td># Now each word has unique index\\nword_index = tokenizer.word_index\\nword_index</td>\n",
       "      <td>This code creates a dictionary, `word_index`, where each unique word in the training sentences is mapped to a unique integer index.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>#apply on train, validation, and test sentences\\n\\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\\nval_sequences = tokenizer.texts_to_sequences(val_sentences)\\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)</td>\n",
       "      <td>This code snippet converts the training, validation, and test sentences into sequences of integers using the fitted tokenizer.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td># Pad the sequences to have the same length\\nmax_length = 15 #arbitrary number\\n\\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\") #post-&gt; 0\\nval_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")</td>\n",
       "      <td>This code snippet pads the sequences of integers for the training, validation, and test sets to a fixed length of 15, ensuring that all sequences have the same length by adding zeros at the end if necessary.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td># flip (key, value)\\nreverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])</td>\n",
       "      <td>This code snippet creates a reverse mapping dictionary where each unique integer index is mapped back to its corresponding word, effectively inverting the `word_index` dictionary.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>#decoding\\ndef decode(sequence):\\n    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])</td>\n",
       "      <td>This code defines a function to decode a sequence of integers back into the original words using the `reverse_word_index` dictionary.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td># We'll use these weights later on to make up for the slightly imbalanced dataset\\nclasses = np.unique(train_df[\"target\"])\\nclass_weights = sklearn.utils.class_weight.compute_class_weight(\\n    \"balanced\", classes=classes, y=train_df[\"target\"]\\n)\\n\\nclass_weights = {clazz : weight for clazz, weight in zip(classes, class_weights)}</td>\n",
       "      <td>This code calculates and stores the class weights for the training dataset to handle class imbalance by ensuring that each class contributes equally to the training process.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td># Commented out the graceful handling of duplicated because the Kaggle kernel version of statistics.mode()\\n# won't handle multimodal results\\n\\n# Duplicates aren't consistently labeled, so we keep one example of the most frequently occuring label\\n# train_df[\"duplicated\"] = train_df.duplicated(subset=\"text\")\\n# duplicated_tweets = train_df.loc[lambda df: df[\"duplicated\"] == True, :]\\n# aggregated_duplicates = duplicated_tweets.groupby(\"text\", as_index=False).aggregate(\\n#     statistics.mode\\n# )\\n\\n# train_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\\n# train_df = train_df.append(aggregated_duplicates, ignore_index=True)\\n\\ntrain_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\\nprint(\"train rows:\", len(train_df.index))\\nprint(\"test rows:\", len(test_df.index))</td>\n",
       "      <td>This code removes duplicate entries from the training DataFrame based on the 'text' column and disables a previously commented section intended for handling multimodal results in duplicated entries.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>class TweetPreProcessor:\\n    \"\"\"\\n    This class does some cleaning and normalization prior to BPE tokenization\\n    \"\"\"\\n\\n    def __init__(self):\\n\\n        self.text_processor = TextPreProcessor(\\n            # terms that will be normalized\\n            normalize=[\\n                \"url\",\\n                \"email\",\\n                \"phone\",\\n                \"user\",\\n                \"time\",\\n                \"date\",\\n            ],\\n            # terms that will be annotated\\n            annotate={\"repeated\", \"elongated\"},\\n            # corpus from which the word statistics are going to be used\\n            # for word segmentation\\n            segmenter=\"twitter\",\\n            # corpus from which the word statistics are going to be used\\n            # for spell correction\\n            spell_correction=True,\\n            corrector=\"twitter\",\\n            unpack_hashtags=False,  # perform word segmentation on hashtags\\n            unpack_contractions=False,  # Unpack contractions (can't -&gt; can not)\\n            spell_correct_elong=True,  # spell correction for elongated words\\n            fix_bad_unicode=True,\\n            tokenizer=Tokenizer(lowercase=True).tokenize,\\n            # list of dictionaries, for replacing tokens extracted from the text,\\n            # with other expressions. You can pass more than one dictionaries.\\n            dicts=[emoticons, slangdict],\\n        )\\n\\n    def preprocess_tweet(self, tweet):\\n        return \" \".join(self.text_processor.pre_process_doc(tweet))\\n    \\n    # this will return the tokenized text     \\n    def __call__(self, tweet):\\n        return self.text_processor.pre_process_doc(tweet)\\n    \\ntweet_preprocessor = TweetPreProcessor()</td>\n",
       "      <td>This code defines a `TweetPreProcessor` class for cleaning and normalizing tweets using various text processing techniques and initializes an instance of this class for further use.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>train_df[\"text\"] = train_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)\\ntest_df[\"text\"] = test_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)</td>\n",
       "      <td>This code applies the `TweetPreProcessor` to preprocess all tweets in the training and test DataFrames.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td># Fill NA\\ntrain_df[\"keyword\"].fillna(\"\", inplace=True)\\ntest_df[\"keyword\"].fillna(\"\", inplace=True)\\n\\n# remove %20 from keywords\\ntrain_df[\"keyword\"] = train_df[\"keyword\"].apply(urllib.parse.unquote)\\ntest_df[\"keyword\"] = test_df[\"keyword\"].apply(urllib.parse.unquote)</td>\n",
       "      <td>This code fills missing values in the 'keyword' column of both the training and test DataFrames with empty strings and removes any '%20' encoding from the keywords using URL decoding.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\\n    train_df[[\"text\", \"keyword\"]], train_df[\"target\"], test_size=0.3, random_state=42, stratify=train_df[\"target\"]\\n)</td>\n",
       "      <td>This code splits the training DataFrame into training and validation sets based on the 'text' and 'keyword' columns, while maintaining the class distribution (stratified by 'target'), and using 30% of the data for validation.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>def tokenize_encode(tweets, max_length=None):\\n    return pretrained_bert_tokenizer(\\n        tweets,\\n        add_special_tokens=True,\\n        truncation=True,\\n        padding=\"max_length\",\\n        max_length=max_length,\\n        return_tensors=\"tf\",\\n    )\\n\\n\\n# need to be explicit about the lengths (instead of just specifying padding=True in the tokenizer)\\n# otherwise train tweets end up being 71 and validation tweets end up as 70, which causes problems/warnings\\nmax_length_tweet = 72\\nmax_length_keyword = 8\\n\\ntrain_tweets_encoded = tokenize_encode(x_train[\"text\"].to_list(), max_length_tweet) \\nvalidation_tweets_encoded = tokenize_encode(x_val[\"text\"].to_list(), max_length_tweet) \\n\\ntrain_keywords_encoded = tokenize_encode(x_train[\"keyword\"].to_list(), max_length_keyword) \\nvalidation_keywords_encoded = tokenize_encode(x_val[\"keyword\"].to_list(), max_length_keyword) \\n\\ntrain_inputs_encoded = dict(train_tweets_encoded)\\ntrain_inputs_encoded[\"keywords\"] = train_keywords_encoded[\"input_ids\"]\\n\\nvalidation_inputs_encoded = dict(validation_tweets_encoded)\\nvalidation_inputs_encoded[\"keywords\"] = validation_keywords_encoded[\"input_ids\"]\\n</td>\n",
       "      <td>This code defines a function to tokenize and encode tweets using the DistilBERT tokenizer, then applies it to the training and validation sets for both tweets and keywords, ensuring uniform lengths, and combines the encoded inputs into dictionaries.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>train_dataset = tf.data.Dataset.from_tensor_slices(\\n    (dict(train_tweets_encoded), y_train)\\n)\\n\\nval_dataset = tf.data.Dataset.from_tensor_slices(\\n    (dict(validation_tweets_encoded), y_val)\\n)\\n\\ntrain_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\\n    (train_inputs_encoded, y_train)\\n)\\n\\nval_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\\n    (validation_inputs_encoded, y_val)\\n)\\n</td>\n",
       "      <td>This code creates TensorFlow datasets from the encoded training and validation data, including both single input (tweets only) and multi-input (tweets and keywords) datasets for model training.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\\n    tokenizer=tweet_preprocessor, min_df=1, ngram_range=(1, 1), norm=\"l2\"\\n)\\n\\ntrain_vectors = tfidf_vectorizer.fit_transform(raw_documents=x_train[\"text\"]).toarray()\\nvalidation_vectors = tfidf_vectorizer.transform(x_val[\"text\"]).toarray()</td>\n",
       "      <td>This code initializes a TF-IDF vectorizer with the `TweetPreProcessor` and applies it to the text columns of the training and validation datasets, transforming the texts into TF-IDF vectors.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>test_tweets_encoded = tokenize_encode(test_df[\"text\"].to_list(), max_length_tweet)\\ntest_inputs_encoded = dict(test_tweets_encoded)\\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)\\n\\ntest_keywords_encoded = tokenize_encode(test_df[\"keyword\"].to_list(), max_length_keyword)\\ntest_inputs_encoded[\"keywords\"] = test_keywords_encoded[\"input_ids\"]\\ntest_multi_input_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)</td>\n",
       "      <td>This code tokenizes and encodes the text and keyword columns of the test dataset, creating TensorFlow datasets for single and multi-input models.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>def filter_text(df):\\n    df['text']=df['text'].str.replace('http\\S+', '', case=False, regex=True)\\n    df['text']=df['text'].str.replace('@\\S+', '', regex=True)\\n    df['text']=df['text'].str.replace('&amp;\\S+', '', regex=True)\\nfilter_text(train)\\nfilter_text(test)</td>\n",
       "      <td>The code defines and applies a function to remove URLs, mentions, and special characters from the 'text' column in both the training and test datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>sw=['the', 'a', 'an', 'in', 'on', 'with', 'by', 'for', 'at',\\n    'about', 'of', 'under', 'to', 'into', 'and', 'or', 'but',\\n    'nor', 'be']</td>\n",
       "      <td>The code defines a list of common stopwords to be potentially used for text preprocessing.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>v = CountVectorizer(stop_words=sw)\\ntrain_v = v.fit_transform(train['text'])\\ntest_v = v.transform(test['text'])</td>\n",
       "      <td>The code initializes a CountVectorizer with custom stopwords, then fits and transforms the training text data and transforms the test text data into a document-term matrix.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td># plot prop of missing for each feature\\nsns.set_theme(style='white')\\nsns.barplot(x=train.columns, y=train.isnull().mean())\\nplt.show()\\n\\n# drop location and keyword\\ntrain.drop(columns=['id', 'keyword', 'location'], inplace=True)\\ntest.drop(columns=['id', 'keyword', 'location'], inplace=True)\\ntrain.drop_duplicates(inplace=True, ignore_index=True)</td>\n",
       "      <td>This code visualizes the proportion of missing values for each feature in the training data and then drops the 'id', 'keyword', and 'location' columns from both the training and testing DataFrames, as well as removing duplicates from the training DataFrame.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>wordnet_lemmatizer = WordNetLemmatizer()\\n\\ndef quick_clean(text):\\n    \"\"\"\\n    adapted from: https://www.kaggle.com/sophiejermy/sj-eda1\\n    \"\"\"\\n#     text = text + ' '\\n    #remove links\\n    text = re.sub(r'(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&amp;?=%.]+', '', text)\\n    #lower case\\n    text = text.lower()    \\n    #remove special characters\\n    text = re.sub(r'[\\W]+', ' ', text)\\n    #remove double spaces\\n    text = re.sub(r'\\s+', ' ', text)\\n    #tokenize\\n    text = word_tokenize(text)\\n    #remove stop words\\n    text = [word for word in text if not word in stopwords.words('english')]    \\n    #lemmatize\\n    text= [wordnet_lemmatizer.lemmatize(word, pos='v') for word in text]\\n    #rejoin text to string\\n    text = ' '.join(text)\\n    return text\\n\\ndef quick_clean_vectorized(col):\\n    return pd.DataFrame(data=col.apply(lambda x: quick_clean(x)).tolist())\\n\\nquiklean_transformer = FunctionTransformer(quick_clean_vectorized) # to use in pipeline\\n</td>\n",
       "      <td>This code snippet defines a text preprocessing function `quick_clean` to clean and lemmatize text data, and a vectorized version `quick_clean_vectorized` for use in a pipeline with `FunctionTransformer`.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>x_train, x_test, y_train, y_test = train_test_split(train.loc[:,train.columns != 'target'], train.target, test_size=0.2)\\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)</td>\n",
       "      <td>This code splits the training data into training and testing sets for features and the target variable, and prints the dimensions of these subsets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', max_features = 300)\\n\\npreprocess = Pipeline(steps=[\\n                    ('clean',   ColumnTransformer([\\n                                    ('cl', quiklean_transformer, 'text')\\n                                    ],\\n                                        remainder='drop')),\\n                    ('TFIDF', ColumnTransformer([\\n                        ('tfidf', tfidf_vectorizer, 0)\\n                    ], \\n                            remainder='passthrough')),\\n                    ('dim_reduce', TruncatedSVD(n_components=250, random_state=42)),\\n                    ('scale', MinMaxScaler())\\n    \\n        ])</td>\n",
       "      <td>This code creates a preprocessing pipeline that includes text cleaning, TF-IDF vectorization, dimensionality reduction, and feature scaling.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>corpus = []\\nfor i in range(0, len(train_len)):\\n  review = re.sub('[^a-zA-Z]', ' ', dataset_train['text'][i])\\n  review = review.lower()\\n  review = review.split()\\n  ps = PorterStemmer()\\n  all_stopwords = stopwords.words('english')\\n  all_stopwords.remove('not')\\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\\n  review = ' '.join(review)\\n  corpus.append(review)</td>\n",
       "      <td>The code preprocesses the text data in the training dataset by cleaning, lowercasing, tokenizing, removing stopwords, stemming, and then storing the processed text in a list called `corpus`.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>from sklearn.feature_extraction.text import CountVectorizer\\ncv = CountVectorizer(max_features = 1500)\\nX = cv.fit_transform(corpus).toarray()\\ny = dataset_train.iloc[:, -1].values</td>\n",
       "      <td>The code transforms the preprocessed text data into a matrix of token counts using `CountVectorizer` and then separates the resulting features and target variables into `X` and `y`, respectively.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>from sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)</td>\n",
       "      <td>The code splits the dataset into training and testing sets, with 80% of the data assigned to training and 20% to testing.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>testcorpus = []\\nfor i in range(0, len(test_len)):\\n  review = re.sub('[^a-zA-Z]', ' ', dataset_test['text'][i])\\n  review = review.lower()\\n  review = review.split()\\n  ps = PorterStemmer()\\n  all_stopwords = stopwords.words('english')\\n  all_stopwords.remove('not')\\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\\n  review = ' '.join(review)\\n  testcorpus.append(review)\\n\\nxtest = cv.transform(testcorpus).toarray()\\ntest_dataframe_prediction = classifier.predict(xtest)</td>\n",
       "      <td>The code preprocesses the 'text' column of the test dataset similarly to the training data, transforms it into a matrix of token counts, and then uses the trained classifier to make predictions on the processed test data.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>keyword_dist = train_df.groupby(\"keyword\")['target'].value_counts().unstack(fill_value=0)\\nkeyword_dist = keyword_dist.add_prefix(keyword_dist.columns.name).rename_axis(columns=None).reset_index()</td>\n",
       "      <td>This code snippet calculates the distribution of the target variable for each keyword in the training dataset and restructures the data into a new DataFrame.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>#word count\\ntrain_df['word_count'] = train_df['text'].apply(lambda x : len(str(x).split()))\\ntest_df['word_count'] = test_df['text'].apply(lambda x : len(str(x).split()))\\n#Unique word count\\ntrain_df['unique_word_count'] = train_df['text'].apply(lambda x : len(set(str(x).split())))\\ntest_df['unique_word_count'] = test_df['text'].apply(lambda x : len(set(str(x).split())))\\n#Count of letters\\ntrain_df['count_letters'] = train_df['text'].apply(lambda x : len(str(x)))\\ntest_df['count_letters'] = test_df['text'].apply(lambda x : len(str(x)))\\n#Count of punctuations\\ntrain_df['count_punctuations'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\\ntest_df['count_punctuations'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\\n#count of stopwords\\ntrain_df['stop_word_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\\ntest_df['stop_word_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\\n#Count of hashtag\\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda x : len([c for c in str(x) if c == '#']))\\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x : len([c for c in str(x) if c == '#']))\\n#Count of mentions\\ntrain_df['mention_count'] = train_df['text'].apply(lambda x : len([c for c in str(x) if c=='@']))\\ntest_df['mention_count'] = test_df['text'].apply(lambda x : len([c for c in str(x) if c=='@']))</td>\n",
       "      <td>This code snippet creates new columns in both the training and testing datasets to count words, unique words, letters, punctuations, stopwords, hashtags, and mentions in the text.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td># Refrenced from Gunes Evitan and Vitalii Mokin Notebook\\ndef clean(tweet): \\n            \\n    # Special characters\\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\\n    tweet = re.sub(r\"å_\", \"\", tweet)\\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \\n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\\n    tweet = re.sub(r\"å¨\", \"\", tweet)\\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\\n    \\n    # Contractions\\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \\n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \\n            \\n    # Character entity references\\n    tweet = re.sub(r\"&amp;gt;\", \"&gt;\", tweet)\\n    tweet = re.sub(r\"&amp;lt;\", \"&lt;\", tweet)\\n    tweet = re.sub(r\"&amp;amp;\", \"&amp;\", tweet)\\n    \\n    # Typos, slang and informal abbreviations\\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\\n    tweet = re.sub(r\"w/\", \"with\", tweet)\\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\\n    tweet = re.sub(r\"&lt;3\", \"love\", tweet)\\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \\n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\\n    \\n    # Hashtags and usernames\\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \\n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \\n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\\n    tweet = re.sub(r\"withBioterrorism&amp;use\", \"with Bioterrorism &amp; use\", tweet)\\n    tweet = re.sub(r\"Hostage&amp;2\", \"Hostage &amp; 2\", tweet)\\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \\n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\\n    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\\n           \\n    # Urls\\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\\n        \\n    # Words with punctuations and special characters\\n    punctuations = '@#!?+&amp;*[]-%.:/();$=&gt;&lt;|{}^' + \"'`\"\\n    for p in punctuations:\\n        tweet = tweet.replace(p, f' {p} ')\\n        \\n    # ... and ..\\n    tweet = tweet.replace('...', ' ... ')\\n    if '...' not in tweet:\\n        tweet = tweet.replace('..', ' ... ')      \\n        \\n    # Acronyms\\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\\n    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \\n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \\n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \\n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \\n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \\n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \\n    \\n    # Grouping same words without embeddings\\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\\n    \\n    #Remove Emoji\\n    tweet = re.sub(u\"\\U0001F600-\\U0001F64F\",\"\", tweet)  # emoticons\\n    tweet = re.sub(u\"\\U0001F300-\\U0001F5FF\",\"\", tweet)  # symbols &amp; pictographs\\n    tweet = re.sub(u\"\\U0001F680-\\U0001F6FF\",\"\", tweet)  # transport &amp; map symbols\\n    tweet = re.sub(u\"\\U0001F1E0-\\U0001F1FF\",\"\", tweet)  # flags (iOS)\\n    tweet = re.sub(u\"\\U00002702-\\U000027B0\",\"\", tweet)\\n    tweet = re.sub(u\"\\U000024C2-\\U0001F251\",\"\", tweet)\\n    \\n    return tweet\\n\\ntrain_df['text_cleaned'] = train_df['text'].apply(lambda s : clean(s))\\ntest_df['text_cleaned'] = test_df['text'].apply(lambda s : clean(s))</td>\n",
       "      <td>This code snippet defines a function for cleaning the text data by handling contractions, special characters, hashtags, emojis, and URLs, then applies this function to the text in both the training and testing datasets to create a cleaned version of the text.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>def encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        # Tokenise text\\n        text = tokenizer.tokenize(text)\\n        #Reduce 2 slots for start and end tag\\n        text = text[:max_len-2]\\n        #Add start and end tag\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        #Padding to be added\\n        pad_len = max_len - len(input_sequence)\\n        #Get token ids\\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        #add padding\\n        tokens += [0] * pad_len\\n        #Create padding mask with 1's of length of input and 0's with padding length\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        #Create segment ids with all 0's \\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)</td>\n",
       "      <td>This code snippet defines a function that tokenizes input text using a tokenizer, adds special tokens for BERT-like models, pads sequences to a specified maximum length, and returns arrays of token IDs, attention masks, and segment IDs.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>train_input = encode(train_df.text_cleaned.values, tokenizer, max_len=160)\\ntest_input = encode(test_df.text_cleaned.values, tokenizer, max_len=160)\\ntrain_labels = train_df.target.values</td>\n",
       "      <td>This code snippet preprocesses the cleaned text data from the training and testing datasets by encoding them using the previously defined `encode` function, setting a maximum length of 160, and extracts the target labels from the training dataset.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\\ntrain_df.at[train_df['id'].isin(ids_with_target_error),'target'] = 0\\ntrain_df[train_df['id'].isin(ids_with_target_error)]</td>\n",
       "      <td>This code corrects errors in the 'target' column for specific rows in the training dataset identified by their 'id', setting their 'target' values to 0.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>set1 = set(train_df[train_df.duplicated(subset=['text'])]['id'].values)</td>\n",
       "      <td>This code creates a set of 'id' values from the training dataset that correspond to duplicate 'text' entries.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>set2 = (train_df[train_df.duplicated(subset=['text','target'])]['id'].values)</td>\n",
       "      <td>This code creates an array of 'id' values from the training dataset that correspond to duplicate rows based on both 'text' and 'target' columns.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td># setting the argument keep=False, drops all the duplicates\\ntrain_df.drop_duplicates(subset=['text'], keep=False, inplace=True)</td>\n",
       "      <td>This code removes all rows with duplicated text entries from the training dataset by setting the `keep` argument to `False` in the `drop_duplicates` method.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>train_df['location'] = train_df['location'].str.lower()\\ntrain_df['location'] = train_df['location'].str.strip()\\ntest_df['location'] = train_df['location'].str.lower()\\ntest_df['location'] = train_df['location'].str.strip()</td>\n",
       "      <td>This code normalizes the 'location' column in both the training and testing datasets by converting the text to lowercase and stripping any leading or trailing whitespace.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>loc_dict = {'united states':'usa',\\n                            'us':'usa',\\n                            'united kingdom':'uk',\\n                              'nyc':'new york',\\n                             'london, uk': 'london',\\n                              'london, england':'london',\\n                            'new york, ny':'new york',\\n                            'everywhere':'worldwide'}</td>\n",
       "      <td>This code defines a dictionary for standardizing various location names to a more consistent format in the dataset.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>train_df['location'].replace(loc_dict, inplace=True)\\ntest_df['location'].replace(loc_dict, inplace=True)</td>\n",
       "      <td>This code uses the previously defined dictionary to replace location names in both the training and testing datasets with their standardized equivalents.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>abbreviations = {\\n    \"$\" : \" dollar \",\\n    \"€\" : \" euro \",\\n    \"4ao\" : \"for adults only\",\\n    \"a.m\" : \"before midday\",\\n    \"a3\" : \"anytime anywhere anyplace\",\\n    \"aamof\" : \"as a matter of fact\",\\n    \"acct\" : \"account\",\\n    \"adih\" : \"another day in hell\",\\n    \"afaic\" : \"as far as i am concerned\",\\n    \"afaict\" : \"as far as i can tell\",\\n    \"afaik\" : \"as far as i know\",\\n    \"afair\" : \"as far as i remember\",\\n    \"afk\" : \"away from keyboard\",\\n    \"app\" : \"application\",\\n    \"approx\" : \"approximately\",\\n    \"apps\" : \"applications\",\\n    \"asap\" : \"as soon as possible\",\\n    \"asl\" : \"age, sex, location\",\\n    \"atk\" : \"at the keyboard\",\\n    \"ave.\" : \"avenue\",\\n    \"aymm\" : \"are you my mother\",\\n    \"ayor\" : \"at your own risk\", \\n    \"b&amp;b\" : \"bed and breakfast\",\\n    \"b+b\" : \"bed and breakfast\",\\n    \"b.c\" : \"before christ\",\\n    \"b2b\" : \"business to business\",\\n    \"b2c\" : \"business to customer\",\\n    \"b4\" : \"before\",\\n    \"b4n\" : \"bye for now\",\\n    \"b@u\" : \"back at you\",\\n    \"bae\" : \"before anyone else\",\\n    \"bak\" : \"back at keyboard\",\\n    \"bbbg\" : \"bye bye be good\",\\n    \"bbc\" : \"british broadcasting corporation\",\\n    \"bbias\" : \"be back in a second\",\\n    \"bbl\" : \"be back later\",\\n    \"bbs\" : \"be back soon\",\\n    \"be4\" : \"before\",\\n    \"bfn\" : \"bye for now\",\\n    \"blvd\" : \"boulevard\",\\n    \"bout\" : \"about\",\\n    \"brb\" : \"be right back\",\\n    \"bros\" : \"brothers\",\\n    \"brt\" : \"be right there\",\\n    \"bsaaw\" : \"big smile and a wink\",\\n    \"btw\" : \"by the way\",\\n    \"bwl\" : \"bursting with laughter\",\\n    \"c/o\" : \"care of\",\\n    \"cet\" : \"central european time\",\\n    \"cf\" : \"compare\",\\n    \"cia\" : \"central intelligence agency\",\\n    \"csl\" : \"can not stop laughing\",\\n    \"cu\" : \"see you\",\\n    \"cul8r\" : \"see you later\",\\n    \"cv\" : \"curriculum vitae\",\\n    \"cwot\" : \"complete waste of time\",\\n    \"cya\" : \"see you\",\\n    \"cyt\" : \"see you tomorrow\",\\n    \"dae\" : \"does anyone else\",\\n    \"dbmib\" : \"do not bother me i am busy\",\\n    \"diy\" : \"do it yourself\",\\n    \"dm\" : \"direct message\",\\n    \"dwh\" : \"during work hours\",\\n    \"e123\" : \"easy as one two three\",\\n    \"eet\" : \"eastern european time\",\\n    \"eg\" : \"example\",\\n    \"embm\" : \"early morning business meeting\",\\n    \"encl\" : \"enclosed\",\\n    \"encl.\" : \"enclosed\",\\n    \"etc\" : \"and so on\",\\n    \"faq\" : \"frequently asked questions\",\\n    \"fawc\" : \"for anyone who cares\",\\n    \"fb\" : \"facebook\",\\n    \"fc\" : \"fingers crossed\",\\n    \"fig\" : \"figure\",\\n    \"fimh\" : \"forever in my heart\", \\n    \"ft.\" : \"feet\",\\n    \"ft\" : \"featuring\",\\n    \"ftl\" : \"for the loss\",\\n    \"ftw\" : \"for the win\",\\n    \"fwiw\" : \"for what it is worth\",\\n    \"fyi\" : \"for your information\",\\n    \"g9\" : \"genius\",\\n    \"gahoy\" : \"get a hold of yourself\",\\n    \"gal\" : \"get a life\",\\n    \"gcse\" : \"general certificate of secondary education\",\\n    \"gfn\" : \"gone for now\",\\n    \"gg\" : \"good game\",\\n    \"gl\" : \"good luck\",\\n    \"glhf\" : \"good luck have fun\",\\n    \"gmt\" : \"greenwich mean time\",\\n    \"gmta\" : \"great minds think alike\",\\n    \"gn\" : \"good night\",\\n    \"g.o.a.t\" : \"greatest of all time\",\\n    \"goat\" : \"greatest of all time\",\\n    \"goi\" : \"get over it\",\\n    \"gps\" : \"global positioning system\",\\n    \"gr8\" : \"great\",\\n    \"gratz\" : \"congratulations\",\\n    \"gyal\" : \"girl\",\\n    \"h&amp;c\" : \"hot and cold\",\\n    \"hp\" : \"horsepower\",\\n    \"hr\" : \"hour\",\\n    \"hrh\" : \"his royal highness\",\\n    \"ht\" : \"height\",\\n    \"ibrb\" : \"i will be right back\",\\n    \"ic\" : \"i see\",\\n    \"icq\" : \"i seek you\",\\n    \"icymi\" : \"in case you missed it\",\\n    \"idc\" : \"i do not care\",\\n    \"idgadf\" : \"i do not give a damn fuck\",\\n    \"idgaf\" : \"i do not give a fuck\",\\n    \"idk\" : \"i do not know\",\\n    \"ie\" : \"that is\",\\n    \"i.e\" : \"that is\",\\n    \"ifyp\" : \"i feel your pain\",\\n    \"IG\" : \"instagram\",\\n    \"iirc\" : \"if i remember correctly\",\\n    \"ilu\" : \"i love you\",\\n    \"ily\" : \"i love you\",\\n    \"imho\" : \"in my humble opinion\",\\n    \"imo\" : \"in my opinion\",\\n    \"imu\" : \"i miss you\",\\n    \"iow\" : \"in other words\",\\n    \"irl\" : \"in real life\",\\n    \"j4f\" : \"just for fun\",\\n    \"jic\" : \"just in case\",\\n    \"jk\" : \"just kidding\",\\n    \"jsyk\" : \"just so you know\",\\n    \"l8r\" : \"later\",\\n    \"lb\" : \"pound\",\\n    \"lbs\" : \"pounds\",\\n    \"ldr\" : \"long distance relationship\",\\n    \"lmao\" : \"laugh my ass off\",\\n    \"lmfao\" : \"laugh my fucking ass off\",\\n    \"lol\" : \"laughing out loud\",\\n    \"ltd\" : \"limited\",\\n    \"ltns\" : \"long time no see\",\\n    \"m8\" : \"mate\",\\n    \"mf\" : \"motherfucker\",\\n    \"mfs\" : \"motherfuckers\",\\n    \"mfw\" : \"my face when\",\\n    \"mofo\" : \"motherfucker\",\\n    \"mph\" : \"miles per hour\",\\n    \"mr\" : \"mister\",\\n    \"mrw\" : \"my reaction when\",\\n    \"ms\" : \"miss\",\\n    \"mte\" : \"my thoughts exactly\",\\n    \"nagi\" : \"not a good idea\",\\n    \"nbc\" : \"national broadcasting company\",\\n    \"nbd\" : \"not big deal\",\\n    \"nfs\" : \"not for sale\",\\n    \"ngl\" : \"not going to lie\",\\n    \"nhs\" : \"national health service\",\\n    \"nrn\" : \"no reply necessary\",\\n    \"nsfl\" : \"not safe for life\",\\n    \"nsfw\" : \"not safe for work\",\\n    \"nth\" : \"nice to have\",\\n    \"nvr\" : \"never\",\\n    \"nyc\" : \"new york city\",\\n    \"oc\" : \"original content\",\\n    \"og\" : \"original\",\\n    \"ohp\" : \"overhead projector\",\\n    \"oic\" : \"oh i see\",\\n    \"omdb\" : \"over my dead body\",\\n    \"omg\" : \"oh my god\",\\n    \"omw\" : \"on my way\",\\n    \"p.a\" : \"per annum\",\\n    \"p.m\" : \"after midday\",\\n    \"pm\" : \"prime minister\",\\n    \"poc\" : \"people of color\",\\n    \"pov\" : \"point of view\",\\n    \"pp\" : \"pages\",\\n    \"ppl\" : \"people\",\\n    \"prw\" : \"parents are watching\",\\n    \"ps\" : \"postscript\",\\n    \"pt\" : \"point\",\\n    \"ptb\" : \"please text back\",\\n    \"pto\" : \"please turn over\",\\n    \"qpsa\" : \"what happens\", #\"que pasa\",\\n    \"ratchet\" : \"rude\",\\n    \"rbtl\" : \"read between the lines\",\\n    \"rlrt\" : \"real life retweet\", \\n    \"rofl\" : \"rolling on the floor laughing\",\\n    \"roflol\" : \"rolling on the floor laughing out loud\",\\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\\n    \"rt\" : \"retweet\",\\n    \"ruok\" : \"are you ok\",\\n    \"sfw\" : \"safe for work\",\\n    \"sk8\" : \"skate\",\\n    \"smh\" : \"shake my head\",\\n    \"sq\" : \"square\",\\n    \"srsly\" : \"seriously\", \\n    \"ssdd\" : \"same stuff different day\",\\n    \"tbh\" : \"to be honest\",\\n    \"tbs\" : \"tablespooful\",\\n    \"tbsp\" : \"tablespooful\",\\n    \"tfw\" : \"that feeling when\",\\n    \"thks\" : \"thank you\",\\n    \"tho\" : \"though\",\\n    \"thx\" : \"thank you\",\\n    \"tia\" : \"thanks in advance\",\\n    \"til\" : \"today i learned\",\\n    \"tl;dr\" : \"too long i did not read\",\\n    \"tldr\" : \"too long i did not read\",\\n    \"tmb\" : \"tweet me back\",\\n    \"tntl\" : \"trying not to laugh\",\\n    \"ttyl\" : \"talk to you later\",\\n    \"u\" : \"you\",\\n    \"u2\" : \"you too\",\\n    \"u4e\" : \"yours for ever\",\\n    \"utc\" : \"coordinated universal time\",\\n    \"w/\" : \"with\",\\n    \"w/o\" : \"without\",\\n    \"w8\" : \"wait\",\\n    \"wassup\" : \"what is up\",\\n    \"wb\" : \"welcome back\",\\n    \"wtf\" : \"what the fuck\",\\n    \"wtg\" : \"way to go\",\\n    \"wtpa\" : \"where the party at\",\\n    \"wuf\" : \"where are you from\",\\n    \"wuzup\" : \"what is up\",\\n    \"wywh\" : \"wish you were here\",\\n    \"yd\" : \"yard\",\\n    \"ygtr\" : \"you got that right\",\\n    \"ynk\" : \"you never know\",\\n    \"zzz\" : \"sleeping bored and tired\"\\n}</td>\n",
       "      <td>This code defines a dictionary to map abbreviations and slang terms commonly found in social media to their expanded forms for text normalization.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>def convert_abb(x):\\n    word_list = x.split()\\n    r_string = []\\n    for word in word_list:\\n        r_string.append(abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word)\\n    return ' '.join(r_string)\\n\\ntest = 'afk hello world!'\\nconvert_abb(test)</td>\n",
       "      <td>This code defines a function `convert_abb` that takes a string, splits it into words, and replaces any abbreviations found in the given dictionary with their expanded forms, then returns the processed string; it also tests this function with an example sentence.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>train_df['text'] = train_df.text.apply(convert_abb)\\ntest_df['text'] = test_df.text.apply(convert_abb)</td>\n",
       "      <td>This code applies the `convert_abb` function to the 'text' column in both the training and testing datasets to replace abbreviations with their expanded forms.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>train_df['clean_text'] = train_df.text.apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', '', x))\\ntest_df['clean_text'] = test_df.text.apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', '', x))</td>\n",
       "      <td>This code creates a 'clean_text' column in both the training and testing datasets by removing URLs from the 'text' column using a regular expression.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>train_df['clean_text'] = train_df.clean_text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))</td>\n",
       "      <td>This code further cleans the 'clean_text' column in both the training and testing datasets by removing non-ASCII characters.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>non_alpha = string.punctuation + '0123456789'</td>\n",
       "      <td>This code creates a string `non_alpha` that contains all punctuation characters and digits to be used later, presumably for text cleaning purposes.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>train_df['clean_text'] = train_df.clean_text.apply(lambda x: x.translate(str.maketrans('','',non_alpha)))\\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: x.translate(str.maketrans('','',non_alpha)))</td>\n",
       "      <td>This code removes all punctuation and digits from the 'clean_text' column in both the training and testing datasets using the previously defined `non_alpha` string.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>train_df['token_text'] = train_df.clean_text.str.lower()\\ntrain_df['token_text'] = train_df.token_text.apply(lambda x: nltk.word_tokenize(x))\\ntest_df['token_text'] = test_df.clean_text.str.lower()\\ntest_df['token_text'] = test_df.token_text.apply(lambda x: nltk.word_tokenize(x))</td>\n",
       "      <td>This code tokenizes the lowercase version of the 'clean_text' column in both the training and testing datasets, creating a new 'token_text' column.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>stopwords = nltk.corpus.stopwords.words(\"english\")\\naddingStopWords = ['im','get','dont','got','amp']\\nstopwords.extend(addingStopWords)\\ntrain_df['token_text'] = train_df['token_text'].apply(lambda x: [word for word in x if word not in stopwords])\\ntest_df['token_text'] = test_df['token_text'].apply(lambda x: [word for word in x if word not in stopwords])</td>\n",
       "      <td>This code removes standard English stopwords, along with some additional specified words, from the tokenized text in the 'token_text' column for both the training and testing datasets.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td># since we have a dataset containing text from social media, there might be many spelling mistakes and words which cannot be found in the word lemmatizer corpus, so they would be remained untouched.\\n# To this end we would use the Porter Stemming which just removes affixes of the words\\nfrom nltk.stem.porter import PorterStemmer\\nps = PorterStemmer()\\ntrain_df['token_text'] = train_df['token_text'].apply(lambda x: [ps.stem(word) for word in x ])\\ntest_df['token_text'] = test_df['token_text'].apply(lambda x: [ps.stem(word) for word in x ])</td>\n",
       "      <td>This code applies the Porter Stemming technique to the tokenized text in both the training and testing datasets to reduce words to their root forms by removing affixes.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>train_df.reset_index(inplace=True, drop=True)</td>\n",
       "      <td>This code resets the index of the training dataset to be sequential, dropping the old index.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>def dummy(doc):\\n    return doc\\ntfidf_vect = TfidfVectorizer(analyzer=dummy)</td>\n",
       "      <td>This code defines a `dummy` function for use in the `TfidfVectorizer` to treat pre-tokenized documents as input without further tokenization steps.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>counts_df_train = pd.DataFrame(matrix_train.toarray())\\ncounts_df_test = pd.DataFrame(matrix_test.toarray())</td>\n",
       "      <td>This code converts the TF-IDF feature matrices for both the training and testing datasets into pandas DataFrames for easier manipulation and analysis.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>train_df['length'] = train_df.text.apply(lambda x: len(x) - x.count(' '))\\ntest_df['length'] = test_df.text.apply(lambda x: len(x) - x.count(' '))</td>\n",
       "      <td>This code calculates the length of each tweet, excluding spaces, and adds this information as a new column called 'length' to both the training and testing datasets.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>train_df['punct_perc'] = train_df.text.apply(lambda x: sum([1 for char in x if char in non_alpha])/(len(x) - x.count(' '))*100)\\ntest_df['punct_perc'] = test_df.text.apply(lambda x: sum([1 for char in x if char in non_alpha])/(len(x) - x.count(' '))*100)</td>\n",
       "      <td>This code calculates the percentage of punctuation and numeric characters relative to the length of each tweet (excluding spaces) and adds this information as a new column called 'punct_perc' to both the training and testing datasets.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>train_df['word_count'] = train_df.token_text.apply(len)\\ntest_df['word_count'] = train_df.token_text.apply(len)</td>\n",
       "      <td>This code calculates the word count of the tokenized text for each tweet and adds this information as a new column called 'word_count' to both the training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>train_df['length_int'] =pd.cut(train_df.length, 14, include_lowest=True)\\n                               #bins=[0, 15, 30, 40, 50,60, 80, 100, 120, 140, 180]\\ntest_df['length_int'] =pd.cut(test_df.length, 14, include_lowest=True)\\n                              #bins=[0, 15, 30, 40, 50,60, 80, 100, 120, 140, 180]</td>\n",
       "      <td>This code creates a new column called 'length_int' in both the training and testing datasets by binning the 'length' values into 14 intervals.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>train_df['length'] = train_df['length']**2.3\\ntest_df['length'] = train_df['length']**2.3</td>\n",
       "      <td>This code transforms the 'length' column in both the training and testing datasets by raising the values to the power of 2.3.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>train_df['punct_perc'] = train_df['punct_perc']**(1/3)\\ntest_df['punct_perc'] = train_df['punct_perc']**(1/3)</td>\n",
       "      <td>This code applies the cube root transformation to the 'punct_perc' column in both the training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td># assign an emotion to each tweet\\ntrain_df['emotion'] = train_df.text.apply(lambda x: te.get_emotion(x))\\ntest_df['emotion'] = test_df.text.apply(lambda x: te.get_emotion(x))</td>\n",
       "      <td>This code assigns an emotion score to each tweet in both the training and testing datasets by applying the `text2emotion` library's `get_emotion` method to the 'text' column.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td># exploding the dictionary into 4 different columns, based on the dictionary keys\\ntrain_df = pd.concat([train_df, pd.DataFrame(train_df['emotion'].tolist())], axis =1)\\ntest_df = pd.concat([test_df, pd.DataFrame(test_df['emotion'].tolist())], axis =1)</td>\n",
       "      <td>This code expands the 'emotion' dictionary column into separate columns, each representing a different emotion, and appends them to the original training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>total_emotions = train_df[['Happy', 'Angry', 'Surprise', 'Sad', 'Fear','target']].groupby('target').sum()</td>\n",
       "      <td>This code calculates the sum of each emotion type for non-disaster and disaster tweets by grouping the relevant emotion columns in the training dataset by the 'target' column.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>mean_emotions = train_df[['Happy', 'Angry', 'Surprise', 'Sad', 'Fear','target']].groupby('target').mean()</td>\n",
       "      <td>This code calculates the mean value of each emotion type for non-disaster and disaster tweets by grouping the relevant emotion columns in the training dataset by the 'target' column.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>train_df['sentiment'] = train_df.text.astype(str).apply(lambda x: sia.polarity_scores(x))\\ntest_df['sentiment'] = test_df.text.astype(str).apply(lambda x: sia.polarity_scores(x))</td>\n",
       "      <td>This code applies the `SentimentIntensityAnalyzer` to each tweet in both the training and testing datasets to calculate sentiment scores, storing the results in a new column called 'sentiment'.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>train_df = pd.concat([train_df, pd.DataFrame(train_df['sentiment'].tolist())], axis =1)\\ntest_df = pd.concat([test_df, pd.DataFrame(test_df['sentiment'].tolist())], axis =1)</td>\n",
       "      <td>This code expands the 'sentiment' dictionary column into separate columns for each sentiment score and appends them to the original training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>mean_sentiment = train_df[['neg', 'neu', 'pos', 'compound','target']].groupby('target').mean()\\ntotal_sentiment = train_df[['neg', 'neu', 'pos', 'compound','target']].groupby('target').sum()</td>\n",
       "      <td>This code calculates the mean and total sentiment scores for 'neg', 'neu', 'pos', and 'compound' by grouping these columns in the training dataset by the 'target' column.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>#full_train_df = pd.concat([train_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int'], axis=1), counts_df_train], axis=1)\\nfull_train_df = pd.concat([train_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int','Happy', 'Angry', 'Surprise', 'Sad', 'Fear'], axis=1), counts_df_train], axis=1)</td>\n",
       "      <td>This code creates a new DataFrame `full_train_df` by concatenating the transformed training dataset, with specific columns dropped, and the TF-IDF feature DataFrame `counts_df_train`.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>#full_test_df = pd.concat([test_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int'], axis=1), counts_df_test], axis=1)\\nfull_test_df = pd.concat([test_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int','Happy', 'Angry', 'Surprise', 'Sad', 'Fear'], axis=1), counts_df_test], axis=1)</td>\n",
       "      <td>This code creates a new DataFrame `full_test_df` by concatenating the transformed testing dataset, with specific columns dropped, and the TF-IDF feature DataFrame `counts_df_test`.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td># deleting unnecessary dataframes to save memory\\ndel train_df\\ndel test_df\\ndel counts_df_train\\ndel counts_df_test</td>\n",
       "      <td>This code deletes the intermediate dataframes (`train_df`, `test_df`, `counts_df_train`, `counts_df_test`) to free up memory.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>count_vectorizer = feature_extraction.text.CountVectorizer()</td>\n",
       "      <td>This code snippet initializes a CountVectorizer to convert a collection of text documents to a matrix of token counts.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\\n\\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])</td>\n",
       "      <td>This code snippet transforms the \"text\" column from the training and test DataFrames into numerical feature vectors using the previously initialized CountVectorizer.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>def bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)</td>\n",
       "      <td>This code defines a function to preprocess and encode text data using the BERT tokenizer, converting text into token IDs and creating corresponding masks and segment IDs for input into a BERT model.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>#vocab file from pre-trained BERT for tokenization\\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\n\\n#returns true/false depending on if we selected cased/uncased bert layer\\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\n\\n#Create the tokenizer\\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\\n\\n#tokenizing the training and testing data\\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\\ntrain_labels = train.target.values</td>\n",
       "      <td>This code creates a BERT tokenizer using the vocabulary file of the pre-trained BERT model and applies this tokenizer to encode the text data from the training and testing datasets, producing tokenized inputs and labels for training.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>stop_words=nltk.corpus.stopwords.words('english')\\ni=0\\n#sc=SpellChecker()\\n#data=pd.concat([train,test])\\nwnl=WordNetLemmatizer()\\nstemmer=SnowballStemmer('english')\\nfor doc in train.text:\\n    doc=re.sub(r'https?://\\S+|www\\.\\S+','',doc)\\n    doc=re.sub(r'&lt;.*?&gt;','',doc)\\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\\n    doc=contractions.fix(doc)\\n    tokens=nltk.word_tokenize(doc)\\n    filtered=[token for token in tokens if token not in stop_words]\\n    doc=' '.join(filtered)\\n    train.text[i]=doc\\n    i+=1\\ni=0\\nfor doc in test.text:\\n    doc=re.sub(r'https?://\\S+|www\\.\\S+','',doc)\\n    doc=re.sub(r'&lt;.*?&gt;','',doc)\\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\\n    doc=contractions.fix(doc)\\n    tokens=nltk.word_tokenize(doc)\\n    filtered=[token for token in tokens if token not in stop_words]\\n    doc=' '.join(filtered)\\n    test.text[i]=doc\\n    i+=1</td>\n",
       "      <td>This code snippet preprocesses the text data in the train and test datasets by removing URLs, HTML tags, non-alphabetic characters, and stopwords, as well as applying lowercasing, lemmatization, and contraction expansion.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>df = df.drop_duplicates().reset_index(drop = True)</td>\n",
       "      <td>This code removes duplicate rows from the DataFrame `df` and resets its index to ensure that the remaining data is unique and the index is continuous.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>#Cleaning the Data</td>\n",
       "      <td>This comment indicates the beginning of a code section that will involve data cleaning operations to prepare the dataset for analysis or model training.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>import re\\n#Conver lowercase remove punctuation and Character and then strip \\ntext = df[\"text\"].iloc[0]\\nprint(text)\\ntext = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\\ntxt = text.split()\\nprint(txt)\\n\\n</td>\n",
       "      <td>This code converts a text sample from the DataFrame `df` to lowercase, removes punctuation and special characters, and strips leading/trailing whitespace before splitting the cleaned text into individual words.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>#remove stopwords\\nimport nltk\\nlst_stopwords = nltk.corpus.stopwords.words(\"english\")\\ntxt = [word for word in txt if word not in lst_stopwords]\\nprint(txt)</td>\n",
       "      <td>This code removes common English stopwords from the previously cleaned and tokenized text by filtering out words found in the NLTK stopwords list.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>#stemming\\nps = nltk.stem.porter.PorterStemmer()\\nprint([ps.stem(word) for word in txt])</td>\n",
       "      <td>This code applies stemming to the filtered words using the Porter Stemmer from the NLTK library to reduce each word to its root form, simplifying the text data.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>#Lemmentization\\nlem = nltk.stem.wordnet.WordNetLemmatizer()\\nprint([lem.lemmatize(word) for word in txt])</td>\n",
       "      <td>This code applies lemmatization to the filtered words using the WordNet Lemmatizer from the NLTK library to convert each word to its base or dictionary form, further refining the text data.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>#to apply all the technique to all the records on dataset\\ndef utils_preprocess_text(text, flg_stemm=True, flg_lemm =True, lst_stopwords=None ):\\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\\n    \\n    #tokenization(convert from string to List)\\n    lst_text = text.split()\\n    #remove stopwords\\n    if lst_stopwords is not None:\\n        lst_text = [word for word in lst_text if word not in\\n                   lst_stopwords]\\n        \\n     #stemming\\n    if flg_stemm == True:\\n        ps = nltk.stem.porter.PorterStemmer()\\n        lst_text = [ps.stem(word) for word in lst_text]\\n        \\n    #Lemmentization\\n    if flg_lemm == True:\\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\\n        lst_text = [lem.lemmatize(word) for word in lst_text]\\n        \\n    # back to string from list\\n    text = \" \".join(lst_text)\\n    return text\\n</td>\n",
       "      <td>This code defines a function `utils_preprocess_text` that preprocesses text by applying lowercase conversion, punctuation removal, tokenization, optional stopword removal, optional stemming, optional lemmatization, and concatenation back to a string for all records in a dataset.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>#apply dataset\\ndf['clean_text'] = df['text'].apply(lambda x: utils_preprocess_text(x, flg_stemm = False, flg_lemm=True))\\ntest['clean_text'] = test['text'].apply(lambda x: utils_preprocess_text(x, flg_stemm = False, flg_lemm=True))</td>\n",
       "      <td>This code applies the text preprocessing function `utils_preprocess_text` to the 'text' column of both the training and test datasets, creating a new column 'clean_text' with the processed text, while enabling lemmatization and disabling stemming.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>#Target Encoding\\n</td>\n",
       "      <td>This comment indicates the beginning of a code section that will involve target encoding, a preprocessing step where categorical variables are converted into numerical values based on the target variable.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>import category_encoders as ce\\n\\n# Target encoding\\nfeatures = ['keyword', 'location']\\nencoder = ce.TargetEncoder(cols=features)\\nencoder.fit(df[features],df['target'])\\n\\ndf = df.join(encoder.transform(df[features]).add_suffix('_target'))\\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))\\n</td>\n",
       "      <td>This code uses the `category_encoders` library to perform target encoding on the 'keyword' and 'location' features, then appends the encoded features as new columns with a '_target' suffix to both the training and test datasets.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>from sklearn.feature_extraction.text import TfidfVectorizer\\n\\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \\n# Only include &gt;=10 occurrences\\n# Have unigrams and bigrams\\ntext_vec = vec_text.fit_transform(df['clean_text'])\\ntext_vec_test = vec_text.transform(test['clean_text'])\\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\\nprint (X_train_text.shape)</td>\n",
       "      <td>This code utilizes the `TfidfVectorizer` from Scikit-learn to convert cleaned text data in both the training and test datasets into numerical feature vectors, including only terms that occur at least 10 times and considering both unigrams and bigrams, before creating corresponding DataFrames for these vectors.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>df = df.join(X_train_text, rsuffix='_text')\\ntest = test.join(X_test_text, rsuffix='_text')\\n</td>\n",
       "      <td>This code appends the TF-IDF feature vectors, stored in DataFrames `X_train_text` and `X_test_text`, to the original training and test data, respectively, merging the text features with the existing data.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td># Get the Bert tokenizer\\ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, \\n                                          do_lower_case=True)</td>\n",
       "      <td>This code snippet initializes a BERT tokenizer from the pretrained model specified by `PRETRAINED_MODEL_NAME` and sets it to convert all text to lowercase.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>def prepare_sequence(text):\\n    \"\"\"\\n    Tokenize and prepare a sequence for the model. It tokenizes the text sequence\\n    adding special tokens ([CLS], [SEP]), padding  to the max length and truncate \\n    reviews longer than the max length.\\n    Return the token IDs, the segment IDs and the mask IDs.\\n    \"\"\"\\n\\n    prepared_sequence = tokenizer.encode_plus(\\n                            text, \\n                            add_special_tokens = True, \\n                            max_length = MAX_LENGHT, \\n                            padding = 'max_length',\\n                            return_attention_mask = True\\n                            )\\n    return prepared_sequence</td>\n",
       "      <td>This code snippet defines a function `prepare_sequence` that tokenizes a given text using the BERT tokenizer, adds special tokens, pads or truncates the text to the maximum length specified by `MAX_LENGTH`, and returns the token IDs, segment IDs, and attention mask IDs.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td># Prepare a test sentence\\ntest_sentence = 'Is this jacksonville?'\\ntest_sentence_encoded = prepare_sequence(test_sentence)\\ntoken_ids = test_sentence_encoded[\"input_ids\"]\\nprint(f'Test sentence:   {test_sentence}')\\nprint(f'Keys:            {test_sentence_encoded.keys()}')\\nprint(f'Tokens:          {tokenizer.convert_ids_to_tokens(token_ids)[:12]}')\\nprint(f'Token IDs:       {token_ids[:12]}')\\nprint(f'Segment IDs:     {test_sentence_encoded[\"token_type_ids\"][:12]}')\\nprint(f'Mask IDs         {test_sentence_encoded[\"attention_mask\"][:12]}')\\nprint(f'Input dimension: {len(token_ids)}')</td>\n",
       "      <td>This code snippet prepares a test sentence by tokenizing it and generating token, segment, and mask IDs using the `prepare_sequence` function, then prints the encoded components and their dimensions for the first portion of the sentence.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\\n    \"\"\"\\n    Map to the expected input to TFBertForSequenceClassification.\\n    \"\"\"\\n    mapped_example = {\\n        \"input_ids\": input_ids,\\n        \"token_type_ids\": token_type_ids,\\n        \"attention_mask\": attention_masks,\\n    }\\n    return mapped_example, label \\n\\ndef encode_examples(texts_and_labels):\\n    \"\"\"\\n    Prepare all sequences of text and build TF dataset.\\n    \"\"\"\\n\\n    input_ids_list = []\\n    token_type_ids_list = []\\n    attention_mask_list = []\\n    label_list = []\\n        \\n    for text, label in texts_and_labels:\\n\\n        bert_input = prepare_sequence(text)\\n\\n        input_ids_list.append(bert_input['input_ids'])\\n        token_type_ids_list.append(bert_input['token_type_ids'])\\n        attention_mask_list.append(bert_input['attention_mask'])\\n        label_list.append([label])\\n\\n    # Create TF dataset\\n    dataset = tf.data.Dataset.from_tensor_slices(\\n        (input_ids_list, attention_mask_list, token_type_ids_list,\\n         label_list)\\n    )\\n    # Map to the expected input to TFBertForSequenceClassification\\n    dataset_mapped = dataset.map(map_example_to_dict)\\n    return dataset_mapped</td>\n",
       "      <td>This code snippet defines two functions: `map_example_to_dict`, which maps tokenized input components to the expected input format for TFBertForSequenceClassification, and `encode_examples`, which prepares sequences of texts by tokenizing them, building TensorFlow datasets, and mapping the resulting components for model input.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>X = train_data[\"text\"]\\ny = train_data[\"target\"]</td>\n",
       "      <td>This code snippet extracts the text data and target variable from the training DataFrame and assigns them to variables `X` and `y`, respectively.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td># Split the training dataset for training and test\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, \\n                                                    random_state=1)</td>\n",
       "      <td>This code snippet splits the extracted text data and target variable into training and validation sets using an 90-10 split and a fixed random state for reproducibility.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>train_dataset = list(zip(X_train, y_train))\\nval_dataset = list(zip(X_val, y_val))</td>\n",
       "      <td>This code snippet combines the training and validation text data with their respective labels into tuples and stores them in lists for further processing.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td># Prepare sequences of text and build TF train dataset\\nds_train_encoded = encode_examples(train_dataset).shuffle(10000).batch(BATCH_SIZE)\\n\\n# Prepare sequences of text and build TF validation dataset\\nds_val_encoded = encode_examples(val_dataset).batch(BATCH_SIZE)</td>\n",
       "      <td>This code snippet encodes the training and validation datasets into sequences of tokenized inputs, creates TensorFlow datasets from these sequences, shuffles and batches the training data, and batches the validation data using the specified batch size.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>def encode_test_examples(texts):\\n    \"\"\"\\n    Prepare all sequences of text and build TF dataset.\\n    \"\"\"\\n\\n    input_ids_list = []\\n    token_type_ids_list = []\\n    attention_mask_list = []\\n        \\n    for text in texts:\\n\\n        bert_input = prepare_sequence(text)\\n\\n        input_ids_list.append(bert_input['input_ids'])\\n        token_type_ids_list.append(bert_input['token_type_ids'])\\n        attention_mask_list.append(bert_input['attention_mask'])\\n\\n    # Create TF dataset\\n    dataset = tf.data.Dataset.from_tensor_slices(\\n        (input_ids_list, attention_mask_list, token_type_ids_list)\\n    )\\n    # Map to the expected input to TFBertForSequenceClassification\\n    dataset_mapped = dataset.map(map_test_example_to_dict)\\n    return dataset_mapped\\n\\ndef map_test_example_to_dict(input_ids, attention_masks, token_type_ids):\\n    \"\"\"\\n    Map to the expected input to TFBertForSequenceClassification.\\n    \"\"\"\\n    mapped_example = {\\n        \"input_ids\": input_ids,\\n        \"token_type_ids\": token_type_ids,\\n        \"attention_mask\": attention_masks,\\n    }\\n    return mapped_example</td>\n",
       "      <td>This code snippet defines two functions: `encode_test_examples`, which prepares sequences of test texts by tokenizing them and building a TensorFlow dataset, and `map_test_example_to_dict`, which maps the tokenized test input components to the expected input format for TFBertForSequenceClassification.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>X_test = test_data[\"text\"]\\ntest_dataset = list(X_test)\\nds_test_encoded = encode_test_examples(test_dataset).batch(BATCH_SIZE)</td>\n",
       "      <td>This code snippet extracts the text data from the test dataset, encodes the test sequences, maps them for model input, and batches them using the specified batch size to form the test dataset for prediction.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td># Only apply 'keyword' columns in full data, because other features cleaned in df_train/test\\ntrain_full = clean_text(train_full,'keyword')\\ntest_full = clean_text(test_full, 'keyword')</td>\n",
       "      <td>This code snippet preprocesses the 'keyword' column in both the original training and test datasets using the `clean_text` function.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td># Adding cleaned data into df_train/test\\ndf_train['keyword'] = train_full['keyword']\\ndf_test['keyword'] = test_full['keyword']</td>\n",
       "      <td>This snippet adds the cleaned 'keyword' column from `train_full` and `test_full` back into the preprocessed training and test DataFrames `df_train` and `df_test`.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>def extract_keywords(text):\\n    potential_keywords = []\\n    TOP_KEYWORD = -1\\n    # Create a list for keyword parts of speech\\n    pos_tag = ['ADJ', 'NOUN', 'PROPN']\\n    doc = nlp_spacy(text)\\n    \\n    for i in doc:\\n        if i.pos_ in pos_tag:\\n            potential_keywords.append(i.text)\\n\\n    document_embed = sentence_enc([text])\\n    potential_embed = sentence_enc(potential_keywords)    \\n    \\n    vector_distances = cosine_similarity(document_embed, potential_embed)\\n    keyword = [potential_keywords[i] for i in vector_distances.argsort()[0][TOP_KEYWORD:]]\\n\\n    return keyword\\n\\ndef keyword_filler(keyword, text):\\n    if pd.isnull(keyword):\\n        try:\\n            keyword = extract_keywords(text)[0]\\n        except:\\n            keyword = '' \\n        \\n    return keyword</td>\n",
       "      <td>This code snippet defines two functions: `extract_keywords` to identify and extract potential keywords from a given text using SpaCy and the Universal Sentence Encoder, and `keyword_filler` to fill missing keywords in the dataset using the extracted keywords.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>df_train.keyword = pd.DataFrame(list(map(keyword_filler, df_train.keyword, df_train.text))).astype(str)\\ndf_test.keyword = pd.DataFrame(list(map(keyword_filler, df_test.keyword, df_test.text))).astype(str)\\n\\nprint('Null Training Keywords =&gt; ', df_train['keyword'].isnull().any())\\nprint('Null Test Keywords =&gt; ', df_test['keyword'].isnull().any())</td>\n",
       "      <td>This snippet applies the `keyword_filler` function to the 'keyword' column of the training and test datasets to handle missing keywords, then checks and prints whether any nulls remain in these columns.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td># Spilt data\\nX_train, X_val, y_train, y_val = train_test_split(df_train[['text','keyword']],\\n                                                    df_train.target, \\n                                                    test_size=0.2, \\n                                                    random_state=42)\\nX_train.shape, X_val.shape</td>\n",
       "      <td>This snippet splits the training dataset into training and validation sets, using an 80-20 split and a random seed for reproducibility, then prints the shapes of the resulting splits.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>train_ds = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\\nval_ds = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(df_test[['text','keyword']]))</td>\n",
       "      <td>This snippet converts the training, validation, and test sets into TensorFlow datasets, facilitating input into TensorFlow models for further processing and training.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>AUTOTUNE = tf.data.experimental.AUTOTUNE\\n\\nBUFFER_SIZE = 1000\\nBATCH_SIZE = 32\\nRANDOM_SEED = 319\\n\\ndef configure_dataset(dataset, shuffle=False, test=False):\\n    if shuffle:\\n        dataset = dataset.cache()\\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\\n                        .prefetch(AUTOTUNE)\\n    elif test:\\n        dataset = dataset.cache()\\\\n                        .batch(BATCH_SIZE, drop_remainder=False)\\\\n                        .prefetch(AUTOTUNE)\\n    else:\\n        dataset = dataset.cache()\\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\\n                        .prefetch(AUTOTUNE)\\n    return dataset</td>\n",
       "      <td>This snippet defines a function to configure TensorFlow datasets for training, validation, or testing by setting caching, shuffling, batching, and prefetching options to optimize data pipeline performance.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>a3 = configure_dataset(train_ds, shuffle=True)\\ndict3 = []\\nfor elem in a3:\\n    dict3.append(elem[0]['text'][0])\\ndict3[:10]</td>\n",
       "      <td>This snippet configures the `train_ds` TensorFlow dataset with shuffling, then extracts and lists the first elements of the 'text' column for the first 10 batches as a sample inspection.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td># Configure the datasets\\ntrain_ds = configure_dataset(train_ds, shuffle=True)\\nval_ds = configure_dataset(val_ds)\\ntest_ds = configure_dataset(test_ds, test=True)</td>\n",
       "      <td>This code snippet configures the training, validation, and test TensorFlow datasets with appropriate settings for shuffling, batching, and prefetching for optimal performance.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td># Free memory\\ndel X_train, X_val, y_train, y_val, df_train, df_test, train_full, test_full</td>\n",
       "      <td>This snippet deletes the intermediate variables related to the training, validation, and test datasets from memory to free up resources.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>train_df = train_df.drop(['id', 'keyword', 'location'], axis = 1)</td>\n",
       "      <td>This code removes the 'id', 'keyword', and 'location' columns from the training DataFrame.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>#remove duplicated rows\\ntrain_df.drop_duplicates(inplace=True)</td>\n",
       "      <td>This code removes duplicate rows from the training DataFrame to ensure data quality and avoid redundancy.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Real_Disaster_text = ' '.join(Real_Disaster_df.text.tolist())</td>\n",
       "      <td>This code concatenates all the text of disaster-related tweets from the filtered DataFrame into a single string for potential further analysis or text processing.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Not_Real_Disaster_text = ' '.join(Not_Real_Disaster_df.text.tolist())</td>\n",
       "      <td>This code concatenates all the text of non-disaster-related tweets from the filtered DataFrame into a single string for potential further analysis or text processing.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td># take text and preprocess 'remove stopwords [a, the, and, thus, ... etc] and punctations[,%$ ..etc] and len of text less than 3' \\ndef clean_text(text):\\n    \"\"\"\\n        text: a string \\n        return: cleaned string\\n    \"\"\"\\n    result = []\\n    for token in simple_preprocess(text):\\n        if token not in STOPWORDS and token not in punctation and  len(token) &gt;= 3 :\\n            token = token.lower() \\n            result.append(token)    \\n    return \" \".join(result)</td>\n",
       "      <td>This code defines a function to preprocess text by removing stopwords, punctuation, and short words, returning a cleaned string for further analysis or modeling.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>train_df['text'] = train_df['text'].map(clean_text)\\ntrain_df.head()</td>\n",
       "      <td>This code applies the `clean_text` function to each text entry in the training DataFrame to clean and preprocess the text data, and then displays the first few rows of the modified DataFrame.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>from sklearn.utils import shuffle\\ntrain_df_shuffled = shuffle(train_df)\\ntrain_df_shuffled.head()</td>\n",
       "      <td>This code shuffles the rows of the training DataFrame to ensure a random distribution of samples and then displays the first few rows of the shuffled DataFrame.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>X = train_df_shuffled['text']\\ny = train_df_shuffled['target']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42, stratify = y)</td>\n",
       "      <td>This code splits the shuffled DataFrame into training and testing sets for the features (X) and target labels (y), using stratified sampling to maintain the class distribution.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>test_df = test_df.drop(['id', 'keyword', 'location'], axis = 1)</td>\n",
       "      <td>This code removes the 'id', 'keyword', and 'location' columns from the testing DataFrame.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>test_df['text'] = test_df['text'].map(clean_text)\\ntest_df.head()</td>\n",
       "      <td>This code applies the `clean_text` function to each text entry in the testing DataFrame to clean and preprocess the text data, and then displays the first few rows of the modified DataFrame.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>sample_submission[\"target\"] = y_pred</td>\n",
       "      <td>This code updates the 'target' column in the sample submission DataFrame with the predicted target labels from the Naive Bayes classifier.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>max_seq_len = 160\\n\\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=max_seq_len)\\ntest_input  = bert_encode(test.text.values, tokenizer, max_len=max_seq_len)\\ntrain_label = train.target.values\\n\\n# Data split\\nX_train, X_test, y_train, y_test = train_test_split(train_input, \\n                                                    train_label, \\n                                                    test_size=0.25,\\n                                                    random_state=42, \\n                                                    shuffle=True)\\n# X_train.shape, X_test.shape = ((5709, 160), (1904, 160))</td>\n",
       "      <td>This code encodes the text data from the training and testing datasets using the BERT tokenizer, sets the maximum sequence length, splits the encoded training data into training and validation sets, and extracts the target labels for training.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td># DROP DUPLICATE SAMPLES WITH CONFLICTING LABELS\\n\\nconflicting = conflicting_check.loc[(conflicting_check.target != 1) &amp; (conflicting_check.target != 0)].index\\ndata = data.drop(data[text.isin(conflicting)].index)\\nprint ('Conflicting samples count:', conflicting.shape[0])</td>\n",
       "      <td>This code snippet drops duplicate samples with conflicting labels from the dataset and then prints the count of such conflicting samples.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td># TOKENIZE ALL THE SENTENCES AND MAP THE TOKENS TO THEIR WORD IDs\\n\\ninput_ids = []\\nattention_masks = []\\n\\n# For every sentence...\\nfor sent in sentences:\\n    # `encode_plus` will:\\n    #   (1) Tokenize the sentence.\\n    #   (2) Prepend the `[CLS]` token to the start.\\n    #   (3) Append the `[SEP]` token to the end.\\n    #   (4) Map tokens to their IDs.\\n    #   (5) Pad or truncate the sentence to `max_length`\\n    #   (6) Create attention masks for [PAD] tokens.\\n    encoded_dict = tokenizer.encode_plus(\\n                        sent,                      # Sentence to encode.\\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\\n                        max_length = 64,           # Pad &amp; truncate all sentences.\\n                        pad_to_max_length = True,\\n                        return_attention_mask = True,   # Construct attn. masks.\\n                        return_tensors = 'pt',     # Return pytorch tensors.\\n                   )\\n    \\n    # Add the encoded sentence to the list.    \\n    input_ids.append(encoded_dict['input_ids'])\\n    \\n    # And its attention mask (simply differentiates padding from non-padding).\\n    attention_masks.append(encoded_dict['attention_mask'])\\n\\n# Convert the lists into tensors.\\ninput_ids = torch.cat(input_ids, dim=0)\\nattention_masks = torch.cat(attention_masks, dim=0)\\nlabels = torch.tensor(labels)\\n\\n# Print sentence 0, now as a list of IDs.\\nprint('Original: ', sentences[0])\\nprint('Token IDs:', input_ids[0])</td>\n",
       "      <td>This code snippet tokenizes all sentences in the dataset, maps the tokens to their IDs, pads or truncates them to a maximum length, constructs attention masks, and converts the results into PyTorch tensors.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td># SPLIT TRAIN DATA INTO TRAIN AND TEST SET\\n# I used small test set (SPLIT=0,999) in order to train the model on the majority of the data, after all parameters were tuned\\n# Use 0,9 or lower to train the model and look at the perfomance/ tune parameters\\n\\nSPLIT = 0.999\\n\\nfrom torch.utils.data import TensorDataset, random_split\\n\\n# Combine the training inputs into a TensorDataset.\\ndataset = TensorDataset(input_ids, attention_masks, labels)\\n\\n# Create a 90-10 train-validation split.\\n\\n# Calculate the number of samples to include in each set.\\ntrain_size = int(SPLIT * len(dataset))\\nval_size = len(dataset) - train_size\\n\\n# Divide the dataset by randomly selecting samples.\\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\\n\\nprint('{:&gt;5,} training samples'.format(train_size))\\nprint('{:&gt;5,} validation samples'.format(val_size))</td>\n",
       "      <td>This code snippet splits the dataset into training and validation sets using a specified split ratio and prints the number of samples in each set.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td># CREATE DATA ITERATOR TO SAVE MEMORY\\n\\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\\n\\n# The DataLoader needs to know our batch size for training, so we specify it \\n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \\n# size of 16 or 32.\\nbatch_size = 32\\n\\n# Create the DataLoaders for our training and validation sets.\\n# We'll take training samples in random order. \\ntrain_dataloader = DataLoader(\\n            train_dataset,  # The training samples.\\n            sampler = RandomSampler(train_dataset), # Select batches randomly\\n            batch_size = batch_size # Trains with this batch size.\\n        )\\n\\n# For validation the order doesn't matter, so we'll just read them sequentially.\\nvalidation_dataloader = DataLoader(\\n            val_dataset, # The validation samples.\\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\\n            batch_size = batch_size # Evaluate with this batch size.\\n        )</td>\n",
       "      <td>This code snippet creates DataLoader objects for both the training and validation datasets, allowing the data to be loaded in batches to save memory, with random sampling for training and sequential sampling for validation.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td># PREPARE PREDICTIONS FOR SUBMISSION\\n\\n# Combine the results across all batches. \\nflat_predictions = np.concatenate(predictions, axis=0)\\n\\n# For each sample, pick the label (0 or 1) with the higher score.\\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()</td>\n",
       "      <td>This code snippet combines the prediction results from all batches and determines the final predicted labels by selecting the label with the higher score for each sample.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td># clean training text\\nl=len(df)\\ndisplay(l)\\ncleanlist=[]\\ntextlength=[]\\nfor i in range(l):\\n    ct=cleantext.clean(df.iloc[i,3], clean_all= True)\\n    cleanlist.append(ct)\\n    lct=len(ct)\\n    textlength.append(lct)\\n</td>\n",
       "      <td>This code snippet cleans the text data in the training dataset using the 'cleantext' library and stores the cleaned text and their lengths in separate lists.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td># combine clean text with training data\\ndf_clean=pd.DataFrame(cleanlist)\\ndf_clean.columns=['cleantext']\\nframes=[df,df_clean]\\nnewdf=pd.concat(frames, axis=1)\\ndisplay(newdf)</td>\n",
       "      <td>This code snippet combines the cleaned text data with the original training dataset into a single DataFrame and displays it.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>l=len(df1)\\ndisplay(l)\\npredlist=[]\\n#l=1\\nfor i in range(l):\\n    ct=cleantext.clean(df1.iloc[i,3], clean_all= True)\\n    new=predictor.predict(ct)\\n    predlist.append(new)</td>\n",
       "      <td>This code snippet cleans the text data in the test dataset, uses the trained predictor to make predictions on the cleaned text, and stores the predictions in a list.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>df_pred=pd.DataFrame(predlist)\\ndf_pred.columns=['target']\\nframes=[df1,df_pred]\\ndf2=pd.concat(frames, axis=1)\\ndisplay(df2.head())</td>\n",
       "      <td>This code snippet combines the prediction results with the original test dataset into a single DataFrame and displays the first few rows of the combined DataFrame.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>df2.loc[df2['target']=='target','target']=1\\ndf2.loc[df2['target']=='not_target','target']=0\\ndisplay(df2['target'].mean())\\ndf2=df2[['id','target']]\\ndisplay(df2.shape)\\ndisplay(df2.head())</td>\n",
       "      <td>This code snippet converts the textual prediction labels to numeric labels (1 for 'target' and 0 for 'not_target'), calculates the mean of the 'target' column, and formats the DataFrame to include only the 'id' and 'target' columns, then displays the shape and first few rows of the resulting DataFrame.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>min_freq = 5\\nabove_threshold = train.location.value_counts() &gt; min_freq\\nfrequent_places = above_threshold.index[above_threshold]\\ndata = train[train.location.isin(frequent_places)].location\\nprint(f'{data.nunique()} unique locations with more than {min_freq} occurrences')</td>\n",
       "      <td>The code snippet filters the locations in the training dataset to retain only those that have more than a specified minimum frequency and prints the number of unique locations that meet this criterion.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>train.drop(['location', 'keyword'], axis=1, inplace=True)\\ntest.drop(['location', 'keyword'], axis=1, inplace=True)</td>\n",
       "      <td>The code snippet removes the 'location' and 'keyword' columns from both the training and test datasets.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>nlp = English()\\ntokenizer = nlp.tokenizer\\ntokens = tokenizer('This is a test!')\\nprint(tokens)\\nprint(type(tokens))\\nprint([t.text for t in tokens])</td>\n",
       "      <td>The code snippet initializes a tokenizer using the SpaCy English model, tokenizes a sample sentence, and prints both the tokens and their text representations.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>text = \"Don't split #hashtags!\"\\nprint('Before:', [t for t in tokenizer(text)])\\n\\nprefixes = list(nlp.Defaults.prefixes)\\nprefixes.remove('#')\\nprefix_regex = spacy.util.compile_prefix_regex(prefixes)\\ntokenizer.prefix_search = prefix_regex.search\\n\\nprint('After:', [t for t in tokenizer(text)])</td>\n",
       "      <td>The code snippet demonstrates how to customize a SpaCy tokenizer to prevent it from splitting hashtags by modifying its prefix handling rules and then tokenizing a sample text before and after the modification.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>text = 'This is  a test\\n , ok?'\\nprint('All tokens:', [t.text for t in tokenizer(text)])\\n\\nprint('Check for is_space():', [t.text for t in tokenizer(text) if not t.is_space])</td>\n",
       "      <td>The code snippet tokenizes a sample text and demonstrates how to filter out tokens that consist solely of whitespace characters.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>train['tokens'] = train['text'].apply(lambda row: [t.text.lower() for t in tokenizer(row) if not t.is_space])\\ntest['tokens'] = test['text'].apply(lambda row: [t.text.lower() for t in tokenizer(row) if not t.is_space])</td>\n",
       "      <td>The code snippet tokenizes the 'text' column in both the training and test datasets, converts the tokens to lowercase, removes whitespace-only tokens, and stores the results in a new 'tokens' column.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>from sklearn.feature_extraction.text import CountVectorizer\\n\\n# min and max document frequency (ratio of documents containing that token)\\nmin_df = 5\\nmax_df = 0.6\\n\\n# limit vocabulary size as a function of the training data\\nmax_features = len(train) * 2\\n\\nvectorizer = CountVectorizer(lowercase=False, tokenizer=lambda x: x, min_df=min_df, max_df=max_df, max_features=max_features, binary=True)\\ntrain_bow = vectorizer.fit_transform(train.tokens)\\ntrain_bow</td>\n",
       "      <td>The code snippet sets up a CountVectorizer to tokenize the pre-processed text data, applying minimum and maximum document frequency thresholds and limiting the vocabulary size, and then transforms the tokenized training data into a sparse bag-of-words representation.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>x = train_bow\\ny = train['target']</td>\n",
       "      <td>The code snippet assigns the bag-of-words representation of the training data to `x` and the target labels to `y` in preparation for modeling.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td># min and max document frequency (ratio of documents containing that token)\\nmin_df = 10\\nmax_df = 0.6\\n\\n# limit vocabulary size as a function of the training data\\nmax_features = len(train) * 2\\n\\n# single words to 3-grams\\nngram_range = (1, 3)\\n\\nvectorizer = CountVectorizer(lowercase=False, tokenizer=lambda x: x, min_df=min_df, max_df=max_df, max_features=max_features, binary=True, ngram_range=ngram_range)\\nx = train_bow = vectorizer.fit_transform(train.tokens)\\n\\nvocab = vectorizer.get_feature_names()\\nword_count = train_bow.toarray().sum(0)\\n\\nplot_top_values(word_count, k, vocab, 'Count', 'Type')</td>\n",
       "      <td>The code snippet reconfigures the CountVectorizer to consider single words to 3-grams, using new minimum and maximum document frequency thresholds, updates the bag-of-words representation of the training dataset, and plots the top 50 most frequent n-grams.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>oov_count = Counter()\\nall_tokens = []\\n\\nfor row in train.tokens:\\n    tokens = [t[1:] if t.startswith('#') else t for t in row]\\n    all_tokens.append(tokens)\\n    oov_count.update(set(t for t in tokens if t not in word_dict))</td>\n",
       "      <td>The code snippet processes the tokenized text data by stripping hashtags from tokens, appends these processed tokens to a list, and tracks the count of out-of-vocabulary (OOV) tokens not found in the GloVe embeddings dictionary.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>test_tokens = []\\nfor row in test.tokens:\\n    tokens = [t[1:] if t.startswith('#') else t for t in row]\\n    test_tokens.append(tokens)</td>\n",
       "      <td>The code snippet processes the test dataset's tokenized text by stripping hashtags from tokens and saves the cleaned tokens into a list.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>words_to_add = [w for w in oov_count if oov_count[w] &gt; 2]\\nfor word in words_to_add:\\n    word_dict[word] = len(word_dict)\\n\\nnew_vectors = torch.zeros((len(words_to_add), embeddings.shape[1]))\\nembeddings = torch.cat([embeddings, new_vectors], dim=0)\\nprint(len(word_dict), embeddings.shape)</td>\n",
       "      <td>The code snippet adds frequently occurring out-of-vocabulary words to the word dictionary, appends zero vectors for these words to the existing embeddings tensor, and prints the updated size of the word dictionary and embeddings tensor.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>def convert_to_indices(all_tokens):\\n    word_indices = []\\n\\n    for tokens in all_tokens:\\n        tweet_inds = torch.tensor([word_dict[t] for t in tokens if t in word_dict], dtype=torch.long)\\n        word_indices.append(tweet_inds)\\n    \\n    return word_indices\\n\\nword_indices = convert_to_indices(all_tokens)\\ntest_word_indices = convert_to_indices(test_tokens)</td>\n",
       "      <td>The code snippet defines a function to convert tokenized text data into indices based on the word dictionary, then applies this function to both the training and test token lists to create index-based representations.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>def collate_as_list(samples):\\n    \"\"\"Function for the DataLoader to combine samples in a batch. Each sample is a (x, y) pair.\"\"\"\\n    x, y = list(zip(*samples))\\n    if y[0] is None:\\n        return x\\n    return x, torch.tensor(y).float()\\n\\n\\nclass WordIndexDataset(Dataset):\\n    def __init__(self, x, y=None):\\n        self.x = x\\n        self.y = y\\n    \\n    def __getitem__(self, i):\\n        if self.y is not None:\\n            return self.x[i], self.y[i]\\n        else:\\n            return self.x[i], None\\n    \\n    def __len__(self):\\n        return len(self.x)\\n</td>\n",
       "      <td>The code snippet defines a custom dataset class `WordIndexDataset` for storing token indices and their corresponding labels, and a collate function `collate_as_list` for the DataLoader to handle batches of these samples.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>validation_size = int(0.1 * len(train))\\nvalidation_inds = np.random.choice(np.arange(len(train)), size=validation_size, replace=False)\\nis_train = np.ones(len(train), dtype=np.bool)\\nis_train[validation_inds] = False\\n\\n# use an object array since we have varied size tensors\\ntweets = np.array(word_indices, dtype=object)\\ntarget = train.target.to_numpy()\\n# train_tweets, valid_tweets, train_target, valid_target = train_test_split(tweets, target, test_size=0.1, stratify=target)\\ntrain_tweets = tweets[is_train].tolist()\\ntrain_target = target[is_train]\\nvalid_tweets = tweets[~is_train].tolist()\\nvalid_target = target[~is_train]\\n\\ntrain_data = WordIndexDataset(train_tweets, train_target)\\nvalid_data = WordIndexDataset(valid_tweets, valid_target)\\ntest_data = WordIndexDataset(test_word_indices)\\ntrain_loader = DataLoader(train_data, batch_size=32, collate_fn=collate_as_list)\\nvalid_loader = DataLoader(valid_data, batch_size=256, collate_fn=collate_as_list)\\ntest_loader = DataLoader(test_data, batch_size=256, collate_fn=collate_as_list)</td>\n",
       "      <td>The code snippet splits the training data into training and validation sets, creates dataset objects for both and the test data, and sets up DataLoader instances to handle batching and collating of the data samples for training, validation, and testing.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td># create tensors of variable sizes\\n# note that the tokenizer returns a tensor with shape [1, num_tokens]\\ntrain_tokens = train.text[is_train].apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\\nvalid_tokens = train.text[~is_train].apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\\ntest_tokens = test.text.apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\\n\\n# add padding to have a fixed size matrix. With bigger datasets we should be careful about memory usage, but this is small enough to skip this kind of optimization\\npadding = tokenizer.pad_token_id\\nx_train = pad_sequence(train_tokens, batch_first=True, padding_value=padding)\\nx_valid = pad_sequence(valid_tokens, batch_first=True, padding_value=padding)\\nx_test = pad_sequence(test_tokens, batch_first=True, padding_value=padding)\\n\\nx_train_mask = x_train != padding\\nx_valid_mask = x_valid != padding\\nx_test_mask = x_test != padding\\nprint(f'x_train shape: {x_train.shape}, x_valid shape: {x_valid.shape}, x_test shape: {x_test.shape}')</td>\n",
       "      <td>The code snippet tokenizes the text data in the training, validation, and test sets using the DistilRoBERTa tokenizer, then pads the token sequences to a fixed length, and creates corresponding attention masks to indicate the valid token positions.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>train_data = TensorDataset(x_train, x_train_mask, torch.tensor(train_target))\\nvalid_data = TensorDataset(x_valid, x_valid_mask, torch.tensor(valid_target))\\ntest_data = TensorDataset(x_test, x_test_mask)\\n\\ntrain_loader = DataLoader(train_data, batch_size=32)\\nvalid_loader = DataLoader(valid_data, batch_size=256)\\ntest_loader = DataLoader(test_data, batch_size=256)</td>\n",
       "      <td>The code snippet creates TensorDataset objects for the training, validation, and test sets that include token sequences, attention masks, and labels (for training and validation), and sets up DataLoader instances for each dataset to handle batching.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>def lowercase_text(text):\\n    return text.lower()\\n\\nX_train.text=X_train.text.apply(lambda x: lowercase_text(x))\\nX_test.text=X_test.text.apply(lambda x: lowercase_text(x))\\nX_train.head()</td>\n",
       "      <td>This code snippet defines a function to convert text to lowercase and applies this transformation to the 'text' column in both the training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>import re\\nimport string\\ndef remove_noise(text):\\n    text = re.sub('\\[.*?\\]', '', text)\\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\\n    text = re.sub('&lt;.*?&gt;+', '', text)\\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\\n    text = re.sub('\\n', '', text)\\n    text = re.sub('\\w*\\d\\w*', '', text)\\n    text = re.sub('ûò', '', text)\\n    return text\\nX_train.text=X_train.text.apply(lambda x: remove_noise(x))\\nX_test.text=X_test.text.apply(lambda x: remove_noise(x))\\nX_train.head()</td>\n",
       "      <td>This code snippet defines a function to remove various types of noise from text data, including punctuation, URLs, and unwanted characters, and applies this cleaning process to the 'text' column in both training and testing datasets.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td># Tokenizing the training and the test set\\nimport nltk\\nfrom nltk.corpus import stopwords\\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\\nX_train['text'] = X_train['text'].apply(lambda x: tokenizer.tokenize(x))\\nX_test['text'] = X_test['text'].apply(lambda x: tokenizer.tokenize(x))\\nX_train['text'].head()</td>\n",
       "      <td>This code snippet imports necessary NLTK modules, initializes a tokenizer to split text into words, and applies this tokenizer to the 'text' column in both the training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td># Removing stopwords belonging to english language\\ndef remove_stopwords(text):\\n    words = [w for w in text if w not in stopwords.words('english')]\\n    return words\\n\\nX_train['text'] = X_train['text'].apply(lambda x : remove_stopwords(x))\\nX_test['text'] = X_test['text'].apply(lambda x : remove_stopwords(x))\\nX_train.head()</td>\n",
       "      <td>This code snippet defines a function to remove English stopwords from tokenized text data and applies this function to the 'text' column in both the training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td># After preprocessing, the text format\\ndef combine_text(list_of_text):\\n    '''Takes a list of text and combines them into one large chunk of text.'''\\n    combined_text = ' '.join(list_of_text)\\n    return combined_text\\n\\nX_train['text'] = X_train['text'].apply(lambda x : combine_text(x))\\nX_test['text'] = X_test['text'].apply(lambda x : combine_text(x))\\n# X_train['text']\\nX_train.head()</td>\n",
       "      <td>This code snippet defines a function to combine a list of words into a single string and applies this function to the 'text' column in both the training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td># Stemming\\nfrom nltk.stem.snowball import SnowballStemmer\\nstemmer = SnowballStemmer(\"english\")\\n\\ndef stemming(text):\\n    text = [stemmer.stem(word) for word in text.split()]\\n    return ' '.join(text)\\n\\n#X_train['text'] = X_train['text'].apply(lambda x : stemming(x))\\n#X_test['text'] = X_test['text'].apply(lambda x : stemming(x))\\n#X_train</td>\n",
       "      <td>This code snippet imports the SnowballStemmer from NLTK, defines a function to apply stemming to each word in the text, and comments out the application of this function to the 'text' column in both the training and testing datasets.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>from sklearn.feature_extraction.text import CountVectorizer\\ncount_vectorizer=CountVectorizer() # analyzer='word', stop_words = \"english\"\\ntrain_vec = count_vectorizer.fit_transform(X_train.text)\\ntest_vec = count_vectorizer.transform(X_test.text)\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nTfidf_vectorizer = TfidfVectorizer() # min_df=2, max_df=0.5, ngram_range=(1, 2)analyzer='word', stop_words = \"english\"analyzer='word', stop_words='english'# , ngram_range=(1, 2), lowercase=True, max_features=150000\\ntrain_tfidf = Tfidf_vectorizer.fit_transform(X_train.text)\\ntest_tfidf = Tfidf_vectorizer.transform(X_test.text)\\n\\nprint(\"train_vec\" ,train_vec[7].todense())\\nprint(\"test_vec\", test_vec[7].todense())\\n\\nprint(\"train_tfidf\" ,train_tfidf[7].todense())\\nprint(\"test_tfidf\", test_vec[7].todense())</td>\n",
       "      <td>This code snippet imports CountVectorizer and TfidfVectorizer from sklearn, fits and transforms the 'text' column of the training data and similarly transforms the testing data using these vectorizers, and then prints the dense representation of a specific transformed entry for both vectorizers.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>def bert_encode(texts, tokenizer, max_len):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)</td>\n",
       "      <td>This code snippet defines a function 'bert_encode' that tokenizes given texts using a BERT tokenizer and formats them into appropriate input representations including tokens, masks, and segment IDs for BERT model input.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>train = train.fillna(' ')\\ntest = test.fillna(' ')</td>\n",
       "      <td>This code snippet fills any missing values in the train and test DataFrames with empty spaces to handle NaNs.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>import tensorflow_hub as hub\\nimport tokenization\\n\\n#%%time\\nmax_len = 60\\n\\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\\n#module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\\nbert_layer = hub.KerasLayer(module_url, trainable=True)\\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\\n\\ntrain_input = bert_encode(train['location']+' '+train['keyword']+' '+train['text'], tokenizer, max_len=max_len)\\ntest_input = bert_encode(test['location']+' '+test['keyword']+' '+test['text'], tokenizer, max_len=max_len)\\ntrain_labels = train.target.values</td>\n",
       "      <td>This code snippet loads a pre-trained BERT model from TensorFlow Hub, initializes a tokenizer, and uses this tokenizer to encode the text data from the train and test DataFrames into BERT-compatible input format while also extracting the target labels for the training data.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>from bs4 import BeautifulSoup # Text Cleaning\\nimport re, string # Regular Expressions, String\\nfrom nltk.corpus import stopwords # stopwords\\nfrom nltk.stem.porter import PorterStemmer # for word stemming\\nfrom nltk.stem import WordNetLemmatizer # for word lemmatization\\nimport unicodedata\\nimport html\\n\\n# set of stopwords to be removed from text\\nstop = set(stopwords.words('english'))\\n\\n# update stopwords to have punctuation too\\nstop.update(list(string.punctuation))\\n\\ndef clean_tweets(text):\\n    \\n    # Remove unwanted html characters\\n    re1 = re.compile(r'  +')\\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&amp;').replace('#146;', \"'\").replace(\\n    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\\n    '&lt;br /&gt;', \"\\n\").replace('\\\\\"', '\"').replace('&lt;unk&gt;', 'u_n').replace(' @.@ ', '.').replace(\\n    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\\n    text = re1.sub(' ', html.unescape(x1))\\n    \\n    # remove non-ascii characters\\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\\n    \\n    # strip html\\n    soup = BeautifulSoup(text, 'html.parser')\\n    text = soup.get_text()\\n    \\n    # remove between square brackets\\n    text = re.sub('\\[[^]]*\\]', '', text)\\n    \\n    # remove URLs\\n    text = re.sub(r'http\\S+', '', text)\\n    \\n    # remove twitter tags\\n    text = text.replace(\"@\", \"\")\\n    \\n    # remove hashtags\\n    text = text.replace(\"#\", \"\")\\n    \\n    # remove all non-alphabetic characters\\n    text = re.sub(r'[^a-zA-Z ]', '', text)\\n    \\n    # remove stopwords from text\\n    final_text = []\\n    for word in text.split():\\n        if word.strip().lower() not in stop:\\n            final_text.append(word.strip().lower())\\n    \\n    text = \" \".join(final_text)\\n    \\n    # lemmatize words\\n    lemmatizer = WordNetLemmatizer()    \\n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\\n    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\\n    \\n    # replace all numbers with \"num\"\\n    text = re.sub(\"\\d\", \"num\", text)\\n    \\n    return text.lower()\\n\\ntrain_data['prep_text'] = train_data['text'].apply(clean_tweets)\\ntrain_data['prep_text'].head(5)</td>\n",
       "      <td>This code defines a function to clean and preprocess textual data, then applies this function to the 'text' column in the training data, storing the cleaned text in a new column 'prep_text'.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>test_data['text'] = test_data['text'].apply(clean_tweets)\\ntest_data['text'].head(5)</td>\n",
       "      <td>This code applies the previously defined `clean_tweets` function to the 'text' column in the test data and stores the cleaned text back into the 'text' column.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>from keras.preprocessing.text import Tokenizer # Text tokenization\\n\\n# Setting up the tokenizer\\nvocab_size = 1000\\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))</td>\n",
       "      <td>This code initializes a Keras Tokenizer with a specified vocabulary size, and then fits it on the combined preprocessed text data from both the training and test datasets to build the vocabulary.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td># Representing texts as one hot encoded sequence\\n\\nX_train_ohe = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'binary')\\nX_test_ohe = tokenizer.texts_to_matrix(test_data['text'], mode = 'binary')\\ny_train = np.array(train_data['target']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_ohe.shape}\")\\nprint(f\"X_test shape: {X_test_ohe.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")</td>\n",
       "      <td>This code converts the preprocessed text data into one-hot encoded sequences using the previously defined tokenizer for both training and test datasets and prints the shapes of the resulting arrays.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>from sklearn.model_selection import train_test_split\\nX_train_ohe, X_val_ohe, y_train, y_val = train_test_split(X_train_ohe, y_train, random_state = 42, test_size = 0.2)\\n\\nprint(f\"X_train shape: {X_train_ohe.shape}\")\\nprint(f\"X_val shape: {X_val_ohe.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")</td>\n",
       "      <td>This code splits the one-hot encoded training data and target labels into training and validation sets, and then prints the shapes of these new datasets.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>X_train_wc = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'count')\\nX_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\\ny_train = np.array(train_data['target']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_wc.shape}\")\\nprint(f\"X_test shape: {X_test_wc.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\n</td>\n",
       "      <td>This code converts the preprocessed text data into count-based encoded sequences using the previously defined tokenizer for both training and test datasets and prints the shapes of the resulting arrays.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>X_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, random_state = 42, test_size = 0.2)\\n\\nprint(f\"X_train shape: {X_train_wc.shape}\")\\nprint(f\"X_val shape: {X_val_wc.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")</td>\n",
       "      <td>This code splits the count-based encoded training data and target labels into training and validation sets, and then prints the shapes of these new datasets.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>X_train_freq = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'freq')\\nX_test_freq = tokenizer.texts_to_matrix(test_data['text'], mode = 'freq')\\ny_train = np.array(train_data['target']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_freq.shape}\")\\nprint(f\"X_test shape: {X_test_freq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")</td>\n",
       "      <td>This code converts the preprocessed text data into frequency-based encoded sequences using the previously defined tokenizer for both training and test datasets and prints the shapes of the resulting arrays.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>X_train_freq, X_val_freq, y_train, y_val = train_test_split(X_train_freq, y_train, test_size = 0.2, random_state = 42)\\nprint(f\"X_train shape: {X_train_freq.shape}\")\\nprint(f\"X_val shape: {X_val_freq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")</td>\n",
       "      <td>This code splits the frequency-based encoded training data and target labels into training and validation sets, and then prints the shapes of these new datasets.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>from sklearn.feature_extraction.text import TfidfVectorizer # Term Frequency - Inverse Document Frequency\\n\\nvectorizer = TfidfVectorizer(max_features = vocab_size)\\nvectorizer.fit(list(train_data['prep_text']) + list(test_data['text']))\\n\\n# Fitting on training and testing data\\nX_train_tfidf = vectorizer.transform(list(train_data['prep_text'])).toarray() \\nX_test_tfidf = vectorizer.transform(list(test_data['text'])).toarray()\\n\\ny_train = np.array(train_data['target']).astype(int)\\n\\nprint(f\"X_train shape {X_train_tfidf.shape}\")\\nprint(f\"X_test shape {X_test_tfidf.shape}\")\\nprint(f\"y_train shape {y_train.shape}\")</td>\n",
       "      <td>This code initializes a TfidfVectorizer to convert the preprocessed text data into TF-IDF representations, fits it on the combined text data from both training and test datasets, transforms the text data into TF-IDF feature arrays, and prints the shapes of the resulting arrays.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size = 0.2, random_state = 42)\\nprint(f\"X_train shape: {X_train_tfidf.shape}\")\\nprint(f\"X_val shape: {X_val_tfidf.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")</td>\n",
       "      <td>This code splits the TF-IDF encoded training data and target labels into training and validation sets, and then prints the shapes of these new datasets.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td># Sequences creation, truncation and padding\\n\\nfrom keras.preprocessing.sequence import pad_sequences\\n\\n# Setting up the tokenizer\\nvocab_size = 10000\\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))\\n\\nmax_len = 15\\nX_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\\nX_test_seq = tokenizer.texts_to_sequences(test_data['text'])\\n\\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\\ny_train = np.array(train_data['target']).astype(int)\\n\\nprint(f\"X_train shape: {X_train_seq.shape}\")\\nprint(f\"X_test shape: {X_test_seq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")</td>\n",
       "      <td>This code initializes a new Keras Tokenizer, fits it on combined preprocessed text data, converts the texts into sequences of tokens, and then truncates and pads these sequences to a specified maximum length, finally printing the shapes of the resulting arrays.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>X_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\\nprint(f\"X_train shape: {X_train_seq.shape}\")\\nprint(f\"X_val shape: {X_val_seq.shape}\")\\nprint(f\"y_train shape: {y_train.shape}\")\\nprint(f\"y_val shape: {y_val.shape}\")</td>\n",
       "      <td>This code splits the tokenized and padded training data sequences and target labels into training and validation sets, and then prints the shapes of these new datasets.</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td># Applying GloVE representations on our corpus\\n\\nembedding_matrix=np.zeros((num_words,100))\\n\\nfor word,i in tokenizer.word_index.items():\\n    if i &lt; num_words:\\n        emb_vec = embedding_dict.get(word)\\n        if emb_vec is not None:\\n            embedding_matrix[i] = emb_vec</td>\n",
       "      <td>This code creates an embedding matrix where each row corresponds to the GloVe embedding vector of a word from the tokenizer's word index, initializing with zeros and filling with the GloVe vectors when available.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td># Since classes are imbalanced, we need to resample the dataframe\\n# First divide by class\\ndf_class_0 = df[df[\"target\"] == 0]\\ndf_class_1 = df[df[\"target\"] == 1]</td>\n",
       "      <td>This code splits the DataFrame into two separate DataFrames based on the target class (0 and 1) to prepare for resampling due to class imbalance.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>df_class_0</td>\n",
       "      <td>This code outputs the DataFrame containing all rows where the \"target\" column is 0, for verification or further processing.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>df_class_1</td>\n",
       "      <td>This code outputs the DataFrame containing all rows where the \"target\" column is 1, for verification or further processing.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td># Second resample - try both under- and over-sampling\\ndf_class_0_under = df_class_0.sample(count_class_1) # undersampling by loosing objects\\ndf_under = pd.concat([df_class_0_under, df_class_1], axis=0)\\n\\ndf_class_1_over = df_class_1.sample(count_class_0, replace=True) # oversampling by duplicaitng objects\\ndf_over = pd.concat([df_class_0, df_class_1_over], axis=0)\\n\\n#df = df_under\\n#df = df_over\\n\\n# Looks like oversampling works better since we use more objects - more training cases</td>\n",
       "      <td>This code performs both undersampling of the majority class and oversampling of the minority class to balance the dataset, and then concatenates the resampled data into two new DataFrames for further analysis.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>train_df, test_df = train_test_split(df, train_size=0.9)</td>\n",
       "      <td>This code splits the DataFrame into training and testing sets, allocating 90% of the data to the training set and 10% to the testing set for model training and evaluation purposes.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>#eng_words = words.words(\"en\")</td>\n",
       "      <td>This code creates a list of English words using NLTK's words corpus to potentially filter or process text data.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>#print(\"wort\" in eng_words)</td>\n",
       "      <td>This code checks if the string \"wort\" is present in the list of English words and prints the result to validate the correctness of the words list.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>snowball = SnowballStemmer(language=\"english\")</td>\n",
       "      <td>This code initializes an instance of the SnowballStemmer for the English language to be used for stemming words during text preprocessing.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>def tokenize_sentence(sentence: str, remove_stop_words: bool = True):\\n    '''Tokenize sentences with nltk dropping non-english words and punctuation and optionally stop words'''\\n    tokens = word_tokenize(sentence, language=\"english\")\\n    #tokens = [i for i in tokens if i in eng_words and i not in string.punctuation]\\n    tokens = [i for i in tokens if i not in string.punctuation]\\n    if remove_stop_words:\\n        tokens = [i for i in tokens if i not in stopwords.words(\"english\")]\\n    tokens = [snowball.stem(i) for i in tokens]\\n    return tokens</td>\n",
       "      <td>This code defines a function that tokenizes a given sentence, removes punctuation, optionally removes stop words, and applies stemming to the remaining tokens for text preprocessing.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>tokenize_sentence(\"the sentence and asdf fy krkr\", False)</td>\n",
       "      <td>This code calls the `tokenize_sentence` function with a sample sentence and the option to not remove stop words, demonstrating the function's tokenization and stemming process.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>vectorizer_params = {\\n    #\"max_features\": 500,\\n    #\"max_features\": None,\\n    #\"tokenizer\": lambda x: tokenize_sentence(x, remove_stop_words=False),\\n    #\"tokenizer\": None,\\n    #\"ngram_range\": (1, 100),\\n    #\"min_df\": 0,\\n    #\"max_df\": 100,\\n    #\"use_idf\": False,\\n    #\"decode_error\": \"replace\",\\n    #\"sublinear_tf\": True,\\n    #\"analyzer\": \"char\"\\n}</td>\n",
       "      <td>This code snippet defines a dictionary with commented-out parameters for a TfidfVectorizer, which suggests various options for customizing text vectorization based on tokenization, n-grams, and other features, though none are currently active.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>vectorizer = TfidfVectorizer(**vectorizer_params)</td>\n",
       "      <td>This code initializes a TfidfVectorizer instance with the specified parameters from the `vectorizer_params` dictionary for transforming text data into TF-IDF features.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>vectorizer</td>\n",
       "      <td>This code outputs the configuration of the initialized TfidfVectorizer instance, showing the parameters set for text vectorization.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>features = vectorizer.fit_transform(train_df[\"text\"])</td>\n",
       "      <td>This code fits the TfidfVectorizer to the text data from the training DataFrame and transforms the text into TF-IDF features for further use in model training.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>feature_names = vectorizer.get_feature_names()</td>\n",
       "      <td>This code retrieves the list of feature names (terms) from the TfidfVectorizer that correspond to the columns of the TF-IDF feature matrix.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>X_train = train_df[\"text\"]</td>\n",
       "      <td>This code assigns the \"text\" column from the training DataFrame to the variable `X_train`, preparing the training text data for further processing or modeling.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>X_train</td>\n",
       "      <td>This code outputs the training text data assigned to `X_train` to verify its contents.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>y_train = train_df[\"target\"]</td>\n",
       "      <td>This code assigns the \"target\" column from the training DataFrame to the variable `y_train`, preparing the training labels for model training.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>y_train</td>\n",
       "      <td>This code outputs the training labels assigned to `y_train` to verify its contents.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>lr_model_params = {\\n    #\"class_weight\": \"balanced\",\\n    #\"class_weight\": None,\\n    #\"class_weight\": {1: 1, 0: 1/class_ratio},\\n    #\"random_state\": 0,\\n    #\"Cs\": 5,\\n    #\"penalty\": \"none\",\\n    #\"penalty\": \"elasticnet\",\\n    \"solver\": \"liblinear\",\\n    #\"l1_ratio\": 0.5,\\n    #\"max_iter\": 10000,\\n    #\"cv\": 10\\n}</td>\n",
       "      <td>This code defines a dictionary with parameters for a logistic regression model, suggesting various options for customizing the model configuration, with only the solver parameter currently active.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>text_n = 10\\nfeatures[text_n]</td>\n",
       "      <td>This code accesses the TF-IDF feature vector for the text sample at index 10 in the training data, likely to inspect its feature representation.</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>model_pipeline = Pipeline([\\n    (\"vectorizer\", vectorizer),\\n    (\"model\", model)\\n]\\n)</td>\n",
       "      <td>This code constructs a scikit-learn Pipeline that chains together the TfidfVectorizer and the logistic regression model, creating a streamlined workflow for transforming text data and making predictions.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>#y_test = y_train\\ny_test = test_df[\"target\"]</td>\n",
       "      <td>This code assigns the \"target\" column from the test DataFrame to the variable `y_test`, preparing the test labels for model evaluation.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>pred_df.drop(columns=[\"keyword\", \"location\", \"text\"], inplace=True)</td>\n",
       "      <td>This code removes the \"keyword\", \"location\", and \"text\" columns from the DataFrame, likely to prepare the data for export or further simplified use.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>def wordcount(x):\\n    length = len(str(x).split())\\n    return length\\ndef charcount(x):\\n    s = x.split()\\n    x = ''.join(s)\\n    return len(x)\\n\\ndef hashtag_count(x):\\n    l = len([t for t in x.split() if t.startswith('#')])\\n    return l\\n\\ndef mentions_count(x):\\n    l = len([t for t in x.split() if t.startswith('@')])\\n    return l\\n\\n\\ntrain_df['char_count'] = train_df['text'].apply(lambda x: charcount(x))\\ntrain_df['word_count'] = train_df['text'].apply(lambda x: wordcount(x))\\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda x: hashtag_count(x))\\ntrain_df['mention_count'] = train_df['text'].apply(lambda x: mentions_count(x))\\ntrain_df['length']=train_df['text'].apply(len)\\n\\ntest_df['char_count'] = test_df['text'].apply(lambda x: charcount(x))\\ntest_df['word_count'] = test_df['text'].apply(lambda x: wordcount(x))\\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x: hashtag_count(x))\\ntest_df['mention_count'] = test_df['text'].apply(lambda x: mentions_count(x))\\ntest_df['length']=test_df['text'].apply(len)\\n\\ntrain_df.head(2)</td>\n",
       "      <td>This code defines functions to calculate various textual features such as word count, character count, hashtag count, and mention count, and then applies these functions to the text data in both the training and testing datasets, adding the resulting values as new columns.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td># Taken from - Craig Thomas https://www.kaggle.com/craigmthomas/logistic-regression-lightgbm-fe\\ntrain_df.drop(\\n    [\\n        6449, 7034, 3589, 3591, 3597, 3600, 3603, 3604, 3610, 3613, 3614, 119, 106, 115,\\n        2666, 2679, 1356, 7609, 3382, 1335, 2655, 2674, 1343, 4291, 4303, 1345, 48, 3374,\\n        7600, 164, 5292, 2352, 4308, 4306, 4310, 1332, 1156, 7610, 2441, 2449, 2454, 2477,\\n        2452, 2456, 3390, 7611, 6656, 1360, 5771, 4351, 5073, 4601, 5665, 7135, 5720, 5723,\\n        5734, 1623, 7533, 7537, 7026, 4834, 4631, 3461, 6366, 6373, 6377, 6378, 6392, 2828,\\n        2841, 1725, 3795, 1251, 7607\\n    ], inplace=True\\n)\\n\\ntrain_df.drop(\\n    [\\n        4290, 4299, 4312, 4221, 4239, 4244, 2830, 2831, 2832, 2833, 4597, 4605, 4618, 4232, 4235, 3240,\\n        3243, 3248, 3251, 3261, 3266, 4285, 4305, 4313, 1214, 1365, 6614, 6616, 1197, 1331, 4379, 4381,\\n        4284, 4286, 4292, 4304, 4309, 4318, 610, 624, 630, 634, 3985, 4013, 4019, 1221, 1349, 6091, 6094, \\n        6103, 6123, 5620, 5641\\n    ], inplace=True\\n)</td>\n",
       "      <td>This code removes specific rows from the training DataFrame based on their indices, likely to clean the dataset by removing duplicates or erroneous entries.</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>def preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\\n    \\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\\n    lst_text = text.split()\\n    if lst_stopwords is not None:\\n        lst_text = [word for word in lst_text if word not in \\n                    lst_stopwords]\\n                \\n    ## Stemming (remove -ing, -ly, ...)\\n    if flg_stemm == True:\\n        ps = nltk.stem.porter.PorterStemmer()\\n        lst_text = [ps.stem(word) for word in lst_text]\\n\\n    if flg_lemm == True:\\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\\n        lst_text = [lem.lemmatize(word) for word in lst_text]\\n            \\n                            \\n    ## back to string from list\\n    text = \" \".join(lst_text)\\n    return text</td>\n",
       "      <td>This code defines a function to preprocess text by converting it to lowercase, removing punctuation, filtering out stopwords, and optionally applying stemming or lemmatization.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>lst_stopwords = nltk.corpus.stopwords.words(\"english\")\\n#lst_stopwords\\n</td>\n",
       "      <td>This code loads the list of English stopwords from the NLTK library into the variable `lst_stopwords`.</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>contractions = { \\n\"ain't\": \"am not\",\\n\"aren't\": \"are not\",\\n\"can't\": \"cannot\",\\n\"can't've\": \"cannot have\",\\n\"'cause\": \"because\",\\n\"could've\": \"could have\",\\n\"couldn't\": \"could not\",\\n\"couldn't've\": \"could not have\",\\n\"didn't\": \"did not\",\\n\"doesn't\": \"does not\",\\n\"don't\": \"do not\",\\n\"hadn't\": \"had not\",\\n\"hadn't've\": \"had not have\",\\n\"hasn't\": \"has not\",\\n\"haven't\": \"have not\",\\n\"he'd\": \"he would\",\\n\"he'd've\": \"he would have\",\\n\"he'll\": \"he will\",\\n\"he'll've\": \"he will have\",\\n\"he's\": \"he is\",\\n\"how'd\": \"how did\",\\n\"how'd'y\": \"how do you\",\\n\"how'll\": \"how will\",\\n\"how's\": \"how does\",\\n\"i'd\": \"i would\",\\n\"i'd've\": \"i would have\",\\n\"i'll\": \"i will\",\\n\"i'll've\": \"i will have\",\\n\"i'm\": \"i am\",\\n\"i've\": \"i have\",\\n\"isn't\": \"is not\",\\n\"it'd\": \"it would\",\\n\"it'd've\": \"it would have\",\\n\"it'll\": \"it will\",\\n\"it'll've\": \"it will have\",\\n\"it's\": \"it is\",\\n\"let's\": \"let us\",\\n\"ma'am\": \"madam\",\\n\"mayn't\": \"may not\",\\n\"might've\": \"might have\",\\n\"mightn't\": \"might not\",\\n\"mightn't've\": \"might not have\",\\n\"must've\": \"must have\",\\n\"mustn't\": \"must not\",\\n\"mustn't've\": \"must not have\",\\n\"needn't\": \"need not\",\\n\"needn't've\": \"need not have\",\\n\"o'clock\": \"of the clock\",\\n\"oughtn't\": \"ought not\",\\n\"oughtn't've\": \"ought not have\",\\n\"shan't\": \"shall not\",\\n\"sha'n't\": \"shall not\",\\n\"shan't've\": \"shall not have\",\\n\"she'd\": \"she would\",\\n\"she'd've\": \"she would have\",\\n\"she'll\": \"she will\",\\n\"she'll've\": \"she will have\",\\n\"she's\": \"she is\",\\n\"should've\": \"should have\",\\n\"shouldn't\": \"should not\",\\n\"shouldn't've\": \"should not have\",\\n\"so've\": \"so have\",\\n\"so's\": \"so is\",\\n\"that'd\": \"that would\",\\n\"that'd've\": \"that would have\",\\n\"that's\": \"that is\",\\n\"there'd\": \"there would\",\\n\"there'd've\": \"there would have\",\\n\"there's\": \"there is\",\\n\"they'd\": \"they would\",\\n\"they'd've\": \"they would have\",\\n\"they'll\": \"they will\",\\n\"they'll've\": \"they will have\",\\n\"they're\": \"they are\",\\n\"they've\": \"they have\",\\n\"to've\": \"to have\",\\n\"wasn't\": \"was not\",\\n\" u \": \" you \",\\n\" ur \": \" your \",\\n\" n \": \" and \",\\n\"won't\": \"would not\",\\n'dis': 'this',\\n'bak': 'back',\\n'brng': 'bring'}\\n\\ndef cont_to_exp(x):\\n    if type(x) is str:\\n        for key in contractions:\\n            value = contractions[key]\\n            x = x.replace(key, value)\\n        return x\\n    else:\\n        return x\\n    \\ntrain_df['text_clean'] = train_df['text'].apply(lambda x: cont_to_exp(x))\\ntest_df['text_clean'] = test_df['text'].apply(lambda x: cont_to_exp(x))\\n\\n\\ndef remove_emails(x):\\n     return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\\n\\n\\ndef remove_urls(x):\\n    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&amp;:/~+#-]*[\\w@?^=%&amp;/~+#-])?', '' , x)\\n\\ndef remove_rt(x):\\n    return re.sub(r'\\brt\\b', '', x).strip()\\n\\ndef remove_special_chars(x):\\n    x = re.sub(r'[^\\w ]+', \"\", x)\\n    x = ' '.join(x.split())\\n    return x\\n\\n\\ndef remove_accented_chars(x):\\n    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\\n    return x\\n\\n\\n\\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_emails(x))\\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_urls(x))\\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_rt(x))\\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_special_chars(x))\\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x: remove_accented_chars(x))</td>\n",
       "      <td>This code defines functions to expand contractions, remove emails, URLs, retweet tags, special characters, and accented characters from text, and applies these transformations to clean the text data in both the training and testing datasets.</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: preprocess_text(x, flg_stemm=True, flg_lemm=False, lst_stopwords=lst_stopwords))\\ntrain_df.head()</td>\n",
       "      <td>This code applies the `preprocess_text` function to the cleaned text data in the training DataFrame, with stemming enabled and stopwords removal, to further process the text.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>vec=TfidfVectorizer(max_features = 10000,ngram_range=(1,4))\\nvec.fit(train_df['text_clean'])</td>\n",
       "      <td>This code initializes a `TfidfVectorizer` with a maximum of 10,000 features and n-gram range of 1 to 4, and fits it to the cleaned text data in the training DataFrame to learn the vocabulary and IDF values.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>matrix = vec.transform(train_df['text_clean']).toarray()\\nfeatures = vec.get_feature_names()\\nmatrix_df = pd.DataFrame(data=matrix, columns=features)\\n</td>\n",
       "      <td>This code transforms the cleaned text data in the training DataFrame into a TF-IDF matrix, retrieves the feature names, and creates a new DataFrame from this matrix with the feature names as column headers.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>matrix_df['length']=train_df['length']\\nmatrix_df['char_count']=train_df['char_count']\\nmatrix_df['word_count']=train_df['word_count']\\nmatrix_df['hashtag_count']=train_df['hashtag_count']\\nmatrix_df['mention_count']=train_df['mention_count']\\ny=train_df['target']</td>\n",
       "      <td>This code adds the textual feature columns (length, char_count, word_count, hashtag_count, mention_count) from the original training DataFrame to the TF-IDF matrix DataFrame and assigns the target variable to `y`.</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>test_df[\"text_clean\"]=test_df['text']\\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_emails(x))\\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_urls(x))\\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_rt(x))\\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_special_chars(x))\\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x: remove_accented_chars(x))\\n\\ntest_df[\"text_clean\"] = test_df[\"text\"].apply(lambda x: preprocess_text(x, flg_stemm=True, flg_lemm=False, lst_stopwords=lst_stopwords))\\ntest_df['length']=test_df['text'].apply(len)\\n\\ntest_df.head()\\n\\n#vec=TfidfVectorizer(max_features = 20000,ngram_range=(1,4))\\n#vec.fit(test_df['text_clean'])\\n\\n\\n\\nmatrix = vec.transform(test_df['text_clean']).toarray()\\nfeatures = vec.get_feature_names()\\nmatrix_df = pd.DataFrame(data=matrix, columns=features)\\n\\nmatrix_df['length']=test_df['length']\\nmatrix_df['char_count']=test_df['char_count']\\nmatrix_df['word_count']=test_df['word_count']\\nmatrix_df['hashtag_count']=test_df['hashtag_count']\\nmatrix_df['mention_count']=test_df['mention_count']</td>\n",
       "      <td>This code cleans and preprocesses the text data in the test DataFrame, transforms it into a TF-IDF matrix using the previously fitted vectorizer, and adds textual feature columns to the resulting DataFrame.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>#Credit: https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\\ndef bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)</td>\n",
       "      <td>This code defines a function to encode text data into BERT-compatible input formats (tokens, masks, and segment IDs) using a tokenizer, with a specified maximum sequence length.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td># Encode the text into tokens, masks, and segment flags\\ntrain_input = bert_encode(train_df.text_clean.values, tokenizer, max_len=160)\\ntest_input = bert_encode(test_df.text_clean.values, tokenizer, max_len=160)\\ntrain_labels = train_df.target.values</td>\n",
       "      <td>This code encodes the text data from the cleaned training and test sets into BERT-compatible input formats (tokens, masks, and segment flags) with a maximum sequence length of 160, and extracts the target labels from the training DataFrame.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>data = pd.concat([train_data, submit_data])\\ndata.shape</td>\n",
       "      <td>This code snippet concatenates the training and submission datasets into a single DataFrame and outputs its dimensions.</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>data['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'https?\\S+'), '', x))\\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'[\\//:,.!?@&amp;\\-\\'\\`\\\"\\_\\n\\#]'), ' ', x))\\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'&lt;.*?&gt;'), '', x))\\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(\"[\"\\n                           u\"\\U0001F600-\\U0001F64F\"  \\n                           u\"\\U0001F300-\\U0001F5FF\"  \\n                           u\"\\U0001F680-\\U0001F6FF\"  \\n                           u\"\\U0001F1E0-\\U0001F1FF\"  \\n                           u\"\\U00002702-\\U000027B0\"\\n                           u\"\\U000024C2-\\U0001F251\"\\n                           \"]+\", flags=re.UNICODE), '', x))\\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'\\d'), '', x))\\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'[^\\w]'), ' ', x))\\ndata['text'] = data['text'].str.lower()</td>\n",
       "      <td>This code snippet applies multiple text preprocessing steps to the 'text' column, including removing URLs, punctuation, HTML tags, emojis, digits, special characters, and converting the text to lowercase.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>'''\\ntext_series = data.loc[:,'text']\\nfor i in range(len(text_series)):\\n    content = text_series.iloc[i]\\n    textblob = TextBlob(content)\\n    text_series.iloc[i] = textblob.correct()\\n'''</td>\n",
       "      <td>This commented-out code snippet intends to iterate through the 'text' column, apply spelling correction using TextBlob's `correct()` method, and update the text entries in the DataFrame.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>clean_train = data[0:train_data.shape[0]]\\nclean_submit = data[train_data.shape[0]:-1]\\n\\nX_train, X_test, y_train, y_test = train_test_split(clean_train['text'], clean_train['target'],\\n                                                   test_size = 0.2, random_state = 4)</td>\n",
       "      <td>This code snippet separates the combined data back into clean training and submission datasets and then splits the clean training set into training and testing subsets for model training and evaluation.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>def tfidf(words):\\n    tfidf_vectorizer = TfidfVectorizer()\\n    data_feature = tfidf_vectorizer.fit_transform(words)\\n    return data_feature, tfidf_vectorizer\\n\\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train.tolist())\\nX_test_tfidf = tfidf_vectorizer.transform(X_test.tolist())</td>\n",
       "      <td>This code snippet defines a function to convert text data into TF-IDF features, applies it to the training text, and uses the resulting TF-IDF vectorizer to transform the test text.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>stop_words = stopwords.words('english')\\nfor word in ['us','no','yet']:\\n    stop_words.append(word)\\n\\ndata_list = []\\ntext_series = data['text']\\nfor i in range(len(text_series)):\\n    content = text_series.iloc[i]\\n    cutwords = [word for word in content.split(' ') if word not in  stop_words if len(word) != 0]\\n    data_list.append(cutwords)</td>\n",
       "      <td>This code snippet extends the list of stopwords, iterates through the 'text' column, removes stopwords and empty tokens from each text, and collects the cleaned words into a list.</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>def get_textVector(data_list, word2vec, textsVectors_list):\\n    for i in range(len(data_list)):\\n        words_perText = data_list[i]\\n        if len(words_perText) &lt; 1:\\n            words_vector = [np.zeros(300)]\\n        else:\\n            words_vector = [word2vec.wv[k]  if k in word2vec_model else  np.zeros(300) for k in words_perText]\\n        text_vector = np.array(words_vector).mean(axis=0)\\n        textsVectors_list.append(text_vector)\\n    return textsVectors_list</td>\n",
       "      <td>This code snippet defines a function to convert text data into average word vectors using a provided Word2Vec model, filling with zero vectors for out-of-vocabulary words, and appending the resulting text vectors to a list.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>textsVectors_list = []\\nget_textVector(data_list, word2vec_model, textsVectors_list)\\nX = np.array(textsVectors_list)</td>\n",
       "      <td>This code snippet calls the `get_textVector` function to generate average word vectors for each text in `data_list`, appends the vectors to `textsVectors_list`, and converts the list to a NumPy array.</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>word2vec_X = X[0:train_data.shape[0]]\\ny = data['target'][0:train_data.shape[0]]\\nword2vec_submit = X[train_data.shape[0]:-1]\\n\\nX_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(word2vec_X, y,\\n                                                   test_size = 0.2, random_state = 4)</td>\n",
       "      <td>This code snippet splits the array `X` back into a training set, a target set (`y`), and a submission set (`word2vec_submit`), then further splits the training set into training and testing subsets for model training and evaluation.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>tokenizer = Tokenizer()\\ntokenizer.fit_on_texts(data_list)\\nsequences = tokenizer.texts_to_sequences(data_list)\\nword_index = tokenizer.word_index\\ncnn_data = pad_sequences(sequences, maxlen = max_sequence_length)\\ncnn_label = to_categorical(np.asarray(train_data['target']))\\nprint('len of word_index:', len(word_index))\\nprint('shape of data tensor:', cnn_data.shape)\\nprint('shape of label tensoe:', cnn_label.shape)</td>\n",
       "      <td>This code snippet tokenizes the texts in `data_list`, converts them to sequences, pads the sequences to a specified maximum length, converts the target labels to categorical format, and prints the size of the word index and the shapes of the resulting data tensors.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>trainCNN_data = cnn_data[0:train_data.shape[0]]\\nX_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(trainCNN_data, cnn_label,\\n                                                   test_size = 0.2, random_state = 4)\\nX_cnn, X_val_cnn, y_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train_cnn,\\n                                                   test_size = 0.2, random_state = 4)</td>\n",
       "      <td>This code snippet splits the CNN-ready data and labels into training and test subsets, and further splits the training subset into training and validation sets, for model training and evaluation.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\\nfor word, i in word_index.items(): \\n    if word in word2vec_model:\\n        embedding_matrix[i] = np.asarray(word2vec_model.wv[word])</td>\n",
       "      <td>This code snippet initializes an embedding matrix and populates it with word vectors from the word2vec model for each word in the tokenizer's word index, leaving zero vectors for words not present in the word2vec model.</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>embedding_layer = Embedding(len(word_index)+1,\\n                           embedding_dim,\\n                           weights = [embedding_matrix],\\n                           input_length = max_sequence_length,\\n                           trainable = False)</td>\n",
       "      <td>This code snippet creates an embedding layer using the pre-trained word vectors stored in the embedding matrix, setting the layer to be non-trainable and specifying the input length and embedding dimensions.</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_df = train_df.drop(columns=[\"id\", \"keywo...</td>\n",
       "      <td>This code snippet drops the \"id\", \"keyword\", a...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def remove_URL(text):\\n    url = re.compile(r'...</td>\n",
       "      <td>This code snippet defines a function to remove...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def remove_html(text):\\n    html=re.compile(r'...</td>\n",
       "      <td>This code snippet defines a function to remove...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def remove_emoji(text):\\n    emoji_pattern = r...</td>\n",
       "      <td>This code snippet defines a function to remove...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_tensor = tokenizer(list(train_df[\"text\"]...</td>\n",
       "      <td>This code snippet tokenizes the text data from...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>class TweetDataset:\\n    def __init__(self, te...</td>\n",
       "      <td>This code snippet defines a custom dataset cla...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_ids, valid_ids = RandomSplitter()(train_...</td>\n",
       "      <td>This code snippet splits the training DataFram...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_tensor = tokenizer(list(test_df[\"text\"]),...</td>\n",
       "      <td>This code snippet tokenizes the text data from...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>class TestDS:\\n    def __init__(self, tensors)...</td>\n",
       "      <td>This code snippet defines a custom dataset cla...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stop_words=nltk.corpus.stopwords.words('englis...</td>\n",
       "      <td>This code processes the text data in the 'text...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>from sklearn.feature_extraction.text import Co...</td>\n",
       "      <td>This code uses the `CountVectorizer` to conver...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from sklearn.feature_extraction.text import Tf...</td>\n",
       "      <td>This code utilizes the `TfidfVectorizer` to co...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>from sklearn.model_selection import train_test...</td>\n",
       "      <td>This code snippet constructs feature data by c...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>my_submission_preds = pipe.predict(test['text'...</td>\n",
       "      <td>This code snippet generates predictions for th...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nlp = spacy.load('en')</td>\n",
       "      <td>This code snippet loads the English language m...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td># remove stopwords,punct\\n# remove duplicate t...</td>\n",
       "      <td>This code snippet processes the training data ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tests = []\\nfor doc in nlp.pipe(test_raw.text)...</td>\n",
       "      <td>This code snippet processes the testing data b...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tf_idf = TfidfVectorizer(max_features=10000).f...</td>\n",
       "      <td>This code snippet creates a TF-IDF Vectorizer ...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>X_train, X_test, y_train, y_test = train_test_...</td>\n",
       "      <td>This code snippet splits the training DataFram...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>def remove_URL(text):\\n    url = re.compile(r\"...</td>\n",
       "      <td>This code defines two functions to preprocess ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>#regex pattern to remove links\\npattern = re.c...</td>\n",
       "      <td>This code snippet demonstrates how URLs in the...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>#for test:\\nfor t in test.text:\\n    matches =...</td>\n",
       "      <td>This code snippet demonstrates how URLs in the...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>#preprocess data frames:\\n#train\\ntrain[\"text\"...</td>\n",
       "      <td>This code snippet preprocesses the text data i...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td># remove stopwords\\nnltk.download('stopwords')...</td>\n",
       "      <td>This code snippet downloads the list of Englis...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>#train\\ntrain[\"text\"] = train.text.map(remove_...</td>\n",
       "      <td>This code snippet preprocesses the text data i...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td># Count unique words\\ndef counter_word(text_co...</td>\n",
       "      <td>This code snippet defines a function to count ...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td># Split dataset into training and validation s...</td>\n",
       "      <td>This code splits the text and target columns o...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>#train/val\\ntrain_sentences = train_sentences....</td>\n",
       "      <td>This code converts the training and validation...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>#test\\ntest_sentences = test.text.to_numpy()</td>\n",
       "      <td>This code converts the text column of the test...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td># Tokenize\\n# vectorize a text corpus by turni...</td>\n",
       "      <td>This code snippet initializes a tokenizer conf...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td># Now each word has unique index\\nword_index =...</td>\n",
       "      <td>This code creates a dictionary, `word_index`, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>#apply on train, validation, and test sentence...</td>\n",
       "      <td>This code snippet converts the training, valid...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td># Pad the sequences to have the same length\\nm...</td>\n",
       "      <td>This code snippet pads the sequences of intege...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td># flip (key, value)\\nreverse_word_index = dict...</td>\n",
       "      <td>This code snippet creates a reverse mapping di...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>#decoding\\ndef decode(sequence):\\n    return \"...</td>\n",
       "      <td>This code defines a function to decode a seque...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td># We'll use these weights later on to make up ...</td>\n",
       "      <td>This code calculates and stores the class weig...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td># Commented out the graceful handling of dupli...</td>\n",
       "      <td>This code removes duplicate entries from the t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>class TweetPreProcessor:\\n    \"\"\"\\n    This cl...</td>\n",
       "      <td>This code defines a `TweetPreProcessor` class ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>train_df[\"text\"] = train_df[\"text\"].apply(twee...</td>\n",
       "      <td>This code applies the `TweetPreProcessor` to p...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td># Fill NA\\ntrain_df[\"keyword\"].fillna(\"\", inpl...</td>\n",
       "      <td>This code fills missing values in the 'keyword...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>x_train, x_val, y_train, y_val = sklearn.model...</td>\n",
       "      <td>This code splits the training DataFrame into t...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>def tokenize_encode(tweets, max_length=None):\\...</td>\n",
       "      <td>This code defines a function to tokenize and e...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>train_dataset = tf.data.Dataset.from_tensor_sl...</td>\n",
       "      <td>This code creates TensorFlow datasets from the...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tfidf_vectorizer = sklearn.feature_extraction....</td>\n",
       "      <td>This code initializes a TF-IDF vectorizer with...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>test_tweets_encoded = tokenize_encode(test_df[...</td>\n",
       "      <td>This code tokenizes and encodes the text and k...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>def filter_text(df):\\n    df['text']=df['text'...</td>\n",
       "      <td>The code defines and applies a function to rem...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>sw=['the', 'a', 'an', 'in', 'on', 'with', 'by'...</td>\n",
       "      <td>The code defines a list of common stopwords to...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>v = CountVectorizer(stop_words=sw)\\ntrain_v = ...</td>\n",
       "      <td>The code initializes a CountVectorizer with cu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td># plot prop of missing for each feature\\nsns.s...</td>\n",
       "      <td>This code visualizes the proportion of missing...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>wordnet_lemmatizer = WordNetLemmatizer()\\n\\nde...</td>\n",
       "      <td>This code snippet defines a text preprocessing...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>x_train, x_test, y_train, y_test = train_test_...</td>\n",
       "      <td>This code splits the training data into traini...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tfidf_vectorizer = TfidfVectorizer(tokenizer=w...</td>\n",
       "      <td>This code creates a preprocessing pipeline tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>corpus = []\\nfor i in range(0, len(train_len))...</td>\n",
       "      <td>The code preprocesses the text data in the tra...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>from sklearn.feature_extraction.text import Co...</td>\n",
       "      <td>The code transforms the preprocessed text data...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>from sklearn.model_selection import train_test...</td>\n",
       "      <td>The code splits the dataset into training and ...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>testcorpus = []\\nfor i in range(0, len(test_le...</td>\n",
       "      <td>The code preprocesses the 'text' column of the...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>keyword_dist = train_df.groupby(\"keyword\")['ta...</td>\n",
       "      <td>This code snippet calculates the distribution ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>#word count\\ntrain_df['word_count'] = train_df...</td>\n",
       "      <td>This code snippet creates new columns in both ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td># Refrenced from Gunes Evitan and Vitalii Moki...</td>\n",
       "      <td>This code snippet defines a function for clean...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>def encode(texts, tokenizer, max_len=512):\\n  ...</td>\n",
       "      <td>This code snippet defines a function that toke...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>train_input = encode(train_df.text_cleaned.val...</td>\n",
       "      <td>This code snippet preprocesses the cleaned tex...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>ids_with_target_error = [328,443,513,2619,3640...</td>\n",
       "      <td>This code corrects errors in the 'target' colu...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>set1 = set(train_df[train_df.duplicated(subset...</td>\n",
       "      <td>This code creates a set of 'id' values from th...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>set2 = (train_df[train_df.duplicated(subset=['...</td>\n",
       "      <td>This code creates an array of 'id' values from...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td># setting the argument keep=False, drops all t...</td>\n",
       "      <td>This code removes all rows with duplicated tex...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>train_df['location'] = train_df['location'].st...</td>\n",
       "      <td>This code normalizes the 'location' column in ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>loc_dict = {'united states':'usa',\\n          ...</td>\n",
       "      <td>This code defines a dictionary for standardizi...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>train_df['location'].replace(loc_dict, inplace...</td>\n",
       "      <td>This code uses the previously defined dictiona...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>abbreviations = {\\n    \"$\" : \" dollar \",\\n    ...</td>\n",
       "      <td>This code defines a dictionary to map abbrevia...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>def convert_abb(x):\\n    word_list = x.split()...</td>\n",
       "      <td>This code defines a function `convert_abb` tha...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>train_df['text'] = train_df.text.apply(convert...</td>\n",
       "      <td>This code applies the `convert_abb` function t...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>train_df['clean_text'] = train_df.text.apply(l...</td>\n",
       "      <td>This code creates a 'clean_text' column in bot...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>train_df['clean_text'] = train_df.clean_text.a...</td>\n",
       "      <td>This code further cleans the 'clean_text' colu...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>non_alpha = string.punctuation + '0123456789'</td>\n",
       "      <td>This code creates a string `non_alpha` that co...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>train_df['clean_text'] = train_df.clean_text.a...</td>\n",
       "      <td>This code removes all punctuation and digits f...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>train_df['token_text'] = train_df.clean_text.s...</td>\n",
       "      <td>This code tokenizes the lowercase version of t...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>stopwords = nltk.corpus.stopwords.words(\"engli...</td>\n",
       "      <td>This code removes standard English stopwords, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td># since we have a dataset containing text from...</td>\n",
       "      <td>This code applies the Porter Stemming techniqu...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>train_df.reset_index(inplace=True, drop=True)</td>\n",
       "      <td>This code resets the index of the training dat...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>def dummy(doc):\\n    return doc\\ntfidf_vect = ...</td>\n",
       "      <td>This code defines a `dummy` function for use i...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>counts_df_train = pd.DataFrame(matrix_train.to...</td>\n",
       "      <td>This code converts the TF-IDF feature matrices...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>train_df['length'] = train_df.text.apply(lambd...</td>\n",
       "      <td>This code calculates the length of each tweet,...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>train_df['punct_perc'] = train_df.text.apply(l...</td>\n",
       "      <td>This code calculates the percentage of punctua...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>train_df['word_count'] = train_df.token_text.a...</td>\n",
       "      <td>This code calculates the word count of the tok...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>train_df['length_int'] =pd.cut(train_df.length...</td>\n",
       "      <td>This code creates a new column called 'length_...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>train_df['length'] = train_df['length']**2.3\\n...</td>\n",
       "      <td>This code transforms the 'length' column in bo...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>train_df['punct_perc'] = train_df['punct_perc'...</td>\n",
       "      <td>This code applies the cube root transformation...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td># assign an emotion to each tweet\\ntrain_df['e...</td>\n",
       "      <td>This code assigns an emotion score to each twe...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td># exploding the dictionary into 4 different co...</td>\n",
       "      <td>This code expands the 'emotion' dictionary col...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>total_emotions = train_df[['Happy', 'Angry', '...</td>\n",
       "      <td>This code calculates the sum of each emotion t...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>mean_emotions = train_df[['Happy', 'Angry', 'S...</td>\n",
       "      <td>This code calculates the mean value of each em...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>train_df['sentiment'] = train_df.text.astype(s...</td>\n",
       "      <td>This code applies the `SentimentIntensityAnaly...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>train_df = pd.concat([train_df, pd.DataFrame(t...</td>\n",
       "      <td>This code expands the 'sentiment' dictionary c...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>mean_sentiment = train_df[['neg', 'neu', 'pos'...</td>\n",
       "      <td>This code calculates the mean and total sentim...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>#full_train_df = pd.concat([train_df.drop(['lo...</td>\n",
       "      <td>This code creates a new DataFrame `full_train_...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>#full_test_df = pd.concat([test_df.drop(['loca...</td>\n",
       "      <td>This code creates a new DataFrame `full_test_d...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td># deleting unnecessary dataframes to save memo...</td>\n",
       "      <td>This code deletes the intermediate dataframes ...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>count_vectorizer = feature_extraction.text.Cou...</td>\n",
       "      <td>This code snippet initializes a CountVectorize...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>train_vectors = count_vectorizer.fit_transform...</td>\n",
       "      <td>This code snippet transforms the \"text\" column...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>def bert_encode(texts, tokenizer, max_len=512)...</td>\n",
       "      <td>This code defines a function to preprocess and...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>#vocab file from pre-trained BERT for tokeniza...</td>\n",
       "      <td>This code creates a BERT tokenizer using the v...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>stop_words=nltk.corpus.stopwords.words('englis...</td>\n",
       "      <td>This code snippet preprocesses the text data i...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>df = df.drop_duplicates().reset_index(drop = T...</td>\n",
       "      <td>This code removes duplicate rows from the Data...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>#Cleaning the Data</td>\n",
       "      <td>This comment indicates the beginning of a code...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>import re\\n#Conver lowercase remove punctuatio...</td>\n",
       "      <td>This code converts a text sample from the Data...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>#remove stopwords\\nimport nltk\\nlst_stopwords ...</td>\n",
       "      <td>This code removes common English stopwords fro...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>#stemming\\nps = nltk.stem.porter.PorterStemmer...</td>\n",
       "      <td>This code applies stemming to the filtered wor...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>#Lemmentization\\nlem = nltk.stem.wordnet.WordN...</td>\n",
       "      <td>This code applies lemmatization to the filtere...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>#to apply all the technique to all the records...</td>\n",
       "      <td>This code defines a function `utils_preprocess...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>#apply dataset\\ndf['clean_text'] = df['text']....</td>\n",
       "      <td>This code applies the text preprocessing funct...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>#Target Encoding\\n</td>\n",
       "      <td>This comment indicates the beginning of a code...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>import category_encoders as ce\\n\\n# Target enc...</td>\n",
       "      <td>This code uses the `category_encoders` library...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>from sklearn.feature_extraction.text import Tf...</td>\n",
       "      <td>This code utilizes the `TfidfVectorizer` from ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>df = df.join(X_train_text, rsuffix='_text')\\nt...</td>\n",
       "      <td>This code appends the TF-IDF feature vectors, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td># Get the Bert tokenizer\\ntokenizer = BertToke...</td>\n",
       "      <td>This code snippet initializes a BERT tokenizer...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>def prepare_sequence(text):\\n    \"\"\"\\n    Toke...</td>\n",
       "      <td>This code snippet defines a function `prepare_...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td># Prepare a test sentence\\ntest_sentence = 'Is...</td>\n",
       "      <td>This code snippet prepares a test sentence by ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>def map_example_to_dict(input_ids, attention_m...</td>\n",
       "      <td>This code snippet defines two functions: `map_...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>X = train_data[\"text\"]\\ny = train_data[\"target\"]</td>\n",
       "      <td>This code snippet extracts the text data and t...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td># Split the training dataset for training and ...</td>\n",
       "      <td>This code snippet splits the extracted text da...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>train_dataset = list(zip(X_train, y_train))\\nv...</td>\n",
       "      <td>This code snippet combines the training and va...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td># Prepare sequences of text and build TF train...</td>\n",
       "      <td>This code snippet encodes the training and val...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>def encode_test_examples(texts):\\n    \"\"\"\\n   ...</td>\n",
       "      <td>This code snippet defines two functions: `enco...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>X_test = test_data[\"text\"]\\ntest_dataset = lis...</td>\n",
       "      <td>This code snippet extracts the text data from ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td># Only apply 'keyword' columns in full data, b...</td>\n",
       "      <td>This code snippet preprocesses the 'keyword' c...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td># Adding cleaned data into df_train/test\\ndf_t...</td>\n",
       "      <td>This snippet adds the cleaned 'keyword' column...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>def extract_keywords(text):\\n    potential_key...</td>\n",
       "      <td>This code snippet defines two functions: `extr...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>df_train.keyword = pd.DataFrame(list(map(keywo...</td>\n",
       "      <td>This snippet applies the `keyword_filler` func...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td># Spilt data\\nX_train, X_val, y_train, y_val =...</td>\n",
       "      <td>This snippet splits the training dataset into ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>train_ds = tf.data.Dataset.from_tensor_slices(...</td>\n",
       "      <td>This snippet converts the training, validation...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>AUTOTUNE = tf.data.experimental.AUTOTUNE\\n\\nBU...</td>\n",
       "      <td>This snippet defines a function to configure T...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>a3 = configure_dataset(train_ds, shuffle=True)...</td>\n",
       "      <td>This snippet configures the `train_ds` TensorF...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td># Configure the datasets\\ntrain_ds = configure...</td>\n",
       "      <td>This code snippet configures the training, val...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td># Free memory\\ndel X_train, X_val, y_train, y_...</td>\n",
       "      <td>This snippet deletes the intermediate variable...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>train_df = train_df.drop(['id', 'keyword', 'lo...</td>\n",
       "      <td>This code removes the 'id', 'keyword', and 'lo...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>#remove duplicated rows\\ntrain_df.drop_duplica...</td>\n",
       "      <td>This code removes duplicate rows from the trai...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Real_Disaster_text = ' '.join(Real_Disaster_df...</td>\n",
       "      <td>This code concatenates all the text of disaste...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Not_Real_Disaster_text = ' '.join(Not_Real_Dis...</td>\n",
       "      <td>This code concatenates all the text of non-dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td># take text and preprocess 'remove stopwords [...</td>\n",
       "      <td>This code defines a function to preprocess tex...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>train_df['text'] = train_df['text'].map(clean_...</td>\n",
       "      <td>This code applies the `clean_text` function to...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>from sklearn.utils import shuffle\\ntrain_df_sh...</td>\n",
       "      <td>This code shuffles the rows of the training Da...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>X = train_df_shuffled['text']\\ny = train_df_sh...</td>\n",
       "      <td>This code splits the shuffled DataFrame into t...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>test_df = test_df.drop(['id', 'keyword', 'loca...</td>\n",
       "      <td>This code removes the 'id', 'keyword', and 'lo...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>test_df['text'] = test_df['text'].map(clean_te...</td>\n",
       "      <td>This code applies the `clean_text` function to...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>sample_submission[\"target\"] = y_pred</td>\n",
       "      <td>This code updates the 'target' column in the s...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>max_seq_len = 160\\n\\ntrain_input = bert_encode...</td>\n",
       "      <td>This code encodes the text data from the train...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td># DROP DUPLICATE SAMPLES WITH CONFLICTING LABE...</td>\n",
       "      <td>This code snippet drops duplicate samples with...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td># TOKENIZE ALL THE SENTENCES AND MAP THE TOKEN...</td>\n",
       "      <td>This code snippet tokenizes all sentences in t...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td># SPLIT TRAIN DATA INTO TRAIN AND TEST SET\\n# ...</td>\n",
       "      <td>This code snippet splits the dataset into trai...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td># CREATE DATA ITERATOR TO SAVE MEMORY\\n\\nfrom ...</td>\n",
       "      <td>This code snippet creates DataLoader objects f...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td># PREPARE PREDICTIONS FOR SUBMISSION\\n\\n# Comb...</td>\n",
       "      <td>This code snippet combines the prediction resu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td># clean training text\\nl=len(df)\\ndisplay(l)\\n...</td>\n",
       "      <td>This code snippet cleans the text data in the ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td># combine clean text with training data\\ndf_cl...</td>\n",
       "      <td>This code snippet combines the cleaned text da...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>l=len(df1)\\ndisplay(l)\\npredlist=[]\\n#l=1\\nfor...</td>\n",
       "      <td>This code snippet cleans the text data in the ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>df_pred=pd.DataFrame(predlist)\\ndf_pred.column...</td>\n",
       "      <td>This code snippet combines the prediction resu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>df2.loc[df2['target']=='target','target']=1\\nd...</td>\n",
       "      <td>This code snippet converts the textual predict...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>min_freq = 5\\nabove_threshold = train.location...</td>\n",
       "      <td>The code snippet filters the locations in the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>train.drop(['location', 'keyword'], axis=1, in...</td>\n",
       "      <td>The code snippet removes the 'location' and 'k...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>nlp = English()\\ntokenizer = nlp.tokenizer\\nto...</td>\n",
       "      <td>The code snippet initializes a tokenizer using...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>text = \"Don't split #hashtags!\"\\nprint('Before...</td>\n",
       "      <td>The code snippet demonstrates how to customize...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>text = 'This is  a test\\n , ok?'\\nprint('All t...</td>\n",
       "      <td>The code snippet tokenizes a sample text and d...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>train['tokens'] = train['text'].apply(lambda r...</td>\n",
       "      <td>The code snippet tokenizes the 'text' column i...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>from sklearn.feature_extraction.text import Co...</td>\n",
       "      <td>The code snippet sets up a CountVectorizer to ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>x = train_bow\\ny = train['target']</td>\n",
       "      <td>The code snippet assigns the bag-of-words repr...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td># min and max document frequency (ratio of doc...</td>\n",
       "      <td>The code snippet reconfigures the CountVectori...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>oov_count = Counter()\\nall_tokens = []\\n\\nfor ...</td>\n",
       "      <td>The code snippet processes the tokenized text ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>test_tokens = []\\nfor row in test.tokens:\\n   ...</td>\n",
       "      <td>The code snippet processes the test dataset's ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>words_to_add = [w for w in oov_count if oov_co...</td>\n",
       "      <td>The code snippet adds frequently occurring out...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>def convert_to_indices(all_tokens):\\n    word_...</td>\n",
       "      <td>The code snippet defines a function to convert...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>def collate_as_list(samples):\\n    \"\"\"Function...</td>\n",
       "      <td>The code snippet defines a custom dataset clas...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>validation_size = int(0.1 * len(train))\\nvalid...</td>\n",
       "      <td>The code snippet splits the training data into...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td># create tensors of variable sizes\\n# note tha...</td>\n",
       "      <td>The code snippet tokenizes the text data in th...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>train_data = TensorDataset(x_train, x_train_ma...</td>\n",
       "      <td>The code snippet creates TensorDataset objects...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>def lowercase_text(text):\\n    return text.low...</td>\n",
       "      <td>This code snippet defines a function to conver...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>import re\\nimport string\\ndef remove_noise(tex...</td>\n",
       "      <td>This code snippet defines a function to remove...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td># Tokenizing the training and the test set\\nim...</td>\n",
       "      <td>This code snippet imports necessary NLTK modul...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td># Removing stopwords belonging to english lang...</td>\n",
       "      <td>This code snippet defines a function to remove...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td># After preprocessing, the text format\\ndef co...</td>\n",
       "      <td>This code snippet defines a function to combin...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td># Stemming\\nfrom nltk.stem.snowball import Sno...</td>\n",
       "      <td>This code snippet imports the SnowballStemmer ...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>from sklearn.feature_extraction.text import Co...</td>\n",
       "      <td>This code snippet imports CountVectorizer and ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>def bert_encode(texts, tokenizer, max_len):\\n ...</td>\n",
       "      <td>This code snippet defines a function 'bert_enc...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>train = train.fillna(' ')\\ntest = test.fillna(...</td>\n",
       "      <td>This code snippet fills any missing values in ...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>import tensorflow_hub as hub\\nimport tokenizat...</td>\n",
       "      <td>This code snippet loads a pre-trained BERT mod...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>from bs4 import BeautifulSoup # Text Cleaning\\...</td>\n",
       "      <td>This code defines a function to clean and prep...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>test_data['text'] = test_data['text'].apply(cl...</td>\n",
       "      <td>This code applies the previously defined `clea...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>from keras.preprocessing.text import Tokenizer...</td>\n",
       "      <td>This code initializes a Keras Tokenizer with a...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td># Representing texts as one hot encoded sequen...</td>\n",
       "      <td>This code converts the preprocessed text data ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>from sklearn.model_selection import train_test...</td>\n",
       "      <td>This code splits the one-hot encoded training ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>X_train_wc = tokenizer.texts_to_matrix(train_d...</td>\n",
       "      <td>This code converts the preprocessed text data ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>X_train_wc, X_val_wc, y_train, y_val = train_t...</td>\n",
       "      <td>This code splits the count-based encoded train...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>X_train_freq = tokenizer.texts_to_matrix(train...</td>\n",
       "      <td>This code converts the preprocessed text data ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>X_train_freq, X_val_freq, y_train, y_val = tra...</td>\n",
       "      <td>This code splits the frequency-based encoded t...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>from sklearn.feature_extraction.text import Tf...</td>\n",
       "      <td>This code initializes a TfidfVectorizer to con...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>X_train_tfidf, X_val_tfidf, y_train, y_val = t...</td>\n",
       "      <td>This code splits the TF-IDF encoded training d...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td># Sequences creation, truncation and padding\\n...</td>\n",
       "      <td>This code initializes a new Keras Tokenizer, f...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>X_train_seq, X_val_seq, y_train, y_val = train...</td>\n",
       "      <td>This code splits the tokenized and padded trai...</td>\n",
       "      <td>4</td>\n",
       "      <td>Machine Learning Data Preparation and Conversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td># Applying GloVE representations on our corpus...</td>\n",
       "      <td>This code creates an embedding matrix where ea...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td># Since classes are imbalanced, we need to res...</td>\n",
       "      <td>This code splits the DataFrame into two separa...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>df_class_0</td>\n",
       "      <td>This code outputs the DataFrame containing all...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>df_class_1</td>\n",
       "      <td>This code outputs the DataFrame containing all...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td># Second resample - try both under- and over-s...</td>\n",
       "      <td>This code performs both undersampling of the m...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>train_df, test_df = train_test_split(df, train...</td>\n",
       "      <td>This code splits the DataFrame into training a...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>#eng_words = words.words(\"en\")</td>\n",
       "      <td>This code creates a list of English words usin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>#print(\"wort\" in eng_words)</td>\n",
       "      <td>This code checks if the string \"wort\" is prese...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>snowball = SnowballStemmer(language=\"english\")</td>\n",
       "      <td>This code initializes an instance of the Snowb...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>def tokenize_sentence(sentence: str, remove_st...</td>\n",
       "      <td>This code defines a function that tokenizes a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>tokenize_sentence(\"the sentence and asdf fy kr...</td>\n",
       "      <td>This code calls the `tokenize_sentence` functi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>vectorizer_params = {\\n    #\"max_features\": 50...</td>\n",
       "      <td>This code snippet defines a dictionary with co...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>vectorizer = TfidfVectorizer(**vectorizer_params)</td>\n",
       "      <td>This code initializes a TfidfVectorizer instan...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>vectorizer</td>\n",
       "      <td>This code outputs the configuration of the ini...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>features = vectorizer.fit_transform(train_df[\"...</td>\n",
       "      <td>This code fits the TfidfVectorizer to the text...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>feature_names = vectorizer.get_feature_names()</td>\n",
       "      <td>This code retrieves the list of feature names ...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>X_train = train_df[\"text\"]</td>\n",
       "      <td>This code assigns the \"text\" column from the t...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>X_train</td>\n",
       "      <td>This code outputs the training text data assig...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>y_train = train_df[\"target\"]</td>\n",
       "      <td>This code assigns the \"target\" column from the...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>y_train</td>\n",
       "      <td>This code outputs the training labels assigned...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>lr_model_params = {\\n    #\"class_weight\": \"bal...</td>\n",
       "      <td>This code defines a dictionary with parameters...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>text_n = 10\\nfeatures[text_n]</td>\n",
       "      <td>This code accesses the TF-IDF feature vector f...</td>\n",
       "      <td>0</td>\n",
       "      <td>Text Data Processing and Preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>model_pipeline = Pipeline([\\n    (\"vectorizer\"...</td>\n",
       "      <td>This code constructs a scikit-learn Pipeline t...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>#y_test = y_train\\ny_test = test_df[\"target\"]</td>\n",
       "      <td>This code assigns the \"target\" column from the...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>pred_df.drop(columns=[\"keyword\", \"location\", \"...</td>\n",
       "      <td>This code removes the \"keyword\", \"location\", a...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>def wordcount(x):\\n    length = len(str(x).spl...</td>\n",
       "      <td>This code defines functions to calculate vario...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td># Taken from - Craig Thomas https://www.kaggle...</td>\n",
       "      <td>This code removes specific rows from the train...</td>\n",
       "      <td>10</td>\n",
       "      <td>Text Data Preprocessing and Analysis Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>def preprocess_text(text, flg_stemm=False, flg...</td>\n",
       "      <td>This code defines a function to preprocess tex...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>lst_stopwords = nltk.corpus.stopwords.words(\"e...</td>\n",
       "      <td>This code loads the list of English stopwords ...</td>\n",
       "      <td>9</td>\n",
       "      <td>DataFrame Preprocessing and Text Vectorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>contractions = { \\n\"ain't\": \"am not\",\\n\"aren't...</td>\n",
       "      <td>This code defines functions to expand contract...</td>\n",
       "      <td>1</td>\n",
       "      <td>Text Data Cleaning and Preprocessing Functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>train_df[\"text_clean\"] = train_df[\"text_clean\"...</td>\n",
       "      <td>This code applies the `preprocess_text` functi...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>vec=TfidfVectorizer(max_features = 10000,ngram...</td>\n",
       "      <td>This code initializes a `TfidfVectorizer` with...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>matrix = vec.transform(train_df['text_clean'])...</td>\n",
       "      <td>This code transforms the cleaned text data in ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>matrix_df['length']=train_df['length']\\nmatrix...</td>\n",
       "      <td>This code adds the textual feature columns (le...</td>\n",
       "      <td>5</td>\n",
       "      <td>Comprehensive Tweet Preprocessing and Feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>test_df[\"text_clean\"]=test_df['text']\\ntest_df...</td>\n",
       "      <td>This code cleans and preprocesses the text dat...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>#Credit: https://www.kaggle.com/xhlulu/disaste...</td>\n",
       "      <td>This code defines a function to encode text da...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td># Encode the text into tokens, masks, and segm...</td>\n",
       "      <td>This code encodes the text data from the clean...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>data = pd.concat([train_data, submit_data])\\nd...</td>\n",
       "      <td>This code snippet concatenates the training an...</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Preprocessing and Feature Engineering Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>data['text'] = data['text'].apply(lambda x: re...</td>\n",
       "      <td>This code snippet applies multiple text prepro...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>'''\\ntext_series = data.loc[:,'text']\\nfor i i...</td>\n",
       "      <td>This commented-out code snippet intends to ite...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>clean_train = data[0:train_data.shape[0]]\\ncle...</td>\n",
       "      <td>This code snippet separates the combined data ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>def tfidf(words):\\n    tfidf_vectorizer = Tfid...</td>\n",
       "      <td>This code snippet defines a function to conver...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>stop_words = stopwords.words('english')\\nfor w...</td>\n",
       "      <td>This code snippet extends the list of stopword...</td>\n",
       "      <td>6</td>\n",
       "      <td>Text Data Preprocessing and Cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>def get_textVector(data_list, word2vec, textsV...</td>\n",
       "      <td>This code snippet defines a function to conver...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>textsVectors_list = []\\nget_textVector(data_li...</td>\n",
       "      <td>This code snippet calls the `get_textVector` f...</td>\n",
       "      <td>7</td>\n",
       "      <td>Text Preprocessing and Data Splitting Techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>word2vec_X = X[0:train_data.shape[0]]\\ny = dat...</td>\n",
       "      <td>This code snippet splits the array `X` back in...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>tokenizer = Tokenizer()\\ntokenizer.fit_on_text...</td>\n",
       "      <td>This code snippet tokenizes the texts in `data...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>trainCNN_data = cnn_data[0:train_data.shape[0]...</td>\n",
       "      <td>This code snippet splits the CNN-ready data an...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>embedding_matrix = np.zeros((len(word_index) +...</td>\n",
       "      <td>This code snippet initializes an embedding mat...</td>\n",
       "      <td>8</td>\n",
       "      <td>Text Data Preprocessing and Vectorization Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>embedding_layer = Embedding(len(word_index)+1,...</td>\n",
       "      <td>This code snippet creates an embedding layer u...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Preprocessing and Tokenization for NLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Code  \\\n",
       "0    train_df = train_df.drop(columns=[\"id\", \"keywo...   \n",
       "1    def remove_URL(text):\\n    url = re.compile(r'...   \n",
       "2    def remove_html(text):\\n    html=re.compile(r'...   \n",
       "3    def remove_emoji(text):\\n    emoji_pattern = r...   \n",
       "4    train_tensor = tokenizer(list(train_df[\"text\"]...   \n",
       "5    class TweetDataset:\\n    def __init__(self, te...   \n",
       "6    train_ids, valid_ids = RandomSplitter()(train_...   \n",
       "7    test_tensor = tokenizer(list(test_df[\"text\"]),...   \n",
       "8    class TestDS:\\n    def __init__(self, tensors)...   \n",
       "9    stop_words=nltk.corpus.stopwords.words('englis...   \n",
       "10   from sklearn.feature_extraction.text import Co...   \n",
       "11   from sklearn.feature_extraction.text import Tf...   \n",
       "12   from sklearn.model_selection import train_test...   \n",
       "13   my_submission_preds = pipe.predict(test['text'...   \n",
       "14                              nlp = spacy.load('en')   \n",
       "15   # remove stopwords,punct\\n# remove duplicate t...   \n",
       "16   tests = []\\nfor doc in nlp.pipe(test_raw.text)...   \n",
       "17   tf_idf = TfidfVectorizer(max_features=10000).f...   \n",
       "18   X_train, X_test, y_train, y_test = train_test_...   \n",
       "19   def remove_URL(text):\\n    url = re.compile(r\"...   \n",
       "20   #regex pattern to remove links\\npattern = re.c...   \n",
       "21   #for test:\\nfor t in test.text:\\n    matches =...   \n",
       "22   #preprocess data frames:\\n#train\\ntrain[\"text\"...   \n",
       "23   # remove stopwords\\nnltk.download('stopwords')...   \n",
       "24   #train\\ntrain[\"text\"] = train.text.map(remove_...   \n",
       "25   # Count unique words\\ndef counter_word(text_co...   \n",
       "26   # Split dataset into training and validation s...   \n",
       "27   #train/val\\ntrain_sentences = train_sentences....   \n",
       "28        #test\\ntest_sentences = test.text.to_numpy()   \n",
       "29   # Tokenize\\n# vectorize a text corpus by turni...   \n",
       "30   # Now each word has unique index\\nword_index =...   \n",
       "31   #apply on train, validation, and test sentence...   \n",
       "32   # Pad the sequences to have the same length\\nm...   \n",
       "33   # flip (key, value)\\nreverse_word_index = dict...   \n",
       "34   #decoding\\ndef decode(sequence):\\n    return \"...   \n",
       "35   # We'll use these weights later on to make up ...   \n",
       "36   # Commented out the graceful handling of dupli...   \n",
       "37   class TweetPreProcessor:\\n    \"\"\"\\n    This cl...   \n",
       "38   train_df[\"text\"] = train_df[\"text\"].apply(twee...   \n",
       "39   # Fill NA\\ntrain_df[\"keyword\"].fillna(\"\", inpl...   \n",
       "40   x_train, x_val, y_train, y_val = sklearn.model...   \n",
       "41   def tokenize_encode(tweets, max_length=None):\\...   \n",
       "42   train_dataset = tf.data.Dataset.from_tensor_sl...   \n",
       "43   tfidf_vectorizer = sklearn.feature_extraction....   \n",
       "44   test_tweets_encoded = tokenize_encode(test_df[...   \n",
       "45   def filter_text(df):\\n    df['text']=df['text'...   \n",
       "46   sw=['the', 'a', 'an', 'in', 'on', 'with', 'by'...   \n",
       "47   v = CountVectorizer(stop_words=sw)\\ntrain_v = ...   \n",
       "48   # plot prop of missing for each feature\\nsns.s...   \n",
       "49   wordnet_lemmatizer = WordNetLemmatizer()\\n\\nde...   \n",
       "50   x_train, x_test, y_train, y_test = train_test_...   \n",
       "51   tfidf_vectorizer = TfidfVectorizer(tokenizer=w...   \n",
       "52   corpus = []\\nfor i in range(0, len(train_len))...   \n",
       "53   from sklearn.feature_extraction.text import Co...   \n",
       "54   from sklearn.model_selection import train_test...   \n",
       "55   testcorpus = []\\nfor i in range(0, len(test_le...   \n",
       "56   keyword_dist = train_df.groupby(\"keyword\")['ta...   \n",
       "57   #word count\\ntrain_df['word_count'] = train_df...   \n",
       "58   # Refrenced from Gunes Evitan and Vitalii Moki...   \n",
       "59   def encode(texts, tokenizer, max_len=512):\\n  ...   \n",
       "60   train_input = encode(train_df.text_cleaned.val...   \n",
       "61   ids_with_target_error = [328,443,513,2619,3640...   \n",
       "62   set1 = set(train_df[train_df.duplicated(subset...   \n",
       "63   set2 = (train_df[train_df.duplicated(subset=['...   \n",
       "64   # setting the argument keep=False, drops all t...   \n",
       "65   train_df['location'] = train_df['location'].st...   \n",
       "66   loc_dict = {'united states':'usa',\\n          ...   \n",
       "67   train_df['location'].replace(loc_dict, inplace...   \n",
       "68   abbreviations = {\\n    \"$\" : \" dollar \",\\n    ...   \n",
       "69   def convert_abb(x):\\n    word_list = x.split()...   \n",
       "70   train_df['text'] = train_df.text.apply(convert...   \n",
       "71   train_df['clean_text'] = train_df.text.apply(l...   \n",
       "72   train_df['clean_text'] = train_df.clean_text.a...   \n",
       "73       non_alpha = string.punctuation + '0123456789'   \n",
       "74   train_df['clean_text'] = train_df.clean_text.a...   \n",
       "75   train_df['token_text'] = train_df.clean_text.s...   \n",
       "76   stopwords = nltk.corpus.stopwords.words(\"engli...   \n",
       "77   # since we have a dataset containing text from...   \n",
       "78       train_df.reset_index(inplace=True, drop=True)   \n",
       "79   def dummy(doc):\\n    return doc\\ntfidf_vect = ...   \n",
       "80   counts_df_train = pd.DataFrame(matrix_train.to...   \n",
       "81   train_df['length'] = train_df.text.apply(lambd...   \n",
       "82   train_df['punct_perc'] = train_df.text.apply(l...   \n",
       "83   train_df['word_count'] = train_df.token_text.a...   \n",
       "84   train_df['length_int'] =pd.cut(train_df.length...   \n",
       "85   train_df['length'] = train_df['length']**2.3\\n...   \n",
       "86   train_df['punct_perc'] = train_df['punct_perc'...   \n",
       "87   # assign an emotion to each tweet\\ntrain_df['e...   \n",
       "88   # exploding the dictionary into 4 different co...   \n",
       "89   total_emotions = train_df[['Happy', 'Angry', '...   \n",
       "90   mean_emotions = train_df[['Happy', 'Angry', 'S...   \n",
       "91   train_df['sentiment'] = train_df.text.astype(s...   \n",
       "92   train_df = pd.concat([train_df, pd.DataFrame(t...   \n",
       "93   mean_sentiment = train_df[['neg', 'neu', 'pos'...   \n",
       "94   #full_train_df = pd.concat([train_df.drop(['lo...   \n",
       "95   #full_test_df = pd.concat([test_df.drop(['loca...   \n",
       "96   # deleting unnecessary dataframes to save memo...   \n",
       "97   count_vectorizer = feature_extraction.text.Cou...   \n",
       "98   train_vectors = count_vectorizer.fit_transform...   \n",
       "99   def bert_encode(texts, tokenizer, max_len=512)...   \n",
       "100  #vocab file from pre-trained BERT for tokeniza...   \n",
       "101  stop_words=nltk.corpus.stopwords.words('englis...   \n",
       "102  df = df.drop_duplicates().reset_index(drop = T...   \n",
       "103                                 #Cleaning the Data   \n",
       "104  import re\\n#Conver lowercase remove punctuatio...   \n",
       "105  #remove stopwords\\nimport nltk\\nlst_stopwords ...   \n",
       "106  #stemming\\nps = nltk.stem.porter.PorterStemmer...   \n",
       "107  #Lemmentization\\nlem = nltk.stem.wordnet.WordN...   \n",
       "108  #to apply all the technique to all the records...   \n",
       "109  #apply dataset\\ndf['clean_text'] = df['text']....   \n",
       "110                                 #Target Encoding\\n   \n",
       "111  import category_encoders as ce\\n\\n# Target enc...   \n",
       "112  from sklearn.feature_extraction.text import Tf...   \n",
       "113  df = df.join(X_train_text, rsuffix='_text')\\nt...   \n",
       "114  # Get the Bert tokenizer\\ntokenizer = BertToke...   \n",
       "115  def prepare_sequence(text):\\n    \"\"\"\\n    Toke...   \n",
       "116  # Prepare a test sentence\\ntest_sentence = 'Is...   \n",
       "117  def map_example_to_dict(input_ids, attention_m...   \n",
       "118   X = train_data[\"text\"]\\ny = train_data[\"target\"]   \n",
       "119  # Split the training dataset for training and ...   \n",
       "120  train_dataset = list(zip(X_train, y_train))\\nv...   \n",
       "121  # Prepare sequences of text and build TF train...   \n",
       "122  def encode_test_examples(texts):\\n    \"\"\"\\n   ...   \n",
       "123  X_test = test_data[\"text\"]\\ntest_dataset = lis...   \n",
       "124  # Only apply 'keyword' columns in full data, b...   \n",
       "125  # Adding cleaned data into df_train/test\\ndf_t...   \n",
       "126  def extract_keywords(text):\\n    potential_key...   \n",
       "127  df_train.keyword = pd.DataFrame(list(map(keywo...   \n",
       "128  # Spilt data\\nX_train, X_val, y_train, y_val =...   \n",
       "129  train_ds = tf.data.Dataset.from_tensor_slices(...   \n",
       "130  AUTOTUNE = tf.data.experimental.AUTOTUNE\\n\\nBU...   \n",
       "131  a3 = configure_dataset(train_ds, shuffle=True)...   \n",
       "132  # Configure the datasets\\ntrain_ds = configure...   \n",
       "133  # Free memory\\ndel X_train, X_val, y_train, y_...   \n",
       "134  train_df = train_df.drop(['id', 'keyword', 'lo...   \n",
       "135  #remove duplicated rows\\ntrain_df.drop_duplica...   \n",
       "136  Real_Disaster_text = ' '.join(Real_Disaster_df...   \n",
       "137  Not_Real_Disaster_text = ' '.join(Not_Real_Dis...   \n",
       "138  # take text and preprocess 'remove stopwords [...   \n",
       "139  train_df['text'] = train_df['text'].map(clean_...   \n",
       "140  from sklearn.utils import shuffle\\ntrain_df_sh...   \n",
       "141  X = train_df_shuffled['text']\\ny = train_df_sh...   \n",
       "142  test_df = test_df.drop(['id', 'keyword', 'loca...   \n",
       "143  test_df['text'] = test_df['text'].map(clean_te...   \n",
       "144               sample_submission[\"target\"] = y_pred   \n",
       "145  max_seq_len = 160\\n\\ntrain_input = bert_encode...   \n",
       "146  # DROP DUPLICATE SAMPLES WITH CONFLICTING LABE...   \n",
       "147  # TOKENIZE ALL THE SENTENCES AND MAP THE TOKEN...   \n",
       "148  # SPLIT TRAIN DATA INTO TRAIN AND TEST SET\\n# ...   \n",
       "149  # CREATE DATA ITERATOR TO SAVE MEMORY\\n\\nfrom ...   \n",
       "150  # PREPARE PREDICTIONS FOR SUBMISSION\\n\\n# Comb...   \n",
       "151  # clean training text\\nl=len(df)\\ndisplay(l)\\n...   \n",
       "152  # combine clean text with training data\\ndf_cl...   \n",
       "153  l=len(df1)\\ndisplay(l)\\npredlist=[]\\n#l=1\\nfor...   \n",
       "154  df_pred=pd.DataFrame(predlist)\\ndf_pred.column...   \n",
       "155  df2.loc[df2['target']=='target','target']=1\\nd...   \n",
       "156  min_freq = 5\\nabove_threshold = train.location...   \n",
       "157  train.drop(['location', 'keyword'], axis=1, in...   \n",
       "158  nlp = English()\\ntokenizer = nlp.tokenizer\\nto...   \n",
       "159  text = \"Don't split #hashtags!\"\\nprint('Before...   \n",
       "160  text = 'This is  a test\\n , ok?'\\nprint('All t...   \n",
       "161  train['tokens'] = train['text'].apply(lambda r...   \n",
       "162  from sklearn.feature_extraction.text import Co...   \n",
       "163                 x = train_bow\\ny = train['target']   \n",
       "164  # min and max document frequency (ratio of doc...   \n",
       "165  oov_count = Counter()\\nall_tokens = []\\n\\nfor ...   \n",
       "166  test_tokens = []\\nfor row in test.tokens:\\n   ...   \n",
       "167  words_to_add = [w for w in oov_count if oov_co...   \n",
       "168  def convert_to_indices(all_tokens):\\n    word_...   \n",
       "169  def collate_as_list(samples):\\n    \"\"\"Function...   \n",
       "170  validation_size = int(0.1 * len(train))\\nvalid...   \n",
       "171  # create tensors of variable sizes\\n# note tha...   \n",
       "172  train_data = TensorDataset(x_train, x_train_ma...   \n",
       "173  def lowercase_text(text):\\n    return text.low...   \n",
       "174  import re\\nimport string\\ndef remove_noise(tex...   \n",
       "175  # Tokenizing the training and the test set\\nim...   \n",
       "176  # Removing stopwords belonging to english lang...   \n",
       "177  # After preprocessing, the text format\\ndef co...   \n",
       "178  # Stemming\\nfrom nltk.stem.snowball import Sno...   \n",
       "179  from sklearn.feature_extraction.text import Co...   \n",
       "180  def bert_encode(texts, tokenizer, max_len):\\n ...   \n",
       "181  train = train.fillna(' ')\\ntest = test.fillna(...   \n",
       "182  import tensorflow_hub as hub\\nimport tokenizat...   \n",
       "183  from bs4 import BeautifulSoup # Text Cleaning\\...   \n",
       "184  test_data['text'] = test_data['text'].apply(cl...   \n",
       "185  from keras.preprocessing.text import Tokenizer...   \n",
       "186  # Representing texts as one hot encoded sequen...   \n",
       "187  from sklearn.model_selection import train_test...   \n",
       "188  X_train_wc = tokenizer.texts_to_matrix(train_d...   \n",
       "189  X_train_wc, X_val_wc, y_train, y_val = train_t...   \n",
       "190  X_train_freq = tokenizer.texts_to_matrix(train...   \n",
       "191  X_train_freq, X_val_freq, y_train, y_val = tra...   \n",
       "192  from sklearn.feature_extraction.text import Tf...   \n",
       "193  X_train_tfidf, X_val_tfidf, y_train, y_val = t...   \n",
       "194  # Sequences creation, truncation and padding\\n...   \n",
       "195  X_train_seq, X_val_seq, y_train, y_val = train...   \n",
       "196  # Applying GloVE representations on our corpus...   \n",
       "197  # Since classes are imbalanced, we need to res...   \n",
       "198                                         df_class_0   \n",
       "199                                         df_class_1   \n",
       "200  # Second resample - try both under- and over-s...   \n",
       "201  train_df, test_df = train_test_split(df, train...   \n",
       "202                     #eng_words = words.words(\"en\")   \n",
       "203                        #print(\"wort\" in eng_words)   \n",
       "204     snowball = SnowballStemmer(language=\"english\")   \n",
       "205  def tokenize_sentence(sentence: str, remove_st...   \n",
       "206  tokenize_sentence(\"the sentence and asdf fy kr...   \n",
       "207  vectorizer_params = {\\n    #\"max_features\": 50...   \n",
       "208  vectorizer = TfidfVectorizer(**vectorizer_params)   \n",
       "209                                         vectorizer   \n",
       "210  features = vectorizer.fit_transform(train_df[\"...   \n",
       "211     feature_names = vectorizer.get_feature_names()   \n",
       "212                         X_train = train_df[\"text\"]   \n",
       "213                                            X_train   \n",
       "214                       y_train = train_df[\"target\"]   \n",
       "215                                            y_train   \n",
       "216  lr_model_params = {\\n    #\"class_weight\": \"bal...   \n",
       "217                      text_n = 10\\nfeatures[text_n]   \n",
       "218  model_pipeline = Pipeline([\\n    (\"vectorizer\"...   \n",
       "219      #y_test = y_train\\ny_test = test_df[\"target\"]   \n",
       "220  pred_df.drop(columns=[\"keyword\", \"location\", \"...   \n",
       "221  def wordcount(x):\\n    length = len(str(x).spl...   \n",
       "222  # Taken from - Craig Thomas https://www.kaggle...   \n",
       "223  def preprocess_text(text, flg_stemm=False, flg...   \n",
       "224  lst_stopwords = nltk.corpus.stopwords.words(\"e...   \n",
       "225  contractions = { \\n\"ain't\": \"am not\",\\n\"aren't...   \n",
       "226  train_df[\"text_clean\"] = train_df[\"text_clean\"...   \n",
       "227  vec=TfidfVectorizer(max_features = 10000,ngram...   \n",
       "228  matrix = vec.transform(train_df['text_clean'])...   \n",
       "229  matrix_df['length']=train_df['length']\\nmatrix...   \n",
       "230  test_df[\"text_clean\"]=test_df['text']\\ntest_df...   \n",
       "231  #Credit: https://www.kaggle.com/xhlulu/disaste...   \n",
       "232  # Encode the text into tokens, masks, and segm...   \n",
       "233  data = pd.concat([train_data, submit_data])\\nd...   \n",
       "234  data['text'] = data['text'].apply(lambda x: re...   \n",
       "235  '''\\ntext_series = data.loc[:,'text']\\nfor i i...   \n",
       "236  clean_train = data[0:train_data.shape[0]]\\ncle...   \n",
       "237  def tfidf(words):\\n    tfidf_vectorizer = Tfid...   \n",
       "238  stop_words = stopwords.words('english')\\nfor w...   \n",
       "239  def get_textVector(data_list, word2vec, textsV...   \n",
       "240  textsVectors_list = []\\nget_textVector(data_li...   \n",
       "241  word2vec_X = X[0:train_data.shape[0]]\\ny = dat...   \n",
       "242  tokenizer = Tokenizer()\\ntokenizer.fit_on_text...   \n",
       "243  trainCNN_data = cnn_data[0:train_data.shape[0]...   \n",
       "244  embedding_matrix = np.zeros((len(word_index) +...   \n",
       "245  embedding_layer = Embedding(len(word_index)+1,...   \n",
       "\n",
       "                                           Description  Cluster  \\\n",
       "0    This code snippet drops the \"id\", \"keyword\", a...        9   \n",
       "1    This code snippet defines a function to remove...        7   \n",
       "2    This code snippet defines a function to remove...        7   \n",
       "3    This code snippet defines a function to remove...        2   \n",
       "4    This code snippet tokenizes the text data from...        2   \n",
       "5    This code snippet defines a custom dataset cla...        2   \n",
       "6    This code snippet splits the training DataFram...        8   \n",
       "7    This code snippet tokenizes the text data from...        2   \n",
       "8    This code snippet defines a custom dataset cla...       10   \n",
       "9    This code processes the text data in the 'text...        1   \n",
       "10   This code uses the `CountVectorizer` to conver...        8   \n",
       "11   This code utilizes the `TfidfVectorizer` to co...        8   \n",
       "12   This code snippet constructs feature data by c...        7   \n",
       "13   This code snippet generates predictions for th...        7   \n",
       "14   This code snippet loads the English language m...        0   \n",
       "15   This code snippet processes the training data ...        1   \n",
       "16   This code snippet processes the testing data b...        6   \n",
       "17   This code snippet creates a TF-IDF Vectorizer ...        7   \n",
       "18   This code snippet splits the training DataFram...        7   \n",
       "19   This code defines two functions to preprocess ...        6   \n",
       "20   This code snippet demonstrates how URLs in the...       10   \n",
       "21   This code snippet demonstrates how URLs in the...       10   \n",
       "22   This code snippet preprocesses the text data i...        7   \n",
       "23   This code snippet downloads the list of Englis...        6   \n",
       "24   This code snippet preprocesses the text data i...        7   \n",
       "25   This code snippet defines a function to count ...       10   \n",
       "26   This code splits the text and target columns o...        7   \n",
       "27   This code converts the training and validation...        4   \n",
       "28   This code converts the text column of the test...        0   \n",
       "29   This code snippet initializes a tokenizer conf...        3   \n",
       "30   This code creates a dictionary, `word_index`, ...        0   \n",
       "31   This code snippet converts the training, valid...        9   \n",
       "32   This code snippet pads the sequences of intege...        4   \n",
       "33   This code snippet creates a reverse mapping di...        9   \n",
       "34   This code defines a function to decode a seque...        9   \n",
       "35   This code calculates and stores the class weig...        3   \n",
       "36   This code removes duplicate entries from the t...        0   \n",
       "37   This code defines a `TweetPreProcessor` class ...        2   \n",
       "38   This code applies the `TweetPreProcessor` to p...        5   \n",
       "39   This code fills missing values in the 'keyword...        7   \n",
       "40   This code splits the training DataFrame into t...        7   \n",
       "41   This code defines a function to tokenize and e...        8   \n",
       "42   This code creates TensorFlow datasets from the...        4   \n",
       "43   This code initializes a TF-IDF vectorizer with...        8   \n",
       "44   This code tokenizes and encodes the text and k...        8   \n",
       "45   The code defines and applies a function to rem...        7   \n",
       "46   The code defines a list of common stopwords to...       10   \n",
       "47   The code initializes a CountVectorizer with cu...        3   \n",
       "48   This code visualizes the proportion of missing...        6   \n",
       "49   This code snippet defines a text preprocessing...        1   \n",
       "50   This code splits the training data into traini...        7   \n",
       "51   This code creates a preprocessing pipeline tha...        2   \n",
       "52   The code preprocesses the text data in the tra...        8   \n",
       "53   The code transforms the preprocessed text data...        3   \n",
       "54   The code splits the dataset into training and ...        7   \n",
       "55   The code preprocesses the 'text' column of the...        8   \n",
       "56   This code snippet calculates the distribution ...        3   \n",
       "57   This code snippet creates new columns in both ...        8   \n",
       "58   This code snippet defines a function for clean...        1   \n",
       "59   This code snippet defines a function that toke...        2   \n",
       "60   This code snippet preprocesses the cleaned tex...        7   \n",
       "61   This code corrects errors in the 'target' colu...        4   \n",
       "62   This code creates a set of 'id' values from th...        9   \n",
       "63   This code creates an array of 'id' values from...        9   \n",
       "64   This code removes all rows with duplicated tex...        7   \n",
       "65   This code normalizes the 'location' column in ...        5   \n",
       "66   This code defines a dictionary for standardizi...        2   \n",
       "67   This code uses the previously defined dictiona...        5   \n",
       "68   This code defines a dictionary to map abbrevia...        1   \n",
       "69   This code defines a function `convert_abb` tha...        6   \n",
       "70   This code applies the `convert_abb` function t...        5   \n",
       "71   This code creates a 'clean_text' column in bot...        5   \n",
       "72   This code further cleans the 'clean_text' colu...        5   \n",
       "73   This code creates a string `non_alpha` that co...        0   \n",
       "74   This code removes all punctuation and digits f...        5   \n",
       "75   This code tokenizes the lowercase version of t...        5   \n",
       "76   This code removes standard English stopwords, ...        5   \n",
       "77   This code applies the Porter Stemming techniqu...        7   \n",
       "78   This code resets the index of the training dat...        9   \n",
       "79   This code defines a `dummy` function for use i...        3   \n",
       "80   This code converts the TF-IDF feature matrices...        4   \n",
       "81   This code calculates the length of each tweet,...        5   \n",
       "82   This code calculates the percentage of punctua...        5   \n",
       "83   This code calculates the word count of the tok...        7   \n",
       "84   This code creates a new column called 'length_...        2   \n",
       "85   This code transforms the 'length' column in bo...        7   \n",
       "86   This code applies the cube root transformation...        7   \n",
       "87   This code assigns an emotion score to each twe...        5   \n",
       "88   This code expands the 'emotion' dictionary col...        7   \n",
       "89   This code calculates the sum of each emotion t...       10   \n",
       "90   This code calculates the mean value of each em...       10   \n",
       "91   This code applies the `SentimentIntensityAnaly...        5   \n",
       "92   This code expands the 'sentiment' dictionary c...        7   \n",
       "93   This code calculates the mean and total sentim...        9   \n",
       "94   This code creates a new DataFrame `full_train_...        9   \n",
       "95   This code creates a new DataFrame `full_test_d...        9   \n",
       "96   This code deletes the intermediate dataframes ...        9   \n",
       "97   This code snippet initializes a CountVectorize...        9   \n",
       "98   This code snippet transforms the \"text\" column...        7   \n",
       "99   This code defines a function to preprocess and...        2   \n",
       "100  This code creates a BERT tokenizer using the v...        8   \n",
       "101  This code snippet preprocesses the text data i...        1   \n",
       "102  This code removes duplicate rows from the Data...        9   \n",
       "103  This comment indicates the beginning of a code...        0   \n",
       "104  This code converts a text sample from the Data...        3   \n",
       "105  This code removes common English stopwords fro...        9   \n",
       "106  This code applies stemming to the filtered wor...        3   \n",
       "107  This code applies lemmatization to the filtere...        3   \n",
       "108  This code defines a function `utils_preprocess...        2   \n",
       "109  This code applies the text preprocessing funct...        5   \n",
       "110  This comment indicates the beginning of a code...        0   \n",
       "111  This code uses the `category_encoders` library...        6   \n",
       "112  This code utilizes the `TfidfVectorizer` from ...        8   \n",
       "113  This code appends the TF-IDF feature vectors, ...        9   \n",
       "114  This code snippet initializes a BERT tokenizer...        2   \n",
       "115  This code snippet defines a function `prepare_...        2   \n",
       "116  This code snippet prepares a test sentence by ...        8   \n",
       "117  This code snippet defines two functions: `map_...        1   \n",
       "118  This code snippet extracts the text data and t...        9   \n",
       "119  This code snippet splits the extracted text da...        2   \n",
       "120  This code snippet combines the training and va...        4   \n",
       "121  This code snippet encodes the training and val...        9   \n",
       "122  This code snippet defines two functions: `enco...        1   \n",
       "123  This code snippet extracts the text data from ...        3   \n",
       "124  This code snippet preprocesses the 'keyword' c...        9   \n",
       "125  This snippet adds the cleaned 'keyword' column...        5   \n",
       "126  This code snippet defines two functions: `extr...        2   \n",
       "127  This snippet applies the `keyword_filler` func...        5   \n",
       "128  This snippet splits the training dataset into ...        2   \n",
       "129  This snippet converts the training, validation...        4   \n",
       "130  This snippet defines a function to configure T...        2   \n",
       "131  This snippet configures the `train_ds` TensorF...        3   \n",
       "132  This code snippet configures the training, val...        9   \n",
       "133  This snippet deletes the intermediate variable...        4   \n",
       "134  This code removes the 'id', 'keyword', and 'lo...        9   \n",
       "135  This code removes duplicate rows from the trai...        9   \n",
       "136  This code concatenates all the text of disaste...        0   \n",
       "137  This code concatenates all the text of non-dis...        0   \n",
       "138  This code defines a function to preprocess tex...        2   \n",
       "139  This code applies the `clean_text` function to...        7   \n",
       "140  This code shuffles the rows of the training Da...        9   \n",
       "141  This code splits the shuffled DataFrame into t...        7   \n",
       "142  This code removes the 'id', 'keyword', and 'lo...        9   \n",
       "143  This code applies the `clean_text` function to...        7   \n",
       "144  This code updates the 'target' column in the s...        9   \n",
       "145  This code encodes the text data from the train...        2   \n",
       "146  This code snippet drops duplicate samples with...        3   \n",
       "147  This code snippet tokenizes all sentences in t...        2   \n",
       "148  This code snippet splits the dataset into trai...        3   \n",
       "149  This code snippet creates DataLoader objects f...        2   \n",
       "150  This code snippet combines the prediction resu...        3   \n",
       "151  This code snippet cleans the text data in the ...        6   \n",
       "152  This code snippet combines the cleaned text da...        3   \n",
       "153  This code snippet cleans the text data in the ...        6   \n",
       "154  This code snippet combines the prediction resu...        3   \n",
       "155  This code snippet converts the textual predict...        6   \n",
       "156  The code snippet filters the locations in the ...        3   \n",
       "157  The code snippet removes the 'location' and 'k...        5   \n",
       "158  The code snippet initializes a tokenizer using...        3   \n",
       "159  The code snippet demonstrates how to customize...        6   \n",
       "160  The code snippet tokenizes a sample text and d...        6   \n",
       "161  The code snippet tokenizes the 'text' column i...        5   \n",
       "162  The code snippet sets up a CountVectorizer to ...        8   \n",
       "163  The code snippet assigns the bag-of-words repr...        9   \n",
       "164  The code snippet reconfigures the CountVectori...        8   \n",
       "165  The code snippet processes the tokenized text ...        6   \n",
       "166  The code snippet processes the test dataset's ...        6   \n",
       "167  The code snippet adds frequently occurring out...        8   \n",
       "168  The code snippet defines a function to convert...        2   \n",
       "169  The code snippet defines a custom dataset clas...       10   \n",
       "170  The code snippet splits the training data into...        4   \n",
       "171  The code snippet tokenizes the text data in th...        8   \n",
       "172  The code snippet creates TensorDataset objects...        4   \n",
       "173  This code snippet defines a function to conver...        7   \n",
       "174  This code snippet defines a function to remove...        1   \n",
       "175  This code snippet imports necessary NLTK modul...        7   \n",
       "176  This code snippet defines a function to remove...        7   \n",
       "177  This code snippet defines a function to combin...        7   \n",
       "178  This code snippet imports the SnowballStemmer ...        7   \n",
       "179  This code snippet imports CountVectorizer and ...        8   \n",
       "180  This code snippet defines a function 'bert_enc...        2   \n",
       "181  This code snippet fills any missing values in ...        9   \n",
       "182  This code snippet loads a pre-trained BERT mod...        8   \n",
       "183  This code defines a function to clean and prep...        1   \n",
       "184  This code applies the previously defined `clea...        7   \n",
       "185  This code initializes a Keras Tokenizer with a...        6   \n",
       "186  This code converts the preprocessed text data ...        8   \n",
       "187  This code splits the one-hot encoded training ...        4   \n",
       "188  This code converts the preprocessed text data ...        8   \n",
       "189  This code splits the count-based encoded train...        4   \n",
       "190  This code converts the preprocessed text data ...        8   \n",
       "191  This code splits the frequency-based encoded t...        4   \n",
       "192  This code initializes a TfidfVectorizer to con...        8   \n",
       "193  This code splits the TF-IDF encoded training d...        4   \n",
       "194  This code initializes a new Keras Tokenizer, f...        8   \n",
       "195  This code splits the tokenized and padded trai...        4   \n",
       "196  This code creates an embedding matrix where ea...       10   \n",
       "197  This code splits the DataFrame into two separa...        3   \n",
       "198  This code outputs the DataFrame containing all...        0   \n",
       "199  This code outputs the DataFrame containing all...        0   \n",
       "200  This code performs both undersampling of the m...        8   \n",
       "201  This code splits the DataFrame into training a...        7   \n",
       "202  This code creates a list of English words usin...        0   \n",
       "203  This code checks if the string \"wort\" is prese...        0   \n",
       "204  This code initializes an instance of the Snowb...        3   \n",
       "205  This code defines a function that tokenizes a ...        2   \n",
       "206  This code calls the `tokenize_sentence` functi...        0   \n",
       "207  This code snippet defines a dictionary with co...       10   \n",
       "208  This code initializes a TfidfVectorizer instan...        9   \n",
       "209  This code outputs the configuration of the ini...        0   \n",
       "210  This code fits the TfidfVectorizer to the text...        9   \n",
       "211  This code retrieves the list of feature names ...        9   \n",
       "212  This code assigns the \"text\" column from the t...        9   \n",
       "213  This code outputs the training text data assig...        0   \n",
       "214  This code assigns the \"target\" column from the...        9   \n",
       "215  This code outputs the training labels assigned...        0   \n",
       "216  This code defines a dictionary with parameters...       10   \n",
       "217  This code accesses the TF-IDF feature vector f...        0   \n",
       "218  This code constructs a scikit-learn Pipeline t...       10   \n",
       "219  This code assigns the \"target\" column from the...        9   \n",
       "220  This code removes the \"keyword\", \"location\", a...        9   \n",
       "221  This code defines functions to calculate vario...        8   \n",
       "222  This code removes specific rows from the train...       10   \n",
       "223  This code defines a function to preprocess tex...        2   \n",
       "224  This code loads the list of English stopwords ...        9   \n",
       "225  This code defines functions to expand contract...        1   \n",
       "226  This code applies the `preprocess_text` functi...        7   \n",
       "227  This code initializes a `TfidfVectorizer` with...        3   \n",
       "228  This code transforms the cleaned text data in ...        3   \n",
       "229  This code adds the textual feature columns (le...        5   \n",
       "230  This code cleans and preprocesses the text dat...        8   \n",
       "231  This code defines a function to encode text da...        2   \n",
       "232  This code encodes the text data from the clean...        7   \n",
       "233  This code snippet concatenates the training an...        3   \n",
       "234  This code snippet applies multiple text prepro...        2   \n",
       "235  This commented-out code snippet intends to ite...        6   \n",
       "236  This code snippet separates the combined data ...        2   \n",
       "237  This code snippet defines a function to conver...        8   \n",
       "238  This code snippet extends the list of stopword...        6   \n",
       "239  This code snippet defines a function to conver...        2   \n",
       "240  This code snippet calls the `get_textVector` f...        7   \n",
       "241  This code snippet splits the array `X` back in...        2   \n",
       "242  This code snippet tokenizes the texts in `data...        8   \n",
       "243  This code snippet splits the CNN-ready data an...        2   \n",
       "244  This code snippet initializes an embedding mat...        8   \n",
       "245  This code snippet creates an embedding layer u...        2   \n",
       "\n",
       "                                                 Title  \n",
       "0       DataFrame Preprocessing and Text Vectorization  \n",
       "1     Text Preprocessing and Data Splitting Techniques  \n",
       "2     Text Preprocessing and Data Splitting Techniques  \n",
       "3          Text Preprocessing and Tokenization for NLP  \n",
       "4          Text Preprocessing and Tokenization for NLP  \n",
       "5          Text Preprocessing and Tokenization for NLP  \n",
       "6    Text Data Preprocessing and Vectorization Tech...  \n",
       "7          Text Preprocessing and Tokenization for NLP  \n",
       "8       Text Data Preprocessing and Analysis Functions  \n",
       "9       Text Data Cleaning and Preprocessing Functions  \n",
       "10   Text Data Preprocessing and Vectorization Tech...  \n",
       "11   Text Data Preprocessing and Vectorization Tech...  \n",
       "12    Text Preprocessing and Data Splitting Techniques  \n",
       "13    Text Preprocessing and Data Splitting Techniques  \n",
       "14                Text Data Processing and Preparation  \n",
       "15      Text Data Cleaning and Preprocessing Functions  \n",
       "16                Text Data Preprocessing and Cleaning  \n",
       "17    Text Preprocessing and Data Splitting Techniques  \n",
       "18    Text Preprocessing and Data Splitting Techniques  \n",
       "19                Text Data Preprocessing and Cleaning  \n",
       "20      Text Data Preprocessing and Analysis Functions  \n",
       "21      Text Data Preprocessing and Analysis Functions  \n",
       "22    Text Preprocessing and Data Splitting Techniques  \n",
       "23                Text Data Preprocessing and Cleaning  \n",
       "24    Text Preprocessing and Data Splitting Techniques  \n",
       "25      Text Data Preprocessing and Analysis Functions  \n",
       "26    Text Preprocessing and Data Splitting Techniques  \n",
       "27    Machine Learning Data Preparation and Conversion  \n",
       "28                Text Data Processing and Preparation  \n",
       "29   Text Preprocessing and Feature Engineering Tec...  \n",
       "30                Text Data Processing and Preparation  \n",
       "31      DataFrame Preprocessing and Text Vectorization  \n",
       "32    Machine Learning Data Preparation and Conversion  \n",
       "33      DataFrame Preprocessing and Text Vectorization  \n",
       "34      DataFrame Preprocessing and Text Vectorization  \n",
       "35   Text Preprocessing and Feature Engineering Tec...  \n",
       "36                Text Data Processing and Preparation  \n",
       "37         Text Preprocessing and Tokenization for NLP  \n",
       "38   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "39    Text Preprocessing and Data Splitting Techniques  \n",
       "40    Text Preprocessing and Data Splitting Techniques  \n",
       "41   Text Data Preprocessing and Vectorization Tech...  \n",
       "42    Machine Learning Data Preparation and Conversion  \n",
       "43   Text Data Preprocessing and Vectorization Tech...  \n",
       "44   Text Data Preprocessing and Vectorization Tech...  \n",
       "45    Text Preprocessing and Data Splitting Techniques  \n",
       "46      Text Data Preprocessing and Analysis Functions  \n",
       "47   Text Preprocessing and Feature Engineering Tec...  \n",
       "48                Text Data Preprocessing and Cleaning  \n",
       "49      Text Data Cleaning and Preprocessing Functions  \n",
       "50    Text Preprocessing and Data Splitting Techniques  \n",
       "51         Text Preprocessing and Tokenization for NLP  \n",
       "52   Text Data Preprocessing and Vectorization Tech...  \n",
       "53   Text Preprocessing and Feature Engineering Tec...  \n",
       "54    Text Preprocessing and Data Splitting Techniques  \n",
       "55   Text Data Preprocessing and Vectorization Tech...  \n",
       "56   Text Preprocessing and Feature Engineering Tec...  \n",
       "57   Text Data Preprocessing and Vectorization Tech...  \n",
       "58      Text Data Cleaning and Preprocessing Functions  \n",
       "59         Text Preprocessing and Tokenization for NLP  \n",
       "60    Text Preprocessing and Data Splitting Techniques  \n",
       "61    Machine Learning Data Preparation and Conversion  \n",
       "62      DataFrame Preprocessing and Text Vectorization  \n",
       "63      DataFrame Preprocessing and Text Vectorization  \n",
       "64    Text Preprocessing and Data Splitting Techniques  \n",
       "65   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "66         Text Preprocessing and Tokenization for NLP  \n",
       "67   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "68      Text Data Cleaning and Preprocessing Functions  \n",
       "69                Text Data Preprocessing and Cleaning  \n",
       "70   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "71   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "72   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "73                Text Data Processing and Preparation  \n",
       "74   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "75   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "76   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "77    Text Preprocessing and Data Splitting Techniques  \n",
       "78      DataFrame Preprocessing and Text Vectorization  \n",
       "79   Text Preprocessing and Feature Engineering Tec...  \n",
       "80    Machine Learning Data Preparation and Conversion  \n",
       "81   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "82   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "83    Text Preprocessing and Data Splitting Techniques  \n",
       "84         Text Preprocessing and Tokenization for NLP  \n",
       "85    Text Preprocessing and Data Splitting Techniques  \n",
       "86    Text Preprocessing and Data Splitting Techniques  \n",
       "87   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "88    Text Preprocessing and Data Splitting Techniques  \n",
       "89      Text Data Preprocessing and Analysis Functions  \n",
       "90      Text Data Preprocessing and Analysis Functions  \n",
       "91   Comprehensive Tweet Preprocessing and Feature ...  \n",
       "92    Text Preprocessing and Data Splitting Techniques  \n",
       "93      DataFrame Preprocessing and Text Vectorization  \n",
       "94      DataFrame Preprocessing and Text Vectorization  \n",
       "95      DataFrame Preprocessing and Text Vectorization  \n",
       "96      DataFrame Preprocessing and Text Vectorization  \n",
       "97      DataFrame Preprocessing and Text Vectorization  \n",
       "98    Text Preprocessing and Data Splitting Techniques  \n",
       "99         Text Preprocessing and Tokenization for NLP  \n",
       "100  Text Data Preprocessing and Vectorization Tech...  \n",
       "101     Text Data Cleaning and Preprocessing Functions  \n",
       "102     DataFrame Preprocessing and Text Vectorization  \n",
       "103               Text Data Processing and Preparation  \n",
       "104  Text Preprocessing and Feature Engineering Tec...  \n",
       "105     DataFrame Preprocessing and Text Vectorization  \n",
       "106  Text Preprocessing and Feature Engineering Tec...  \n",
       "107  Text Preprocessing and Feature Engineering Tec...  \n",
       "108        Text Preprocessing and Tokenization for NLP  \n",
       "109  Comprehensive Tweet Preprocessing and Feature ...  \n",
       "110               Text Data Processing and Preparation  \n",
       "111               Text Data Preprocessing and Cleaning  \n",
       "112  Text Data Preprocessing and Vectorization Tech...  \n",
       "113     DataFrame Preprocessing and Text Vectorization  \n",
       "114        Text Preprocessing and Tokenization for NLP  \n",
       "115        Text Preprocessing and Tokenization for NLP  \n",
       "116  Text Data Preprocessing and Vectorization Tech...  \n",
       "117     Text Data Cleaning and Preprocessing Functions  \n",
       "118     DataFrame Preprocessing and Text Vectorization  \n",
       "119        Text Preprocessing and Tokenization for NLP  \n",
       "120   Machine Learning Data Preparation and Conversion  \n",
       "121     DataFrame Preprocessing and Text Vectorization  \n",
       "122     Text Data Cleaning and Preprocessing Functions  \n",
       "123  Text Preprocessing and Feature Engineering Tec...  \n",
       "124     DataFrame Preprocessing and Text Vectorization  \n",
       "125  Comprehensive Tweet Preprocessing and Feature ...  \n",
       "126        Text Preprocessing and Tokenization for NLP  \n",
       "127  Comprehensive Tweet Preprocessing and Feature ...  \n",
       "128        Text Preprocessing and Tokenization for NLP  \n",
       "129   Machine Learning Data Preparation and Conversion  \n",
       "130        Text Preprocessing and Tokenization for NLP  \n",
       "131  Text Preprocessing and Feature Engineering Tec...  \n",
       "132     DataFrame Preprocessing and Text Vectorization  \n",
       "133   Machine Learning Data Preparation and Conversion  \n",
       "134     DataFrame Preprocessing and Text Vectorization  \n",
       "135     DataFrame Preprocessing and Text Vectorization  \n",
       "136               Text Data Processing and Preparation  \n",
       "137               Text Data Processing and Preparation  \n",
       "138        Text Preprocessing and Tokenization for NLP  \n",
       "139   Text Preprocessing and Data Splitting Techniques  \n",
       "140     DataFrame Preprocessing and Text Vectorization  \n",
       "141   Text Preprocessing and Data Splitting Techniques  \n",
       "142     DataFrame Preprocessing and Text Vectorization  \n",
       "143   Text Preprocessing and Data Splitting Techniques  \n",
       "144     DataFrame Preprocessing and Text Vectorization  \n",
       "145        Text Preprocessing and Tokenization for NLP  \n",
       "146  Text Preprocessing and Feature Engineering Tec...  \n",
       "147        Text Preprocessing and Tokenization for NLP  \n",
       "148  Text Preprocessing and Feature Engineering Tec...  \n",
       "149        Text Preprocessing and Tokenization for NLP  \n",
       "150  Text Preprocessing and Feature Engineering Tec...  \n",
       "151               Text Data Preprocessing and Cleaning  \n",
       "152  Text Preprocessing and Feature Engineering Tec...  \n",
       "153               Text Data Preprocessing and Cleaning  \n",
       "154  Text Preprocessing and Feature Engineering Tec...  \n",
       "155               Text Data Preprocessing and Cleaning  \n",
       "156  Text Preprocessing and Feature Engineering Tec...  \n",
       "157  Comprehensive Tweet Preprocessing and Feature ...  \n",
       "158  Text Preprocessing and Feature Engineering Tec...  \n",
       "159               Text Data Preprocessing and Cleaning  \n",
       "160               Text Data Preprocessing and Cleaning  \n",
       "161  Comprehensive Tweet Preprocessing and Feature ...  \n",
       "162  Text Data Preprocessing and Vectorization Tech...  \n",
       "163     DataFrame Preprocessing and Text Vectorization  \n",
       "164  Text Data Preprocessing and Vectorization Tech...  \n",
       "165               Text Data Preprocessing and Cleaning  \n",
       "166               Text Data Preprocessing and Cleaning  \n",
       "167  Text Data Preprocessing and Vectorization Tech...  \n",
       "168        Text Preprocessing and Tokenization for NLP  \n",
       "169     Text Data Preprocessing and Analysis Functions  \n",
       "170   Machine Learning Data Preparation and Conversion  \n",
       "171  Text Data Preprocessing and Vectorization Tech...  \n",
       "172   Machine Learning Data Preparation and Conversion  \n",
       "173   Text Preprocessing and Data Splitting Techniques  \n",
       "174     Text Data Cleaning and Preprocessing Functions  \n",
       "175   Text Preprocessing and Data Splitting Techniques  \n",
       "176   Text Preprocessing and Data Splitting Techniques  \n",
       "177   Text Preprocessing and Data Splitting Techniques  \n",
       "178   Text Preprocessing and Data Splitting Techniques  \n",
       "179  Text Data Preprocessing and Vectorization Tech...  \n",
       "180        Text Preprocessing and Tokenization for NLP  \n",
       "181     DataFrame Preprocessing and Text Vectorization  \n",
       "182  Text Data Preprocessing and Vectorization Tech...  \n",
       "183     Text Data Cleaning and Preprocessing Functions  \n",
       "184   Text Preprocessing and Data Splitting Techniques  \n",
       "185               Text Data Preprocessing and Cleaning  \n",
       "186  Text Data Preprocessing and Vectorization Tech...  \n",
       "187   Machine Learning Data Preparation and Conversion  \n",
       "188  Text Data Preprocessing and Vectorization Tech...  \n",
       "189   Machine Learning Data Preparation and Conversion  \n",
       "190  Text Data Preprocessing and Vectorization Tech...  \n",
       "191   Machine Learning Data Preparation and Conversion  \n",
       "192  Text Data Preprocessing and Vectorization Tech...  \n",
       "193   Machine Learning Data Preparation and Conversion  \n",
       "194  Text Data Preprocessing and Vectorization Tech...  \n",
       "195   Machine Learning Data Preparation and Conversion  \n",
       "196     Text Data Preprocessing and Analysis Functions  \n",
       "197  Text Preprocessing and Feature Engineering Tec...  \n",
       "198               Text Data Processing and Preparation  \n",
       "199               Text Data Processing and Preparation  \n",
       "200  Text Data Preprocessing and Vectorization Tech...  \n",
       "201   Text Preprocessing and Data Splitting Techniques  \n",
       "202               Text Data Processing and Preparation  \n",
       "203               Text Data Processing and Preparation  \n",
       "204  Text Preprocessing and Feature Engineering Tec...  \n",
       "205        Text Preprocessing and Tokenization for NLP  \n",
       "206               Text Data Processing and Preparation  \n",
       "207     Text Data Preprocessing and Analysis Functions  \n",
       "208     DataFrame Preprocessing and Text Vectorization  \n",
       "209               Text Data Processing and Preparation  \n",
       "210     DataFrame Preprocessing and Text Vectorization  \n",
       "211     DataFrame Preprocessing and Text Vectorization  \n",
       "212     DataFrame Preprocessing and Text Vectorization  \n",
       "213               Text Data Processing and Preparation  \n",
       "214     DataFrame Preprocessing and Text Vectorization  \n",
       "215               Text Data Processing and Preparation  \n",
       "216     Text Data Preprocessing and Analysis Functions  \n",
       "217               Text Data Processing and Preparation  \n",
       "218     Text Data Preprocessing and Analysis Functions  \n",
       "219     DataFrame Preprocessing and Text Vectorization  \n",
       "220     DataFrame Preprocessing and Text Vectorization  \n",
       "221  Text Data Preprocessing and Vectorization Tech...  \n",
       "222     Text Data Preprocessing and Analysis Functions  \n",
       "223        Text Preprocessing and Tokenization for NLP  \n",
       "224     DataFrame Preprocessing and Text Vectorization  \n",
       "225     Text Data Cleaning and Preprocessing Functions  \n",
       "226   Text Preprocessing and Data Splitting Techniques  \n",
       "227  Text Preprocessing and Feature Engineering Tec...  \n",
       "228  Text Preprocessing and Feature Engineering Tec...  \n",
       "229  Comprehensive Tweet Preprocessing and Feature ...  \n",
       "230  Text Data Preprocessing and Vectorization Tech...  \n",
       "231        Text Preprocessing and Tokenization for NLP  \n",
       "232   Text Preprocessing and Data Splitting Techniques  \n",
       "233  Text Preprocessing and Feature Engineering Tec...  \n",
       "234        Text Preprocessing and Tokenization for NLP  \n",
       "235               Text Data Preprocessing and Cleaning  \n",
       "236        Text Preprocessing and Tokenization for NLP  \n",
       "237  Text Data Preprocessing and Vectorization Tech...  \n",
       "238               Text Data Preprocessing and Cleaning  \n",
       "239        Text Preprocessing and Tokenization for NLP  \n",
       "240   Text Preprocessing and Data Splitting Techniques  \n",
       "241        Text Preprocessing and Tokenization for NLP  \n",
       "242  Text Data Preprocessing and Vectorization Tech...  \n",
       "243        Text Preprocessing and Tokenization for NLP  \n",
       "244  Text Data Preprocessing and Vectorization Tech...  \n",
       "245        Text Preprocessing and Tokenization for NLP  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate over the cells\n",
    "for i, cell in enumerate(cells):\n",
    "    # Get the code, description, cluster, and title\n",
    "    code = cell[\"code\"]\n",
    "    desc = cell[\"desc\"]\n",
    "    cluster = labels[i]\n",
    "    title = titles[str(labels[i])]\n",
    "\n",
    "    # Append the data to the list\n",
    "    data.append([code, desc, cluster, title])\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(data, columns=[\"Code\", \"Description\", \"Cluster\", \"Title\"])\n",
    "# Set the maximum number of rows to display\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display the dataframe\n",
    "display(HTML(df.to_html()))\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
