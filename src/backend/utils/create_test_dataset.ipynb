{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "code4ML_path = \"/home/ryounis/Documents/Zurich/PEACHLab/data/Code4ML\"\n",
    "\n",
    "kernels_meta = pd.read_csv(f\"{code4ML_path}/kernels_meta.csv\")\n",
    "markup_data = pd.read_csv(f\"{code4ML_path}/markup_data.csv\")\n",
    "vertices = pd.read_csv(f\"{code4ML_path}/vertices.csv\")\n",
    "code_blocks = pd.read_csv(f\"{code4ML_path}/code_blocks.csv\")\n",
    "data_preds = pd.read_csv(f\"{code4ML_path}/data_with_preds.csv\")\n",
    "\n",
    "code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_count = kernels_meta.groupby('comp_name').size()\n",
    "n_kernels_per_comp = kernels_meta['comp_name'].value_counts()\n",
    "N_HIGHEST_COMP = 4\n",
    "top_comp = kernels_count.nlargest(N_HIGHEST_COMP).index[N_HIGHEST_COMP-1]\n",
    "print(f\"Competition with most kernels: '{top_comp}'  ({kernels_count.nlargest(N_HIGHEST_COMP).iloc[N_HIGHEST_COMP-1]} kernels)\\n\")\n",
    "for name in n_kernels_per_comp.index:\n",
    "    print(f\"{name}: {n_kernels_per_comp[name]} kernels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = kernels_meta[kernels_meta['comp_name'] == top_comp]\n",
    "top_comp_code_blocks = code_blocks[code_blocks['kernel_id'].isin(kernels['kernel_id'])]\n",
    "merged_df = pd.merge(top_comp_code_blocks, kernels_meta, on='kernel_id')\n",
    "top_comp_code_blocks = merged_df.drop(columns=['kaggle_score', 'kaggle_comments', 'kaggle_upvotes', 'comp_name'])\n",
    "top_comp_code_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(top_comp_code_blocks.merge(data_preds, on='code_blocks_index'), vertices, left_on='predicted_graph_vertex_id', right_on='graph_vertex_id')\n",
    "merged_df.drop(['code_blocks_index', 'predicted_graph_vertex_id'], axis=1, inplace=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dir = \"../../data/test_datasets\"\n",
    "merged_df.to_csv(f'{test_dataset_dir}/{top_comp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utils.constants import BLANK_IPYNB_JSON\n",
    "import json\n",
    "import os\n",
    "\n",
    "test_ipynb_dir = f\"../../data/test_datasets/{top_comp}\"\n",
    "if not os.path.exists(test_ipynb_dir):\n",
    "    os.makedirs(test_ipynb_dir)\n",
    "\n",
    "distinct_kernel_links = merged_df['kernel_link'].unique()\n",
    "notebooks = []\n",
    "for i, kernel_link in enumerate(distinct_kernel_links):\n",
    "    print(f\"Notebook {i+1}/{len(distinct_kernel_links)}\", end='\\r')\n",
    "    kernel_df = merged_df[merged_df['kernel_link'] == kernel_link]\n",
    "    print(f\"Num cells kernel_df: {len(kernel_df)}\")\n",
    "    \n",
    "    ipynb = {\n",
    "        \"cells\": [],\n",
    "        \"metadata\": {\n",
    "            \"kernelspec\": {\n",
    "                \"display_name\": \"Python 3\",\n",
    "                \"language\": \"python\",\n",
    "                \"name\": \"python3\"\n",
    "            },\n",
    "            \"language_info\": {\n",
    "                \"codemirror_mode\": {\n",
    "                    \"name\": \"ipython\",\n",
    "                    \"version\": 3\n",
    "                },\n",
    "                \"file_extension\": \".py\",\n",
    "                \"mimetype\": \"text/x-python\",\n",
    "                \"name\": \"python\",\n",
    "                \"nbconvert_exporter\": \"python\",\n",
    "                \"pygments_lexer\": \"ipython3\",\n",
    "                \"version\": \"3.10.12\"\n",
    "            }\n",
    "        },\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 4    \n",
    "    }\n",
    "    print(f\"Num cells ipynb: {len(ipynb['cells'])}\")\n",
    "    for row in kernel_df.iterrows():\n",
    "        ipynb['cells'].append({\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": None,\n",
    "            \"metadata\": {\n",
    "                \"class\": row[1][\"graph_vertex_class\"],\n",
    "                \"subclass\": row[1][\"graph_vertex_subclass\"],\n",
    "                \"subclass_id\": row[1][\"graph_vertex_id\"],\n",
    "                \"predicted_subclass_probability\": row[1][\"predicted_graph_vertex__probability\"],\n",
    "                \"notebook_id\": row[1]['kernel_id'],\n",
    "            },\n",
    "            \"source\": row[1]['code_block']\n",
    "        })\n",
    "    notebooks.append(ipynb)\n",
    "    with open(f\"{test_ipynb_dir}/{kernel_link.split('/')[-1]}.ipynb\", \"w\") as f:json.dump(ipynb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utils.constants import FIRST_LAYER_LABELS, SECOND_LAYER_LABELS, BLANK_IPYNB_JSON\n",
    "from Classifiers.GPTClassifier import GPTClassifier\n",
    "with open('../../secrets/api_key.txt', 'r') as f: api_key = f'{f.read()}'\n",
    "\n",
    "LABELS = FIRST_LAYER_LABELS\n",
    "# LABELS = SECOND_LAYER_LABELS\n",
    "\n",
    "\n",
    "print(f\"Initializing classifier...\")\n",
    "prompt = f\"\"\"You will be given each code cell of the same jupyter notebook of a machine learning task.\n",
    "First, classify the code into one {', '.join(LABELS[:-1])} or {LABELS[-1]}.\n",
    "Consider the previously classified code snippets for context.\n",
    "Then, describe what the code snippet does in strictly one sentence.\n",
    "Explain your reasoning for the classification and then output the desired format at the end.\n",
    "Desired format:\n",
    "Class: <class_label>\n",
    "Description: <desctiption_sentence>\n",
    "\"\"\" \n",
    "classifier = GPTClassifier(api_key=api_key, prompt=prompt, labels=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLANK_IPYNB_JSON[\"cells\"] = []\n",
    "BLANK_IPYNB_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper_functions import notebook_extract_code, notebook_add_class_labels\n",
    "\n",
    "notebooks = []\n",
    "for kernel_id in merged_df['kernel_id'].unique()[:20]:\n",
    "    notebook_json = BLANK_IPYNB_JSON.copy()\n",
    "    notebook_json['cells'] = []\n",
    "    for row in merged_df[merged_df['kernel_id'] == kernel_id].iterrows():\n",
    "        notebook_json['cells'].append({\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": None,\n",
    "            \"metadata\": {\n",
    "                \"graph_vertex_id\": row[1][\"graph_vertex_id\"],\n",
    "                \"predicted_graph_vertex__probability\": row[1][\"predicted_graph_vertex__probability\"],\n",
    "                \"notebook_id\": row[1]['kernel_id'],\n",
    "            },\n",
    "            \"source\": row[1]['code_block']\n",
    "        })\n",
    "    notebooks.append(notebook_json)\n",
    "# notebook_code = notebook_extract_code(notebook_json)\n",
    "# cell_labels = classifier.classify_notebook(notebook_code)\n",
    "# notebook_json = notebook_add_class_labels(notebook_json, cell_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notebooks = []\n",
    "notebook_cell_labels = []\n",
    "for i, notebook_json in enumerate(notebooks):\n",
    "    print(f\"Notebook {i+1}/{len(notebooks)}\", end='\\r')\n",
    "    \n",
    "    notebook_code = notebook_extract_code(notebook_json)\n",
    "    cell_labels = classifier.classify_notebook(notebook_code)\n",
    "    notebook_json = notebook_add_class_labels(notebook_json, cell_labels)\n",
    "    labeled_notebooks.append(notebook_json)\n",
    "    notebook_cell_labels.append(cell_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for notebook in notebook_cell_labels:\n",
    "    for elem in notebook:\n",
    "        embeddings.append(elem[1])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "clusterer: HDBSCAN = HDBSCAN(\n",
    "    min_cluster_size=4,                 # Minimum number of samples to form a cluster\n",
    "    min_samples=2,                      # Minimum number of samples in a neighborhood to be considered as a core point\n",
    "    cluster_selection_epsilon=0,     # If 2 clusters are less than epsilon apart, they get merged\n",
    ")\n",
    "\n",
    "clusterer.fit(embeddings)\n",
    "for label in set(clusterer.labels_):\n",
    "    print(f\"Cluster {label}: {len([x for x in clusterer.labels_ if x == label])} cells\")\n",
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook['cells']:\n",
    "        cell['metadata']['cluster_label'] = clusterer.labels_[counter]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notebooks[0][\"cells\"]\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook[\"cells\"]:\n",
    "        true_labels.append(cell[\"metadata\"][\"graph_vertex_id\"])\n",
    "        predicted_labels.append(cell[\"metadata\"][\"cluster_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def count_misclustered_elements(true_labels, predicted_labels):\n",
    "    # Convert the labels to numpy arrays for easier manipulation\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    \n",
    "    unique_true_labels = np.unique(true_labels)\n",
    "    unique_predicted_labels = np.unique(predicted_labels)\n",
    "    \n",
    "    cost_matrix = np.zeros((len(unique_true_labels), len(unique_predicted_labels)), dtype=int)\n",
    "    \n",
    "    for i, true_label in enumerate(unique_true_labels):\n",
    "        for j, predicted_label in enumerate(unique_predicted_labels):\n",
    "            cost_matrix[i, j] = np.sum((true_labels == true_label) & (predicted_labels != predicted_label))\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    misclustered_count = cost_matrix[row_ind, col_ind].sum()\n",
    "    \n",
    "    return misclustered_count\n",
    "\n",
    "misclustered_count = count_misclustered_elements(true_labels, predicted_labels)\n",
    "print(f\"Number of misclustered elements: {misclustered_count}/{len(true_labels)}\")\n",
    "print(f\"Score: {1 - misclustered_count/len(true_labels)} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook['cells']:\n",
    "        cell['metadata']['cluster_label'] = clusterer.labels_[counter]\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
