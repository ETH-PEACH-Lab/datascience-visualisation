{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_blocks_index</th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17493583</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17493583</td>\n",
       "      <td>1</td>\n",
       "      <td>import os\\nimport zipfile\\nfrom subprocess imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17493583</td>\n",
       "      <td>2</td>\n",
       "      <td>def reduce_mem_usage(df):\\n    \"\"\" iterate thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17493583</td>\n",
       "      <td>3</td>\n",
       "      <td>import numpy as np</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17493583</td>\n",
       "      <td>4</td>\n",
       "      <td>orders_df = reduce_mem_usage(pd.read_csv('../w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599348</th>\n",
       "      <td>2599348</td>\n",
       "      <td>8902213</td>\n",
       "      <td>22</td>\n",
       "      <td>f=[]\\n\\nfor root, dirs, files in os.walk(\"../o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599349</th>\n",
       "      <td>2599349</td>\n",
       "      <td>8902213</td>\n",
       "      <td>23</td>\n",
       "      <td>my_model.fit_generator(\\n        train_data_ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599350</th>\n",
       "      <td>2599350</td>\n",
       "      <td>8902213</td>\n",
       "      <td>24</td>\n",
       "      <td>test_generator.reset()\\n\\npred = my_model.pred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599351</th>\n",
       "      <td>2599351</td>\n",
       "      <td>8902213</td>\n",
       "      <td>25</td>\n",
       "      <td>import cv2\\n\\n\\nfrom matplotlib import pyplot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599352</th>\n",
       "      <td>2599352</td>\n",
       "      <td>8902213</td>\n",
       "      <td>26</td>\n",
       "      <td>results_df = pd.DataFrame(\\n    {\\n        'id...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2599353 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         code_blocks_index  kernel_id  code_block_id  \\\n",
       "0                        0   17493583              0   \n",
       "1                        1   17493583              1   \n",
       "2                        2   17493583              2   \n",
       "3                        3   17493583              3   \n",
       "4                        4   17493583              4   \n",
       "...                    ...        ...            ...   \n",
       "2599348            2599348    8902213             22   \n",
       "2599349            2599349    8902213             23   \n",
       "2599350            2599350    8902213             24   \n",
       "2599351            2599351    8902213             25   \n",
       "2599352            2599352    8902213             26   \n",
       "\n",
       "                                                code_block  \n",
       "0        # This Python 3 environment comes with many he...  \n",
       "1        import os\\nimport zipfile\\nfrom subprocess imp...  \n",
       "2        def reduce_mem_usage(df):\\n    \"\"\" iterate thr...  \n",
       "3                                       import numpy as np  \n",
       "4        orders_df = reduce_mem_usage(pd.read_csv('../w...  \n",
       "...                                                    ...  \n",
       "2599348  f=[]\\n\\nfor root, dirs, files in os.walk(\"../o...  \n",
       "2599349  my_model.fit_generator(\\n        train_data_ge...  \n",
       "2599350  test_generator.reset()\\n\\npred = my_model.pred...  \n",
       "2599351  import cv2\\n\\n\\nfrom matplotlib import pyplot ...  \n",
       "2599352  results_df = pd.DataFrame(\\n    {\\n        'id...  \n",
       "\n",
       "[2599353 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import kaggle\n",
    "\n",
    "\n",
    "code4ML_path = \"/home/ryounis/Documents/Zurich/PEACHLab/data/Code4ML\"\n",
    "\n",
    "kernels_meta = pd.read_csv(f\"{code4ML_path}/kernels_meta.csv\")\n",
    "markup_data = pd.read_csv(f\"{code4ML_path}/markup_data.csv\")\n",
    "vertices = pd.read_csv(f\"{code4ML_path}/vertices.csv\")\n",
    "code_blocks = pd.read_csv(f\"{code4ML_path}/code_blocks.csv\")\n",
    "data_preds = pd.read_csv(f\"{code4ML_path}/data_with_preds.csv\")\n",
    "\n",
    "code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>kaggle_score</th>\n",
       "      <th>kaggle_comments</th>\n",
       "      <th>kaggle_upvotes</th>\n",
       "      <th>kernel_link</th>\n",
       "      <th>comp_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23299654</td>\n",
       "      <td>0.963920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/antonsharandin/lr1-anton-sharandin</td>\n",
       "      <td>Digit Recognizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23156584</td>\n",
       "      <td>0.964420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/aleksandrkolbin/kolbin-8305-lr1</td>\n",
       "      <td>Digit Recognizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23095035</td>\n",
       "      <td>0.973640</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>/sumeetbohra/a-very-basic-neural-network</td>\n",
       "      <td>Digit Recognizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23100959</td>\n",
       "      <td>0.964670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/denisshvetsov811/lr1-shvetsov8305</td>\n",
       "      <td>Digit Recognizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23077471</td>\n",
       "      <td>0.098960</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>/mdjafrilalamshihab/digit-recognizer-using-cla...</td>\n",
       "      <td>Digit Recognizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23098</th>\n",
       "      <td>14881993</td>\n",
       "      <td>0.730710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/diyadodwad/tutorial</td>\n",
       "      <td>Forest Cover Type Prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23099</th>\n",
       "      <td>15675072</td>\n",
       "      <td>0.673460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/maximkovito/nerve-segmentation-unet</td>\n",
       "      <td>Ultrasound Nerve Segmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23100</th>\n",
       "      <td>15717600</td>\n",
       "      <td>0.659580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/maxfarafonov/notebook-lab-3</td>\n",
       "      <td>Ultrasound Nerve Segmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23101</th>\n",
       "      <td>1990791</td>\n",
       "      <td>0.934961</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>/naturebalance/starter-approach-on-gift-matchi...</td>\n",
       "      <td>Santa Gift Matching Challenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23102</th>\n",
       "      <td>504569</td>\n",
       "      <td>0.745514</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>/lemonkoala/greedy-v2</td>\n",
       "      <td>Santa Gift Matching Challenge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23103 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kernel_id  kaggle_score  kaggle_comments  kaggle_upvotes  \\\n",
       "0       23299654      0.963920                0               0   \n",
       "1       23156584      0.964420                0               0   \n",
       "2       23095035      0.973640                3               4   \n",
       "3       23100959      0.964670                0               0   \n",
       "4       23077471      0.098960                4               7   \n",
       "...          ...           ...              ...             ...   \n",
       "23098   14881993      0.730710                0               0   \n",
       "23099   15675072      0.673460                0               0   \n",
       "23100   15717600      0.659580                0               0   \n",
       "23101    1990791      0.934961                2               3   \n",
       "23102     504569      0.745514                0              11   \n",
       "\n",
       "                                             kernel_link  \\\n",
       "0                    /antonsharandin/lr1-anton-sharandin   \n",
       "1                       /aleksandrkolbin/kolbin-8305-lr1   \n",
       "2               /sumeetbohra/a-very-basic-neural-network   \n",
       "3                     /denisshvetsov811/lr1-shvetsov8305   \n",
       "4      /mdjafrilalamshihab/digit-recognizer-using-cla...   \n",
       "...                                                  ...   \n",
       "23098                               /diyadodwad/tutorial   \n",
       "23099               /maximkovito/nerve-segmentation-unet   \n",
       "23100                       /maxfarafonov/notebook-lab-3   \n",
       "23101  /naturebalance/starter-approach-on-gift-matchi...   \n",
       "23102                              /lemonkoala/greedy-v2   \n",
       "\n",
       "                           comp_name  \n",
       "0                   Digit Recognizer  \n",
       "1                   Digit Recognizer  \n",
       "2                   Digit Recognizer  \n",
       "3                   Digit Recognizer  \n",
       "4                   Digit Recognizer  \n",
       "...                              ...  \n",
       "23098   Forest Cover Type Prediction  \n",
       "23099  Ultrasound Nerve Segmentation  \n",
       "23100  Ultrasound Nerve Segmentation  \n",
       "23101  Santa Gift Matching Challenge  \n",
       "23102  Santa Gift Matching Challenge  \n",
       "\n",
       "[23103 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition with most kernels: 'Natural Language Processing with Disaster Tweets'  (725 kernels)\n",
      "\n",
      "Titanic - Machine Learning from Disaster: 1963 kernels\n",
      "Digit Recognizer: 1300 kernels\n",
      "House Prices - Advanced Regression Techniques: 1013 kernels\n",
      "Natural Language Processing with Disaster Tweets: 725 kernels\n",
      "Quora Insincere Questions Classification: 679 kernels\n",
      "Mechanisms of Action (MoA) Prediction: 660 kernels\n",
      "COVID19 Global Forecasting (Week 4): 434 kernels\n",
      "Santander Customer Transaction Prediction: 418 kernels\n",
      "PUBG Finish Placement Prediction (Kernels Only): 394 kernels\n",
      "Tweet Sentiment Extraction: 386 kernels\n",
      "Cassava Leaf Disease Classification: 383 kernels\n",
      "Kannada MNIST: 362 kernels\n",
      "Aerial Cactus Identification: 345 kernels\n",
      "SIIM-ISIC Melanoma Classification: 325 kernels\n",
      "Riiid Answer Correctness Prediction: 322 kernels\n",
      "IEEE-CIS Fraud Detection: 303 kernels\n",
      "APTOS 2019 Blindness Detection: 303 kernels\n",
      "Predict Future Sales: 295 kernels\n",
      "COVID19 Global Forecasting (Week 2): 288 kernels\n",
      "Global Wheat Detection: 272 kernels\n",
      "Jane Street Market Prediction: 272 kernels\n",
      "Costa Rican Household Poverty Level Prediction: 259 kernels\n",
      "Jigsaw Unintended Bias in Toxicity Classification: 244 kernels\n",
      "Toxic Comment Classification Challenge: 241 kernels\n",
      "Home Credit Default Risk: 235 kernels\n",
      "OSIC Pulmonary Fibrosis Progression: 217 kernels\n",
      "Jigsaw Multilingual Toxic Comment Classification: 213 kernels\n",
      "Petals to the Metal - Flower Classification on TPU: 211 kernels\n",
      "Tabular Playground Series - Jan 2021: 209 kernels\n",
      "PetFinder.my Adoption Prediction: 201 kernels\n",
      "Mercari Price Suggestion Challenge: 192 kernels\n",
      "LANL Earthquake Prediction: 191 kernels\n",
      "M5 Forecasting - Accuracy: 184 kernels\n",
      "Flower Classification with TPUs: 180 kernels\n",
      "Instant Gratification: 179 kernels\n",
      "University of Liverpool - Ion Switching: 178 kernels\n",
      "Google QUEST Q&A Labeling: 178 kernels\n",
      "What's Cooking? (Kernels Only): 162 kernels\n",
      "ASHRAE - Great Energy Predictor III: 161 kernels\n",
      "Plant Pathology 2020 - FGVC7: 157 kernels\n",
      "Generative Dog Images: 155 kernels\n",
      "Bengali.AI Handwritten Grapheme Classification: 153 kernels\n",
      "Bike Sharing Demand: 151 kernels\n",
      "COVID19 Global Forecasting (Week 1): 147 kernels\n",
      "COVID19 Global Forecasting (Week 3): 144 kernels\n",
      "NFL Big Data Bowl: 142 kernels\n",
      "New York City Taxi Fare Prediction: 142 kernels\n",
      "Don't Overfit! II: 141 kernels\n",
      "Dogs vs. Cats Redux: Kernels Edition: 140 kernels\n",
      "Two Sigma: Using News to Predict Stock Movements: 136 kernels\n",
      "Categorical Feature Encoding Challenge: 135 kernels\n",
      "New York City Taxi Trip Duration: 130 kernels\n",
      "Categorical Feature Encoding Challenge II: 129 kernels\n",
      "Severstal: Steel Defect Detection: 120 kernels\n",
      "TMDB Box Office Prediction: 114 kernels\n",
      "Santander Value Prediction Challenge: 114 kernels\n",
      "OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction: 114 kernels\n",
      "Histopathologic Cancer Detection: 113 kernels\n",
      "Deepfake Detection Challenge: 112 kernels\n",
      "Google Analytics Customer Revenue Prediction: 112 kernels\n",
      "Prostate cANcer graDe Assessment (PANDA) Challenge: 110 kernels\n",
      "TGS Salt Identification Challenge: 110 kernels\n",
      "Predicting Molecular Properties: 104 kernels\n",
      "Contradictory, My Dear Watson: 101 kernels\n",
      "CareerCon 2019 - Help Navigate Robots: 101 kernels\n",
      "Humpback Whale Identification: 95 kernels\n",
      "Cornell Birdcall Identification: 94 kernels\n",
      "RANZCR CLiP - Catheter and Line Position Challenge: 83 kernels\n",
      "Tabular Playground Series - Feb 2021: 82 kernels\n",
      "Leaf Classification: 81 kernels\n",
      "Ghouls, Goblins, and Ghosts... Boo!: 77 kernels\n",
      "TReNDS Neuroimaging: 74 kernels\n",
      "Porto Seguro’s Safe Driver Prediction: 74 kernels\n",
      "Lyft Motion Prediction for Autonomous Vehicles: 72 kernels\n",
      "Abstraction and Reasoning Challenge: 72 kernels\n",
      "Rossmann Store Sales: 71 kernels\n",
      "Quick, Draw! Doodle Recognition Challenge: 70 kernels\n",
      "Forest Cover Type Prediction: 69 kernels\n",
      "Dog Breed Identification: 68 kernels\n",
      "Traveling Santa 2018 - Prime Paths: 67 kernels\n",
      "Plant Seedlings Classification: 67 kernels\n",
      "Microsoft Malware Prediction: 65 kernels\n",
      "Two Sigma Connect: Rental Listing Inquiries: 64 kernels\n",
      "San Francisco Crime Classification: 63 kernels\n",
      "Spooky Author Identification: 62 kernels\n",
      "Avito Demand Prediction Challenge: 62 kernels\n",
      "HuBMAP - Hacking the Kidney: 61 kernels\n",
      "RSNA STR Pulmonary Embolism Detection: 57 kernels\n",
      "Bag of Words Meets Bags of Popcorn: 56 kernels\n",
      "Santa's Workshop Tour 2019: 55 kernels\n",
      "Quora Question Pairs: 54 kernels\n",
      "Santander Customer Satisfaction: 54 kernels\n",
      "ALASKA2 Image Steganalysis: 53 kernels\n",
      "Google Landmark Recognition 2020: 53 kernels\n",
      "Rainforest Connection Species Audio Detection: 52 kernels\n",
      "Freesound Audio Tagging 2019: 52 kernels\n",
      "Human Protein Atlas Image Classification: 52 kernels\n",
      "TalkingData AdTracking Fraud Detection Challenge: 51 kernels\n",
      "INGV - Volcanic Eruption Prediction: 49 kernels\n",
      "TensorFlow 2.0 Question Answering: 49 kernels\n",
      "Restaurant Revenue Prediction: 49 kernels\n",
      "DonorsChoose.org Application Screening: 46 kernels\n",
      "Instacart Market Basket Analysis: 45 kernels\n",
      "VSB Power Line Fault Detection: 43 kernels\n",
      "iMet Collection 2019 - FGVC6: 41 kernels\n",
      "Gendered Pronoun Resolution: 41 kernels\n",
      "Otto Group Product Classification Challenge: 41 kernels\n",
      "Google Cloud & NCAA® ML Competition 2020-NCAAM: 41 kernels\n",
      "Understanding Clouds from Satellite Images: 40 kernels\n",
      "Mercedes-Benz Greener Manufacturing: 40 kernels\n",
      "Sentiment Analysis on Movie Reviews: 39 kernels\n",
      "RSNA Pneumonia Detection Challenge: 38 kernels\n",
      "Conway's Reverse Game of Life 2020: 37 kernels\n",
      "What's Cooking?: 37 kernels\n",
      "Facial Keypoints Detection: 36 kernels\n",
      "Recruit Restaurant Visitor Forecasting: 35 kernels\n",
      "Northeastern SMILE Lab - Recognizing Faces in the Wild: 35 kernels\n",
      "M5 Forecasting - Uncertainty: 34 kernels\n",
      "Statoil/C-CORE Iceberg Classifier Challenge: 34 kernels\n",
      "Airbnb New User Bookings: 33 kernels\n",
      "Airbus Ship Detection Challenge: 33 kernels\n",
      "Kobe Bryant Shot Selection: 31 kernels\n",
      "RSNA Intracranial Hemorrhage Detection: 30 kernels\n",
      "Sberbank Russian Housing Market: 30 kernels\n",
      "PLAsTiCC Astronomical Classification: 29 kernels\n",
      "SIIM-ACR Pneumothorax Segmentation: 28 kernels\n",
      "Indoor Location & Navigation: 28 kernels\n",
      "Walmart Recruiting - Store Sales Forecasting: 25 kernels\n",
      "Zillow Prize: Zillow’s Home Value Prediction (Zestimate): 25 kernels\n",
      "Elo Merchant Category Recommendation: 25 kernels\n",
      "Google Cloud & NCAA® ML Competition 2020-NCAAW: 24 kernels\n",
      "Homework for Students: 22 kernels\n",
      "Personality Profile Prediction: 22 kernels\n",
      "Human Protein Atlas - Single Cell Classification: 21 kernels\n",
      "Santander Product Recommendation: 21 kernels\n",
      "Allstate Claims Severity: 21 kernels\n",
      "Recursion Cellular Image Classification: 21 kernels\n",
      "VinBigData Chest X-ray Abnormalities Detection: 21 kernels\n",
      "Don't Get Kicked!: 20 kernels\n",
      "IESB - 2019: 20 kernels\n",
      "Give Me Some Credit: 20 kernels\n",
      "Atividade_3_PMR3508: 20 kernels\n",
      "2019 Data Science Bowl: 19 kernels\n",
      "Предсказание положения космических объектов: 19 kernels\n",
      "MLH - Pokemon Challenge: 19 kernels\n",
      "Google Cloud & NCAA® ML Competition 2019-Men's: 18 kernels\n",
      "iWildCam 2019 - FGVC6: 18 kernels\n",
      "2019  ML competition with KISTI: 18 kernels\n",
      "State Farm Distracted Driver Detection: 18 kernels\n",
      "PMR3508 - Tarefa 1 - 3508 Adult Dataset: 18 kernels\n",
      "Exam for Students20200129: 17 kernels\n",
      "Google Landmark Retrieval 2020: 17 kernels\n",
      "Nomad2018 Predicting Transparent Conductors: 17 kernels\n",
      "PadhAI: Text - Non Text Classification Level 4b: 16 kernels\n",
      "NBA Rookies: 16 kernels\n",
      "Google Cloud & NCAA® ML Competition 2019-Women's: 16 kernels\n",
      "BNP Paribas Cardif Claims Management: 15 kernels\n",
      "Shelter Animal Outcomes: 15 kernels\n",
      "TalkingData Mobile User Demographics: 15 kernels\n",
      "QSTP - Deep Learning 2019: 15 kernels\n",
      "Freesound General-Purpose Audio Tagging Challenge: 14 kernels\n",
      "Predicting Red Hat Business Value: 14 kernels\n",
      "Expedia Hotel Recommendations: 13 kernels\n",
      "Peking University/Baidu - Autonomous Driving: 13 kernels\n",
      "Prudential Life Insurance Assessment: 12 kernels\n",
      "Corporación Favorita Grocery Sales Forecasting: 12 kernels\n",
      "How Much Did It Rain? II: 11 kernels\n",
      "Amazon.com - Employee Access Challenge: 11 kernels\n",
      "PadhAI: Text - Non Text Classification Level 4a: 11 kernels\n",
      "Open Images 2019 - Object Detection: 11 kernels\n",
      "Homesite Quote Conversion: 10 kernels\n",
      "Planet: Understanding the Amazon from Space: 10 kernels\n",
      "I’m Something of a Painter Myself: 10 kernels\n",
      "Eval Lab 2 F464: 10 kernels\n",
      "Diabetes Diagnosis: 10 kernels\n",
      "Cloud Faculty Institute Workshop: 10 kernels\n",
      "Ultrasound Nerve Segmentation: 10 kernels\n",
      "IESB Sul - IGM - Maio 2019: 10 kernels\n",
      "DSNet: fastai Hackathon: 10 kernels\n",
      "YKC-2nd: 9 kernels\n",
      "Predictive Equipment Failures: 9 kernels\n",
      "PadhAI: Hindi Vowel - Consonant Classification: 9 kernels\n",
      "iWildCam 2020 - FGVC7: 9 kernels\n",
      "National Data Science Challenge 2019 - Advanced: 8 kernels\n",
      "Grupo Bimbo Inventory Demand: 8 kernels\n",
      "Home Depot Product Search Relevance: 8 kernels\n",
      "Humpback Whale Identification Challenge: 8 kernels\n",
      "Hash Code Archive - Drone Delivery: 8 kernels\n",
      "AI Academy Intermediate Class Competition 1: 7 kernels\n",
      "ML 4 Money: 7 kernels\n",
      "ASN10e Final Submission - Detect COML Faces: 7 kernels\n",
      "WSDM - KKBox's Music Recommendation Challenge: 7 kernels\n",
      "[Student] Shopee Code League - Product Detection: 6 kernels\n",
      "Google Cloud & NCAA® ML Competition 2018-Men's: 6 kernels\n",
      "IEEE's Signal Processing Society - Camera Model Identification: 6 kernels\n",
      "WSDM - KKBox's Churn Prediction Challenge: 6 kernels\n",
      "Invasive Species Monitoring: 6 kernels\n",
      "Sarcasmo: 6 kernels\n",
      "Seleksi Calon Asisten GAIB: 6 kernels\n",
      "Whose line is it anyway?: 6 kernels\n",
      "IESB Norte - IGM - Maio 2019: 6 kernels\n",
      "Competição DSA de Machine Learning: 6 kernels\n",
      "ISSM2020 AI Challenge: 5 kernels\n",
      "Hackathon Auto_matic: 5 kernels\n",
      "StumbleUpon Evergreen Classification Challenge: 5 kernels\n",
      "Santa Gift Matching Challenge: 5 kernels\n",
      "TrackML Particle Tracking Challenge: 5 kernels\n",
      "Classifying Movie Reviews: 5 kernels\n",
      "WiDS Datathon 2019: 5 kernels\n",
      "Coupon Purchase Prediction: 5 kernels\n",
      "Santa 2019: Revenge of the Accountants: 5 kernels\n",
      "Rock, Paper, Scissors: 5 kernels\n",
      "DM-Assignment 1: 5 kernels\n",
      "hackStat 2.0: 5 kernels\n",
      "Bosch Production Line Performance: 5 kernels\n",
      "Web Enthusiasts' Club NITK Recruitment: 5 kernels\n",
      "Web Traffic Time Series Forecasting: 4 kernels\n",
      "Loan Default Prediction - Imperial College London: 4 kernels\n",
      "Recommender Systems: 4 kernels\n",
      "Outbrain Click Prediction: 4 kernels\n",
      "Google Cloud & NCAA® ML Competition 2018-Women's: 4 kernels\n",
      "car-classification: 4 kernels\n",
      "DS特論2019年度 演習課題2: 4 kernels\n",
      "DL in NLP Spring 2019. Classification: 4 kernels\n",
      "The Nature Conservancy Fisheries Monitoring: 4 kernels\n",
      "Text classification: 4 kernels\n",
      "Predicting a Biological Response: 4 kernels\n",
      "ML in biology: 4 kernels\n",
      "Google Landmark Recognition Challenge: 4 kernels\n",
      "Property price prediction challenge: 4 kernels\n",
      "UI DS Summer School: 4 kernels\n",
      "Lyft 3D Object Detection for Autonomous Vehicles: 4 kernels\n",
      "Hackathon Sentimento: 4 kernels\n",
      "Telstra Network Disruptions: 4 kernels\n",
      "AILAB ML Training #1: 4 kernels\n",
      "Basic Regression Competition: 3 kernels\n",
      "Halite by Two Sigma: 3 kernels\n",
      "Python for Data science ITEA: 3 kernels\n",
      "I-RICH ML COMPETITION: 3 kernels\n",
      "Fake News e ML: 3 kernels\n",
      "Facebook V: Predicting Check Ins: 3 kernels\n",
      "Market Basket - ID NDSC 2020: 3 kernels\n",
      "Click-Through Rate Prediction: 3 kernels\n",
      "PadhAI: Tamil Vowel - Consonant Classification: 3 kernels\n",
      "Hackathon Sentimento_v2: 3 kernels\n",
      "SERPRO - Abalone: 3 kernels\n",
      "VSU ML 1 Regression: 3 kernels\n",
      "Personalized Medicine: Redefining Cancer Treatment: 3 kernels\n",
      "Sentiment Analysis in Russian: 3 kernels\n",
      "Multiple regression for time series data: 3 kernels\n",
      "Brain Cancer Classification: 3 kernels\n",
      "[ACM] Recommender System Practice: 3 kernels\n",
      "NFL 1st and Future - Impact Detection: 3 kernels\n",
      "Santa's Uncertain Bags: 3 kernels\n",
      "Used Cars Price Prediction: 3 kernels\n",
      "COVID-19 diagnostic: 2 kernels\n",
      "Predict the Income - <DECODE> WITH BOARD: 2 kernels\n",
      "Pycon Korea 2018 - Tutorial: 2 kernels\n",
      "Regression Evaluative Lab: 2 kernels\n",
      "OCRV Test Task: 2 kernels\n",
      "notMNIST Competition: 2 kernels\n",
      "[Open] Shopee Code League - Logistics: 2 kernels\n",
      "Conway's Reverse Game of Life: 2 kernels\n",
      "Higgs Boson Machine Learning Challenge: 2 kernels\n",
      "empty: 2 kernels\n",
      "MLClass Dubai by ODS, Lecture 6 HW: 2 kernels\n",
      "Классификация изображений: 2 kernels\n",
      "Texts classification: 2 kernels\n",
      "ML Challenge: 2 kernels\n",
      "West Nile Virus Prediction: 2 kernels\n",
      "VietAI Advance Course - Retinal Disease Detection: 2 kernels\n",
      "Characters classification: 2 kernels\n",
      "House pricing: 2 kernels\n",
      "UCI-HAR: 2 kernels\n",
      "Schnell-Mal-Klassifizieren: 2 kernels\n",
      "CIFAR-10 - Object Recognition in Images: 2 kernels\n",
      "SERPRO - Iris: 2 kernels\n",
      "ClassificationOFShields: 2 kernels\n",
      "Задержка рейса самолета: 2 kernels\n",
      "Pneumonia Diagnosis: 2 kernels\n",
      "UMUC DATA 650 Summer 2019 Competition: 2 kernels\n",
      "Птица или самолет: 2 kernels\n",
      "House Price Prediction: 2 kernels\n",
      "Machine Learning Lab - CAS Data Science FS 20: 2 kernels\n",
      "Tap30 Challenge: 2 kernels\n",
      "Data Champions Android App Malware Prediction: 2 kernels\n",
      "Diabetes Classification: 2 kernels\n",
      "IES Data Mining(WS 18/19): 2 kernels\n",
      "Language Identification: 2 kernels\n",
      "March Machine Learning Mania 2016: 2 kernels\n",
      "2018 Data Science Bowl: 2 kernels\n",
      "CSC: HW4 spring19: 2 kernels\n",
      "YKC-cup-1st: 2 kernels\n",
      "Digit Classification DL Workshop: 2 kernels\n",
      "Influencers in Social Networks: 2 kernels\n",
      "AILAB ML Training #0: 2 kernels\n",
      "Open Images 2019 - Instance Segmentation: 1 kernels\n",
      "Find me that fish: 1 kernels\n",
      "Painter by Numbers: 1 kernels\n",
      "Avito Duplicate Ads Detection: 1 kernels\n",
      "Diabetic Retinopathy Detection: 1 kernels\n",
      "Summer Analytics 2020 Capstone Project: 1 kernels\n",
      "Similarity Search Project: 1 kernels\n",
      "Galaxy Zoo - The Galaxy Challenge: 1 kernels\n",
      "Flavours of Physics: Finding τ  →  μμμ: 1 kernels\n",
      "Cdiscount’s Image Classification Challenge: 1 kernels\n",
      "DL for exploration geophysics: 1 kernels\n",
      "Liberty Mutual Group: Property Inspection Prediction: 1 kernels\n",
      "Data Series Summarization Project (v3): 1 kernels\n",
      "Car loan default: 1 kernels\n",
      "KNIT_HACKS: 1 kernels\n",
      "Carvana Image Masking Challenge: 1 kernels\n",
      "Data Science - Master: 1 kernels\n",
      "Oxford Fast AI Week 2: 1 kernels\n",
      "[T] PadhAI: Text - Non Text Classification Level 1: 1 kernels\n",
      "EPAM: Exercise 1 -  Sentiment Analysis: 1 kernels\n",
      "UC Irvine Math 10 Winter 2020: 1 kernels\n",
      "The Kaggle Master: 1 kernels\n",
      "Competencia-Series-Temporales: 1 kernels\n",
      "SQL Saturday Madrid ML Challenge: 1 kernels\n",
      "IA1819: 1 kernels\n",
      "DeepNLP HSE Course: 1 kernels\n",
      "BU CS506 Spring 2020 Midterm: 1 kernels\n",
      "Focus start 2020: 1 kernels\n",
      "Predicting Age Groups: 1 kernels\n",
      "ML Hackathon 2019 Q1: 1 kernels\n",
      "Tobigs13_7week_competition: 1 kernels\n",
      "Pitch estimation and voicing detection: 1 kernels\n",
      "COMP 750/850 Project 1: 1 kernels\n",
      "Tweet Sentiment Analysis: 1 kernels\n",
      "Fashion MNIST challenge2019: 1 kernels\n",
      "2019 SMHRD 경진대회 ( 지능형 ): 1 kernels\n",
      "SkillFactory | Final hackathon: 1 kernels\n",
      "SMEMI309 - Final evaluation challenge 2020: 1 kernels\n",
      "Predict the missing pixel value v2: 1 kernels\n",
      "Tracy Regression: 1 kernels\n",
      "Japanese Review Rating Prediction: 1 kernels\n",
      "SYDE 522 (Winter 2020): 1 kernels\n",
      "HEROZ Internal Competition: 1 kernels\n",
      "Классификация компьютерных атак: 1 kernels\n",
      "JAMP Hackathon Drive 1: 1 kernels\n",
      "AS-bow-2019-2020: 1 kernels\n",
      "NTUST: Information Retrieval and Applications: 1 kernels\n",
      "TensorFlow Speech Recognition Challenge: 1 kernels\n",
      "Springleaf Marketing Response: 1 kernels\n",
      "Hungry Geese: 1 kernels\n",
      "Gallivanters: 1 kernels\n",
      "Cats vs Dogs vs More: 1 kernels\n",
      "Aesthetic Visual Analysis: 1 kernels\n",
      "University of applied sciences Mannheim: 1 kernels\n",
      "CSM/SEM6420 workshop: 1 kernels\n",
      "KaggleDays Paris: 1 kernels\n",
      "2019S UTS Data Analytics Assignment 3: 1 kernels\n",
      "EC524: Heart-disease classification: 1 kernels\n",
      "Predição de Churn: 1 kernels\n",
      "IEEE PES BDC DataThon , Year-2020: 1 kernels\n",
      "Птица или самолет?: 1 kernels\n",
      "AI for Clinical Data Analytics HW2: 1 kernels\n",
      "Data Mining Lab2: 1 kernels\n",
      "Predicting user conversions: 1 kernels\n",
      "Avaliação de Carros: 1 kernels\n",
      "InClass Competition at Tokyo Metropolitan Univ.: 1 kernels\n",
      "Bad comments: 1 kernels\n",
      "Хакатон от Кафедры ТПСТС МФТИ (level 1): 1 kernels\n",
      "Who is a Friend?: 1 kernels\n",
      "Chh-OLA: 1 kernels\n",
      "Flatiron School: 1 kernels\n",
      "NCTU BDALAB 2020 Onboard: 1 kernels\n",
      "Night at Cameo: 1 kernels\n",
      "Time Series Classification: 1 kernels\n",
      "House Sales : 1 kernels\n",
      "Car Classification(Project Vision): 1 kernels\n",
      "Great Energy Predictor Shootout I: 1 kernels\n",
      "Pneumonia Texture Analysis: 1 kernels\n",
      "MLDM Classification Competition: 1 kernels\n",
      "Korean Gender Bias Detection: 1 kernels\n",
      "Einfhrung in Kaggle InClass Competitions</title>: 1 kernels\n",
      "GL Hack: Landmarks: 1 kernels\n",
      "[DM&PR WS19/20] Machine learning competition: 1 kernels\n",
      "iFood - 2019 at FGVC6: 1 kernels\n",
      "Fashion MNIST-ITBA-LAB 2020: 1 kernels\n",
      "finec-1941-hw6: 1 kernels\n",
      "Word vectors: 1 kernels\n",
      "compass-canada: 1 kernels\n",
      "DMA Kaggle Challenge: 1 kernels\n",
      "DSI-US-8 Project 2 Regression Challenge: 1 kernels\n",
      "Cleaned vs Dirty: 1 kernels\n",
      "Penyisihan Datavidia 2019: 1 kernels\n",
      "Fashion MNIST challenge201907: 1 kernels\n",
      "ML Hackathon 2019 Q2: 1 kernels\n",
      "Data-Driven Business Analytics: 1 kernels\n",
      "Traffic signs classification: 1 kernels\n",
      "IIITB ML Project: SFO Crime Classification: 1 kernels\n",
      "Temperature Forecasting: 1 kernels\n",
      "[DM&PR WS18/19] Machine learning competition: 1 kernels\n",
      "Kharagpur Data Analytics Group: 1 kernels\n",
      "Анализ потребительской корзины: 1 kernels\n",
      "Heart Disease Prediction: 1 kernels\n",
      "NMLO Contest 3 - Regression: 1 kernels\n",
      "ods_class_cs231n: 1 kernels\n",
      "Predice el futuro: 1 kernels\n",
      "ACM Summer'19 Inclass-1: 1 kernels\n",
      "ACM Machine Learning (SVNIT): 1 kernels\n",
      "HTA Tagging: 1 kernels\n",
      "Oracle Graph ML Contest at Polimi: 1 kernels\n",
      "GirlsGoIT competition 2020: 1 kernels\n",
      "FIA ML T5: 1 kernels\n",
      "Bike Sharing Demand for Education(): 1 kernels\n",
      "Challenge GH: 1 kernels\n",
      "Python Class - Practice: 1 kernels\n",
      "Test Competition Please Ignore: 1 kernels\n",
      "Movie Genre Classification: 1 kernels\n",
      "kaggle18011884: 1 kernels\n",
      "Sentence Relatedness: 1 kernels\n",
      "Team ISTE's Datathon: 1 kernels\n",
      "Anokha AI Adept: 1 kernels\n",
      "HEROZ Internal Competition Extra2: 1 kernels\n",
      "Machine Learning Lab - CAS Data Science HS 20: 1 kernels\n",
      "Digit recognition: 1 kernels\n",
      "Computational Intelligence Project: 1 kernels\n",
      "Technidus machine learning competition 2: 1 kernels\n",
      "Fieldguide Challenge: Moths & Butterflies: 1 kernels\n",
      "Multi-label Bird Species Classification - NIPS 2013: 1 kernels\n"
     ]
    }
   ],
   "source": [
    "kernels_count = kernels_meta.groupby('comp_name').size()\n",
    "n_kernels_per_comp = kernels_meta['comp_name'].value_counts()\n",
    "N_HIGHEST_COMP = 4\n",
    "top_comp = kernels_count.nlargest(N_HIGHEST_COMP).index[N_HIGHEST_COMP-1]\n",
    "print(f\"Competition with most kernels: '{top_comp}'  ({kernels_count.nlargest(N_HIGHEST_COMP).iloc[N_HIGHEST_COMP-1]} kernels)\\n\")\n",
    "for name in n_kernels_per_comp.index:\n",
    "    print(f\"{name}: {n_kernels_per_comp[name]} kernels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_blocks_index</th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9741</td>\n",
       "      <td>14113092</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9742</td>\n",
       "      <td>14113092</td>\n",
       "      <td>1</td>\n",
       "      <td>train = pd.read_csv(\"../input/nlp-getting-star...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9743</td>\n",
       "      <td>14113092</td>\n",
       "      <td>2</td>\n",
       "      <td>def lowercase_text(text):\\n    return text.low...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9744</td>\n",
       "      <td>14113092</td>\n",
       "      <td>3</td>\n",
       "      <td>import re\\nimport string\\ndef remove_noise(tex...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9745</td>\n",
       "      <td>14113092</td>\n",
       "      <td>4</td>\n",
       "      <td># Tokenizing the training and the test set\\nim...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22621</th>\n",
       "      <td>2462840</td>\n",
       "      <td>10038839</td>\n",
       "      <td>108</td>\n",
       "      <td>#train BERT\\nhistory_bert = BERT_large.fit([tr...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22622</th>\n",
       "      <td>2462841</td>\n",
       "      <td>10038839</td>\n",
       "      <td>109</td>\n",
       "      <td>#load model with best losses\\nBERT_large.load_...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22623</th>\n",
       "      <td>2462842</td>\n",
       "      <td>10038839</td>\n",
       "      <td>110</td>\n",
       "      <td>#save as dataframe\\nsubmission_bert = pd.DataF...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22624</th>\n",
       "      <td>2462843</td>\n",
       "      <td>10038839</td>\n",
       "      <td>111</td>\n",
       "      <td>#and last but not least, submit\\nsubmission_be...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22625</th>\n",
       "      <td>2464127</td>\n",
       "      <td>9100265</td>\n",
       "      <td>0</td>\n",
       "      <td>import pandas as pd # data processing, CSV fil...</td>\n",
       "      <td>/moradnejad/distweets-perfect-score-for-evalua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22626 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       code_blocks_index  kernel_id  code_block_id  \\\n",
       "0                   9741   14113092              0   \n",
       "1                   9742   14113092              1   \n",
       "2                   9743   14113092              2   \n",
       "3                   9744   14113092              3   \n",
       "4                   9745   14113092              4   \n",
       "...                  ...        ...            ...   \n",
       "22621            2462840   10038839            108   \n",
       "22622            2462841   10038839            109   \n",
       "22623            2462842   10038839            110   \n",
       "22624            2462843   10038839            111   \n",
       "22625            2464127    9100265              0   \n",
       "\n",
       "                                              code_block  \\\n",
       "0      # This Python 3 environment comes with many he...   \n",
       "1      train = pd.read_csv(\"../input/nlp-getting-star...   \n",
       "2      def lowercase_text(text):\\n    return text.low...   \n",
       "3      import re\\nimport string\\ndef remove_noise(tex...   \n",
       "4      # Tokenizing the training and the test set\\nim...   \n",
       "...                                                  ...   \n",
       "22621  #train BERT\\nhistory_bert = BERT_large.fit([tr...   \n",
       "22622  #load model with best losses\\nBERT_large.load_...   \n",
       "22623  #save as dataframe\\nsubmission_bert = pd.DataF...   \n",
       "22624  #and last but not least, submit\\nsubmission_be...   \n",
       "22625  import pandas as pd # data processing, CSV fil...   \n",
       "\n",
       "                                             kernel_link  \n",
       "0                     /nicknosorogov/distweetrhinosceros  \n",
       "1                     /nicknosorogov/distweetrhinosceros  \n",
       "2                     /nicknosorogov/distweetrhinosceros  \n",
       "3                     /nicknosorogov/distweetrhinosceros  \n",
       "4                     /nicknosorogov/distweetrhinosceros  \n",
       "...                                                  ...  \n",
       "22621  /tuckerarrants/disaster-tweets-eda-glove-rnns-...  \n",
       "22622  /tuckerarrants/disaster-tweets-eda-glove-rnns-...  \n",
       "22623  /tuckerarrants/disaster-tweets-eda-glove-rnns-...  \n",
       "22624  /tuckerarrants/disaster-tweets-eda-glove-rnns-...  \n",
       "22625  /moradnejad/distweets-perfect-score-for-evalua...  \n",
       "\n",
       "[22626 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels = kernels_meta[kernels_meta['comp_name'] == top_comp]\n",
    "top_comp_code_blocks = code_blocks[code_blocks['kernel_id'].isin(kernels['kernel_id'])]\n",
    "merged_df = pd.merge(top_comp_code_blocks, kernels_meta, on='kernel_id')\n",
    "top_comp_code_blocks = merged_df.drop(columns=['kaggle_score', 'kaggle_comments', 'kaggle_upvotes', 'comp_name'])\n",
    "top_comp_code_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "      <th>predicted_graph_vertex__probability</th>\n",
       "      <th>graph_vertex_id</th>\n",
       "      <th>graph_vertex_class</th>\n",
       "      <th>graph_vertex_subclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14113092</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>88</td>\n",
       "      <td>Debug</td>\n",
       "      <td>list_files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14113092</td>\n",
       "      <td>1</td>\n",
       "      <td>train = pd.read_csv(\"../input/nlp-getting-star...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.999676</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14113092</td>\n",
       "      <td>2</td>\n",
       "      <td>def lowercase_text(text):\\n    return text.low...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.922104</td>\n",
       "      <td>20</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>categorify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14113092</td>\n",
       "      <td>3</td>\n",
       "      <td>import re\\nimport string\\ndef remove_noise(tex...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.751464</td>\n",
       "      <td>20</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>categorify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14113092</td>\n",
       "      <td>4</td>\n",
       "      <td># Tokenizing the training and the test set\\nim...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.923835</td>\n",
       "      <td>8</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>feature_engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22621</th>\n",
       "      <td>10038839</td>\n",
       "      <td>108</td>\n",
       "      <td>#train BERT\\nhistory_bert = BERT_large.fit([tr...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>7</td>\n",
       "      <td>Model_Train</td>\n",
       "      <td>train_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22622</th>\n",
       "      <td>10038839</td>\n",
       "      <td>109</td>\n",
       "      <td>#load model with best losses\\nBERT_large.load_...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.984101</td>\n",
       "      <td>48</td>\n",
       "      <td>Model_Evaluation</td>\n",
       "      <td>predict_on_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22623</th>\n",
       "      <td>10038839</td>\n",
       "      <td>110</td>\n",
       "      <td>#save as dataframe\\nsubmission_bert = pd.DataF...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.845476</td>\n",
       "      <td>55</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>prepare_output</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22624</th>\n",
       "      <td>10038839</td>\n",
       "      <td>111</td>\n",
       "      <td>#and last but not least, submit\\nsubmission_be...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.998979</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22625</th>\n",
       "      <td>9100265</td>\n",
       "      <td>0</td>\n",
       "      <td>import pandas as pd # data processing, CSV fil...</td>\n",
       "      <td>/moradnejad/distweets-perfect-score-for-evalua...</td>\n",
       "      <td>0.998242</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22626 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kernel_id  code_block_id  \\\n",
       "0       14113092              0   \n",
       "1       14113092              1   \n",
       "2       14113092              2   \n",
       "3       14113092              3   \n",
       "4       14113092              4   \n",
       "...          ...            ...   \n",
       "22621   10038839            108   \n",
       "22622   10038839            109   \n",
       "22623   10038839            110   \n",
       "22624   10038839            111   \n",
       "22625    9100265              0   \n",
       "\n",
       "                                              code_block  \\\n",
       "0      # This Python 3 environment comes with many he...   \n",
       "1      train = pd.read_csv(\"../input/nlp-getting-star...   \n",
       "2      def lowercase_text(text):\\n    return text.low...   \n",
       "3      import re\\nimport string\\ndef remove_noise(tex...   \n",
       "4      # Tokenizing the training and the test set\\nim...   \n",
       "...                                                  ...   \n",
       "22621  #train BERT\\nhistory_bert = BERT_large.fit([tr...   \n",
       "22622  #load model with best losses\\nBERT_large.load_...   \n",
       "22623  #save as dataframe\\nsubmission_bert = pd.DataF...   \n",
       "22624  #and last but not least, submit\\nsubmission_be...   \n",
       "22625  import pandas as pd # data processing, CSV fil...   \n",
       "\n",
       "                                             kernel_link  \\\n",
       "0                     /nicknosorogov/distweetrhinosceros   \n",
       "1                     /nicknosorogov/distweetrhinosceros   \n",
       "2                     /nicknosorogov/distweetrhinosceros   \n",
       "3                     /nicknosorogov/distweetrhinosceros   \n",
       "4                     /nicknosorogov/distweetrhinosceros   \n",
       "...                                                  ...   \n",
       "22621  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22622  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22623  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22624  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22625  /moradnejad/distweets-perfect-score-for-evalua...   \n",
       "\n",
       "       predicted_graph_vertex__probability  graph_vertex_id  \\\n",
       "0                                 0.999220               88   \n",
       "1                                 0.999676               45   \n",
       "2                                 0.922104               20   \n",
       "3                                 0.751464               20   \n",
       "4                                 0.923835                8   \n",
       "...                                    ...              ...   \n",
       "22621                             0.999671                7   \n",
       "22622                             0.984101               48   \n",
       "22623                             0.845476               55   \n",
       "22624                             0.998979               25   \n",
       "22625                             0.998242               25   \n",
       "\n",
       "      graph_vertex_class graph_vertex_subclass  \n",
       "0                  Debug            list_files  \n",
       "1        Data_Extraction         load_from_csv  \n",
       "2         Data_Transform            categorify  \n",
       "3         Data_Transform            categorify  \n",
       "4         Data_Transform   feature_engineering  \n",
       "...                  ...                   ...  \n",
       "22621        Model_Train           train_model  \n",
       "22622   Model_Evaluation       predict_on_test  \n",
       "22623        Data_Export        prepare_output  \n",
       "22624        Data_Export           save_to_csv  \n",
       "22625        Data_Export           save_to_csv  \n",
       "\n",
       "[22626 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(top_comp_code_blocks.merge(data_preds, on='code_blocks_index'), vertices, left_on='predicted_graph_vertex_id', right_on='graph_vertex_id')\n",
    "merged_df.drop(['code_blocks_index', 'predicted_graph_vertex_id'], axis=1, inplace=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "725"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_kernel_links = kernels['kernel_link'].unique()\n",
    "len(distinct_kernel_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "      <th>predicted_graph_vertex__probability</th>\n",
       "      <th>graph_vertex_id</th>\n",
       "      <th>graph_vertex_class</th>\n",
       "      <th>graph_vertex_subclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>22956883</td>\n",
       "      <td>0</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nfrom ...</td>\n",
       "      <td>/renraeldab/disaster-tweets</td>\n",
       "      <td>0.999302</td>\n",
       "      <td>22</td>\n",
       "      <td>Environment</td>\n",
       "      <td>import_modules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22956883</td>\n",
       "      <td>1</td>\n",
       "      <td>train = pd.read_csv('/kaggle/input/nlp-getting...</td>\n",
       "      <td>/renraeldab/disaster-tweets</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>22956883</td>\n",
       "      <td>2</td>\n",
       "      <td>def filter_text(df):\\n    df['text']=df['text'...</td>\n",
       "      <td>/renraeldab/disaster-tweets</td>\n",
       "      <td>0.995599</td>\n",
       "      <td>8</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>feature_engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>22956883</td>\n",
       "      <td>3</td>\n",
       "      <td>sw=['the', 'a', 'an', 'in', 'on', 'with', 'by'...</td>\n",
       "      <td>/renraeldab/disaster-tweets</td>\n",
       "      <td>0.997215</td>\n",
       "      <td>77</td>\n",
       "      <td>Other</td>\n",
       "      <td>define_variables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>22956883</td>\n",
       "      <td>4</td>\n",
       "      <td>v = CountVectorizer(stop_words=sw)\\ntrain_v = ...</td>\n",
       "      <td>/renraeldab/disaster-tweets</td>\n",
       "      <td>0.997267</td>\n",
       "      <td>8</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>feature_engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>22956883</td>\n",
       "      <td>5</td>\n",
       "      <td>y = train['target']</td>\n",
       "      <td>/renraeldab/disaster-tweets</td>\n",
       "      <td>0.999064</td>\n",
       "      <td>21</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>prepare_x_and_y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>22956883</td>\n",
       "      <td>6</td>\n",
       "      <td>clf = MultinomialNB()\\nscorer = make_scorer(f1...</td>\n",
       "      <td>/renraeldab/disaster-tweets</td>\n",
       "      <td>0.983758</td>\n",
       "      <td>6</td>\n",
       "      <td>Hyperparam_Tuning</td>\n",
       "      <td>train_on_grid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>22956883</td>\n",
       "      <td>7</td>\n",
       "      <td>y = model.predict(test_v)\\noutput = pd.DataFra...</td>\n",
       "      <td>/renraeldab/disaster-tweets</td>\n",
       "      <td>0.999141</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kernel_id  code_block_id  \\\n",
       "43   22956883              0   \n",
       "44   22956883              1   \n",
       "45   22956883              2   \n",
       "46   22956883              3   \n",
       "47   22956883              4   \n",
       "48   22956883              5   \n",
       "49   22956883              6   \n",
       "50   22956883              7   \n",
       "\n",
       "                                           code_block  \\\n",
       "43  import numpy as np\\nimport pandas as pd\\nfrom ...   \n",
       "44  train = pd.read_csv('/kaggle/input/nlp-getting...   \n",
       "45  def filter_text(df):\\n    df['text']=df['text'...   \n",
       "46  sw=['the', 'a', 'an', 'in', 'on', 'with', 'by'...   \n",
       "47  v = CountVectorizer(stop_words=sw)\\ntrain_v = ...   \n",
       "48                                y = train['target']   \n",
       "49  clf = MultinomialNB()\\nscorer = make_scorer(f1...   \n",
       "50  y = model.predict(test_v)\\noutput = pd.DataFra...   \n",
       "\n",
       "                    kernel_link  predicted_graph_vertex__probability  \\\n",
       "43  /renraeldab/disaster-tweets                             0.999302   \n",
       "44  /renraeldab/disaster-tweets                             0.999751   \n",
       "45  /renraeldab/disaster-tweets                             0.995599   \n",
       "46  /renraeldab/disaster-tweets                             0.997215   \n",
       "47  /renraeldab/disaster-tweets                             0.997267   \n",
       "48  /renraeldab/disaster-tweets                             0.999064   \n",
       "49  /renraeldab/disaster-tweets                             0.983758   \n",
       "50  /renraeldab/disaster-tweets                             0.999141   \n",
       "\n",
       "    graph_vertex_id graph_vertex_class graph_vertex_subclass  \n",
       "43               22        Environment        import_modules  \n",
       "44               45    Data_Extraction         load_from_csv  \n",
       "45                8     Data_Transform   feature_engineering  \n",
       "46               77              Other      define_variables  \n",
       "47                8     Data_Transform   feature_engineering  \n",
       "48               21     Data_Transform       prepare_x_and_y  \n",
       "49                6  Hyperparam_Tuning         train_on_grid  \n",
       "50               25        Data_Export           save_to_csv  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_kernel_link = '/nicknosorogov/distweetrhinosceros'\n",
    "filtered_df = merged_df[merged_df['kernel_link'] == distinct_kernel_links[3]].sort_values('code_block_id', ascending=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dir = \"../../data/test_datasets\"\n",
    "merged_df.to_csv(f'{test_dataset_dir}/{top_comp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual cell for saving the classified notebooks in a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:04<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error pulling kernel: /andrej0marinchenko/nlp-with-disaster-tweets-new-b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metadata': {'kernelspec': {'language': 'python',\n",
       "   'display_name': 'Python 3',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'pygments_lexer': 'ipython3',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'version': '3.6.4',\n",
       "   'file_extension': '.py',\n",
       "   'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'name': 'python',\n",
       "   'mimetype': 'text/x-python'}},\n",
       " 'nbformat_minor': 4,\n",
       " 'nbformat': 4,\n",
       " 'cells': [{'cell_type': 'code',\n",
       "   'source': 'import numpy as np \\nimport pandas as pd \\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom wordcloud import WordCloud\\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\\n\\n#sklearn \\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils.class_weight import compute_sample_weight\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\nfrom sklearn.feature_extraction.text import TfidfTransformer\\nfrom sklearn.metrics import accuracy_score, confusion_matrix\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import SGDClassifier\\n\\n# nlp preprocessing lib\\nimport gensim\\nfrom gensim.utils import simple_preprocess\\nfrom gensim.parsing.preprocessing import STOPWORDS\\nimport string \\npunctation = string.punctuation',\n",
       "   'metadata': {'_uuid': '8f2839f25d086af736a60e9eeb907d3b93b6e0e5',\n",
       "    '_cell_guid': 'b1076dfc-b9ad-4769-8c92-a6c4dae69d19',\n",
       "    'execution': {'iopub.status.busy': '2021-12-05T16:16:55.748647Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:55.748973Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:56.96196Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:55.748919Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:56.961193Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Environment',\n",
       "    'subclass': 'import_modules',\n",
       "    'subclass_id': 22,\n",
       "    'predicted_subclass_probability': 0.99901414,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")',\n",
       "   'metadata': {'_uuid': 'd629ff2d2480ee46fbb7e2d37f6b5fab8052498a',\n",
       "    '_cell_guid': '79c7e3d0-c299-4dcb-8224-4455121ee9b0',\n",
       "    'execution': {'iopub.status.busy': '2021-12-05T16:16:56.963548Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:56.963855Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:57.001781Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:56.963798Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:57.001119Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Extraction',\n",
       "    'subclass': 'load_from_csv',\n",
       "    'subclass_id': 45,\n",
       "    'predicted_subclass_probability': 0.99975425,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'markdown',\n",
       "   'source': \"### EDA\\n\\nLet's Explore our data..\",\n",
       "   'metadata': {}},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'train_df.head()',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:58.000369Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:58.000655Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:58.018194Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:58.000605Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:58.017122Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_table',\n",
       "    'subclass_id': 41,\n",
       "    'predicted_subclass_probability': 0.9997545,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"train_df = train_df.drop(['id', 'keyword', 'location'], axis = 1)\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:58.162114Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:58.162346Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:58.168013Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:58.162302Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:58.167271Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'drop_column',\n",
       "    'subclass_id': 10,\n",
       "    'predicted_subclass_probability': 0.9992505,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'train_df.shape',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:58.315099Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:58.315339Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:58.31996Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:58.315297Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:58.319269Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_shape',\n",
       "    'subclass_id': 58,\n",
       "    'predicted_subclass_probability': 0.9995821,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'train_df.columns',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:58.599402Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:58.599645Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:58.605338Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:58.599601Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:58.60456Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_columns',\n",
       "    'subclass_id': 71,\n",
       "    'predicted_subclass_probability': 0.9984144,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'train_df.info()',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:58.613963Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:58.614178Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:58.622945Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:58.614136Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:58.621888Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_table_attributes',\n",
       "    'subclass_id': 40,\n",
       "    'predicted_subclass_probability': 0.9993624,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'train_df.describe()',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:58.741931Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:58.742191Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:58.761592Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:58.742136Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:58.760866Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_table_attributes',\n",
       "    'subclass_id': 40,\n",
       "    'predicted_subclass_probability': 0.9994492,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'train_df[train_df[\"target\"] == 1][\"text\"].values[0]',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:58.90265Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:58.902903Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:58.912447Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:58.902859Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:58.911349Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_table',\n",
       "    'subclass_id': 41,\n",
       "    'predicted_subclass_probability': 0.95915043,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'train_df[train_df[\"target\"] == 1][\"text\"].values[1]',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:59.054724Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:59.054974Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:59.065416Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:59.054931Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:59.064666Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_table',\n",
       "    'subclass_id': 41,\n",
       "    'predicted_subclass_probability': 0.5788779,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'print(\"Number of duplicates in data : {}\".format(len(train_df[train_df.duplicated()])))',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:59.433068Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:59.433331Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:59.445806Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:59.433283Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:59.445171Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'EDA',\n",
       "    'subclass': 'count_duplicates',\n",
       "    'subclass_id': 38,\n",
       "    'predicted_subclass_probability': 0.8543922,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'print(\"Duplicated rows before remove them : \")\\ntrain_df[train_df.duplicated(keep=False)].sort_values(by=\"text\").head(8)',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:59.448782Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:59.448995Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:59.467426Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:59.448955Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:59.466679Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'EDA',\n",
       "    'subclass': 'count_duplicates',\n",
       "    'subclass_id': 38,\n",
       "    'predicted_subclass_probability': 0.859677,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': '#remove duplicated rows\\ntrain_df.drop_duplicates(inplace=True)',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:59.584508Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:59.58473Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:59.594558Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:59.584687Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:59.593708Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'remove_duplicates',\n",
       "    'subclass_id': 19,\n",
       "    'predicted_subclass_probability': 0.8869491,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'print(\"Number of duplicates in data : {}\".format(len(train_df[train_df.duplicated()])))',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:59.806891Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:59.807119Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:16:59.818813Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:59.807077Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:59.817883Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'EDA',\n",
       "    'subclass': 'count_duplicates',\n",
       "    'subclass_id': 38,\n",
       "    'predicted_subclass_probability': 0.8543922,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"train_df['target'].value_counts()\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:16:59.992166Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:16:59.992406Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:00.000485Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:16:59.992362Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:16:59.999494Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'EDA',\n",
       "    'subclass': 'count_values',\n",
       "    'subclass_id': 72,\n",
       "    'predicted_subclass_probability': 0.9995184,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': '# count plot \"Histogram\" of Frequencies of Subjects for true news\\nplt.figure(figsize=(10,6))\\nplt.title(\"Frequencies of tweets for Disaster\")\\nsns.countplot(x = \\'target\\', data = train_df)\\nplt.xlabel(\\'Disaster Type\\')',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:00.191495Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:00.191738Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:00.409702Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:00.191691Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:00.408897Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Visualization',\n",
       "    'subclass': 'distribution',\n",
       "    'subclass_id': 33,\n",
       "    'predicted_subclass_probability': 0.9293306,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"Real_Disaster_df = train_df[train_df['target'] == 1]\\nReal_Disaster_df.head()\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:00.643514Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:00.643773Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:00.655147Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:00.643714Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:00.654275Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_table',\n",
       "    'subclass_id': 41,\n",
       "    'predicted_subclass_probability': 0.5334526,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"Not_Real_Disaster_df = train_df[train_df['target'] == 0]\\nNot_Real_Disaster_df.head()\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:00.657002Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:00.657407Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:00.669227Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:00.65724Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:00.668464Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'filter',\n",
       "    'subclass_id': 14,\n",
       "    'predicted_subclass_probability': 0.5007087,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"Real_Disaster_text = ' '.join(Real_Disaster_df.text.tolist())\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:00.793286Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:00.793509Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:00.798116Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:00.793466Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:00.797042Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'string_transform',\n",
       "    'subclass_id': 78,\n",
       "    'predicted_subclass_probability': 0.9610504,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'wordcloud_true = WordCloud().generate(Real_Disaster_text)\\nplt.figure(figsize=(10,10))\\nplt.imshow(wordcloud_true)\\nplt.axis(\\'off\\')\\nplt.title(\"Word Cloud of Real Disaster news\")\\nplt.tight_layout(pad=0)\\nplt.show()',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:01.026292Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:01.026542Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:01.775788Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:01.026497Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:01.774916Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Visualization',\n",
       "    'subclass': 'distribution',\n",
       "    'subclass_id': 33,\n",
       "    'predicted_subclass_probability': 0.96466434,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"Not_Real_Disaster_text = ' '.join(Not_Real_Disaster_df.text.tolist())\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:03.113388Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:03.113668Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:03.118719Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:03.113618Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:03.117544Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'string_transform',\n",
       "    'subclass_id': 78,\n",
       "    'predicted_subclass_probability': 0.9576982,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'wordcloud_true = WordCloud().generate(Not_Real_Disaster_text)\\nplt.figure(figsize=(10,10))\\nplt.imshow(wordcloud_true)\\nplt.axis(\\'off\\')\\nplt.title(\"Word Cloud of Not RealDisaster twittes\")\\nplt.tight_layout(pad=0)\\nplt.show()\\n',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:04.058122Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:04.058411Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:04.912536Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:04.058362Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:04.91172Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Visualization',\n",
       "    'subclass': 'distribution',\n",
       "    'subclass_id': 33,\n",
       "    'predicted_subclass_probability': 0.9845956,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'markdown',\n",
       "   'source': '### Text Preprocessing',\n",
       "   'metadata': {}},\n",
       "  {'cell_type': 'code',\n",
       "   'source': '# take text and preprocess \\'remove stopwords [a, the, and, thus, ... etc] and punctations[,%$ ..etc] and len of text less than 3\\' \\ndef clean_text(text):\\n    \"\"\"\\n        text: a string \\n        return: cleaned string\\n    \"\"\"\\n    result = []\\n    for token in simple_preprocess(text):\\n        if token not in STOPWORDS and token not in punctation and  len(token) >= 3 :\\n            token = token.lower() \\n            result.append(token)    \\n    return \" \".join(result)',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:05.097237Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:05.097516Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:05.104607Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:05.09747Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:05.103719Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'string_transform',\n",
       "    'subclass_id': 78,\n",
       "    'predicted_subclass_probability': 0.9118299,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"train_df['text'] = train_df['text'].map(clean_text)\\ntrain_df.head()\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:05.473166Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:05.47346Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:05.978468Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:05.473412Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:05.977705Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'categorify',\n",
       "    'subclass_id': 20,\n",
       "    'predicted_subclass_probability': 0.99127215,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'from sklearn.utils import shuffle\\ntrain_df_shuffled = shuffle(train_df)\\ntrain_df_shuffled.head()',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:05.980065Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:05.980328Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:05.99133Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:05.980281Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:05.990552Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'normalization',\n",
       "    'subclass_id': 18,\n",
       "    'predicted_subclass_probability': 0.7203666,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"X = train_df_shuffled['text']\\ny = train_df_shuffled['target']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42, stratify = y)\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:06.176598Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:06.176881Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:06.19323Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:06.176832Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:06.192549Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'split',\n",
       "    'subclass_id': 13,\n",
       "    'predicted_subclass_probability': 0.995934,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'X_test',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:27:27.913358Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:27:27.913642Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:27:27.920548Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:27:27.913592Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:27:27.919684Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_table',\n",
       "    'subclass_id': 41,\n",
       "    'predicted_subclass_probability': 0.99974364,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"from sklearn.model_selection import cross_val_score\\nnb_classifier = Pipeline([('vect', CountVectorizer()),\\n               ('tfidf', TfidfTransformer()),\\n               ('clf', MultinomialNB()),])\\n\\nnb_classifier.fit(X_train, y_train)\\n\\ny_pred = nb_classifier.predict(X_test)\\nprint('accuracy {}'.format(accuracy_score(y_pred, y_test)))\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:06.918119Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:06.918405Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:07.088125Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:06.918357Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:07.087339Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Model_Train',\n",
       "    'subclass': 'train_model',\n",
       "    'subclass_id': 7,\n",
       "    'predicted_subclass_probability': 0.7887582,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"sgd = Pipeline([('vect', CountVectorizer()),\\n                ('tfidf', TfidfTransformer()),\\n                ('clf', SGDClassifier(loss='epsilon_insensitive', penalty='l2',alpha=1e-3, random_state=42, max_iter=1000, tol=None)),])\\n\\n\\nsgd.fit(X_train, y_train)\\ny_pred = sgd.predict(X_test)\\nprint('accuracy {}'.format(accuracy_score(y_pred, y_test)))\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:07.467448Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:07.46774Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:08.228692Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:07.467682Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:08.228008Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Model_Train',\n",
       "    'subclass': 'train_model',\n",
       "    'subclass_id': 7,\n",
       "    'predicted_subclass_probability': 0.95042425,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"test_df = test_df.drop(['id', 'keyword', 'location'], axis = 1)\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:17:08.268608Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:17:08.268867Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:17:08.27416Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:17:08.268821Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:17:08.273142Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'drop_column',\n",
       "    'subclass_id': 10,\n",
       "    'predicted_subclass_probability': 0.99925584,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"test_df['text'] = test_df['text'].map(clean_text)\\ntest_df.head()\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:18:29.183879Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:18:29.184184Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:18:29.348973Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:18:29.184129Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:18:29.34806Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Transform',\n",
       "    'subclass': 'categorify',\n",
       "    'subclass_id': 20,\n",
       "    'predicted_subclass_probability': 0.99245125,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': '',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:27:00.926637Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:27:00.926961Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:27:00.932523Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:27:00.926909Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:27:00.93177Z'},\n",
       "    'trusted': True},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': '',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:27:45.464456Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:27:45.465214Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:27:45.476023Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:27:45.464956Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:27:45.474867Z'},\n",
       "    'trusted': True},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': \"y_pred = nb_classifier.predict(test_df['text'])\",\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:27:50.182577Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:27:50.182904Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:27:50.242686Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:27:50.182849Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:27:50.24198Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Model_Evaluation',\n",
       "    'subclass': 'predict_on_test',\n",
       "    'subclass_id': 48,\n",
       "    'predicted_subclass_probability': 0.994578,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:27:56.635717Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:27:56.63605Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:27:56.645974Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:27:56.635996Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:27:56.645178Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Extraction',\n",
       "    'subclass': 'load_from_csv',\n",
       "    'subclass_id': 45,\n",
       "    'predicted_subclass_probability': 0.99969256,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'sample_submission[\"target\"] = y_pred',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:28:06.907479Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:28:06.907777Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:28:06.913414Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:28:06.907715Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:28:06.911427Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Export',\n",
       "    'subclass': 'prepare_output',\n",
       "    'subclass_id': 55,\n",
       "    'predicted_subclass_probability': 0.7656245,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'sample_submission.head()',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-09-02T09:43:16.073977Z',\n",
       "     'iopub.execute_input': '2021-09-02T09:43:16.074349Z',\n",
       "     'iopub.status.idle': '2021-09-02T09:43:16.099374Z',\n",
       "     'shell.execute_reply.started': '2021-09-02T09:43:16.074293Z',\n",
       "     'shell.execute_reply': '2021-09-02T09:43:16.098344Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Debug',\n",
       "    'subclass': 'show_table',\n",
       "    'subclass_id': 41,\n",
       "    'predicted_subclass_probability': 0.99975234,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'code',\n",
       "   'source': 'sample_submission.to_csv(\"submission.csv\", index=False)',\n",
       "   'metadata': {'execution': {'iopub.status.busy': '2021-12-05T16:28:12.851597Z',\n",
       "     'iopub.execute_input': '2021-12-05T16:28:12.851899Z',\n",
       "     'iopub.status.idle': '2021-12-05T16:28:12.988588Z',\n",
       "     'shell.execute_reply.started': '2021-12-05T16:28:12.851847Z',\n",
       "     'shell.execute_reply': '2021-12-05T16:28:12.987911Z'},\n",
       "    'trusted': True,\n",
       "    'class': 'Data_Export',\n",
       "    'subclass': 'save_to_csv',\n",
       "    'subclass_id': 25,\n",
       "    'predicted_subclass_probability': 0.999154,\n",
       "    'notebook_id': 20205650},\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'cell_type': 'markdown',\n",
       "   'source': 'Now, in the viewer, you can submit the above file to the competition! Good luck!',\n",
       "   'metadata': {}}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utils.constants import BLANK_IPYNB_JSON\n",
    "from utils.helper_functions import load_notebook\n",
    "import json\n",
    "import os\n",
    "import tempfile \n",
    "from kaggle.api.kaggle_api_extended import ApiException\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_ipynb_dir = f\"./tmp/{top_comp}\"\n",
    "if not os.path.exists(test_ipynb_dir): os.makedirs(test_ipynb_dir)\n",
    "\n",
    "FIRST_N_KERNELS = 10\n",
    "\n",
    "notebooks = []\n",
    "for kernel_link in tqdm(distinct_kernel_links[:FIRST_N_KERNELS]):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        kernel_df = merged_df[merged_df['kernel_link'] == kernel_link].sort_values('code_block_id', ascending=True)\n",
    "        try:\n",
    "            kaggle.api.kernels_pull(kernel_link[1:], path=temp_dir, metadata=True)\n",
    "            notebook_path = f\"{temp_dir}/{kernel_link.split('/')[-1]}.ipynb\"\n",
    "            notebook = load_notebook(notebook_path)\n",
    "        except ApiException as e:\n",
    "            print(f\"Error pulling kernel: {kernel_link}\")\n",
    "            continue\n",
    "        \n",
    "    code_cell_counter = 0\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'code' and len(cell['source']):\n",
    "            row = kernel_df.iloc[code_cell_counter]\n",
    "            code_cell_counter += 1\n",
    "\n",
    "            cell[\"metadata\"] = {\n",
    "                **cell[\"metadata\"],\n",
    "                \"class\": row[\"graph_vertex_class\"],\n",
    "                \"subclass\": row[\"graph_vertex_subclass\"],\n",
    "                \"subclass_id\": int(row[\"graph_vertex_id\"]),\n",
    "                \"predicted_subclass_probability\": row[\"predicted_graph_vertex__probability\"],\n",
    "                \"notebook_id\": int(row['kernel_id']),\n",
    "            }    \n",
    "    notebooks.append(notebook)\n",
    "    with open(f\"{test_ipynb_dir}/{kernel_link.split('/')[-1]}.ipynb\", \"w\") as f:json.dump(notebook, f)\n",
    "    \n",
    "notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utils.constants import FIRST_LAYER_LABELS, SECOND_LAYER_LABELS, BLANK_IPYNB_JSON\n",
    "from Classifiers.GPTClassifier import GPTClassifier\n",
    "with open('../../secrets/api_key.txt', 'r') as f: api_key = f'{f.read()}'\n",
    "\n",
    "LABELS = FIRST_LAYER_LABELS\n",
    "# LABELS = SECOND_LAYER_LABELS\n",
    "\n",
    "\n",
    "print(f\"Initializing classifier...\")\n",
    "prompt = f\"\"\"You will be given each code cell of the same jupyter notebook of a machine learning task.\n",
    "First, classify the code into one {', '.join(LABELS[:-1])} or {LABELS[-1]}.\n",
    "Consider the previously classified code snippets for context.\n",
    "Then, describe what the code snippet does in strictly one sentence.\n",
    "Explain your reasoning for the classification and then output the desired format at the end.\n",
    "Desired format:\n",
    "Class: <class_label>\n",
    "Description: <desctiption_sentence>\n",
    "\"\"\" \n",
    "classifier = GPTClassifier(api_key=api_key, prompt=prompt, labels=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLANK_IPYNB_JSON[\"cells\"] = []\n",
    "BLANK_IPYNB_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper_functions import notebook_extract_code, notebook_add_class_labels\n",
    "\n",
    "notebooks = []\n",
    "for kernel_id in merged_df['kernel_id'].unique()[:20]:\n",
    "    notebook_json = BLANK_IPYNB_JSON.copy()\n",
    "    notebook_json['cells'] = []\n",
    "    for row in merged_df[merged_df['kernel_id'] == kernel_id].iterrows():\n",
    "        notebook_json['cells'].append({\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": None,\n",
    "            \"metadata\": {\n",
    "                \"graph_vertex_id\": row[1][\"graph_vertex_id\"],\n",
    "                \"predicted_graph_vertex__probability\": row[1][\"predicted_graph_vertex__probability\"],\n",
    "                \"notebook_id\": row[1]['kernel_id'],\n",
    "            },\n",
    "            \"source\": row[1]['code_block']\n",
    "        })\n",
    "    notebooks.append(notebook_json)\n",
    "# notebook_code = notebook_extract_code(notebook_json)\n",
    "# cell_labels = classifier.classify_notebook(notebook_code)\n",
    "# notebook_json = notebook_add_class_labels(notebook_json, cell_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notebooks = []\n",
    "notebook_cell_labels = []\n",
    "for i, notebook_json in enumerate(notebooks):\n",
    "    print(f\"Notebook {i+1}/{len(notebooks)}\", end='\\r')\n",
    "    \n",
    "    notebook_code = notebook_extract_code(notebook_json)\n",
    "    cell_labels = classifier.classify_notebook(notebook_code)\n",
    "    notebook_json = notebook_add_class_labels(notebook_json, cell_labels)\n",
    "    labeled_notebooks.append(notebook_json)\n",
    "    notebook_cell_labels.append(cell_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for notebook in notebook_cell_labels:\n",
    "    for elem in notebook:\n",
    "        embeddings.append(elem[1])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "clusterer: HDBSCAN = HDBSCAN(\n",
    "    min_cluster_size=4,                 # Minimum number of samples to form a cluster\n",
    "    min_samples=2,                      # Minimum number of samples in a neighborhood to be considered as a core point\n",
    "    cluster_selection_epsilon=0,     # If 2 clusters are less than epsilon apart, they get merged\n",
    ")\n",
    "\n",
    "clusterer.fit(embeddings)\n",
    "for label in set(clusterer.labels_):\n",
    "    print(f\"Cluster {label}: {len([x for x in clusterer.labels_ if x == label])} cells\")\n",
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook['cells']:\n",
    "        cell['metadata']['cluster_label'] = clusterer.labels_[counter]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notebooks[0][\"cells\"]\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook[\"cells\"]:\n",
    "        true_labels.append(cell[\"metadata\"][\"graph_vertex_id\"])\n",
    "        predicted_labels.append(cell[\"metadata\"][\"cluster_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def count_misclustered_elements(true_labels, predicted_labels):\n",
    "    # Convert the labels to numpy arrays for easier manipulation\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    \n",
    "    unique_true_labels = np.unique(true_labels)\n",
    "    unique_predicted_labels = np.unique(predicted_labels)\n",
    "    \n",
    "    cost_matrix = np.zeros((len(unique_true_labels), len(unique_predicted_labels)), dtype=int)\n",
    "    \n",
    "    for i, true_label in enumerate(unique_true_labels):\n",
    "        for j, predicted_label in enumerate(unique_predicted_labels):\n",
    "            cost_matrix[i, j] = np.sum((true_labels == true_label) & (predicted_labels != predicted_label))\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    misclustered_count = cost_matrix[row_ind, col_ind].sum()\n",
    "    \n",
    "    return misclustered_count\n",
    "\n",
    "misclustered_count = count_misclustered_elements(true_labels, predicted_labels)\n",
    "print(f\"Number of misclustered elements: {misclustered_count}/{len(true_labels)}\")\n",
    "print(f\"Score: {1 - misclustered_count/len(true_labels)} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook['cells']:\n",
    "        cell['metadata']['cluster_label'] = clusterer.labels_[counter]\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
