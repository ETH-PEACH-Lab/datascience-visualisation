{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_blocks_index</th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17493583</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17493583</td>\n",
       "      <td>1</td>\n",
       "      <td>import os\\nimport zipfile\\nfrom subprocess imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17493583</td>\n",
       "      <td>2</td>\n",
       "      <td>def reduce_mem_usage(df):\\n    \"\"\" iterate thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17493583</td>\n",
       "      <td>3</td>\n",
       "      <td>import numpy as np</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17493583</td>\n",
       "      <td>4</td>\n",
       "      <td>orders_df = reduce_mem_usage(pd.read_csv('../w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599348</th>\n",
       "      <td>2599348</td>\n",
       "      <td>8902213</td>\n",
       "      <td>22</td>\n",
       "      <td>f=[]\\n\\nfor root, dirs, files in os.walk(\"../o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599349</th>\n",
       "      <td>2599349</td>\n",
       "      <td>8902213</td>\n",
       "      <td>23</td>\n",
       "      <td>my_model.fit_generator(\\n        train_data_ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599350</th>\n",
       "      <td>2599350</td>\n",
       "      <td>8902213</td>\n",
       "      <td>24</td>\n",
       "      <td>test_generator.reset()\\n\\npred = my_model.pred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599351</th>\n",
       "      <td>2599351</td>\n",
       "      <td>8902213</td>\n",
       "      <td>25</td>\n",
       "      <td>import cv2\\n\\n\\nfrom matplotlib import pyplot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599352</th>\n",
       "      <td>2599352</td>\n",
       "      <td>8902213</td>\n",
       "      <td>26</td>\n",
       "      <td>results_df = pd.DataFrame(\\n    {\\n        'id...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2599353 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         code_blocks_index  kernel_id  code_block_id  \\\n",
       "0                        0   17493583              0   \n",
       "1                        1   17493583              1   \n",
       "2                        2   17493583              2   \n",
       "3                        3   17493583              3   \n",
       "4                        4   17493583              4   \n",
       "...                    ...        ...            ...   \n",
       "2599348            2599348    8902213             22   \n",
       "2599349            2599349    8902213             23   \n",
       "2599350            2599350    8902213             24   \n",
       "2599351            2599351    8902213             25   \n",
       "2599352            2599352    8902213             26   \n",
       "\n",
       "                                                code_block  \n",
       "0        # This Python 3 environment comes with many he...  \n",
       "1        import os\\nimport zipfile\\nfrom subprocess imp...  \n",
       "2        def reduce_mem_usage(df):\\n    \"\"\" iterate thr...  \n",
       "3                                       import numpy as np  \n",
       "4        orders_df = reduce_mem_usage(pd.read_csv('../w...  \n",
       "...                                                    ...  \n",
       "2599348  f=[]\\n\\nfor root, dirs, files in os.walk(\"../o...  \n",
       "2599349  my_model.fit_generator(\\n        train_data_ge...  \n",
       "2599350  test_generator.reset()\\n\\npred = my_model.pred...  \n",
       "2599351  import cv2\\n\\n\\nfrom matplotlib import pyplot ...  \n",
       "2599352  results_df = pd.DataFrame(\\n    {\\n        'id...  \n",
       "\n",
       "[2599353 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import kaggle\n",
    "\n",
    "\n",
    "code4ML_path = \"/home/ryounis/Documents/Zurich/PEACHLab/data/Code4ML\"\n",
    "\n",
    "kernels_meta = pd.read_csv(f\"{code4ML_path}/kernels_meta.csv\")\n",
    "markup_data = pd.read_csv(f\"{code4ML_path}/markup_data.csv\")\n",
    "vertices = pd.read_csv(f\"{code4ML_path}/vertices.csv\")\n",
    "code_blocks = pd.read_csv(f\"{code4ML_path}/code_blocks.csv\")\n",
    "data_preds = pd.read_csv(f\"{code4ML_path}/data_with_preds.csv\")\n",
    "\n",
    "code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition with most kernels: 'Natural Language Processing with Disaster Tweets'  (725 kernels)\n",
      "\n",
      "Titanic - Machine Learning from Disaster: 1963 kernels\n",
      "Digit Recognizer: 1300 kernels\n",
      "House Prices - Advanced Regression Techniques: 1013 kernels\n",
      "Natural Language Processing with Disaster Tweets: 725 kernels\n",
      "Quora Insincere Questions Classification: 679 kernels\n",
      "Mechanisms of Action (MoA) Prediction: 660 kernels\n",
      "COVID19 Global Forecasting (Week 4): 434 kernels\n",
      "Santander Customer Transaction Prediction: 418 kernels\n",
      "PUBG Finish Placement Prediction (Kernels Only): 394 kernels\n",
      "Tweet Sentiment Extraction: 386 kernels\n",
      "Cassava Leaf Disease Classification: 383 kernels\n",
      "Kannada MNIST: 362 kernels\n",
      "Aerial Cactus Identification: 345 kernels\n",
      "SIIM-ISIC Melanoma Classification: 325 kernels\n",
      "Riiid Answer Correctness Prediction: 322 kernels\n",
      "IEEE-CIS Fraud Detection: 303 kernels\n",
      "APTOS 2019 Blindness Detection: 303 kernels\n",
      "Predict Future Sales: 295 kernels\n",
      "COVID19 Global Forecasting (Week 2): 288 kernels\n",
      "Global Wheat Detection: 272 kernels\n",
      "Jane Street Market Prediction: 272 kernels\n",
      "Costa Rican Household Poverty Level Prediction: 259 kernels\n",
      "Jigsaw Unintended Bias in Toxicity Classification: 244 kernels\n",
      "Toxic Comment Classification Challenge: 241 kernels\n",
      "Home Credit Default Risk: 235 kernels\n",
      "OSIC Pulmonary Fibrosis Progression: 217 kernels\n",
      "Jigsaw Multilingual Toxic Comment Classification: 213 kernels\n",
      "Petals to the Metal - Flower Classification on TPU: 211 kernels\n",
      "Tabular Playground Series - Jan 2021: 209 kernels\n",
      "PetFinder.my Adoption Prediction: 201 kernels\n",
      "Mercari Price Suggestion Challenge: 192 kernels\n",
      "LANL Earthquake Prediction: 191 kernels\n",
      "M5 Forecasting - Accuracy: 184 kernels\n",
      "Flower Classification with TPUs: 180 kernels\n",
      "Instant Gratification: 179 kernels\n",
      "University of Liverpool - Ion Switching: 178 kernels\n",
      "Google QUEST Q&A Labeling: 178 kernels\n",
      "What's Cooking? (Kernels Only): 162 kernels\n",
      "ASHRAE - Great Energy Predictor III: 161 kernels\n",
      "Plant Pathology 2020 - FGVC7: 157 kernels\n",
      "Generative Dog Images: 155 kernels\n",
      "Bengali.AI Handwritten Grapheme Classification: 153 kernels\n",
      "Bike Sharing Demand: 151 kernels\n",
      "COVID19 Global Forecasting (Week 1): 147 kernels\n",
      "COVID19 Global Forecasting (Week 3): 144 kernels\n",
      "NFL Big Data Bowl: 142 kernels\n",
      "New York City Taxi Fare Prediction: 142 kernels\n",
      "Don't Overfit! II: 141 kernels\n",
      "Dogs vs. Cats Redux: Kernels Edition: 140 kernels\n",
      "Two Sigma: Using News to Predict Stock Movements: 136 kernels\n",
      "Categorical Feature Encoding Challenge: 135 kernels\n",
      "New York City Taxi Trip Duration: 130 kernels\n",
      "Categorical Feature Encoding Challenge II: 129 kernels\n",
      "Severstal: Steel Defect Detection: 120 kernels\n",
      "TMDB Box Office Prediction: 114 kernels\n",
      "Santander Value Prediction Challenge: 114 kernels\n",
      "OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction: 114 kernels\n",
      "Histopathologic Cancer Detection: 113 kernels\n",
      "Deepfake Detection Challenge: 112 kernels\n",
      "Google Analytics Customer Revenue Prediction: 112 kernels\n",
      "Prostate cANcer graDe Assessment (PANDA) Challenge: 110 kernels\n",
      "TGS Salt Identification Challenge: 110 kernels\n",
      "Predicting Molecular Properties: 104 kernels\n",
      "Contradictory, My Dear Watson: 101 kernels\n",
      "CareerCon 2019 - Help Navigate Robots: 101 kernels\n",
      "Humpback Whale Identification: 95 kernels\n",
      "Cornell Birdcall Identification: 94 kernels\n",
      "RANZCR CLiP - Catheter and Line Position Challenge: 83 kernels\n",
      "Tabular Playground Series - Feb 2021: 82 kernels\n",
      "Leaf Classification: 81 kernels\n",
      "Ghouls, Goblins, and Ghosts... Boo!: 77 kernels\n",
      "TReNDS Neuroimaging: 74 kernels\n",
      "Porto Seguro’s Safe Driver Prediction: 74 kernels\n",
      "Lyft Motion Prediction for Autonomous Vehicles: 72 kernels\n",
      "Abstraction and Reasoning Challenge: 72 kernels\n",
      "Rossmann Store Sales: 71 kernels\n",
      "Quick, Draw! Doodle Recognition Challenge: 70 kernels\n",
      "Forest Cover Type Prediction: 69 kernels\n",
      "Dog Breed Identification: 68 kernels\n",
      "Traveling Santa 2018 - Prime Paths: 67 kernels\n",
      "Plant Seedlings Classification: 67 kernels\n",
      "Microsoft Malware Prediction: 65 kernels\n",
      "Two Sigma Connect: Rental Listing Inquiries: 64 kernels\n",
      "San Francisco Crime Classification: 63 kernels\n",
      "Spooky Author Identification: 62 kernels\n",
      "Avito Demand Prediction Challenge: 62 kernels\n",
      "HuBMAP - Hacking the Kidney: 61 kernels\n",
      "RSNA STR Pulmonary Embolism Detection: 57 kernels\n",
      "Bag of Words Meets Bags of Popcorn: 56 kernels\n",
      "Santa's Workshop Tour 2019: 55 kernels\n",
      "Quora Question Pairs: 54 kernels\n",
      "Santander Customer Satisfaction: 54 kernels\n",
      "ALASKA2 Image Steganalysis: 53 kernels\n",
      "Google Landmark Recognition 2020: 53 kernels\n",
      "Rainforest Connection Species Audio Detection: 52 kernels\n",
      "Freesound Audio Tagging 2019: 52 kernels\n",
      "Human Protein Atlas Image Classification: 52 kernels\n",
      "TalkingData AdTracking Fraud Detection Challenge: 51 kernels\n",
      "INGV - Volcanic Eruption Prediction: 49 kernels\n",
      "TensorFlow 2.0 Question Answering: 49 kernels\n",
      "Restaurant Revenue Prediction: 49 kernels\n",
      "DonorsChoose.org Application Screening: 46 kernels\n",
      "Instacart Market Basket Analysis: 45 kernels\n",
      "VSB Power Line Fault Detection: 43 kernels\n",
      "iMet Collection 2019 - FGVC6: 41 kernels\n",
      "Gendered Pronoun Resolution: 41 kernels\n",
      "Otto Group Product Classification Challenge: 41 kernels\n",
      "Google Cloud & NCAA® ML Competition 2020-NCAAM: 41 kernels\n",
      "Understanding Clouds from Satellite Images: 40 kernels\n",
      "Mercedes-Benz Greener Manufacturing: 40 kernels\n",
      "Sentiment Analysis on Movie Reviews: 39 kernels\n",
      "RSNA Pneumonia Detection Challenge: 38 kernels\n",
      "Conway's Reverse Game of Life 2020: 37 kernels\n",
      "What's Cooking?: 37 kernels\n",
      "Facial Keypoints Detection: 36 kernels\n",
      "Recruit Restaurant Visitor Forecasting: 35 kernels\n",
      "Northeastern SMILE Lab - Recognizing Faces in the Wild: 35 kernels\n",
      "M5 Forecasting - Uncertainty: 34 kernels\n",
      "Statoil/C-CORE Iceberg Classifier Challenge: 34 kernels\n",
      "Airbnb New User Bookings: 33 kernels\n",
      "Airbus Ship Detection Challenge: 33 kernels\n",
      "Kobe Bryant Shot Selection: 31 kernels\n",
      "RSNA Intracranial Hemorrhage Detection: 30 kernels\n",
      "Sberbank Russian Housing Market: 30 kernels\n",
      "PLAsTiCC Astronomical Classification: 29 kernels\n",
      "SIIM-ACR Pneumothorax Segmentation: 28 kernels\n",
      "Indoor Location & Navigation: 28 kernels\n",
      "Walmart Recruiting - Store Sales Forecasting: 25 kernels\n",
      "Zillow Prize: Zillow’s Home Value Prediction (Zestimate): 25 kernels\n",
      "Elo Merchant Category Recommendation: 25 kernels\n",
      "Google Cloud & NCAA® ML Competition 2020-NCAAW: 24 kernels\n",
      "Homework for Students: 22 kernels\n",
      "Personality Profile Prediction: 22 kernels\n",
      "Human Protein Atlas - Single Cell Classification: 21 kernels\n",
      "Santander Product Recommendation: 21 kernels\n",
      "Allstate Claims Severity: 21 kernels\n",
      "Recursion Cellular Image Classification: 21 kernels\n",
      "VinBigData Chest X-ray Abnormalities Detection: 21 kernels\n",
      "Don't Get Kicked!: 20 kernels\n",
      "IESB - 2019: 20 kernels\n",
      "Give Me Some Credit: 20 kernels\n",
      "Atividade_3_PMR3508: 20 kernels\n",
      "2019 Data Science Bowl: 19 kernels\n",
      "Предсказание положения космических объектов: 19 kernels\n",
      "MLH - Pokemon Challenge: 19 kernels\n",
      "Google Cloud & NCAA® ML Competition 2019-Men's: 18 kernels\n",
      "iWildCam 2019 - FGVC6: 18 kernels\n",
      "2019  ML competition with KISTI: 18 kernels\n",
      "State Farm Distracted Driver Detection: 18 kernels\n",
      "PMR3508 - Tarefa 1 - 3508 Adult Dataset: 18 kernels\n",
      "Exam for Students20200129: 17 kernels\n",
      "Google Landmark Retrieval 2020: 17 kernels\n",
      "Nomad2018 Predicting Transparent Conductors: 17 kernels\n",
      "PadhAI: Text - Non Text Classification Level 4b: 16 kernels\n",
      "NBA Rookies: 16 kernels\n",
      "Google Cloud & NCAA® ML Competition 2019-Women's: 16 kernels\n",
      "BNP Paribas Cardif Claims Management: 15 kernels\n",
      "Shelter Animal Outcomes: 15 kernels\n",
      "TalkingData Mobile User Demographics: 15 kernels\n",
      "QSTP - Deep Learning 2019: 15 kernels\n",
      "Freesound General-Purpose Audio Tagging Challenge: 14 kernels\n",
      "Predicting Red Hat Business Value: 14 kernels\n",
      "Expedia Hotel Recommendations: 13 kernels\n",
      "Peking University/Baidu - Autonomous Driving: 13 kernels\n",
      "Prudential Life Insurance Assessment: 12 kernels\n",
      "Corporación Favorita Grocery Sales Forecasting: 12 kernels\n",
      "How Much Did It Rain? II: 11 kernels\n",
      "Amazon.com - Employee Access Challenge: 11 kernels\n",
      "PadhAI: Text - Non Text Classification Level 4a: 11 kernels\n",
      "Open Images 2019 - Object Detection: 11 kernels\n",
      "Homesite Quote Conversion: 10 kernels\n",
      "Planet: Understanding the Amazon from Space: 10 kernels\n",
      "I’m Something of a Painter Myself: 10 kernels\n",
      "Eval Lab 2 F464: 10 kernels\n",
      "Diabetes Diagnosis: 10 kernels\n",
      "Cloud Faculty Institute Workshop: 10 kernels\n",
      "Ultrasound Nerve Segmentation: 10 kernels\n",
      "IESB Sul - IGM - Maio 2019: 10 kernels\n",
      "DSNet: fastai Hackathon: 10 kernels\n",
      "YKC-2nd: 9 kernels\n",
      "Predictive Equipment Failures: 9 kernels\n",
      "PadhAI: Hindi Vowel - Consonant Classification: 9 kernels\n",
      "iWildCam 2020 - FGVC7: 9 kernels\n",
      "National Data Science Challenge 2019 - Advanced: 8 kernels\n",
      "Grupo Bimbo Inventory Demand: 8 kernels\n",
      "Home Depot Product Search Relevance: 8 kernels\n",
      "Humpback Whale Identification Challenge: 8 kernels\n",
      "Hash Code Archive - Drone Delivery: 8 kernels\n",
      "AI Academy Intermediate Class Competition 1: 7 kernels\n",
      "ML 4 Money: 7 kernels\n",
      "ASN10e Final Submission - Detect COML Faces: 7 kernels\n",
      "WSDM - KKBox's Music Recommendation Challenge: 7 kernels\n",
      "[Student] Shopee Code League - Product Detection: 6 kernels\n",
      "Google Cloud & NCAA® ML Competition 2018-Men's: 6 kernels\n",
      "IEEE's Signal Processing Society - Camera Model Identification: 6 kernels\n",
      "WSDM - KKBox's Churn Prediction Challenge: 6 kernels\n",
      "Invasive Species Monitoring: 6 kernels\n",
      "Sarcasmo: 6 kernels\n",
      "Seleksi Calon Asisten GAIB: 6 kernels\n",
      "Whose line is it anyway?: 6 kernels\n",
      "IESB Norte - IGM - Maio 2019: 6 kernels\n",
      "Competição DSA de Machine Learning: 6 kernels\n",
      "ISSM2020 AI Challenge: 5 kernels\n",
      "Hackathon Auto_matic: 5 kernels\n",
      "StumbleUpon Evergreen Classification Challenge: 5 kernels\n",
      "Santa Gift Matching Challenge: 5 kernels\n",
      "TrackML Particle Tracking Challenge: 5 kernels\n",
      "Classifying Movie Reviews: 5 kernels\n",
      "WiDS Datathon 2019: 5 kernels\n",
      "Coupon Purchase Prediction: 5 kernels\n",
      "Santa 2019: Revenge of the Accountants: 5 kernels\n",
      "Rock, Paper, Scissors: 5 kernels\n",
      "DM-Assignment 1: 5 kernels\n",
      "hackStat 2.0: 5 kernels\n",
      "Bosch Production Line Performance: 5 kernels\n",
      "Web Enthusiasts' Club NITK Recruitment: 5 kernels\n",
      "Web Traffic Time Series Forecasting: 4 kernels\n",
      "Loan Default Prediction - Imperial College London: 4 kernels\n",
      "Recommender Systems: 4 kernels\n",
      "Outbrain Click Prediction: 4 kernels\n",
      "Google Cloud & NCAA® ML Competition 2018-Women's: 4 kernels\n",
      "car-classification: 4 kernels\n",
      "DS特論2019年度 演習課題2: 4 kernels\n",
      "DL in NLP Spring 2019. Classification: 4 kernels\n",
      "The Nature Conservancy Fisheries Monitoring: 4 kernels\n",
      "Text classification: 4 kernels\n",
      "Predicting a Biological Response: 4 kernels\n",
      "ML in biology: 4 kernels\n",
      "Google Landmark Recognition Challenge: 4 kernels\n",
      "Property price prediction challenge: 4 kernels\n",
      "UI DS Summer School: 4 kernels\n",
      "Lyft 3D Object Detection for Autonomous Vehicles: 4 kernels\n",
      "Hackathon Sentimento: 4 kernels\n",
      "Telstra Network Disruptions: 4 kernels\n",
      "AILAB ML Training #1: 4 kernels\n",
      "Basic Regression Competition: 3 kernels\n",
      "Halite by Two Sigma: 3 kernels\n",
      "Python for Data science ITEA: 3 kernels\n",
      "I-RICH ML COMPETITION: 3 kernels\n",
      "Fake News e ML: 3 kernels\n",
      "Facebook V: Predicting Check Ins: 3 kernels\n",
      "Market Basket - ID NDSC 2020: 3 kernels\n",
      "Click-Through Rate Prediction: 3 kernels\n",
      "PadhAI: Tamil Vowel - Consonant Classification: 3 kernels\n",
      "Hackathon Sentimento_v2: 3 kernels\n",
      "SERPRO - Abalone: 3 kernels\n",
      "VSU ML 1 Regression: 3 kernels\n",
      "Personalized Medicine: Redefining Cancer Treatment: 3 kernels\n",
      "Sentiment Analysis in Russian: 3 kernels\n",
      "Multiple regression for time series data: 3 kernels\n",
      "Brain Cancer Classification: 3 kernels\n",
      "[ACM] Recommender System Practice: 3 kernels\n",
      "NFL 1st and Future - Impact Detection: 3 kernels\n",
      "Santa's Uncertain Bags: 3 kernels\n",
      "Used Cars Price Prediction: 3 kernels\n",
      "COVID-19 diagnostic: 2 kernels\n",
      "Predict the Income - <DECODE> WITH BOARD: 2 kernels\n",
      "Pycon Korea 2018 - Tutorial: 2 kernels\n",
      "Regression Evaluative Lab: 2 kernels\n",
      "OCRV Test Task: 2 kernels\n",
      "notMNIST Competition: 2 kernels\n",
      "[Open] Shopee Code League - Logistics: 2 kernels\n",
      "Conway's Reverse Game of Life: 2 kernels\n",
      "Higgs Boson Machine Learning Challenge: 2 kernels\n",
      "empty: 2 kernels\n",
      "MLClass Dubai by ODS, Lecture 6 HW: 2 kernels\n",
      "Классификация изображений: 2 kernels\n",
      "Texts classification: 2 kernels\n",
      "ML Challenge: 2 kernels\n",
      "West Nile Virus Prediction: 2 kernels\n",
      "VietAI Advance Course - Retinal Disease Detection: 2 kernels\n",
      "Characters classification: 2 kernels\n",
      "House pricing: 2 kernels\n",
      "UCI-HAR: 2 kernels\n",
      "Schnell-Mal-Klassifizieren: 2 kernels\n",
      "CIFAR-10 - Object Recognition in Images: 2 kernels\n",
      "SERPRO - Iris: 2 kernels\n",
      "ClassificationOFShields: 2 kernels\n",
      "Задержка рейса самолета: 2 kernels\n",
      "Pneumonia Diagnosis: 2 kernels\n",
      "UMUC DATA 650 Summer 2019 Competition: 2 kernels\n",
      "Птица или самолет: 2 kernels\n",
      "House Price Prediction: 2 kernels\n",
      "Machine Learning Lab - CAS Data Science FS 20: 2 kernels\n",
      "Tap30 Challenge: 2 kernels\n",
      "Data Champions Android App Malware Prediction: 2 kernels\n",
      "Diabetes Classification: 2 kernels\n",
      "IES Data Mining(WS 18/19): 2 kernels\n",
      "Language Identification: 2 kernels\n",
      "March Machine Learning Mania 2016: 2 kernels\n",
      "2018 Data Science Bowl: 2 kernels\n",
      "CSC: HW4 spring19: 2 kernels\n",
      "YKC-cup-1st: 2 kernels\n",
      "Digit Classification DL Workshop: 2 kernels\n",
      "Influencers in Social Networks: 2 kernels\n",
      "AILAB ML Training #0: 2 kernels\n",
      "Open Images 2019 - Instance Segmentation: 1 kernels\n",
      "Find me that fish: 1 kernels\n",
      "Painter by Numbers: 1 kernels\n",
      "Avito Duplicate Ads Detection: 1 kernels\n",
      "Diabetic Retinopathy Detection: 1 kernels\n",
      "Summer Analytics 2020 Capstone Project: 1 kernels\n",
      "Similarity Search Project: 1 kernels\n",
      "Galaxy Zoo - The Galaxy Challenge: 1 kernels\n",
      "Flavours of Physics: Finding τ  →  μμμ: 1 kernels\n",
      "Cdiscount’s Image Classification Challenge: 1 kernels\n",
      "DL for exploration geophysics: 1 kernels\n",
      "Liberty Mutual Group: Property Inspection Prediction: 1 kernels\n",
      "Data Series Summarization Project (v3): 1 kernels\n",
      "Car loan default: 1 kernels\n",
      "KNIT_HACKS: 1 kernels\n",
      "Carvana Image Masking Challenge: 1 kernels\n",
      "Data Science - Master: 1 kernels\n",
      "Oxford Fast AI Week 2: 1 kernels\n",
      "[T] PadhAI: Text - Non Text Classification Level 1: 1 kernels\n",
      "EPAM: Exercise 1 -  Sentiment Analysis: 1 kernels\n",
      "UC Irvine Math 10 Winter 2020: 1 kernels\n",
      "The Kaggle Master: 1 kernels\n",
      "Competencia-Series-Temporales: 1 kernels\n",
      "SQL Saturday Madrid ML Challenge: 1 kernels\n",
      "IA1819: 1 kernels\n",
      "DeepNLP HSE Course: 1 kernels\n",
      "BU CS506 Spring 2020 Midterm: 1 kernels\n",
      "Focus start 2020: 1 kernels\n",
      "Predicting Age Groups: 1 kernels\n",
      "ML Hackathon 2019 Q1: 1 kernels\n",
      "Tobigs13_7week_competition: 1 kernels\n",
      "Pitch estimation and voicing detection: 1 kernels\n",
      "COMP 750/850 Project 1: 1 kernels\n",
      "Tweet Sentiment Analysis: 1 kernels\n",
      "Fashion MNIST challenge2019: 1 kernels\n",
      "2019 SMHRD 경진대회 ( 지능형 ): 1 kernels\n",
      "SkillFactory | Final hackathon: 1 kernels\n",
      "SMEMI309 - Final evaluation challenge 2020: 1 kernels\n",
      "Predict the missing pixel value v2: 1 kernels\n",
      "Tracy Regression: 1 kernels\n",
      "Japanese Review Rating Prediction: 1 kernels\n",
      "SYDE 522 (Winter 2020): 1 kernels\n",
      "HEROZ Internal Competition: 1 kernels\n",
      "Классификация компьютерных атак: 1 kernels\n",
      "JAMP Hackathon Drive 1: 1 kernels\n",
      "AS-bow-2019-2020: 1 kernels\n",
      "NTUST: Information Retrieval and Applications: 1 kernels\n",
      "TensorFlow Speech Recognition Challenge: 1 kernels\n",
      "Springleaf Marketing Response: 1 kernels\n",
      "Hungry Geese: 1 kernels\n",
      "Gallivanters: 1 kernels\n",
      "Cats vs Dogs vs More: 1 kernels\n",
      "Aesthetic Visual Analysis: 1 kernels\n",
      "University of applied sciences Mannheim: 1 kernels\n",
      "CSM/SEM6420 workshop: 1 kernels\n",
      "KaggleDays Paris: 1 kernels\n",
      "2019S UTS Data Analytics Assignment 3: 1 kernels\n",
      "EC524: Heart-disease classification: 1 kernels\n",
      "Predição de Churn: 1 kernels\n",
      "IEEE PES BDC DataThon , Year-2020: 1 kernels\n",
      "Птица или самолет?: 1 kernels\n",
      "AI for Clinical Data Analytics HW2: 1 kernels\n",
      "Data Mining Lab2: 1 kernels\n",
      "Predicting user conversions: 1 kernels\n",
      "Avaliação de Carros: 1 kernels\n",
      "InClass Competition at Tokyo Metropolitan Univ.: 1 kernels\n",
      "Bad comments: 1 kernels\n",
      "Хакатон от Кафедры ТПСТС МФТИ (level 1): 1 kernels\n",
      "Who is a Friend?: 1 kernels\n",
      "Chh-OLA: 1 kernels\n",
      "Flatiron School: 1 kernels\n",
      "NCTU BDALAB 2020 Onboard: 1 kernels\n",
      "Night at Cameo: 1 kernels\n",
      "Time Series Classification: 1 kernels\n",
      "House Sales : 1 kernels\n",
      "Car Classification(Project Vision): 1 kernels\n",
      "Great Energy Predictor Shootout I: 1 kernels\n",
      "Pneumonia Texture Analysis: 1 kernels\n",
      "MLDM Classification Competition: 1 kernels\n",
      "Korean Gender Bias Detection: 1 kernels\n",
      "Einfhrung in Kaggle InClass Competitions</title>: 1 kernels\n",
      "GL Hack: Landmarks: 1 kernels\n",
      "[DM&PR WS19/20] Machine learning competition: 1 kernels\n",
      "iFood - 2019 at FGVC6: 1 kernels\n",
      "Fashion MNIST-ITBA-LAB 2020: 1 kernels\n",
      "finec-1941-hw6: 1 kernels\n",
      "Word vectors: 1 kernels\n",
      "compass-canada: 1 kernels\n",
      "DMA Kaggle Challenge: 1 kernels\n",
      "DSI-US-8 Project 2 Regression Challenge: 1 kernels\n",
      "Cleaned vs Dirty: 1 kernels\n",
      "Penyisihan Datavidia 2019: 1 kernels\n",
      "Fashion MNIST challenge201907: 1 kernels\n",
      "ML Hackathon 2019 Q2: 1 kernels\n",
      "Data-Driven Business Analytics: 1 kernels\n",
      "Traffic signs classification: 1 kernels\n",
      "IIITB ML Project: SFO Crime Classification: 1 kernels\n",
      "Temperature Forecasting: 1 kernels\n",
      "[DM&PR WS18/19] Machine learning competition: 1 kernels\n",
      "Kharagpur Data Analytics Group: 1 kernels\n",
      "Анализ потребительской корзины: 1 kernels\n",
      "Heart Disease Prediction: 1 kernels\n",
      "NMLO Contest 3 - Regression: 1 kernels\n",
      "ods_class_cs231n: 1 kernels\n",
      "Predice el futuro: 1 kernels\n",
      "ACM Summer'19 Inclass-1: 1 kernels\n",
      "ACM Machine Learning (SVNIT): 1 kernels\n",
      "HTA Tagging: 1 kernels\n",
      "Oracle Graph ML Contest at Polimi: 1 kernels\n",
      "GirlsGoIT competition 2020: 1 kernels\n",
      "FIA ML T5: 1 kernels\n",
      "Bike Sharing Demand for Education(): 1 kernels\n",
      "Challenge GH: 1 kernels\n",
      "Python Class - Practice: 1 kernels\n",
      "Test Competition Please Ignore: 1 kernels\n",
      "Movie Genre Classification: 1 kernels\n",
      "kaggle18011884: 1 kernels\n",
      "Sentence Relatedness: 1 kernels\n",
      "Team ISTE's Datathon: 1 kernels\n",
      "Anokha AI Adept: 1 kernels\n",
      "HEROZ Internal Competition Extra2: 1 kernels\n",
      "Machine Learning Lab - CAS Data Science HS 20: 1 kernels\n",
      "Digit recognition: 1 kernels\n",
      "Computational Intelligence Project: 1 kernels\n",
      "Technidus machine learning competition 2: 1 kernels\n",
      "Fieldguide Challenge: Moths & Butterflies: 1 kernels\n",
      "Multi-label Bird Species Classification - NIPS 2013: 1 kernels\n"
     ]
    }
   ],
   "source": [
    "kernels_count = kernels_meta.groupby('comp_name').size()\n",
    "n_kernels_per_comp = kernels_meta['comp_name'].value_counts()\n",
    "N_HIGHEST_COMP = 4\n",
    "top_comp = kernels_count.nlargest(N_HIGHEST_COMP).index[N_HIGHEST_COMP-1]\n",
    "print(f\"Competition with most kernels: '{top_comp}'  ({kernels_count.nlargest(N_HIGHEST_COMP).iloc[N_HIGHEST_COMP-1]} kernels)\\n\")\n",
    "for name in n_kernels_per_comp.index:\n",
    "    print(f\"{name}: {n_kernels_per_comp[name]} kernels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_blocks_index</th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9741</td>\n",
       "      <td>14113092</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9742</td>\n",
       "      <td>14113092</td>\n",
       "      <td>1</td>\n",
       "      <td>train = pd.read_csv(\"../input/nlp-getting-star...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9743</td>\n",
       "      <td>14113092</td>\n",
       "      <td>2</td>\n",
       "      <td>def lowercase_text(text):\\n    return text.low...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9744</td>\n",
       "      <td>14113092</td>\n",
       "      <td>3</td>\n",
       "      <td>import re\\nimport string\\ndef remove_noise(tex...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9745</td>\n",
       "      <td>14113092</td>\n",
       "      <td>4</td>\n",
       "      <td># Tokenizing the training and the test set\\nim...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22621</th>\n",
       "      <td>2462840</td>\n",
       "      <td>10038839</td>\n",
       "      <td>108</td>\n",
       "      <td>#train BERT\\nhistory_bert = BERT_large.fit([tr...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22622</th>\n",
       "      <td>2462841</td>\n",
       "      <td>10038839</td>\n",
       "      <td>109</td>\n",
       "      <td>#load model with best losses\\nBERT_large.load_...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22623</th>\n",
       "      <td>2462842</td>\n",
       "      <td>10038839</td>\n",
       "      <td>110</td>\n",
       "      <td>#save as dataframe\\nsubmission_bert = pd.DataF...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22624</th>\n",
       "      <td>2462843</td>\n",
       "      <td>10038839</td>\n",
       "      <td>111</td>\n",
       "      <td>#and last but not least, submit\\nsubmission_be...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22625</th>\n",
       "      <td>2464127</td>\n",
       "      <td>9100265</td>\n",
       "      <td>0</td>\n",
       "      <td>import pandas as pd # data processing, CSV fil...</td>\n",
       "      <td>/moradnejad/distweets-perfect-score-for-evalua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22626 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       code_blocks_index  kernel_id  code_block_id  \\\n",
       "0                   9741   14113092              0   \n",
       "1                   9742   14113092              1   \n",
       "2                   9743   14113092              2   \n",
       "3                   9744   14113092              3   \n",
       "4                   9745   14113092              4   \n",
       "...                  ...        ...            ...   \n",
       "22621            2462840   10038839            108   \n",
       "22622            2462841   10038839            109   \n",
       "22623            2462842   10038839            110   \n",
       "22624            2462843   10038839            111   \n",
       "22625            2464127    9100265              0   \n",
       "\n",
       "                                              code_block  \\\n",
       "0      # This Python 3 environment comes with many he...   \n",
       "1      train = pd.read_csv(\"../input/nlp-getting-star...   \n",
       "2      def lowercase_text(text):\\n    return text.low...   \n",
       "3      import re\\nimport string\\ndef remove_noise(tex...   \n",
       "4      # Tokenizing the training and the test set\\nim...   \n",
       "...                                                  ...   \n",
       "22621  #train BERT\\nhistory_bert = BERT_large.fit([tr...   \n",
       "22622  #load model with best losses\\nBERT_large.load_...   \n",
       "22623  #save as dataframe\\nsubmission_bert = pd.DataF...   \n",
       "22624  #and last but not least, submit\\nsubmission_be...   \n",
       "22625  import pandas as pd # data processing, CSV fil...   \n",
       "\n",
       "                                             kernel_link  \n",
       "0                     /nicknosorogov/distweetrhinosceros  \n",
       "1                     /nicknosorogov/distweetrhinosceros  \n",
       "2                     /nicknosorogov/distweetrhinosceros  \n",
       "3                     /nicknosorogov/distweetrhinosceros  \n",
       "4                     /nicknosorogov/distweetrhinosceros  \n",
       "...                                                  ...  \n",
       "22621  /tuckerarrants/disaster-tweets-eda-glove-rnns-...  \n",
       "22622  /tuckerarrants/disaster-tweets-eda-glove-rnns-...  \n",
       "22623  /tuckerarrants/disaster-tweets-eda-glove-rnns-...  \n",
       "22624  /tuckerarrants/disaster-tweets-eda-glove-rnns-...  \n",
       "22625  /moradnejad/distweets-perfect-score-for-evalua...  \n",
       "\n",
       "[22626 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels = kernels_meta[kernels_meta['comp_name'] == top_comp]\n",
    "top_comp_code_blocks = code_blocks[code_blocks['kernel_id'].isin(kernels['kernel_id'])]\n",
    "merged_df = pd.merge(top_comp_code_blocks, kernels_meta, on='kernel_id')\n",
    "top_comp_code_blocks = merged_df.drop(columns=['kaggle_score', 'kaggle_comments', 'kaggle_upvotes', 'comp_name'])\n",
    "top_comp_code_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "      <th>predicted_graph_vertex__probability</th>\n",
       "      <th>graph_vertex_id</th>\n",
       "      <th>graph_vertex_class</th>\n",
       "      <th>graph_vertex_subclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14113092</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>88</td>\n",
       "      <td>Debug</td>\n",
       "      <td>list_files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14113092</td>\n",
       "      <td>1</td>\n",
       "      <td>train = pd.read_csv(\"../input/nlp-getting-star...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.999676</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14113092</td>\n",
       "      <td>2</td>\n",
       "      <td>def lowercase_text(text):\\n    return text.low...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.922104</td>\n",
       "      <td>20</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>categorify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14113092</td>\n",
       "      <td>3</td>\n",
       "      <td>import re\\nimport string\\ndef remove_noise(tex...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.751464</td>\n",
       "      <td>20</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>categorify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14113092</td>\n",
       "      <td>4</td>\n",
       "      <td># Tokenizing the training and the test set\\nim...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.923835</td>\n",
       "      <td>8</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>feature_engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22621</th>\n",
       "      <td>10038839</td>\n",
       "      <td>108</td>\n",
       "      <td>#train BERT\\nhistory_bert = BERT_large.fit([tr...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>7</td>\n",
       "      <td>Model_Train</td>\n",
       "      <td>train_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22622</th>\n",
       "      <td>10038839</td>\n",
       "      <td>109</td>\n",
       "      <td>#load model with best losses\\nBERT_large.load_...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.984101</td>\n",
       "      <td>48</td>\n",
       "      <td>Model_Evaluation</td>\n",
       "      <td>predict_on_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22623</th>\n",
       "      <td>10038839</td>\n",
       "      <td>110</td>\n",
       "      <td>#save as dataframe\\nsubmission_bert = pd.DataF...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.845476</td>\n",
       "      <td>55</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>prepare_output</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22624</th>\n",
       "      <td>10038839</td>\n",
       "      <td>111</td>\n",
       "      <td>#and last but not least, submit\\nsubmission_be...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.998979</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22625</th>\n",
       "      <td>9100265</td>\n",
       "      <td>0</td>\n",
       "      <td>import pandas as pd # data processing, CSV fil...</td>\n",
       "      <td>/moradnejad/distweets-perfect-score-for-evalua...</td>\n",
       "      <td>0.998242</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22626 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kernel_id  code_block_id  \\\n",
       "0       14113092              0   \n",
       "1       14113092              1   \n",
       "2       14113092              2   \n",
       "3       14113092              3   \n",
       "4       14113092              4   \n",
       "...          ...            ...   \n",
       "22621   10038839            108   \n",
       "22622   10038839            109   \n",
       "22623   10038839            110   \n",
       "22624   10038839            111   \n",
       "22625    9100265              0   \n",
       "\n",
       "                                              code_block  \\\n",
       "0      # This Python 3 environment comes with many he...   \n",
       "1      train = pd.read_csv(\"../input/nlp-getting-star...   \n",
       "2      def lowercase_text(text):\\n    return text.low...   \n",
       "3      import re\\nimport string\\ndef remove_noise(tex...   \n",
       "4      # Tokenizing the training and the test set\\nim...   \n",
       "...                                                  ...   \n",
       "22621  #train BERT\\nhistory_bert = BERT_large.fit([tr...   \n",
       "22622  #load model with best losses\\nBERT_large.load_...   \n",
       "22623  #save as dataframe\\nsubmission_bert = pd.DataF...   \n",
       "22624  #and last but not least, submit\\nsubmission_be...   \n",
       "22625  import pandas as pd # data processing, CSV fil...   \n",
       "\n",
       "                                             kernel_link  \\\n",
       "0                     /nicknosorogov/distweetrhinosceros   \n",
       "1                     /nicknosorogov/distweetrhinosceros   \n",
       "2                     /nicknosorogov/distweetrhinosceros   \n",
       "3                     /nicknosorogov/distweetrhinosceros   \n",
       "4                     /nicknosorogov/distweetrhinosceros   \n",
       "...                                                  ...   \n",
       "22621  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22622  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22623  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22624  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22625  /moradnejad/distweets-perfect-score-for-evalua...   \n",
       "\n",
       "       predicted_graph_vertex__probability  graph_vertex_id  \\\n",
       "0                                 0.999220               88   \n",
       "1                                 0.999676               45   \n",
       "2                                 0.922104               20   \n",
       "3                                 0.751464               20   \n",
       "4                                 0.923835                8   \n",
       "...                                    ...              ...   \n",
       "22621                             0.999671                7   \n",
       "22622                             0.984101               48   \n",
       "22623                             0.845476               55   \n",
       "22624                             0.998979               25   \n",
       "22625                             0.998242               25   \n",
       "\n",
       "      graph_vertex_class graph_vertex_subclass  \n",
       "0                  Debug            list_files  \n",
       "1        Data_Extraction         load_from_csv  \n",
       "2         Data_Transform            categorify  \n",
       "3         Data_Transform            categorify  \n",
       "4         Data_Transform   feature_engineering  \n",
       "...                  ...                   ...  \n",
       "22621        Model_Train           train_model  \n",
       "22622   Model_Evaluation       predict_on_test  \n",
       "22623        Data_Export        prepare_output  \n",
       "22624        Data_Export           save_to_csv  \n",
       "22625        Data_Export           save_to_csv  \n",
       "\n",
       "[22626 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(top_comp_code_blocks.merge(data_preds, on='code_blocks_index'), vertices, left_on='predicted_graph_vertex_id', right_on='graph_vertex_id')\n",
    "merged_df.drop(['code_blocks_index', 'predicted_graph_vertex_id'], axis=1, inplace=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "      <th>predicted_graph_vertex__probability</th>\n",
       "      <th>graph_vertex_id</th>\n",
       "      <th>graph_vertex_class</th>\n",
       "      <th>graph_vertex_subclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14113092</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>88</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>list_files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14113092</td>\n",
       "      <td>1</td>\n",
       "      <td>train = pd.read_csv(\"../input/nlp-getting-star...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.999676</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14113092</td>\n",
       "      <td>2</td>\n",
       "      <td>def lowercase_text(text):\\n    return text.low...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.922104</td>\n",
       "      <td>20</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>categorify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14113092</td>\n",
       "      <td>3</td>\n",
       "      <td>import re\\nimport string\\ndef remove_noise(tex...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.751464</td>\n",
       "      <td>20</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>categorify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14113092</td>\n",
       "      <td>4</td>\n",
       "      <td># Tokenizing the training and the test set\\nim...</td>\n",
       "      <td>/nicknosorogov/distweetrhinosceros</td>\n",
       "      <td>0.923835</td>\n",
       "      <td>8</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>feature_engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22621</th>\n",
       "      <td>10038839</td>\n",
       "      <td>108</td>\n",
       "      <td>#train BERT\\nhistory_bert = BERT_large.fit([tr...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>7</td>\n",
       "      <td>Model_Train</td>\n",
       "      <td>train_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22622</th>\n",
       "      <td>10038839</td>\n",
       "      <td>109</td>\n",
       "      <td>#load model with best losses\\nBERT_large.load_...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.984101</td>\n",
       "      <td>48</td>\n",
       "      <td>Model_Evaluation</td>\n",
       "      <td>predict_on_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22623</th>\n",
       "      <td>10038839</td>\n",
       "      <td>110</td>\n",
       "      <td>#save as dataframe\\nsubmission_bert = pd.DataF...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.845476</td>\n",
       "      <td>55</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>prepare_output</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22624</th>\n",
       "      <td>10038839</td>\n",
       "      <td>111</td>\n",
       "      <td>#and last but not least, submit\\nsubmission_be...</td>\n",
       "      <td>/tuckerarrants/disaster-tweets-eda-glove-rnns-...</td>\n",
       "      <td>0.998979</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22625</th>\n",
       "      <td>9100265</td>\n",
       "      <td>0</td>\n",
       "      <td>import pandas as pd # data processing, CSV fil...</td>\n",
       "      <td>/moradnejad/distweets-perfect-score-for-evalua...</td>\n",
       "      <td>0.998242</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22626 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kernel_id  code_block_id  \\\n",
       "0       14113092              0   \n",
       "1       14113092              1   \n",
       "2       14113092              2   \n",
       "3       14113092              3   \n",
       "4       14113092              4   \n",
       "...          ...            ...   \n",
       "22621   10038839            108   \n",
       "22622   10038839            109   \n",
       "22623   10038839            110   \n",
       "22624   10038839            111   \n",
       "22625    9100265              0   \n",
       "\n",
       "                                              code_block  \\\n",
       "0      # This Python 3 environment comes with many he...   \n",
       "1      train = pd.read_csv(\"../input/nlp-getting-star...   \n",
       "2      def lowercase_text(text):\\n    return text.low...   \n",
       "3      import re\\nimport string\\ndef remove_noise(tex...   \n",
       "4      # Tokenizing the training and the test set\\nim...   \n",
       "...                                                  ...   \n",
       "22621  #train BERT\\nhistory_bert = BERT_large.fit([tr...   \n",
       "22622  #load model with best losses\\nBERT_large.load_...   \n",
       "22623  #save as dataframe\\nsubmission_bert = pd.DataF...   \n",
       "22624  #and last but not least, submit\\nsubmission_be...   \n",
       "22625  import pandas as pd # data processing, CSV fil...   \n",
       "\n",
       "                                             kernel_link  \\\n",
       "0                     /nicknosorogov/distweetrhinosceros   \n",
       "1                     /nicknosorogov/distweetrhinosceros   \n",
       "2                     /nicknosorogov/distweetrhinosceros   \n",
       "3                     /nicknosorogov/distweetrhinosceros   \n",
       "4                     /nicknosorogov/distweetrhinosceros   \n",
       "...                                                  ...   \n",
       "22621  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22622  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22623  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22624  /tuckerarrants/disaster-tweets-eda-glove-rnns-...   \n",
       "22625  /moradnejad/distweets-perfect-score-for-evalua...   \n",
       "\n",
       "       predicted_graph_vertex__probability  graph_vertex_id  \\\n",
       "0                                 0.999220               88   \n",
       "1                                 0.999676               45   \n",
       "2                                 0.922104               20   \n",
       "3                                 0.751464               20   \n",
       "4                                 0.923835                8   \n",
       "...                                    ...              ...   \n",
       "22621                             0.999671                7   \n",
       "22622                             0.984101               48   \n",
       "22623                             0.845476               55   \n",
       "22624                             0.998979               25   \n",
       "22625                             0.998242               25   \n",
       "\n",
       "              graph_vertex_class graph_vertex_subclass  \n",
       "0      Exploratory_Data_Analysis            list_files  \n",
       "1                Data_Extraction         load_from_csv  \n",
       "2                 Data_Transform            categorify  \n",
       "3                 Data_Transform            categorify  \n",
       "4                 Data_Transform   feature_engineering  \n",
       "...                          ...                   ...  \n",
       "22621                Model_Train           train_model  \n",
       "22622           Model_Evaluation       predict_on_test  \n",
       "22623                Data_Export        prepare_output  \n",
       "22624                Data_Export           save_to_csv  \n",
       "22625                Data_Export           save_to_csv  \n",
       "\n",
       "[22626 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['graph_vertex_class'] == 'EDA', 'graph_vertex_class'] = 'Exploratory_Data_Analysis'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Hyperparam_Tuning', 'graph_vertex_class'] = 'Model_Train'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Model_Interpretation', 'graph_vertex_class'] = 'Visualization'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Debug', 'graph_vertex_class'] = 'Exploratory_Data_Analysis'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Other', 'graph_vertex_class'] = 'Exploratory_Data_Analysis'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Environment', 'graph_vertex_class'] = 'Imports_and_Environment'\n",
    "\n",
    "LABELS = [\n",
    "    \"Data_Transform\",\n",
    "    \"Data_Extraction\",\n",
    "    \"Visualization\",\n",
    "    \"Model_Train\",\n",
    "    \"Model_Evaluation\",\n",
    "    \"Imports_and_Environment\",\n",
    "    \"Data_Export\",\n",
    "    \"Exploratory_Data_Analysis\", \n",
    "    # \"Other\" # \"Other\"\n",
    "]\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "725"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_kernel_links = kernels['kernel_link'].unique()\n",
    "len(distinct_kernel_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_df[merged_df['kernel_link'] == distinct_kernel_links[3]].sort_values('code_block_id', ascending=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dir = \"../../data/test_datasets\"\n",
    "merged_df.to_csv(f'{test_dataset_dir}/{top_comp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual cell for saving the classified notebooks in a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:05<00:10,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error pulling kernel: /andrej0marinchenko/nlp-with-disaster-tweets-new-b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:09<00:06,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error pulling kernel: /sohelranaccselab/nlp-disaster-tweets-using-bert-for-beginner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:18<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28 notebooks to /home/ryounis/Documents/Zurich/PEACHLab/datascience-visualisation/src/backend/data/datasets/Natural Language Processing with Disaster Tweets/unclassified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utils.constants import BLANK_IPYNB_JSON\n",
    "from utils.helper_functions import load_notebook\n",
    "import json\n",
    "import os\n",
    "import tempfile \n",
    "from kaggle.api.kaggle_api_extended import ApiException\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_dataset_dir = f\"datascience-visualisation/data/datasets/{top_comp}/unclassified\"\n",
    "if not os.path.exists(test_dataset_dir): os.makedirs(test_dataset_dir)\n",
    "\n",
    "FIRST_N_KERNELS = 30\n",
    "\n",
    "notebooks = []\n",
    "for kernel_link in tqdm(distinct_kernel_links[:FIRST_N_KERNELS]):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        kernel_df = merged_df[merged_df['kernel_link'] == kernel_link].sort_values('code_block_id', ascending=True)\n",
    "        try:\n",
    "            kaggle.api.kernels_pull(kernel_link[1:], path=temp_dir, metadata=True)\n",
    "            notebook_path = f\"{temp_dir}/{kernel_link.split('/')[-1]}.ipynb\"\n",
    "            notebook = load_notebook(notebook_path)\n",
    "        except ApiException as e:\n",
    "            print(f\"Error pulling kernel: {kernel_link}\")\n",
    "            continue\n",
    "        \n",
    "    code_cell_counter = 0\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'code' and len(cell['source']):\n",
    "            row = kernel_df.iloc[code_cell_counter]\n",
    "            code_cell_counter += 1\n",
    "\n",
    "            cell[\"metadata\"] = {\n",
    "                **cell[\"metadata\"],\n",
    "                \"class\": row[\"graph_vertex_class\"],\n",
    "                \"subclass\": row[\"graph_vertex_subclass\"],\n",
    "                \"subclass_id\": int(row[\"graph_vertex_id\"]),\n",
    "                \"predicted_subclass_probability\": row[\"predicted_graph_vertex__probability\"],\n",
    "                \"notebook_id\": int(row['kernel_id']),\n",
    "            }    \n",
    "    notebook[\"metadata\"][\"labels\"] = LABELS\n",
    "    notebooks.append(notebook)\n",
    "    with open(f\"{test_dataset_dir}/{kernel_link.split('/')[-1]}.ipynb\", \"w\") as f:json.dump(notebook, f)\n",
    "    \n",
    "print(f\"Saved {len(notebooks)} notebooks to {test_dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "clusterer = HDBSCAN(\n",
    "    min_cluster_size=4,\n",
    "    min_samples=2,\n",
    "    cluster_selection_epsilon=.0,\n",
    "    max_cluster_size=None,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "clusters = {}\n",
    "for key, group in grouped_cells.items(): \n",
    "    X = np.array([cell[\"embedding\"] for cell in group])\n",
    "    \n",
    "    if len(group) < 4:\n",
    "        labels = [-1] * len(group)\n",
    "    else:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(X)\n",
    "        embeddings_pca = pca.transform(X)\n",
    "        clusterer.fit(embeddings_pca)\n",
    "        labels = clusterer.labels_\n",
    "        \n",
    "    clusters[key] = {str(label): [] for label in set(labels)}\n",
    "    for cell, label, embedding in zip(group, labels, embeddings_pca):\n",
    "        clusters[key][str(label)].append({\n",
    "            \"cell\": cell,\n",
    "            \"embedding\": embedding\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, elems in clusters.items():\n",
    "    print(f\"Class: {key}\")\n",
    "    for label, group in elems.items():\n",
    "        print(f\"    {label}: {len(group)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot of the embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "for key, clusts in clusters.items():\n",
    "    \n",
    "    \n",
    "    plt.scatter(\n",
    "        x=embeddings_pca[:, 0],\n",
    "        y=embeddings_pca[:, 1],\n",
    "        cmap='viridis',\n",
    "        alpha=0.5,\n",
    "        label=key\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('Clusters Visualization')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper_functions import notebook_extract_code, notebook_add_class_labels\n",
    "\n",
    "notebooks = []\n",
    "for kernel_id in merged_df['kernel_id'].unique()[:20]:\n",
    "    notebook_json = BLANK_IPYNB_JSON.copy()\n",
    "    notebook_json['cells'] = []\n",
    "    for row in merged_df[merged_df['kernel_id'] == kernel_id].iterrows():\n",
    "        notebook_json['cells'].append({\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": None,\n",
    "            \"metadata\": {\n",
    "                \"graph_vertex_id\": row[1][\"graph_vertex_id\"],\n",
    "                \"predicted_graph_vertex__probability\": row[1][\"predicted_graph_vertex__probability\"],\n",
    "                \"notebook_id\": row[1]['kernel_id'],\n",
    "            },\n",
    "            \"source\": row[1]['code_block']\n",
    "        })\n",
    "    notebooks.append(notebook_json)\n",
    "# notebook_code = notebook_extract_code(notebook_json)\n",
    "# cell_labels = classifier.classify_notebook(notebook_code)\n",
    "# notebook_json = notebook_add_class_labels(notebook_json, cell_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notebooks = []\n",
    "notebook_cell_labels = []\n",
    "for i, notebook_json in enumerate(notebooks):\n",
    "    print(f\"Notebook {i+1}/{len(notebooks)}\", end='\\r')\n",
    "    \n",
    "    notebook_code = notebook_extract_code(notebook_json)\n",
    "    cell_labels = classifier.classify_notebook(notebook_code)\n",
    "    notebook_json = notebook_add_class_labels(notebook_json, cell_labels)\n",
    "    labeled_notebooks.append(notebook_json)\n",
    "    notebook_cell_labels.append(cell_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for notebook in notebook_cell_labels:\n",
    "    for elem in notebook:\n",
    "        embeddings.append(elem[1])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "clusterer: HDBSCAN = HDBSCAN(\n",
    "    min_cluster_size=4,                 # Minimum number of samples to form a cluster\n",
    "    min_samples=2,                      # Minimum number of samples in a neighborhood to be considered as a core point\n",
    "    cluster_selection_epsilon=0,     # If 2 clusters are less than epsilon apart, they get merged\n",
    ")\n",
    "\n",
    "clusterer.fit(embeddings)\n",
    "for label in set(clusterer.labels_):\n",
    "    print(f\"Cluster {label}: {len([x for x in clusterer.labels_ if x == label])} cells\")\n",
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook['cells']:\n",
    "        cell['metadata']['cluster_label'] = clusterer.labels_[counter]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notebooks[0][\"cells\"]\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook[\"cells\"]:\n",
    "        true_labels.append(cell[\"metadata\"][\"graph_vertex_id\"])\n",
    "        predicted_labels.append(cell[\"metadata\"][\"cluster_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def count_misclustered_elements(true_labels, predicted_labels):\n",
    "    # Convert the labels to numpy arrays for easier manipulation\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    \n",
    "    unique_true_labels = np.unique(true_labels)\n",
    "    unique_predicted_labels = np.unique(predicted_labels)\n",
    "    \n",
    "    cost_matrix = np.zeros((len(unique_true_labels), len(unique_predicted_labels)), dtype=int)\n",
    "    \n",
    "    for i, true_label in enumerate(unique_true_labels):\n",
    "        for j, predicted_label in enumerate(unique_predicted_labels):\n",
    "            cost_matrix[i, j] = np.sum((true_labels == true_label) & (predicted_labels != predicted_label))\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    misclustered_count = cost_matrix[row_ind, col_ind].sum()\n",
    "    \n",
    "    return misclustered_count\n",
    "\n",
    "misclustered_count = count_misclustered_elements(true_labels, predicted_labels)\n",
    "print(f\"Number of misclustered elements: {misclustered_count}/{len(true_labels)}\")\n",
    "print(f\"Score: {1 - misclustered_count/len(true_labels)} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook['cells']:\n",
    "        cell['metadata']['cluster_label'] = clusterer.labels_[counter]\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
