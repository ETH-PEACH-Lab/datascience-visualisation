{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_blocks_index</th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17493583</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17493583</td>\n",
       "      <td>1</td>\n",
       "      <td>import os\\nimport zipfile\\nfrom subprocess imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17493583</td>\n",
       "      <td>2</td>\n",
       "      <td>def reduce_mem_usage(df):\\n    \"\"\" iterate thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17493583</td>\n",
       "      <td>3</td>\n",
       "      <td>import numpy as np</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17493583</td>\n",
       "      <td>4</td>\n",
       "      <td>orders_df = reduce_mem_usage(pd.read_csv('../w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599348</th>\n",
       "      <td>2599348</td>\n",
       "      <td>8902213</td>\n",
       "      <td>22</td>\n",
       "      <td>f=[]\\n\\nfor root, dirs, files in os.walk(\"../o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599349</th>\n",
       "      <td>2599349</td>\n",
       "      <td>8902213</td>\n",
       "      <td>23</td>\n",
       "      <td>my_model.fit_generator(\\n        train_data_ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599350</th>\n",
       "      <td>2599350</td>\n",
       "      <td>8902213</td>\n",
       "      <td>24</td>\n",
       "      <td>test_generator.reset()\\n\\npred = my_model.pred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599351</th>\n",
       "      <td>2599351</td>\n",
       "      <td>8902213</td>\n",
       "      <td>25</td>\n",
       "      <td>import cv2\\n\\n\\nfrom matplotlib import pyplot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599352</th>\n",
       "      <td>2599352</td>\n",
       "      <td>8902213</td>\n",
       "      <td>26</td>\n",
       "      <td>results_df = pd.DataFrame(\\n    {\\n        'id...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2599353 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         code_blocks_index  kernel_id  code_block_id  \\\n",
       "0                        0   17493583              0   \n",
       "1                        1   17493583              1   \n",
       "2                        2   17493583              2   \n",
       "3                        3   17493583              3   \n",
       "4                        4   17493583              4   \n",
       "...                    ...        ...            ...   \n",
       "2599348            2599348    8902213             22   \n",
       "2599349            2599349    8902213             23   \n",
       "2599350            2599350    8902213             24   \n",
       "2599351            2599351    8902213             25   \n",
       "2599352            2599352    8902213             26   \n",
       "\n",
       "                                                code_block  \n",
       "0        # This Python 3 environment comes with many he...  \n",
       "1        import os\\nimport zipfile\\nfrom subprocess imp...  \n",
       "2        def reduce_mem_usage(df):\\n    \"\"\" iterate thr...  \n",
       "3                                       import numpy as np  \n",
       "4        orders_df = reduce_mem_usage(pd.read_csv('../w...  \n",
       "...                                                    ...  \n",
       "2599348  f=[]\\n\\nfor root, dirs, files in os.walk(\"../o...  \n",
       "2599349  my_model.fit_generator(\\n        train_data_ge...  \n",
       "2599350  test_generator.reset()\\n\\npred = my_model.pred...  \n",
       "2599351  import cv2\\n\\n\\nfrom matplotlib import pyplot ...  \n",
       "2599352  results_df = pd.DataFrame(\\n    {\\n        'id...  \n",
       "\n",
       "[2599353 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import kaggle\n",
    "\n",
    "\n",
    "code4ML_path = \"/home/ryounis/Documents/Zurich/PEACHLab/data/Code4ML\"\n",
    "\n",
    "kernels_meta = pd.read_csv(f\"{code4ML_path}/kernels_meta.csv\")\n",
    "markup_data = pd.read_csv(f\"{code4ML_path}/markup_data.csv\")\n",
    "vertices = pd.read_csv(f\"{code4ML_path}/vertices.csv\")\n",
    "code_blocks = pd.read_csv(f\"{code4ML_path}/code_blocks.csv\")\n",
    "data_preds = pd.read_csv(f\"{code4ML_path}/data_with_preds.csv\")\n",
    "\n",
    "code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition with most kernels: 'Titanic - Machine Learning from Disaster'  (1963 kernels)\n",
      "\n",
      "Titanic - Machine Learning from Disaster: 1963 kernels\n",
      "Digit Recognizer: 1300 kernels\n",
      "House Prices - Advanced Regression Techniques: 1013 kernels\n",
      "Natural Language Processing with Disaster Tweets: 725 kernels\n",
      "Quora Insincere Questions Classification: 679 kernels\n",
      "Mechanisms of Action (MoA) Prediction: 660 kernels\n",
      "COVID19 Global Forecasting (Week 4): 434 kernels\n",
      "Santander Customer Transaction Prediction: 418 kernels\n",
      "PUBG Finish Placement Prediction (Kernels Only): 394 kernels\n",
      "Tweet Sentiment Extraction: 386 kernels\n",
      "Cassava Leaf Disease Classification: 383 kernels\n",
      "Kannada MNIST: 362 kernels\n",
      "Aerial Cactus Identification: 345 kernels\n",
      "SIIM-ISIC Melanoma Classification: 325 kernels\n",
      "Riiid Answer Correctness Prediction: 322 kernels\n",
      "IEEE-CIS Fraud Detection: 303 kernels\n",
      "APTOS 2019 Blindness Detection: 303 kernels\n",
      "Predict Future Sales: 295 kernels\n",
      "COVID19 Global Forecasting (Week 2): 288 kernels\n",
      "Global Wheat Detection: 272 kernels\n",
      "Jane Street Market Prediction: 272 kernels\n",
      "Costa Rican Household Poverty Level Prediction: 259 kernels\n",
      "Jigsaw Unintended Bias in Toxicity Classification: 244 kernels\n",
      "Toxic Comment Classification Challenge: 241 kernels\n",
      "Home Credit Default Risk: 235 kernels\n",
      "OSIC Pulmonary Fibrosis Progression: 217 kernels\n",
      "Jigsaw Multilingual Toxic Comment Classification: 213 kernels\n",
      "Petals to the Metal - Flower Classification on TPU: 211 kernels\n",
      "Tabular Playground Series - Jan 2021: 209 kernels\n",
      "PetFinder.my Adoption Prediction: 201 kernels\n",
      "Mercari Price Suggestion Challenge: 192 kernels\n",
      "LANL Earthquake Prediction: 191 kernels\n",
      "M5 Forecasting - Accuracy: 184 kernels\n",
      "Flower Classification with TPUs: 180 kernels\n",
      "Instant Gratification: 179 kernels\n",
      "University of Liverpool - Ion Switching: 178 kernels\n",
      "Google QUEST Q&A Labeling: 178 kernels\n",
      "What's Cooking? (Kernels Only): 162 kernels\n",
      "ASHRAE - Great Energy Predictor III: 161 kernels\n",
      "Plant Pathology 2020 - FGVC7: 157 kernels\n",
      "Generative Dog Images: 155 kernels\n",
      "Bengali.AI Handwritten Grapheme Classification: 153 kernels\n",
      "Bike Sharing Demand: 151 kernels\n",
      "COVID19 Global Forecasting (Week 1): 147 kernels\n",
      "COVID19 Global Forecasting (Week 3): 144 kernels\n",
      "NFL Big Data Bowl: 142 kernels\n",
      "New York City Taxi Fare Prediction: 142 kernels\n",
      "Don't Overfit! II: 141 kernels\n",
      "Dogs vs. Cats Redux: Kernels Edition: 140 kernels\n",
      "Two Sigma: Using News to Predict Stock Movements: 136 kernels\n",
      "Categorical Feature Encoding Challenge: 135 kernels\n",
      "New York City Taxi Trip Duration: 130 kernels\n",
      "Categorical Feature Encoding Challenge II: 129 kernels\n",
      "Severstal: Steel Defect Detection: 120 kernels\n",
      "TMDB Box Office Prediction: 114 kernels\n",
      "Santander Value Prediction Challenge: 114 kernels\n",
      "OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction: 114 kernels\n",
      "Histopathologic Cancer Detection: 113 kernels\n",
      "Deepfake Detection Challenge: 112 kernels\n",
      "Google Analytics Customer Revenue Prediction: 112 kernels\n",
      "Prostate cANcer graDe Assessment (PANDA) Challenge: 110 kernels\n",
      "TGS Salt Identification Challenge: 110 kernels\n",
      "Predicting Molecular Properties: 104 kernels\n",
      "Contradictory, My Dear Watson: 101 kernels\n",
      "CareerCon 2019 - Help Navigate Robots: 101 kernels\n",
      "Humpback Whale Identification: 95 kernels\n",
      "Cornell Birdcall Identification: 94 kernels\n",
      "RANZCR CLiP - Catheter and Line Position Challenge: 83 kernels\n",
      "Tabular Playground Series - Feb 2021: 82 kernels\n",
      "Leaf Classification: 81 kernels\n",
      "Ghouls, Goblins, and Ghosts... Boo!: 77 kernels\n",
      "TReNDS Neuroimaging: 74 kernels\n",
      "Porto Seguro’s Safe Driver Prediction: 74 kernels\n",
      "Lyft Motion Prediction for Autonomous Vehicles: 72 kernels\n",
      "Abstraction and Reasoning Challenge: 72 kernels\n",
      "Rossmann Store Sales: 71 kernels\n",
      "Quick, Draw! Doodle Recognition Challenge: 70 kernels\n",
      "Forest Cover Type Prediction: 69 kernels\n",
      "Dog Breed Identification: 68 kernels\n",
      "Traveling Santa 2018 - Prime Paths: 67 kernels\n",
      "Plant Seedlings Classification: 67 kernels\n",
      "Microsoft Malware Prediction: 65 kernels\n",
      "Two Sigma Connect: Rental Listing Inquiries: 64 kernels\n",
      "San Francisco Crime Classification: 63 kernels\n",
      "Spooky Author Identification: 62 kernels\n",
      "Avito Demand Prediction Challenge: 62 kernels\n",
      "HuBMAP - Hacking the Kidney: 61 kernels\n",
      "RSNA STR Pulmonary Embolism Detection: 57 kernels\n",
      "Bag of Words Meets Bags of Popcorn: 56 kernels\n",
      "Santa's Workshop Tour 2019: 55 kernels\n",
      "Quora Question Pairs: 54 kernels\n",
      "Santander Customer Satisfaction: 54 kernels\n",
      "ALASKA2 Image Steganalysis: 53 kernels\n",
      "Google Landmark Recognition 2020: 53 kernels\n",
      "Rainforest Connection Species Audio Detection: 52 kernels\n",
      "Freesound Audio Tagging 2019: 52 kernels\n",
      "Human Protein Atlas Image Classification: 52 kernels\n",
      "TalkingData AdTracking Fraud Detection Challenge: 51 kernels\n",
      "INGV - Volcanic Eruption Prediction: 49 kernels\n",
      "TensorFlow 2.0 Question Answering: 49 kernels\n",
      "Restaurant Revenue Prediction: 49 kernels\n",
      "DonorsChoose.org Application Screening: 46 kernels\n",
      "Instacart Market Basket Analysis: 45 kernels\n",
      "VSB Power Line Fault Detection: 43 kernels\n",
      "iMet Collection 2019 - FGVC6: 41 kernels\n",
      "Gendered Pronoun Resolution: 41 kernels\n",
      "Otto Group Product Classification Challenge: 41 kernels\n",
      "Google Cloud & NCAA® ML Competition 2020-NCAAM: 41 kernels\n",
      "Understanding Clouds from Satellite Images: 40 kernels\n",
      "Mercedes-Benz Greener Manufacturing: 40 kernels\n",
      "Sentiment Analysis on Movie Reviews: 39 kernels\n",
      "RSNA Pneumonia Detection Challenge: 38 kernels\n",
      "Conway's Reverse Game of Life 2020: 37 kernels\n",
      "What's Cooking?: 37 kernels\n",
      "Facial Keypoints Detection: 36 kernels\n",
      "Recruit Restaurant Visitor Forecasting: 35 kernels\n",
      "Northeastern SMILE Lab - Recognizing Faces in the Wild: 35 kernels\n",
      "M5 Forecasting - Uncertainty: 34 kernels\n",
      "Statoil/C-CORE Iceberg Classifier Challenge: 34 kernels\n",
      "Airbnb New User Bookings: 33 kernels\n",
      "Airbus Ship Detection Challenge: 33 kernels\n",
      "Kobe Bryant Shot Selection: 31 kernels\n",
      "RSNA Intracranial Hemorrhage Detection: 30 kernels\n",
      "Sberbank Russian Housing Market: 30 kernels\n",
      "PLAsTiCC Astronomical Classification: 29 kernels\n",
      "SIIM-ACR Pneumothorax Segmentation: 28 kernels\n",
      "Indoor Location & Navigation: 28 kernels\n",
      "Walmart Recruiting - Store Sales Forecasting: 25 kernels\n",
      "Zillow Prize: Zillow’s Home Value Prediction (Zestimate): 25 kernels\n",
      "Elo Merchant Category Recommendation: 25 kernels\n",
      "Google Cloud & NCAA® ML Competition 2020-NCAAW: 24 kernels\n",
      "Homework for Students: 22 kernels\n",
      "Personality Profile Prediction: 22 kernels\n",
      "Human Protein Atlas - Single Cell Classification: 21 kernels\n",
      "Santander Product Recommendation: 21 kernels\n",
      "Allstate Claims Severity: 21 kernels\n",
      "Recursion Cellular Image Classification: 21 kernels\n",
      "VinBigData Chest X-ray Abnormalities Detection: 21 kernels\n",
      "Don't Get Kicked!: 20 kernels\n",
      "IESB - 2019: 20 kernels\n",
      "Give Me Some Credit: 20 kernels\n",
      "Atividade_3_PMR3508: 20 kernels\n",
      "2019 Data Science Bowl: 19 kernels\n",
      "Предсказание положения космических объектов: 19 kernels\n",
      "MLH - Pokemon Challenge: 19 kernels\n",
      "Google Cloud & NCAA® ML Competition 2019-Men's: 18 kernels\n",
      "iWildCam 2019 - FGVC6: 18 kernels\n",
      "2019  ML competition with KISTI: 18 kernels\n",
      "State Farm Distracted Driver Detection: 18 kernels\n",
      "PMR3508 - Tarefa 1 - 3508 Adult Dataset: 18 kernels\n",
      "Exam for Students20200129: 17 kernels\n",
      "Google Landmark Retrieval 2020: 17 kernels\n",
      "Nomad2018 Predicting Transparent Conductors: 17 kernels\n",
      "PadhAI: Text - Non Text Classification Level 4b: 16 kernels\n",
      "NBA Rookies: 16 kernels\n",
      "Google Cloud & NCAA® ML Competition 2019-Women's: 16 kernels\n",
      "BNP Paribas Cardif Claims Management: 15 kernels\n",
      "Shelter Animal Outcomes: 15 kernels\n",
      "TalkingData Mobile User Demographics: 15 kernels\n",
      "QSTP - Deep Learning 2019: 15 kernels\n",
      "Freesound General-Purpose Audio Tagging Challenge: 14 kernels\n",
      "Predicting Red Hat Business Value: 14 kernels\n",
      "Expedia Hotel Recommendations: 13 kernels\n",
      "Peking University/Baidu - Autonomous Driving: 13 kernels\n",
      "Prudential Life Insurance Assessment: 12 kernels\n",
      "Corporación Favorita Grocery Sales Forecasting: 12 kernels\n",
      "How Much Did It Rain? II: 11 kernels\n",
      "Amazon.com - Employee Access Challenge: 11 kernels\n",
      "PadhAI: Text - Non Text Classification Level 4a: 11 kernels\n",
      "Open Images 2019 - Object Detection: 11 kernels\n",
      "Homesite Quote Conversion: 10 kernels\n",
      "Planet: Understanding the Amazon from Space: 10 kernels\n",
      "I’m Something of a Painter Myself: 10 kernels\n",
      "Eval Lab 2 F464: 10 kernels\n",
      "Diabetes Diagnosis: 10 kernels\n",
      "Cloud Faculty Institute Workshop: 10 kernels\n",
      "Ultrasound Nerve Segmentation: 10 kernels\n",
      "IESB Sul - IGM - Maio 2019: 10 kernels\n",
      "DSNet: fastai Hackathon: 10 kernels\n",
      "YKC-2nd: 9 kernels\n",
      "Predictive Equipment Failures: 9 kernels\n",
      "PadhAI: Hindi Vowel - Consonant Classification: 9 kernels\n",
      "iWildCam 2020 - FGVC7: 9 kernels\n",
      "National Data Science Challenge 2019 - Advanced: 8 kernels\n",
      "Grupo Bimbo Inventory Demand: 8 kernels\n",
      "Home Depot Product Search Relevance: 8 kernels\n",
      "Humpback Whale Identification Challenge: 8 kernels\n",
      "Hash Code Archive - Drone Delivery: 8 kernels\n",
      "AI Academy Intermediate Class Competition 1: 7 kernels\n",
      "ML 4 Money: 7 kernels\n",
      "ASN10e Final Submission - Detect COML Faces: 7 kernels\n",
      "WSDM - KKBox's Music Recommendation Challenge: 7 kernels\n",
      "[Student] Shopee Code League - Product Detection: 6 kernels\n",
      "Google Cloud & NCAA® ML Competition 2018-Men's: 6 kernels\n",
      "IEEE's Signal Processing Society - Camera Model Identification: 6 kernels\n",
      "WSDM - KKBox's Churn Prediction Challenge: 6 kernels\n",
      "Invasive Species Monitoring: 6 kernels\n",
      "Sarcasmo: 6 kernels\n",
      "Seleksi Calon Asisten GAIB: 6 kernels\n",
      "Whose line is it anyway?: 6 kernels\n",
      "IESB Norte - IGM - Maio 2019: 6 kernels\n",
      "Competição DSA de Machine Learning: 6 kernels\n",
      "ISSM2020 AI Challenge: 5 kernels\n",
      "Hackathon Auto_matic: 5 kernels\n",
      "StumbleUpon Evergreen Classification Challenge: 5 kernels\n",
      "Santa Gift Matching Challenge: 5 kernels\n",
      "TrackML Particle Tracking Challenge: 5 kernels\n",
      "Classifying Movie Reviews: 5 kernels\n",
      "WiDS Datathon 2019: 5 kernels\n",
      "Coupon Purchase Prediction: 5 kernels\n",
      "Santa 2019: Revenge of the Accountants: 5 kernels\n",
      "Rock, Paper, Scissors: 5 kernels\n",
      "DM-Assignment 1: 5 kernels\n",
      "hackStat 2.0: 5 kernels\n",
      "Bosch Production Line Performance: 5 kernels\n",
      "Web Enthusiasts' Club NITK Recruitment: 5 kernels\n",
      "Web Traffic Time Series Forecasting: 4 kernels\n",
      "Loan Default Prediction - Imperial College London: 4 kernels\n",
      "Recommender Systems: 4 kernels\n",
      "Outbrain Click Prediction: 4 kernels\n",
      "Google Cloud & NCAA® ML Competition 2018-Women's: 4 kernels\n",
      "car-classification: 4 kernels\n",
      "DS特論2019年度 演習課題2: 4 kernels\n",
      "DL in NLP Spring 2019. Classification: 4 kernels\n",
      "The Nature Conservancy Fisheries Monitoring: 4 kernels\n",
      "Text classification: 4 kernels\n",
      "Predicting a Biological Response: 4 kernels\n",
      "ML in biology: 4 kernels\n",
      "Google Landmark Recognition Challenge: 4 kernels\n",
      "Property price prediction challenge: 4 kernels\n",
      "UI DS Summer School: 4 kernels\n",
      "Lyft 3D Object Detection for Autonomous Vehicles: 4 kernels\n",
      "Hackathon Sentimento: 4 kernels\n",
      "Telstra Network Disruptions: 4 kernels\n",
      "AILAB ML Training #1: 4 kernels\n",
      "Basic Regression Competition: 3 kernels\n",
      "Halite by Two Sigma: 3 kernels\n",
      "Python for Data science ITEA: 3 kernels\n",
      "I-RICH ML COMPETITION: 3 kernels\n",
      "Fake News e ML: 3 kernels\n",
      "Facebook V: Predicting Check Ins: 3 kernels\n",
      "Market Basket - ID NDSC 2020: 3 kernels\n",
      "Click-Through Rate Prediction: 3 kernels\n",
      "PadhAI: Tamil Vowel - Consonant Classification: 3 kernels\n",
      "Hackathon Sentimento_v2: 3 kernels\n",
      "SERPRO - Abalone: 3 kernels\n",
      "VSU ML 1 Regression: 3 kernels\n",
      "Personalized Medicine: Redefining Cancer Treatment: 3 kernels\n",
      "Sentiment Analysis in Russian: 3 kernels\n",
      "Multiple regression for time series data: 3 kernels\n",
      "Brain Cancer Classification: 3 kernels\n",
      "[ACM] Recommender System Practice: 3 kernels\n",
      "NFL 1st and Future - Impact Detection: 3 kernels\n",
      "Santa's Uncertain Bags: 3 kernels\n",
      "Used Cars Price Prediction: 3 kernels\n",
      "COVID-19 diagnostic: 2 kernels\n",
      "Predict the Income - <DECODE> WITH BOARD: 2 kernels\n",
      "Pycon Korea 2018 - Tutorial: 2 kernels\n",
      "Regression Evaluative Lab: 2 kernels\n",
      "OCRV Test Task: 2 kernels\n",
      "notMNIST Competition: 2 kernels\n",
      "[Open] Shopee Code League - Logistics: 2 kernels\n",
      "Conway's Reverse Game of Life: 2 kernels\n",
      "Higgs Boson Machine Learning Challenge: 2 kernels\n",
      "empty: 2 kernels\n",
      "MLClass Dubai by ODS, Lecture 6 HW: 2 kernels\n",
      "Классификация изображений: 2 kernels\n",
      "Texts classification: 2 kernels\n",
      "ML Challenge: 2 kernels\n",
      "West Nile Virus Prediction: 2 kernels\n",
      "VietAI Advance Course - Retinal Disease Detection: 2 kernels\n",
      "Characters classification: 2 kernels\n",
      "House pricing: 2 kernels\n",
      "UCI-HAR: 2 kernels\n",
      "Schnell-Mal-Klassifizieren: 2 kernels\n",
      "CIFAR-10 - Object Recognition in Images: 2 kernels\n",
      "SERPRO - Iris: 2 kernels\n",
      "ClassificationOFShields: 2 kernels\n",
      "Задержка рейса самолета: 2 kernels\n",
      "Pneumonia Diagnosis: 2 kernels\n",
      "UMUC DATA 650 Summer 2019 Competition: 2 kernels\n",
      "Птица или самолет: 2 kernels\n",
      "House Price Prediction: 2 kernels\n",
      "Machine Learning Lab - CAS Data Science FS 20: 2 kernels\n",
      "Tap30 Challenge: 2 kernels\n",
      "Data Champions Android App Malware Prediction: 2 kernels\n",
      "Diabetes Classification: 2 kernels\n",
      "IES Data Mining(WS 18/19): 2 kernels\n",
      "Language Identification: 2 kernels\n",
      "March Machine Learning Mania 2016: 2 kernels\n",
      "2018 Data Science Bowl: 2 kernels\n",
      "CSC: HW4 spring19: 2 kernels\n",
      "YKC-cup-1st: 2 kernels\n",
      "Digit Classification DL Workshop: 2 kernels\n",
      "Influencers in Social Networks: 2 kernels\n",
      "AILAB ML Training #0: 2 kernels\n",
      "Open Images 2019 - Instance Segmentation: 1 kernels\n",
      "Find me that fish: 1 kernels\n",
      "Painter by Numbers: 1 kernels\n",
      "Avito Duplicate Ads Detection: 1 kernels\n",
      "Diabetic Retinopathy Detection: 1 kernels\n",
      "Summer Analytics 2020 Capstone Project: 1 kernels\n",
      "Similarity Search Project: 1 kernels\n",
      "Galaxy Zoo - The Galaxy Challenge: 1 kernels\n",
      "Flavours of Physics: Finding τ  →  μμμ: 1 kernels\n",
      "Cdiscount’s Image Classification Challenge: 1 kernels\n",
      "DL for exploration geophysics: 1 kernels\n",
      "Liberty Mutual Group: Property Inspection Prediction: 1 kernels\n",
      "Data Series Summarization Project (v3): 1 kernels\n",
      "Car loan default: 1 kernels\n",
      "KNIT_HACKS: 1 kernels\n",
      "Carvana Image Masking Challenge: 1 kernels\n",
      "Data Science - Master: 1 kernels\n",
      "Oxford Fast AI Week 2: 1 kernels\n",
      "[T] PadhAI: Text - Non Text Classification Level 1: 1 kernels\n",
      "EPAM: Exercise 1 -  Sentiment Analysis: 1 kernels\n",
      "UC Irvine Math 10 Winter 2020: 1 kernels\n",
      "The Kaggle Master: 1 kernels\n",
      "Competencia-Series-Temporales: 1 kernels\n",
      "SQL Saturday Madrid ML Challenge: 1 kernels\n",
      "IA1819: 1 kernels\n",
      "DeepNLP HSE Course: 1 kernels\n",
      "BU CS506 Spring 2020 Midterm: 1 kernels\n",
      "Focus start 2020: 1 kernels\n",
      "Predicting Age Groups: 1 kernels\n",
      "ML Hackathon 2019 Q1: 1 kernels\n",
      "Tobigs13_7week_competition: 1 kernels\n",
      "Pitch estimation and voicing detection: 1 kernels\n",
      "COMP 750/850 Project 1: 1 kernels\n",
      "Tweet Sentiment Analysis: 1 kernels\n",
      "Fashion MNIST challenge2019: 1 kernels\n",
      "2019 SMHRD 경진대회 ( 지능형 ): 1 kernels\n",
      "SkillFactory | Final hackathon: 1 kernels\n",
      "SMEMI309 - Final evaluation challenge 2020: 1 kernels\n",
      "Predict the missing pixel value v2: 1 kernels\n",
      "Tracy Regression: 1 kernels\n",
      "Japanese Review Rating Prediction: 1 kernels\n",
      "SYDE 522 (Winter 2020): 1 kernels\n",
      "HEROZ Internal Competition: 1 kernels\n",
      "Классификация компьютерных атак: 1 kernels\n",
      "JAMP Hackathon Drive 1: 1 kernels\n",
      "AS-bow-2019-2020: 1 kernels\n",
      "NTUST: Information Retrieval and Applications: 1 kernels\n",
      "TensorFlow Speech Recognition Challenge: 1 kernels\n",
      "Springleaf Marketing Response: 1 kernels\n",
      "Hungry Geese: 1 kernels\n",
      "Gallivanters: 1 kernels\n",
      "Cats vs Dogs vs More: 1 kernels\n",
      "Aesthetic Visual Analysis: 1 kernels\n",
      "University of applied sciences Mannheim: 1 kernels\n",
      "CSM/SEM6420 workshop: 1 kernels\n",
      "KaggleDays Paris: 1 kernels\n",
      "2019S UTS Data Analytics Assignment 3: 1 kernels\n",
      "EC524: Heart-disease classification: 1 kernels\n",
      "Predição de Churn: 1 kernels\n",
      "IEEE PES BDC DataThon , Year-2020: 1 kernels\n",
      "Птица или самолет?: 1 kernels\n",
      "AI for Clinical Data Analytics HW2: 1 kernels\n",
      "Data Mining Lab2: 1 kernels\n",
      "Predicting user conversions: 1 kernels\n",
      "Avaliação de Carros: 1 kernels\n",
      "InClass Competition at Tokyo Metropolitan Univ.: 1 kernels\n",
      "Bad comments: 1 kernels\n",
      "Хакатон от Кафедры ТПСТС МФТИ (level 1): 1 kernels\n",
      "Who is a Friend?: 1 kernels\n",
      "Chh-OLA: 1 kernels\n",
      "Flatiron School: 1 kernels\n",
      "NCTU BDALAB 2020 Onboard: 1 kernels\n",
      "Night at Cameo: 1 kernels\n",
      "Time Series Classification: 1 kernels\n",
      "House Sales : 1 kernels\n",
      "Car Classification(Project Vision): 1 kernels\n",
      "Great Energy Predictor Shootout I: 1 kernels\n",
      "Pneumonia Texture Analysis: 1 kernels\n",
      "MLDM Classification Competition: 1 kernels\n",
      "Korean Gender Bias Detection: 1 kernels\n",
      "Einfhrung in Kaggle InClass Competitions</title>: 1 kernels\n",
      "GL Hack: Landmarks: 1 kernels\n",
      "[DM&PR WS19/20] Machine learning competition: 1 kernels\n",
      "iFood - 2019 at FGVC6: 1 kernels\n",
      "Fashion MNIST-ITBA-LAB 2020: 1 kernels\n",
      "finec-1941-hw6: 1 kernels\n",
      "Word vectors: 1 kernels\n",
      "compass-canada: 1 kernels\n",
      "DMA Kaggle Challenge: 1 kernels\n",
      "DSI-US-8 Project 2 Regression Challenge: 1 kernels\n",
      "Cleaned vs Dirty: 1 kernels\n",
      "Penyisihan Datavidia 2019: 1 kernels\n",
      "Fashion MNIST challenge201907: 1 kernels\n",
      "ML Hackathon 2019 Q2: 1 kernels\n",
      "Data-Driven Business Analytics: 1 kernels\n",
      "Traffic signs classification: 1 kernels\n",
      "IIITB ML Project: SFO Crime Classification: 1 kernels\n",
      "Temperature Forecasting: 1 kernels\n",
      "[DM&PR WS18/19] Machine learning competition: 1 kernels\n",
      "Kharagpur Data Analytics Group: 1 kernels\n",
      "Анализ потребительской корзины: 1 kernels\n",
      "Heart Disease Prediction: 1 kernels\n",
      "NMLO Contest 3 - Regression: 1 kernels\n",
      "ods_class_cs231n: 1 kernels\n",
      "Predice el futuro: 1 kernels\n",
      "ACM Summer'19 Inclass-1: 1 kernels\n",
      "ACM Machine Learning (SVNIT): 1 kernels\n",
      "HTA Tagging: 1 kernels\n",
      "Oracle Graph ML Contest at Polimi: 1 kernels\n",
      "GirlsGoIT competition 2020: 1 kernels\n",
      "FIA ML T5: 1 kernels\n",
      "Bike Sharing Demand for Education(): 1 kernels\n",
      "Challenge GH: 1 kernels\n",
      "Python Class - Practice: 1 kernels\n",
      "Test Competition Please Ignore: 1 kernels\n",
      "Movie Genre Classification: 1 kernels\n",
      "kaggle18011884: 1 kernels\n",
      "Sentence Relatedness: 1 kernels\n",
      "Team ISTE's Datathon: 1 kernels\n",
      "Anokha AI Adept: 1 kernels\n",
      "HEROZ Internal Competition Extra2: 1 kernels\n",
      "Machine Learning Lab - CAS Data Science HS 20: 1 kernels\n",
      "Digit recognition: 1 kernels\n",
      "Computational Intelligence Project: 1 kernels\n",
      "Technidus machine learning competition 2: 1 kernels\n",
      "Fieldguide Challenge: Moths & Butterflies: 1 kernels\n",
      "Multi-label Bird Species Classification - NIPS 2013: 1 kernels\n"
     ]
    }
   ],
   "source": [
    "kernels_count = kernels_meta.groupby('comp_name').size()\n",
    "n_kernels_per_comp = kernels_meta['comp_name'].value_counts()\n",
    "N_HIGHEST_COMP = 1\n",
    "top_comp = kernels_count.nlargest(N_HIGHEST_COMP).index[N_HIGHEST_COMP-1]\n",
    "print(f\"Competition with most kernels: '{top_comp}'  ({kernels_count.nlargest(N_HIGHEST_COMP).iloc[N_HIGHEST_COMP-1]} kernels)\\n\")\n",
    "for name in n_kernels_per_comp.index:\n",
    "    print(f\"{name}: {n_kernels_per_comp[name]} kernels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_blocks_index</th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7019</td>\n",
       "      <td>21768463</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7020</td>\n",
       "      <td>21768463</td>\n",
       "      <td>1</td>\n",
       "      <td>train_data = pd.read_csv(\"/kaggle/input/titani...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7021</td>\n",
       "      <td>21768463</td>\n",
       "      <td>2</td>\n",
       "      <td>test_data = pd.read_csv(\"/kaggle/input/titanic...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7022</td>\n",
       "      <td>21768463</td>\n",
       "      <td>3</td>\n",
       "      <td>women = train_data.loc[train_data.Sex == 'fema...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7023</td>\n",
       "      <td>21768463</td>\n",
       "      <td>4</td>\n",
       "      <td>men = train_data.loc[train_data.Sex == 'male']...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76485</th>\n",
       "      <td>2547403</td>\n",
       "      <td>10882513</td>\n",
       "      <td>5</td>\n",
       "      <td># clf = RandomForestClassifier()\\nclf = Linear...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76486</th>\n",
       "      <td>2547404</td>\n",
       "      <td>10882513</td>\n",
       "      <td>6</td>\n",
       "      <td>_ = clf.fit(x,y)</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76487</th>\n",
       "      <td>2547405</td>\n",
       "      <td>10882513</td>\n",
       "      <td>7</td>\n",
       "      <td>pred_valid = clf.predict(x_valid)\\naccuracy_sc...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76488</th>\n",
       "      <td>2547406</td>\n",
       "      <td>10882513</td>\n",
       "      <td>8</td>\n",
       "      <td>pred = clf.predict(x_test)</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76489</th>\n",
       "      <td>2547407</td>\n",
       "      <td>10882513</td>\n",
       "      <td>9</td>\n",
       "      <td>sub = pd.DataFrame(test_df['PassengerId'])\\nsu...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76490 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       code_blocks_index  kernel_id  code_block_id  \\\n",
       "0                   7019   21768463              0   \n",
       "1                   7020   21768463              1   \n",
       "2                   7021   21768463              2   \n",
       "3                   7022   21768463              3   \n",
       "4                   7023   21768463              4   \n",
       "...                  ...        ...            ...   \n",
       "76485            2547403   10882513              5   \n",
       "76486            2547404   10882513              6   \n",
       "76487            2547405   10882513              7   \n",
       "76488            2547406   10882513              8   \n",
       "76489            2547407   10882513              9   \n",
       "\n",
       "                                              code_block  \\\n",
       "0      # This Python 3 environment comes with many he...   \n",
       "1      train_data = pd.read_csv(\"/kaggle/input/titani...   \n",
       "2      test_data = pd.read_csv(\"/kaggle/input/titanic...   \n",
       "3      women = train_data.loc[train_data.Sex == 'fema...   \n",
       "4      men = train_data.loc[train_data.Sex == 'male']...   \n",
       "...                                                  ...   \n",
       "76485  # clf = RandomForestClassifier()\\nclf = Linear...   \n",
       "76486                                   _ = clf.fit(x,y)   \n",
       "76487  pred_valid = clf.predict(x_valid)\\naccuracy_sc...   \n",
       "76488                         pred = clf.predict(x_test)   \n",
       "76489  sub = pd.DataFrame(test_df['PassengerId'])\\nsu...   \n",
       "\n",
       "                              kernel_link  \n",
       "0      /nisharahysmith/notebookda32490206  \n",
       "1      /nisharahysmith/notebookda32490206  \n",
       "2      /nisharahysmith/notebookda32490206  \n",
       "3      /nisharahysmith/notebookda32490206  \n",
       "4      /nisharahysmith/notebookda32490206  \n",
       "...                                   ...  \n",
       "76485          /hs1592/adkkhn-krj-titanic  \n",
       "76486          /hs1592/adkkhn-krj-titanic  \n",
       "76487          /hs1592/adkkhn-krj-titanic  \n",
       "76488          /hs1592/adkkhn-krj-titanic  \n",
       "76489          /hs1592/adkkhn-krj-titanic  \n",
       "\n",
       "[76490 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels = kernels_meta[kernels_meta['comp_name'] == top_comp]\n",
    "top_comp_code_blocks = code_blocks[code_blocks['kernel_id'].isin(kernels['kernel_id'])]\n",
    "merged_df = pd.merge(top_comp_code_blocks, kernels_meta, on='kernel_id')\n",
    "top_comp_code_blocks = merged_df.drop(columns=['kaggle_score', 'kaggle_comments', 'kaggle_upvotes', 'comp_name'])\n",
    "top_comp_code_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "      <th>predicted_graph_vertex__probability</th>\n",
       "      <th>graph_vertex_id</th>\n",
       "      <th>graph_vertex_class</th>\n",
       "      <th>graph_vertex_subclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21768463</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>88</td>\n",
       "      <td>Debug</td>\n",
       "      <td>list_files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21768463</td>\n",
       "      <td>1</td>\n",
       "      <td>train_data = pd.read_csv(\"/kaggle/input/titani...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.999642</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21768463</td>\n",
       "      <td>2</td>\n",
       "      <td>test_data = pd.read_csv(\"/kaggle/input/titanic...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.999662</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21768463</td>\n",
       "      <td>3</td>\n",
       "      <td>women = train_data.loc[train_data.Sex == 'fema...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.459239</td>\n",
       "      <td>77</td>\n",
       "      <td>Other</td>\n",
       "      <td>define_variables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21768463</td>\n",
       "      <td>4</td>\n",
       "      <td>men = train_data.loc[train_data.Sex == 'male']...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.590113</td>\n",
       "      <td>77</td>\n",
       "      <td>Other</td>\n",
       "      <td>define_variables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76485</th>\n",
       "      <td>10882513</td>\n",
       "      <td>5</td>\n",
       "      <td># clf = RandomForestClassifier()\\nclf = Linear...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.977052</td>\n",
       "      <td>76</td>\n",
       "      <td>Debug</td>\n",
       "      <td>commented</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76486</th>\n",
       "      <td>10882513</td>\n",
       "      <td>6</td>\n",
       "      <td>_ = clf.fit(x,y)</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>7</td>\n",
       "      <td>Model_Train</td>\n",
       "      <td>train_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76487</th>\n",
       "      <td>10882513</td>\n",
       "      <td>7</td>\n",
       "      <td>pred_valid = clf.predict(x_valid)\\naccuracy_sc...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.992896</td>\n",
       "      <td>48</td>\n",
       "      <td>Model_Evaluation</td>\n",
       "      <td>predict_on_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76488</th>\n",
       "      <td>10882513</td>\n",
       "      <td>8</td>\n",
       "      <td>pred = clf.predict(x_test)</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.994685</td>\n",
       "      <td>48</td>\n",
       "      <td>Model_Evaluation</td>\n",
       "      <td>predict_on_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76489</th>\n",
       "      <td>10882513</td>\n",
       "      <td>9</td>\n",
       "      <td>sub = pd.DataFrame(test_df['PassengerId'])\\nsu...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.999286</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76490 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kernel_id  code_block_id  \\\n",
       "0       21768463              0   \n",
       "1       21768463              1   \n",
       "2       21768463              2   \n",
       "3       21768463              3   \n",
       "4       21768463              4   \n",
       "...          ...            ...   \n",
       "76485   10882513              5   \n",
       "76486   10882513              6   \n",
       "76487   10882513              7   \n",
       "76488   10882513              8   \n",
       "76489   10882513              9   \n",
       "\n",
       "                                              code_block  \\\n",
       "0      # This Python 3 environment comes with many he...   \n",
       "1      train_data = pd.read_csv(\"/kaggle/input/titani...   \n",
       "2      test_data = pd.read_csv(\"/kaggle/input/titanic...   \n",
       "3      women = train_data.loc[train_data.Sex == 'fema...   \n",
       "4      men = train_data.loc[train_data.Sex == 'male']...   \n",
       "...                                                  ...   \n",
       "76485  # clf = RandomForestClassifier()\\nclf = Linear...   \n",
       "76486                                   _ = clf.fit(x,y)   \n",
       "76487  pred_valid = clf.predict(x_valid)\\naccuracy_sc...   \n",
       "76488                         pred = clf.predict(x_test)   \n",
       "76489  sub = pd.DataFrame(test_df['PassengerId'])\\nsu...   \n",
       "\n",
       "                              kernel_link  \\\n",
       "0      /nisharahysmith/notebookda32490206   \n",
       "1      /nisharahysmith/notebookda32490206   \n",
       "2      /nisharahysmith/notebookda32490206   \n",
       "3      /nisharahysmith/notebookda32490206   \n",
       "4      /nisharahysmith/notebookda32490206   \n",
       "...                                   ...   \n",
       "76485          /hs1592/adkkhn-krj-titanic   \n",
       "76486          /hs1592/adkkhn-krj-titanic   \n",
       "76487          /hs1592/adkkhn-krj-titanic   \n",
       "76488          /hs1592/adkkhn-krj-titanic   \n",
       "76489          /hs1592/adkkhn-krj-titanic   \n",
       "\n",
       "       predicted_graph_vertex__probability  graph_vertex_id  \\\n",
       "0                                 0.999220               88   \n",
       "1                                 0.999642               45   \n",
       "2                                 0.999662               45   \n",
       "3                                 0.459239               77   \n",
       "4                                 0.590113               77   \n",
       "...                                    ...              ...   \n",
       "76485                             0.977052               76   \n",
       "76486                             0.999666                7   \n",
       "76487                             0.992896               48   \n",
       "76488                             0.994685               48   \n",
       "76489                             0.999286               25   \n",
       "\n",
       "      graph_vertex_class graph_vertex_subclass  \n",
       "0                  Debug            list_files  \n",
       "1        Data_Extraction         load_from_csv  \n",
       "2        Data_Extraction         load_from_csv  \n",
       "3                  Other      define_variables  \n",
       "4                  Other      define_variables  \n",
       "...                  ...                   ...  \n",
       "76485              Debug             commented  \n",
       "76486        Model_Train           train_model  \n",
       "76487   Model_Evaluation       predict_on_test  \n",
       "76488   Model_Evaluation       predict_on_test  \n",
       "76489        Data_Export           save_to_csv  \n",
       "\n",
       "[76490 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(top_comp_code_blocks.merge(data_preds, on='code_blocks_index'), vertices, left_on='predicted_graph_vertex_id', right_on='graph_vertex_id')\n",
    "merged_df.drop(['code_blocks_index', 'predicted_graph_vertex_id'], axis=1, inplace=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "      <th>predicted_graph_vertex__probability</th>\n",
       "      <th>graph_vertex_id</th>\n",
       "      <th>graph_vertex_class</th>\n",
       "      <th>graph_vertex_subclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21768463</td>\n",
       "      <td>0</td>\n",
       "      <td># This Python 3 environment comes with many he...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>88</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>list_files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21768463</td>\n",
       "      <td>1</td>\n",
       "      <td>train_data = pd.read_csv(\"/kaggle/input/titani...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.999642</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21768463</td>\n",
       "      <td>2</td>\n",
       "      <td>test_data = pd.read_csv(\"/kaggle/input/titanic...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.999662</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21768463</td>\n",
       "      <td>3</td>\n",
       "      <td>women = train_data.loc[train_data.Sex == 'fema...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.459239</td>\n",
       "      <td>77</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>define_variables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21768463</td>\n",
       "      <td>4</td>\n",
       "      <td>men = train_data.loc[train_data.Sex == 'male']...</td>\n",
       "      <td>/nisharahysmith/notebookda32490206</td>\n",
       "      <td>0.590113</td>\n",
       "      <td>77</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>define_variables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76485</th>\n",
       "      <td>10882513</td>\n",
       "      <td>5</td>\n",
       "      <td># clf = RandomForestClassifier()\\nclf = Linear...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.977052</td>\n",
       "      <td>76</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>commented</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76486</th>\n",
       "      <td>10882513</td>\n",
       "      <td>6</td>\n",
       "      <td>_ = clf.fit(x,y)</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>7</td>\n",
       "      <td>Model_Train</td>\n",
       "      <td>train_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76487</th>\n",
       "      <td>10882513</td>\n",
       "      <td>7</td>\n",
       "      <td>pred_valid = clf.predict(x_valid)\\naccuracy_sc...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.992896</td>\n",
       "      <td>48</td>\n",
       "      <td>Model_Evaluation</td>\n",
       "      <td>predict_on_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76488</th>\n",
       "      <td>10882513</td>\n",
       "      <td>8</td>\n",
       "      <td>pred = clf.predict(x_test)</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.994685</td>\n",
       "      <td>48</td>\n",
       "      <td>Model_Evaluation</td>\n",
       "      <td>predict_on_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76489</th>\n",
       "      <td>10882513</td>\n",
       "      <td>9</td>\n",
       "      <td>sub = pd.DataFrame(test_df['PassengerId'])\\nsu...</td>\n",
       "      <td>/hs1592/adkkhn-krj-titanic</td>\n",
       "      <td>0.999286</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76490 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kernel_id  code_block_id  \\\n",
       "0       21768463              0   \n",
       "1       21768463              1   \n",
       "2       21768463              2   \n",
       "3       21768463              3   \n",
       "4       21768463              4   \n",
       "...          ...            ...   \n",
       "76485   10882513              5   \n",
       "76486   10882513              6   \n",
       "76487   10882513              7   \n",
       "76488   10882513              8   \n",
       "76489   10882513              9   \n",
       "\n",
       "                                              code_block  \\\n",
       "0      # This Python 3 environment comes with many he...   \n",
       "1      train_data = pd.read_csv(\"/kaggle/input/titani...   \n",
       "2      test_data = pd.read_csv(\"/kaggle/input/titanic...   \n",
       "3      women = train_data.loc[train_data.Sex == 'fema...   \n",
       "4      men = train_data.loc[train_data.Sex == 'male']...   \n",
       "...                                                  ...   \n",
       "76485  # clf = RandomForestClassifier()\\nclf = Linear...   \n",
       "76486                                   _ = clf.fit(x,y)   \n",
       "76487  pred_valid = clf.predict(x_valid)\\naccuracy_sc...   \n",
       "76488                         pred = clf.predict(x_test)   \n",
       "76489  sub = pd.DataFrame(test_df['PassengerId'])\\nsu...   \n",
       "\n",
       "                              kernel_link  \\\n",
       "0      /nisharahysmith/notebookda32490206   \n",
       "1      /nisharahysmith/notebookda32490206   \n",
       "2      /nisharahysmith/notebookda32490206   \n",
       "3      /nisharahysmith/notebookda32490206   \n",
       "4      /nisharahysmith/notebookda32490206   \n",
       "...                                   ...   \n",
       "76485          /hs1592/adkkhn-krj-titanic   \n",
       "76486          /hs1592/adkkhn-krj-titanic   \n",
       "76487          /hs1592/adkkhn-krj-titanic   \n",
       "76488          /hs1592/adkkhn-krj-titanic   \n",
       "76489          /hs1592/adkkhn-krj-titanic   \n",
       "\n",
       "       predicted_graph_vertex__probability  graph_vertex_id  \\\n",
       "0                                 0.999220               88   \n",
       "1                                 0.999642               45   \n",
       "2                                 0.999662               45   \n",
       "3                                 0.459239               77   \n",
       "4                                 0.590113               77   \n",
       "...                                    ...              ...   \n",
       "76485                             0.977052               76   \n",
       "76486                             0.999666                7   \n",
       "76487                             0.992896               48   \n",
       "76488                             0.994685               48   \n",
       "76489                             0.999286               25   \n",
       "\n",
       "              graph_vertex_class graph_vertex_subclass  \n",
       "0      Exploratory_Data_Analysis            list_files  \n",
       "1                Data_Extraction         load_from_csv  \n",
       "2                Data_Extraction         load_from_csv  \n",
       "3      Exploratory_Data_Analysis      define_variables  \n",
       "4      Exploratory_Data_Analysis      define_variables  \n",
       "...                          ...                   ...  \n",
       "76485  Exploratory_Data_Analysis             commented  \n",
       "76486                Model_Train           train_model  \n",
       "76487           Model_Evaluation       predict_on_test  \n",
       "76488           Model_Evaluation       predict_on_test  \n",
       "76489                Data_Export           save_to_csv  \n",
       "\n",
       "[76490 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['graph_vertex_class'] == 'EDA', 'graph_vertex_class'] = 'Exploratory_Data_Analysis'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Hyperparam_Tuning', 'graph_vertex_class'] = 'Model_Train'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Model_Interpretation', 'graph_vertex_class'] = 'Visualization'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Debug', 'graph_vertex_class'] = 'Exploratory_Data_Analysis'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Other', 'graph_vertex_class'] = 'Exploratory_Data_Analysis'\n",
    "merged_df.loc[merged_df['graph_vertex_class'] == 'Environment', 'graph_vertex_class'] = 'Imports_and_Environment'\n",
    "\n",
    "LABELS = [\n",
    "    \"Data_Transform\",\n",
    "    \"Data_Extraction\",\n",
    "    \"Visualization\",\n",
    "    \"Model_Train\",\n",
    "    \"Model_Evaluation\",\n",
    "    \"Imports_and_Environment\",\n",
    "    \"Data_Export\",\n",
    "    \"Exploratory_Data_Analysis\", \n",
    "    # \"Other\" # \"Other\"\n",
    "]\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rhythmcam/automl-search-best-titanic-model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_kernel_links = kernels['kernel_link'].unique()\n",
    "distinct_kernel_links[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel_id</th>\n",
       "      <th>code_block_id</th>\n",
       "      <th>code_block</th>\n",
       "      <th>kernel_link</th>\n",
       "      <th>predicted_graph_vertex__probability</th>\n",
       "      <th>graph_vertex_id</th>\n",
       "      <th>graph_vertex_class</th>\n",
       "      <th>graph_vertex_subclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>21322805</td>\n",
       "      <td>0</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\n\\nfil...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>21322805</td>\n",
       "      <td>1</td>\n",
       "      <td>filename_test = \"/kaggle/input/titanic/test.cs...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>21322805</td>\n",
       "      <td>2</td>\n",
       "      <td>filename_submit=\"../input/titanic/gender_submi...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999699</td>\n",
       "      <td>45</td>\n",
       "      <td>Data_Extraction</td>\n",
       "      <td>load_from_csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>21322805</td>\n",
       "      <td>3</td>\n",
       "      <td>#大浜先生の質問\\nanimals = pd.DataFrame({'Cows': [12,...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.998248</td>\n",
       "      <td>12</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>create_dataframe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>21322805</td>\n",
       "      <td>4</td>\n",
       "      <td>#Goatがmaxの時のTrue/Falesリスト\\nprint(animals[\"Goat...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.956523</td>\n",
       "      <td>40</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>show_table_attributes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>21322805</td>\n",
       "      <td>5</td>\n",
       "      <td>data = pd.concat([train, test], sort=False)\\ndata</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>11</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>concatenate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>21322805</td>\n",
       "      <td>6</td>\n",
       "      <td>data.isnull().sum()</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.998980</td>\n",
       "      <td>39</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>count_missing_values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>21322805</td>\n",
       "      <td>7</td>\n",
       "      <td>data['Sex'].replace(['male', 'female'], [0, 1]...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.798071</td>\n",
       "      <td>20</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>categorify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>21322805</td>\n",
       "      <td>8</td>\n",
       "      <td>data</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999773</td>\n",
       "      <td>41</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>show_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>21322805</td>\n",
       "      <td>9</td>\n",
       "      <td>data['Embarked'].value_counts()</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999534</td>\n",
       "      <td>72</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>count_values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>21322805</td>\n",
       "      <td>10</td>\n",
       "      <td>data['Embarked'].fillna(('S'), inplace=True)</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.534734</td>\n",
       "      <td>16</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>data_type_conversions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>21322805</td>\n",
       "      <td>11</td>\n",
       "      <td>data['Embarked'].value_counts()</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999534</td>\n",
       "      <td>72</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>count_values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>21322805</td>\n",
       "      <td>12</td>\n",
       "      <td>data.isnull().sum()</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.998980</td>\n",
       "      <td>39</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>count_missing_values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>21322805</td>\n",
       "      <td>13</td>\n",
       "      <td>data['Embarked'] = data['Embarked'].map({'S': ...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.937349</td>\n",
       "      <td>16</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>data_type_conversions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>21322805</td>\n",
       "      <td>14</td>\n",
       "      <td>data</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999773</td>\n",
       "      <td>41</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>show_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>21322805</td>\n",
       "      <td>15</td>\n",
       "      <td>data['Fare'].fillna(np.mean(data['Fare']), inp...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.735726</td>\n",
       "      <td>8</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>feature_engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>21322805</td>\n",
       "      <td>16</td>\n",
       "      <td>age_avg = data['Age'].mean()\\nage_std = data['...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999155</td>\n",
       "      <td>40</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>show_table_attributes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>21322805</td>\n",
       "      <td>17</td>\n",
       "      <td>data</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999773</td>\n",
       "      <td>41</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>show_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>21322805</td>\n",
       "      <td>18</td>\n",
       "      <td>delete_columns = ['Name', 'PassengerId', 'SibS...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>10</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>drop_column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>21322805</td>\n",
       "      <td>19</td>\n",
       "      <td>train = data[:len(train)]\\ntest = data[len(tra...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.991172</td>\n",
       "      <td>13</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>split</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>21322805</td>\n",
       "      <td>20</td>\n",
       "      <td>y_train = train['Survived']\\nX_train = train.d...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>21</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>prepare_x_and_y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>21322805</td>\n",
       "      <td>21</td>\n",
       "      <td>X_train.head()</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>41</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>show_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>21322805</td>\n",
       "      <td>22</td>\n",
       "      <td>y_train.head()</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>41</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>show_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>21322805</td>\n",
       "      <td>23</td>\n",
       "      <td>from sklearn.model_selection import train_test...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.990771</td>\n",
       "      <td>13</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>split</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>21322805</td>\n",
       "      <td>24</td>\n",
       "      <td>categorical_features = ['Embarked', 'Pclass', ...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.998681</td>\n",
       "      <td>77</td>\n",
       "      <td>Exploratory_Data_Analysis</td>\n",
       "      <td>define_variables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>21322805</td>\n",
       "      <td>25</td>\n",
       "      <td>import lightgbm as lgb\\n\\n\\nlgb_train = lgb.Da...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.996879</td>\n",
       "      <td>7</td>\n",
       "      <td>Model_Train</td>\n",
       "      <td>train_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>21322805</td>\n",
       "      <td>26</td>\n",
       "      <td>y_pred = (y_pred &gt; 0.5).astype(int)\\ny_pred[:10]</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.982947</td>\n",
       "      <td>16</td>\n",
       "      <td>Data_Transform</td>\n",
       "      <td>data_type_conversions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>21322805</td>\n",
       "      <td>27</td>\n",
       "      <td>sub = pd.read_csv('../input/titanic/gender_sub...</td>\n",
       "      <td>/ryotaroyabe/notebook-titanic</td>\n",
       "      <td>0.999163</td>\n",
       "      <td>25</td>\n",
       "      <td>Data_Export</td>\n",
       "      <td>save_to_csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     kernel_id  code_block_id  \\\n",
       "117   21322805              0   \n",
       "118   21322805              1   \n",
       "119   21322805              2   \n",
       "120   21322805              3   \n",
       "121   21322805              4   \n",
       "122   21322805              5   \n",
       "123   21322805              6   \n",
       "124   21322805              7   \n",
       "125   21322805              8   \n",
       "126   21322805              9   \n",
       "127   21322805             10   \n",
       "128   21322805             11   \n",
       "129   21322805             12   \n",
       "130   21322805             13   \n",
       "131   21322805             14   \n",
       "132   21322805             15   \n",
       "133   21322805             16   \n",
       "134   21322805             17   \n",
       "135   21322805             18   \n",
       "136   21322805             19   \n",
       "137   21322805             20   \n",
       "138   21322805             21   \n",
       "139   21322805             22   \n",
       "140   21322805             23   \n",
       "141   21322805             24   \n",
       "142   21322805             25   \n",
       "143   21322805             26   \n",
       "144   21322805             27   \n",
       "\n",
       "                                            code_block  \\\n",
       "117  import numpy as np\\nimport pandas as pd\\n\\nfil...   \n",
       "118  filename_test = \"/kaggle/input/titanic/test.cs...   \n",
       "119  filename_submit=\"../input/titanic/gender_submi...   \n",
       "120  #大浜先生の質問\\nanimals = pd.DataFrame({'Cows': [12,...   \n",
       "121  #Goatがmaxの時のTrue/Falesリスト\\nprint(animals[\"Goat...   \n",
       "122  data = pd.concat([train, test], sort=False)\\ndata   \n",
       "123                                data.isnull().sum()   \n",
       "124  data['Sex'].replace(['male', 'female'], [0, 1]...   \n",
       "125                                               data   \n",
       "126                    data['Embarked'].value_counts()   \n",
       "127       data['Embarked'].fillna(('S'), inplace=True)   \n",
       "128                    data['Embarked'].value_counts()   \n",
       "129                                data.isnull().sum()   \n",
       "130  data['Embarked'] = data['Embarked'].map({'S': ...   \n",
       "131                                               data   \n",
       "132  data['Fare'].fillna(np.mean(data['Fare']), inp...   \n",
       "133  age_avg = data['Age'].mean()\\nage_std = data['...   \n",
       "134                                               data   \n",
       "135  delete_columns = ['Name', 'PassengerId', 'SibS...   \n",
       "136  train = data[:len(train)]\\ntest = data[len(tra...   \n",
       "137  y_train = train['Survived']\\nX_train = train.d...   \n",
       "138                                     X_train.head()   \n",
       "139                                     y_train.head()   \n",
       "140  from sklearn.model_selection import train_test...   \n",
       "141  categorical_features = ['Embarked', 'Pclass', ...   \n",
       "142  import lightgbm as lgb\\n\\n\\nlgb_train = lgb.Da...   \n",
       "143   y_pred = (y_pred > 0.5).astype(int)\\ny_pred[:10]   \n",
       "144  sub = pd.read_csv('../input/titanic/gender_sub...   \n",
       "\n",
       "                       kernel_link  predicted_graph_vertex__probability  \\\n",
       "117  /ryotaroyabe/notebook-titanic                             0.999664   \n",
       "118  /ryotaroyabe/notebook-titanic                             0.999738   \n",
       "119  /ryotaroyabe/notebook-titanic                             0.999699   \n",
       "120  /ryotaroyabe/notebook-titanic                             0.998248   \n",
       "121  /ryotaroyabe/notebook-titanic                             0.956523   \n",
       "122  /ryotaroyabe/notebook-titanic                             0.999525   \n",
       "123  /ryotaroyabe/notebook-titanic                             0.998980   \n",
       "124  /ryotaroyabe/notebook-titanic                             0.798071   \n",
       "125  /ryotaroyabe/notebook-titanic                             0.999773   \n",
       "126  /ryotaroyabe/notebook-titanic                             0.999534   \n",
       "127  /ryotaroyabe/notebook-titanic                             0.534734   \n",
       "128  /ryotaroyabe/notebook-titanic                             0.999534   \n",
       "129  /ryotaroyabe/notebook-titanic                             0.998980   \n",
       "130  /ryotaroyabe/notebook-titanic                             0.937349   \n",
       "131  /ryotaroyabe/notebook-titanic                             0.999773   \n",
       "132  /ryotaroyabe/notebook-titanic                             0.735726   \n",
       "133  /ryotaroyabe/notebook-titanic                             0.999155   \n",
       "134  /ryotaroyabe/notebook-titanic                             0.999773   \n",
       "135  /ryotaroyabe/notebook-titanic                             0.999055   \n",
       "136  /ryotaroyabe/notebook-titanic                             0.991172   \n",
       "137  /ryotaroyabe/notebook-titanic                             0.999324   \n",
       "138  /ryotaroyabe/notebook-titanic                             0.999755   \n",
       "139  /ryotaroyabe/notebook-titanic                             0.999751   \n",
       "140  /ryotaroyabe/notebook-titanic                             0.990771   \n",
       "141  /ryotaroyabe/notebook-titanic                             0.998681   \n",
       "142  /ryotaroyabe/notebook-titanic                             0.996879   \n",
       "143  /ryotaroyabe/notebook-titanic                             0.982947   \n",
       "144  /ryotaroyabe/notebook-titanic                             0.999163   \n",
       "\n",
       "     graph_vertex_id         graph_vertex_class  graph_vertex_subclass  \n",
       "117               45            Data_Extraction          load_from_csv  \n",
       "118               45            Data_Extraction          load_from_csv  \n",
       "119               45            Data_Extraction          load_from_csv  \n",
       "120               12             Data_Transform       create_dataframe  \n",
       "121               40  Exploratory_Data_Analysis  show_table_attributes  \n",
       "122               11             Data_Transform            concatenate  \n",
       "123               39  Exploratory_Data_Analysis   count_missing_values  \n",
       "124               20             Data_Transform             categorify  \n",
       "125               41  Exploratory_Data_Analysis             show_table  \n",
       "126               72  Exploratory_Data_Analysis           count_values  \n",
       "127               16             Data_Transform  data_type_conversions  \n",
       "128               72  Exploratory_Data_Analysis           count_values  \n",
       "129               39  Exploratory_Data_Analysis   count_missing_values  \n",
       "130               16             Data_Transform  data_type_conversions  \n",
       "131               41  Exploratory_Data_Analysis             show_table  \n",
       "132                8             Data_Transform    feature_engineering  \n",
       "133               40  Exploratory_Data_Analysis  show_table_attributes  \n",
       "134               41  Exploratory_Data_Analysis             show_table  \n",
       "135               10             Data_Transform            drop_column  \n",
       "136               13             Data_Transform                  split  \n",
       "137               21             Data_Transform        prepare_x_and_y  \n",
       "138               41  Exploratory_Data_Analysis             show_table  \n",
       "139               41  Exploratory_Data_Analysis             show_table  \n",
       "140               13             Data_Transform                  split  \n",
       "141               77  Exploratory_Data_Analysis       define_variables  \n",
       "142                7                Model_Train            train_model  \n",
       "143               16             Data_Transform  data_type_conversions  \n",
       "144               25                Data_Export            save_to_csv  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = merged_df[merged_df['kernel_link'] == distinct_kernel_links[3]].sort_values('code_block_id', ascending=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dir = \"../../data/test_datasets\"\n",
    "merged_df.to_csv(f'{test_dataset_dir}/{top_comp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual cell for saving the classified notebooks in a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20 notebooks to /home/ryounis/Documents/Zurich/PEACHLab/datascience-visualisation/data/datasets/Titanic - Machine Learning from Disaster/unclassified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utils.constants import BLANK_IPYNB_JSON\n",
    "from utils.helper_functions import load_notebook\n",
    "import json\n",
    "import os\n",
    "import tempfile \n",
    "from kaggle.api.kaggle_api_extended import ApiException\n",
    "from tqdm import tqdm\n",
    "test_dataset_dir = f\"/home/ryounis/Documents/Zurich/PEACHLab/datascience-visualisation/data/datasets/{top_comp}/unclassified\"\n",
    "if not os.path.exists(test_dataset_dir): os.makedirs(test_dataset_dir)\n",
    "\n",
    "FIRST_N_KERNELS = 20\n",
    "\n",
    "notebooks = []\n",
    "for kernel_link in tqdm(distinct_kernel_links[:FIRST_N_KERNELS]):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        kernel_df = merged_df[merged_df['kernel_link'] == kernel_link].sort_values('code_block_id', ascending=True)\n",
    "        try:\n",
    "            kaggle.api.kernels_pull(kernel_link[1:], path=temp_dir, metadata=True)\n",
    "            notebook_path = f\"{temp_dir}/{kernel_link.split('/')[-1]}.ipynb\"\n",
    "            notebook = load_notebook(notebook_path)\n",
    "        except ApiException as e:\n",
    "            print(f\"Error pulling kernel: {kernel_link}\")\n",
    "            continue\n",
    "        \n",
    "    code_cell_counter = 0\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'code' and len(cell['source']):\n",
    "            row = kernel_df.iloc[code_cell_counter]\n",
    "            code_cell_counter += 1\n",
    "\n",
    "            cell[\"metadata\"] = {\n",
    "                **cell[\"metadata\"],\n",
    "                \"class\": row[\"graph_vertex_class\"],\n",
    "                \"subclass\": row[\"graph_vertex_subclass\"],\n",
    "                \"subclass_id\": int(row[\"graph_vertex_id\"]),\n",
    "                \"predicted_subclass_probability\": row[\"predicted_graph_vertex__probability\"],\n",
    "                \"notebook_id\": int(row['kernel_id']),\n",
    "            }    \n",
    "    notebook[\"metadata\"][\"labels\"] = LABELS\n",
    "    notebooks.append(notebook)\n",
    "    filename = \"_\".join(kernel_link.split('/')[1:])\n",
    "    with open(f\"{test_dataset_dir}/{filename}.ipynb\", \"w\") as f:json.dump(notebook, f)\n",
    "    \n",
    "print(f\"Saved {len(notebooks)} notebooks to {test_dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "clusterer = HDBSCAN(\n",
    "    min_cluster_size=4,\n",
    "    min_samples=2,\n",
    "    cluster_selection_epsilon=.0,\n",
    "    max_cluster_size=None,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "clusters = {}\n",
    "for key, group in grouped_cells.items(): \n",
    "    X = np.array([cell[\"embedding\"] for cell in group])\n",
    "    \n",
    "    if len(group) < 4:\n",
    "        labels = [-1] * len(group)\n",
    "    else:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(X)\n",
    "        embeddings_pca = pca.transform(X)\n",
    "        clusterer.fit(embeddings_pca)\n",
    "        labels = clusterer.labels_\n",
    "        \n",
    "    clusters[key] = {str(label): [] for label in set(labels)}\n",
    "    for cell, label, embedding in zip(group, labels, embeddings_pca):\n",
    "        clusters[key][str(label)].append({\n",
    "            \"cell\": cell,\n",
    "            \"embedding\": embedding\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, elems in clusters.items():\n",
    "    print(f\"Class: {key}\")\n",
    "    for label, group in elems.items():\n",
    "        print(f\"    {label}: {len(group)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot of the embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "for key, clusts in clusters.items():\n",
    "    \n",
    "    \n",
    "    plt.scatter(\n",
    "        x=embeddings_pca[:, 0],\n",
    "        y=embeddings_pca[:, 1],\n",
    "        cmap='viridis',\n",
    "        alpha=0.5,\n",
    "        label=key\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('Clusters Visualization')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper_functions import notebook_extract_code, notebook_add_class_labels\n",
    "\n",
    "notebooks = []\n",
    "for kernel_id in merged_df['kernel_id'].unique()[:20]:\n",
    "    notebook_json = BLANK_IPYNB_JSON.copy()\n",
    "    notebook_json['cells'] = []\n",
    "    for row in merged_df[merged_df['kernel_id'] == kernel_id].iterrows():\n",
    "        notebook_json['cells'].append({\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": None,\n",
    "            \"metadata\": {\n",
    "                \"graph_vertex_id\": row[1][\"graph_vertex_id\"],\n",
    "                \"predicted_graph_vertex__probability\": row[1][\"predicted_graph_vertex__probability\"],\n",
    "                \"notebook_id\": row[1]['kernel_id'],\n",
    "            },\n",
    "            \"source\": row[1]['code_block']\n",
    "        })\n",
    "    notebooks.append(notebook_json)\n",
    "# notebook_code = notebook_extract_code(notebook_json)\n",
    "# cell_labels = classifier.classify_notebook(notebook_code)\n",
    "# notebook_json = notebook_add_class_labels(notebook_json, cell_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notebooks = []\n",
    "notebook_cell_labels = []\n",
    "for i, notebook_json in enumerate(notebooks):\n",
    "    print(f\"Notebook {i+1}/{len(notebooks)}\", end='\\r')\n",
    "    \n",
    "    notebook_code = notebook_extract_code(notebook_json)\n",
    "    cell_labels = classifier.classify_notebook(notebook_code)\n",
    "    notebook_json = notebook_add_class_labels(notebook_json, cell_labels)\n",
    "    labeled_notebooks.append(notebook_json)\n",
    "    notebook_cell_labels.append(cell_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for notebook in notebook_cell_labels:\n",
    "    for elem in notebook:\n",
    "        embeddings.append(elem[1])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "clusterer: HDBSCAN = HDBSCAN(\n",
    "    min_cluster_size=4,                 # Minimum number of samples to form a cluster\n",
    "    min_samples=2,                      # Minimum number of samples in a neighborhood to be considered as a core point\n",
    "    cluster_selection_epsilon=0,     # If 2 clusters are less than epsilon apart, they get merged\n",
    ")\n",
    "\n",
    "clusterer.fit(embeddings)\n",
    "for label in set(clusterer.labels_):\n",
    "    print(f\"Cluster {label}: {len([x for x in clusterer.labels_ if x == label])} cells\")\n",
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook['cells']:\n",
    "        cell['metadata']['cluster_label'] = clusterer.labels_[counter]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notebooks[0][\"cells\"]\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook[\"cells\"]:\n",
    "        true_labels.append(cell[\"metadata\"][\"graph_vertex_id\"])\n",
    "        predicted_labels.append(cell[\"metadata\"][\"cluster_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def count_misclustered_elements(true_labels, predicted_labels):\n",
    "    # Convert the labels to numpy arrays for easier manipulation\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    \n",
    "    unique_true_labels = np.unique(true_labels)\n",
    "    unique_predicted_labels = np.unique(predicted_labels)\n",
    "    \n",
    "    cost_matrix = np.zeros((len(unique_true_labels), len(unique_predicted_labels)), dtype=int)\n",
    "    \n",
    "    for i, true_label in enumerate(unique_true_labels):\n",
    "        for j, predicted_label in enumerate(unique_predicted_labels):\n",
    "            cost_matrix[i, j] = np.sum((true_labels == true_label) & (predicted_labels != predicted_label))\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    misclustered_count = cost_matrix[row_ind, col_ind].sum()\n",
    "    \n",
    "    return misclustered_count\n",
    "\n",
    "misclustered_count = count_misclustered_elements(true_labels, predicted_labels)\n",
    "print(f\"Number of misclustered elements: {misclustered_count}/{len(true_labels)}\")\n",
    "print(f\"Score: {1 - misclustered_count/len(true_labels)} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for notebook in labeled_notebooks:\n",
    "    for cell in notebook['cells']:\n",
    "        cell['metadata']['cluster_label'] = clusterer.labels_[counter]\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
